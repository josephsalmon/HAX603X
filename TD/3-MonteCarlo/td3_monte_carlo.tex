\documentclass[11pt]{td_um}

%------------------------------
\def\version{cor}
%\def\version{eno}
%------------------------------

\input{../../common/header.tex}

\newcommand{\va}{variable aléatoire\xspace}
\newcommand{\vas}{variables aléatoires\xspace}
\newcommand{\evs}{événements\xspace}
\newcommand{\ev}{événement\xspace}
\newcommand{\fdr}{fonction de répartition\xspace}
\usepackage{xspace} % gestion des espaces apres les macros
\usepackage[ruled]{algorithm2e} % to write algorithms

\title{TD 3 - Méthode de Monte Carlo}

\begin{document}

\maketitle


\bigskip

\bigskip



\begin{exo}{} % [Approximation de $\pi$]
    On veut approcher la valeur de $\pi$ en utilisant une méthode de Monte-Carlo à partir de l’une des deux intégrales suivantes :
    $$
    I_1= 4 \int_0^1 \sqrt{1-x^2} dx \quad \hbox{ et } \quad I_2= \int_{\mathbb R^2} \one_{\{ x^2+ y^2 \leq 1\}} dx dy
    $$
    \begin{enumerate}
        \item  Montrer que $I_1=I_2=\pi$. Que représentent ces intégrales d'un point de vue géométrique ?
%--------------------------------------------
            \cor{
                L'intégrale $\int_0^1\sqrt{1-x^2} dx$ représente l'aire du quart de disque unité soit $\pi /4$. (La courbe représentative de la fonction définie sur $[0,1]$ par $x\mapsto \sqrt{1-x^2}$ est le quart de cercle de rayon 1 et de centre l'origine du repère orthonormé.) Ainsi $I_1=4\times \pi /4=\pi$. \\
                De façon plus calculatoire, on fait le changement de variable $x=\sin\theta$
                \begin{align*}
                    I_1&=4\int_{0}^{\frac{\pi}{2}}\sqrt{1-\sin^2\theta}\cos\theta d\theta\\
                    &=4\int_{0}^{\frac{\pi}{2}}\cos^2\theta d\theta\\
                    &=4\int_{0}^{\frac{\pi}{2}}\left(\frac{e^{i\theta}+e^{-i\theta}}{2}\right)^2 d\theta\\
                    &=\int_{0}^{\frac{\pi}{2}}e^{2i\theta}+e^{-i2\theta}+2 d\theta\\
                    &=\left[\frac{e^{2i\theta}}{2i}+\frac{e^{-2i\theta}}{2i}+2\theta\right]_{0}^{\frac{\pi}{2}}\\
                    &=\frac{i}{2i}+\frac{-i}{2i}+\pi\ =\ \pi.
                \end{align*}

                Pour l'intégrale $I_2$, on fait le changement de variables coordonnées cartésiennes - coordonnées polaires : $x=r \cos(\theta)$, $y=r \sin(\theta)$
                \begin{eqnarray*}
                    I_2&=&  \int_{\mathbb R^2} \one_{\{ x^2+ y^2 \leq 1\}} dx dy \\
                    &=& \iint_{[0, 2\pi] \times [0,1]} \!\!\!\!\!\! r dr d\theta \\
                    &=& \int_0^{2\pi} \frac12 \, d\theta = \pi
                \end{eqnarray*}
                Quant à $I_2$, cette intégrale double représente l'aire du disque unité.
            }
%--------------------------------------------
        \item Exprimer $I_1$ et $I_2$ comme des espérances.
%--------------------------------------------
            \cor{
                Exprimons $I_1$ comme une espérance :
                $$
                I_1=4 \mathbb E\left(\sqrt{1-X^2}\right) \quad \hbox{où } X\sim U(0,1)
                $$
                et pour $I_2$ remarquons qu'en prenant $X\sim U(-1,1)$ et $Y\sim U(-1,1)$ indépendantes, on a
                $$
                \mathbb P\left(X^2+Y^2 \leq 1\right)=\mathbb E \left( \one_{ \{X^2+Y^2\leq 1\}}\right)
                = \int_{-1}^1 \int_{-1}^1 \one_{\{x^2+y^2\leq 1\}} \, \frac12 \times \frac12 dxdy=\frac14 I_2
                $$
                d'où :
                $$
                I_2=4 \mathbb E \left( \one_{ \{X^2+Y^2\leq 1\}}\right)
                $$
            }
%--------------------------------------------
        \item Écrire le pseudo code des algorithmes qui calculent les approximations $I_{1,n}$ et $\hat I_{2,n}$ de $\pi$ par méthode de Monte-Carlo, appliquée à $I_1$ et à $I_2$.
%--------------------------------------------
            \cor{
                On approche $I_1$ par $\frac{4}{n} \sum_{i=1}^n \sqrt{1-X_i^2}$ où $X_1, \cdots, X_n$ sont i.i.d de loi $U(0,1)$

                \begin{algorithm}[H]
                    \SetAlgoLined
                    \textbf{Entrée:} une taille d'échantillon $n$

                    \begin{enumerate}
                        \item  Générer $X_1$, $X_2$, ..., $X_n$ selon une loi uniforme sur $[0,1]$
                        \item  Calculer $Y_1=\sqrt{1-X_1}$, ..., $Y_n=\sqrt{1-X_n}$
                    \end{enumerate}
                    \textbf{Sortie:}  renvoyer la moyenne arithmétique des $Y_1$, $Y_2$, \dots, $Y_n$ et la multiplier par $4$.
                    \caption{Approximation de $I_1$.}
                \end{algorithm}

                On approche $I_2$ par $\frac{4}{n} \sum_{i=1}^n \one_{ \{X^2+Y^2\leq 1\}}$ où  $X_1, \cdots, X_n$ sont i.i.d de loi $U(-1,1)$ et  $Y_1, \cdots, Y_n$ sont i.i.d de loi $U(-1,1)$

                \begin{algorithm}[H]
                    \SetAlgoLined
                    \textbf{Entrée:} une taille d'échantillon $n$

                    \begin{enumerate}
                        \item  $S\leftarrow 0$
                        \item  Générer $X_1$, $X_2$, ..., $X_n$ selon une loi uniforme sur $[-1,1]$
                        \item  Générer $Y_1$, $Y_2$, ..., $Y_n$ selon une loi uniforme sur $[-1,1]$
                        \item  Pour $i$ allant de $1$ à $n$:
                            SI $X_i^2+Y_i^2$ est inférieur ou égal à $1$ ALORS $S \leftarrow S+1$
                            FIN SI
                            FIN de boucle i
                    \end{enumerate}
                    \textbf{Sortie:} Retourner la valeur $\tfrac{4S}{n}$.
                \end{algorithm}
            }
%--------------------------------------------
    \end{enumerate}
\end{exo}

%=========================
\begin{exo}{} % [Probabilités de dépassement d'une Cauchy]
    Supposons que nous souhaitions estimer la probabilité qu'une variable aléatoire
    $X \sim \mbox{Cauchy}(0, 1)$ soit plus grande que 2.

    \begin{enumerate}
        \item Calculer $I = \P(X \geq 2)$.
            \cor{
                \begin{equation*}
                    I = \mathbb P(X\geq 2) = \int_2^\infty \frac{1}{\pi (1 + x^2)} dx = - \frac{\arctan 2}{\pi} + \frac{1}{2} \approx 0.148.
                \end{equation*}
            }
%--------------------------------------------
        \item Trouver un estimateur basique par Monte--Carlo pour $I$ et déterminer sa variance.
%--------------------------------------------
            \cor{
                L'estimateur par Monte--Carlo classique s'écrit
                \begin{equation*}
                    I_n = \frac{1}{n} \sum_{i=1}^n \one_{\{X_i > 2\}}, \qquad
                    X_1, \ldots, X_n \stackrel{\rm iid}{\sim} \mbox{Cauchy}(0,1),
                \end{equation*}
                et sa variance est
                \begin{equation*}
                    \mbox{Var}(I_n) = \frac{\mbox{Var}(\one_{\{X > 2\}})}{n} =
                    \frac{I (1 - I)}{n} \approx \frac{0.126}{n}
                \end{equation*}
            }
%--------------------------------------------
        \item Trouver un estimateur antithétique (simple) pour $I$ et
            déterminer sa variance.
%--------------------------------------------
            \cor{
                Puisqu'une $\mbox{Cauchy}(0,1)$ est symétrique autour de 0,
                un estimateur antithétique s'écrit alors
                \begin{equation*}
                    I_n^a = \frac{1}{2n} \sum_{i=1}^n \left(\one_{\{X_i > 2\}} +
                    \one_{\{-X_i > 2\}} \right) = \frac{1}{2n} \sum_{i=1}^n
                    \one_{\{|X_i| > 2\}}, \qquad X_1, \ldots, X_n \stackrel{\rm
                    iid}{\sim} \mbox{Cauchy}(0,1),
                \end{equation*}
                et sa variance est
                \begin{equation*}
                    \mbox{Var}(I_n^a) =  \frac{\mbox{Var}(\one_{\{|X| >
                    2\}})}{4n} = \frac{2 I (1 - 2 I)}{4 n} = \frac{I (1
                    - 2 I)}{2 n} \approx \frac{0.052}{n}.
                \end{equation*}
            }
%--------------------------------------------
        \item Montrer que
            \begin{equation*}
                \tilde{I}_n = \frac{1}{2} - \frac{1}{n} \sum_{i=1}^n
                \frac{2}{\pi (1 + X_i^2)}, \qquad X_i \stackrel{\rm iid}{\sim}
                U(0,2),
            \end{equation*}
            est un estimateur sans biais de $I$, i.e., $\mathbb{E}[\tilde{I}_n] = I$ et calculer sa variance.
%--------------------------------------------
            \cor{
                \begin{align*}
                    \mathbb{E}[\tilde{I}_n] &= \frac{1}{2} - \mathbb{E}\left[
                    \frac{2}{\pi (1 + X^2)} \right] =
                    \frac{1}{2} - \int_0^2 \frac{1}{\pi
                    (1 + u^2)} \mbox{d$u$}\\
                    &= \int_0^\infty \frac{1}{\pi (1 + u^2)} \mbox{d$u$} -
                    \int_0^2 \frac{1}{\pi (1 + u^2)} \mbox{d$u$}\\
                    &= I.
                \end{align*}

                Pour la variance maintenant
                \begin{align*}
                    \mbox{Var}(\tilde{I}_n) &= \frac{4}{\pi^2 n} \mbox{Var}\{(1 + X^2)^{-1}\}\\
                    &= \frac{4}{\pi^2 n} \left[ \mathbb{E}\{(1 + X^2)^{-2}\} -
                    \mathbb{E}\{(1 + X^2)^{-1}\}^2 \right].
                \end{align*}
                Or on a
                \begin{align*}
                    \mathbb{E}\{(1 + X^2)^{-2}\} &= \frac{1}{2} \int_0^2 (1 +
                    x^2)^{-2} \mbox{d$x$} =
                    \frac{1}{2} \int_0^2 \frac{1 +
                    x^2 - x^2}{(1 + x^2)^2}
                    \mbox{d$x$}\\
                    &= \frac{1}{2} \int_0^2 \frac{1}{(1
                    + x^2)} \mbox{d$x$} - \frac{1}{2}
                    \int_0^2 \frac{x^2}{(1 + x^2)^2} \mbox{d$x$}\\
                    &= \frac{1}{2} [\arctan x]_0^2 -
                    \frac{1}{2}\int_0^2
                    \underbrace{x}_u  \underbrace{\frac{x}{(1 +
                    x^2)^2}}_{v'} \mbox{d$x$}\\
                    &= \frac{1}{2} [\arctan x]_0^2 -
                    \frac{1}{2} \left[-\frac{x}{2 (1 + x^2)} \right]_0^2 -
                    \int_0^2 \frac{1}{2 (1 + x^2)} \mbox{d$x$}\\
                    &= \frac{1}{2} \left[\frac{x}{2 (1 + x^2)} + \frac{1}{2}
                    \arctan x \right]_0^2\\
                    &= \frac{1}{10} + \frac{1}{4} \arctan 2.
                \end{align*}

                On en déduit donc que
                \begin{align*}
                    \mbox{Var}(\tilde{I}_n) &= \frac{4}{\pi^2 n} \left\{\frac{1}{10}
                        + \frac{1}{4} \arctan 2 -
                        \left(\frac{\arctan 2}{2} \right)^2
                    \right\}\\
                    &\approx \frac{0.0285}{n}.
                \end{align*}

            }
%--------------------------------------------
        \item Faire de même pour l'estimateur
            \begin{equation*}
                \hat{I}_n = \frac{1}{n} \sum_{i=1}^n \left\{ \frac{1}{2 \pi
                (1 + X_i^2)}  \right\}, \qquad X_1, \ldots, X_n \stackrel{\rm
                iid}{\sim} U(0,1/2).
            \end{equation*}
%--------------------------------------------
            \cor{
                C'est à peu près la même chose. On a
                \begin{align*}
                    \mathbb{E}[\hat{I}_n] &= 2 \int_0^{1/2} \frac{1}{2 \pi (1 +
                    x^2)} \mbox{d$x$} \stackrel{x \mapsto x^{-1}}{=} \int_2^\infty
                    \frac{1}{\pi x^2 (1 + x^{-2})}
                    \mbox{d$x$}\\
                    &= \int_2^\infty \frac{1}{\pi (1 + x^2)} \mbox{d$x$}\\
                    &= I.
                \end{align*}

                Puisque l'on a
                \begin{align*}
                    \mbox{Var} \left\{\frac{1}{2 \pi (1 + X^2)} \right\}
                    &= \mathbb{E} \left\{\frac{1}{4 \pi^2 (1 + X^2)^2} \right\} - \psi^2\\
                    &= \frac{1}{4 \pi^2} \times 2 \left[\frac{x}{2 (1 +
                    x^2)} + \frac{1}{2} \arctan x \right]_0^{1/2} - I^2\\
                    &= \frac{1}{4 \pi^2} \left( \frac{2}{5} + \arctan\frac{1}{2} \right) - I^2\\
                    &\approx 9.55 \times 10^{-5},
                \end{align*}
                et donc
                \begin{equation*}
                    \mbox{Var}(\hat{I}_n) \approx \frac{9.55 \times 10^{-5}}{n}.
                \end{equation*}
                Ce dernier estimateur est, à $n$ fixé, environ 23 fois plus précis
                que notre premier estimateur.

            }
%--------------------------------------------
    \end{enumerate}
\end{exo}



\begin{exo}{} %[Aiguille de Buffon]
    L'expérience originelle de Buffon pour approcher $\pi$ consistait à jeter une aiguille de longueur
    $\ell$ sur une grille de ligne parallèles, les lignes étant distantes de
    $d$, et compter le nombre de fois où l'aiguille croise une ligne.
    \begin{figure}[h!]
        \centering
        \begin{tikzpicture}
            %% les lattes de parquet
            \foreach \x in {-4, -2, 0, 2, 4}
            {\draw [thick,brown] (\x, -2) -- (\x, 2);}

            \draw [|-|,blue] (2,-1) -- (4,-1);
            \draw [blue] (3, -1) node[below] {{\tiny $d$}};

            %% l'aiguille
            \draw [thin] (-1.75,-1) -- (-0.25, 0);
            \draw [blue,|-|] (-1.9,-0.8) -- (-0.4,0.2);
            \draw [blue] (-1.4, -0.2) node[above] {{\tiny $\ell$}};

            %% l'angle
            \draw [dashed] (-1,-0.5) -- (0,-0.5) ;
            \draw (-0.5,-0.5) arc (0:35:0.5);
            \draw [blue] (-0.5,-0.25) node[right] {{\tiny $\theta$}};

            %% La distance à la jointure la plus proche
            \draw [blue,|-|] (-1,-0.75) -- (0, -0.75);
            \draw [blue] (-0.5, -0.75) node[below] {{\tiny $X$}};
        \end{tikzpicture}
        \caption{L'aiguille de Buffon. Ici l'aiguille est de longueur $\ell$
            et les lignes parallèles espacées de $d$. Les variables aléatoires
            $X$ et $\theta$ représentent respectivement la distance du milieu
            de l'aiguille à la ligne la plus proche et l'angle formé avec
        l'axe horizontal.}
        \label{fig:buffon}
    \end{figure}
    \begin{enumerate}
        \item Montrez que, pour $\ell \leq d$, la probabilité que l'aiguille soit à cheval sur une ligne est
            $2 \ell / (\pi d)$. En déduire un estimateur pour
            $I= 1 / \pi$.
%-----------------------
            \cor{
                D'après la Figure~\ref{fig:buffon} et puisque l'aiguille est jetée
                complètement au hasard, cela définit deux v.a. $X \sim U[0, d / 2]$
                et $\theta \sim U[-\pi/2, \pi/2]$. On a donc
                \begin{align*}
                    \mathbb P(\text{aiguille à cheval}) &= \mathbb P\left( \frac{\ell}{2} \cos
                    \theta \geq X \right) =
                    \int_{-\pi/2}^{\pi/2}
                    \int_0^{\ell \cos(\theta) / 2}
                    \frac{2}{d} \mbox{d$x$} \frac{1}{\pi} \mbox{d$\theta$}\\
                    &= \int_{-\pi/2}^{\pi/2} \frac{\ell \cos \theta}{d \pi}
                    \mbox{d$\theta$} = \frac{2 \ell}{d \pi}
                \end{align*}

                D'après l'équation précédente on a donc que
                \begin{equation*}
                    \pi^{-1} = \frac{d \mathbb P(\text{aiguille à cheval})}{2 \ell},
                \end{equation*}
                et un estimateur de $I= 1 / \pi$ est donc
                \begin{equation*}
                    I_n = \frac{d}{2 \ell n} \sum_{i=1}^n \one_{\{\ell \cos
                    \theta_i \geq 2 X_i\}}, \qquad (X_1, \theta_1), \ldots, (X_n,
                    \theta_n) \stackrel{\rm iid}{\sim} U[0, d/2] \times U[-\pi/2,
                    \pi/2].
                \end{equation*}
            }
%-----------------------
        \item Trouvez la variance de cet estimateur et déduisez en un choix optimal pour
            $\ell$ et $d$.
%-----------------------
            \cor{
                La variance se trouve en calculant
                \begin{equation*}
                    \mbox{Var}(I_n) = \frac{d^2}{4 \ell^2 n} \mbox{Var}(\one_{\{\ell \cos\theta_i \geq 2 X_i\}}) =  \frac{I}{n} \left ( \frac{d}{2 \ell} - I \right),
                \end{equation*}
                et le choix optimal pour $\ell$ et $d$, i.e., celui minimisant la
                variance correspond au cas où $\ell = d$, puisque $\ell$ ne peut
                pas être strictement plus grand que $d$.
            }
%-----------------------
        \item  Avec ce choix optimal, construisez un estimateur de $\pi$.
%-----------------------
            \cor{
                Lorsque $\ell = d = 1$, un estimateur de $\pi$ est alors
                \begin{equation*}
                    \hat{\pi}_n = 2 n \left(\sum_{i=1}^n \one_{\{ \cos \theta_i \geq
                    2 X_i\}}\right)^{-1}, \qquad (X_1, \theta_1), \ldots, (X_n,
                    \theta_n) \stackrel{\rm iid}{\sim} U[0, 1/2] \times U[-\pi/2,
                    \pi/2].
                \end{equation*}
            }
%-----------------------
    \end{enumerate}
\end{exo}



\begin{exo}{} %[Probabilités de dépassement d'une Normale]
    Soit $Z \sim N(0,1)$. Nous cherchons à évaluer la
    probabilité $I = \mathbb P(Z >4.5)$.
    \begin{enumerate}
        \item Donnez un estimateur basique pour $I$.
%-----------------------
            \cor{
                L'estimateur est
                \begin{equation*}
                    I_n = \frac{1}{n} \sum_{i=1}^n \one_{\{Z_i > 4.5\}}, \qquad Z_1, \ldots, Z_n \stackrel{\rm iid}{\sim} N(0,1)
                \end{equation*}
            }
%-----------------------
        \item Soit $Y = 4.5 + E$ où $E \sim \mbox{Exp}(1)$. Déterminez la
            densité de probabilité $h$ de $Y$ et trouvez comment simuler des
            réalisations de $Y$.
%-----------------------
            \cor{
                Soit $y > 4.5$, on a
                \begin{equation*}
                    \mathbb P(Y \leq y) = \mathbb P(E \leq y - 4.5) = 1 - \exp(-y + 4.5)
                \end{equation*}
                et la densité en dérivant par rapport à $y$, i.e.,
                \begin{equation*}
                    h(y) = \exp\{-(y - 4.5)\}, \qquad y \geq 4.5
                \end{equation*}

                On simule facilement selon l'algorithme
                \begin{enumerate}
                    \item Simuler $E \sim \mbox{Exp}(1)$ (via la fonction de répartition inverse)
                    \item Retournez $Y = 4.5 + E$.
                \end{enumerate}
            }
%-----------------------
        \item Trouvez un estimateur par échantillonnage préférentiel de
            $I$ basés sur des tirages de $Y$.
%-----------------------
            \cor{
                D'après le cours, on a
                \begin{align*}
                    I &= \mathbb P(Z >4.5)\\
                    &=\int_\bbR \one_{z >4.5}\frac{1}{\sqrt{2 \pi}} \exp\left\{-\frac{z^2}{2}\right\}dz\\
                    &=\int_\bbR \one_{z >4.5}
                    \frac{1}{\sqrt{2 \pi}\exp\{-(z - 4.5)\}} \exp\left\{-\frac{z^2}{2}\right\}\exp\{-(z - 4.5)\}dz\\
                    &=\bbE[\frac{1}{\sqrt{2 \pi}} \exp\left\{-\frac{Y^2}{2}+ Y - 4.5 \right\}]
                \end{align*}
                puisque $\mathbb P(Y > 4.5) = 1$.
                L'estimateur est alors
                \begin{align*}
                    I_n^e&=  \frac{1}{n} \sum_{i=1}^n \frac{1}{\sqrt{2 \pi}}
                    \exp\left\{-\frac{Y_i^2}{2} + Y_i - 4.5 \right\}.
                \end{align*}
            }
%-----------------------
    \end{enumerate}
\end{exo}



\begin{exo}{} %
    On veut utiliser la m\'ethode de Monte Carlo pour calculer le prix $C$ d'une option européenne d'achat (call) au temps 0 dans le mod\`ele de Black et Scholes. Une option européenne est un actif financier caractérisé par un prix d'exercice $K$, une date échéance $T$ et un actif risqué de valeur initiale $x$ suivant le mod\`ele de Black et Scholes. Son prix s'exprime par la formule
    \begin{eqnarray*}
        C&=&\mathbb{E}\left[\left(x\mathrm{e}^{\sigma\sqrt{T}X-\frac{\sigma^2T}{2}}-K\mathrm{e}^{-rT}\right)_+\right],
%&=&\mathbb{E}\left[(K\mathrm{e}^{-rT}-x\mathrm{e}^{\sigma\sqrt{T}X-\frac{\sigma^2T}{2}})\one_{\{X\leq -d_2\}}\right]
    \end{eqnarray*}
    où $X$ est une variable al\'eatoire de loi normale centr\'ee r\'eduite et $(\cdot)_+$ désigne la partie positive,
    $r$ est le taux d'intérêt sans risque, et $\sigma$ la volatilité du modèle de Black et Scholes.
    \begin{enumerate}
        \item Donner un estimateur basique de $C$.
%-----------------------
            \cor{
                L'estimateur Monte Carlo basique est
                $$C_n=\frac{1}{n}\sum_{k=1}^n\left(x\mathrm{e}^{\sigma\sqrt{T}X_k-\frac{\sigma^2T}{2}}-K\mathrm{e}^{-rT}\right)_+,$$
                avec $X_k$ iid de loi $\scrN(0,1)$.
            }
%-----------------------
        \item Donner un estimateur de antithétique simple de $C$.
%-----------------------
            \cor{
                La loi normale centrée réduite est symétrique. On peut donc poser
                $$C_n^a=\frac{1}{2n}\sum_{k=1}^n\left(x\mathrm{e}^{\sigma\sqrt{T}X_k-\frac{\sigma^2T}{2}}-K\mathrm{e}^{-rT}\right)_++\left(x\mathrm{e}^{-\sigma\sqrt{T}X_k-\frac{\sigma^2T}{2}}-K\mathrm{e}^{-rT}\right)_+,$$
                avec $X_k$ iid de loi $\scrN(0,1)$.
            }
%-----------------------
            Dans toute la suite, on suppose que $x=K$ et $r=\sigma^2/2$.
        \item  Montrer que
            \begin{align*}
                C=\frac{x\mathrm{e}^{-rT}}{\sqrt{2\pi}}\mathbb{E}\Big[\frac{\mathrm{e}^{\sigma\sqrt{2TU}}-1}{\sqrt{2U}}\Big],
            \end{align*}
            o\`u $U$ est une variable al\'eatoire de loi exponentielle de param\`etre $1$.

%-----------------------
            \cor{
                En utilisant $x=K$ et $r=\sigma^2/2$, il vient
                \begin{align*}
                    C
                    &=\mathbb{E}\left[\left(x\mathrm{e}^{\sigma\sqrt{T}X-\frac{\sigma^2T}{2}}-x\mathrm{e}^{-\frac{\sigma^2T}{2}}\right)_+\right]\\
                    &=\mathbb{E}\left[\left(\mathrm{e}^{\sigma\sqrt{T}X}-1\right)_+\right]\\
                    &=x\mathrm{e}^{-\frac{\sigma^2T}{2}}\mathbb{E}[(\mathrm{e}^{\sigma\sqrt{T}X}-1)\one_{\{X\geq0\}}]
                \end{align*}
                car l'exponentielle dépasse $1$ si et seulement si $X$ est positif.
                En faisant le changement de variable $u=y^2/2$ on obtient
                \begin{eqnarray*}
                    C&=&\frac{x\mathrm{e}^{-rT}}{\sqrt{2\pi}}\int_0^{+\infty}(\mathrm{e}^{\sigma\sqrt{T}y}-1)\mathrm{e}^{-y^2/2}dy\\
%&=&\frac{x\mathrm{e}^{-rT}}{\sqrt{2\pi}}\int_0^{+\infty}\frac{\mathrm{e}^{\sigma\sqrt{T}y}-1}{\sigma\sqrt{T}y}\sigma\sqrt{T}y\mathrm{e}^{-y^2/2}dy\\
                    &=&\frac{x\mathrm{e}^{-rT}}{\sqrt{2\pi}}\int_0^{+\infty}\frac{\mathrm{e}^{\sigma\sqrt{T}\sqrt{2u}}-1}{\sqrt{2u}}\mathrm{e}^{-u}du\\
                    &=&\frac{x\mathrm{e}^{-rT}}{\sqrt{2\pi}}\mathbb{E}\Big[\frac{\mathrm{e}^{\sigma\sqrt{2TU}}-1}{\sqrt{2U}}\Big]
                \end{eqnarray*}
                o\`u $U$ est une variable al\'eatoire de loi exponentielle de param\`etre $1$.
            }
%-----------------------
        \item  En déduire un nouvel estimateur d'échantillonnage préférentiel de $C$.
%-----------------------
            \cor{
                On a
                $$C_n^e=\frac{x\mathrm{e}^{-rT}}{n\sqrt{2\pi}}\sum_{k=1}^n\frac{\mathrm{e}^{\sigma\sqrt{2TU_k}}-1}{\sqrt{2U_k}},$$
                avec $U_k$ iid de loi $\scrE(1)$.
            }
%-----------------------
        \item Dans le modèle de Balck et Scholes, le prix $P$ d'une option européenne de vente (put) au temps 0 avec prix d'exercice $K$, date échéance $T$ et actif risqué de valeur initiale $x$ est
            \begin{eqnarray*}
                P&=&\mathbb{E}\left[\left(K\mathrm{e}^{-rT}-x\mathrm{e}^{\sigma\sqrt{T}X-\frac{\sigma^2T}{2}}\right)_+\right].
%&=&\mathbb{E}\left[(K\mathrm{e}^{-rT}-x\mathrm{e}^{\sigma\sqrt{T}X-\frac{\sigma^2T}{2}})\one_{\{X\leq -d_2\}}\right]
            \end{eqnarray*}
            Démontrer la relation de parité call-put : $C=P+x(1-\mathrm{e}^{-rT})$.

%-----------------------
            \cor{
                On a
                \begin{align*}
                    C-P&=\mathbb{E}\left[\left(x\mathrm{e}^{\sigma\sqrt{T}X-\frac{\sigma^2T}{2}}-K\mathrm{e}^{-rT}\right)_+\right]-\mathbb{E}\left[\left(K\mathrm{e}^{-rT}-x\mathrm{e}^{\sigma\sqrt{T}X-\frac{\sigma^2T}{2}}\right)_+\right]\\
                    &=\mathbb{E}\left[\left(x\mathrm{e}^{\sigma\sqrt{T}X-\frac{\sigma^2T}{2}}-K\mathrm{e}^{-rT}\right)_+-\left(x\mathrm{e}^{\sigma\sqrt{T}X-\frac{\sigma^2T}{2}}-K\mathrm{e}^{-rT}\right)_-\right]\\
                    &=\mathbb{E}\left[x\mathrm{e}^{\sigma\sqrt{T}X-\frac{\sigma^2T}{2}}-K\mathrm{e}^{-rT}\right]\\
                    &=x\mathrm{e}^{-rT}\mathbb{E}\left[\mathrm{e}^{\sigma\sqrt{T}X}-1\right]
                \end{align*}
                Or
                \begin{align*}
                    \mathbb{E}\left[\mathrm{e}^{\sigma\sqrt{T}X}\right]
                    &=\frac{1}{\sqrt{2\pi}}\int_{-\infty}^{\infty}\mathrm{e}^{\sigma\sqrt{T}x}\mathrm{e}^{x^2/2}dx\\
                    &=\frac{1}{\sqrt{2\pi}}\int_{-\infty}^{\infty}\mathrm{e}^{-\frac{1}{2}(x-\sigma\sqrt{T})^2}dx\\
                    &=\mathrm{e}^{\sigma^2T/2}\frac{1}{\sqrt{2\pi}}\int_{-\infty}^{\infty}\mathrm{e}^{-\frac{1}{2}(x-\sigma\sqrt{T})^2}dx\\
                    &=\mathrm{e}^{\sigma^2T/2}\ =\ \mathrm{e}^{rT}.
                \end{align*}
                donc on a
                \begin{align*}
                    C-P&=x\mathrm{e}^{-rT}\mathbb{E}\left[\mathrm{e}^{\sigma\sqrt{T}X}-1\right]\\
                    &=x\mathrm{e}^{-rT}(\mathrm{e}^{rT}-1)\ =\ x(1-\mathrm{e}^{-rT}).
                \end{align*}
            }
%-----------------------
        \item En déduire un estimateur par variable de contrôle de $C$.
%-----------------------
            \cor{
                A partir de la relation de parité call-put $C=P+x(1-\mathrm{e}^{-rT})$, on peut estimer $C$ par
                $$C_n^c=\frac{1}{n}\sum_{k=1}^n\left(K\mathrm{e}^{-rT}-x\mathrm{e}^{\sigma\sqrt{T}X_k-\frac{\sigma^2T}{2}}\right)_++x(1-\mathrm{e}^{-rT}),$$
                avec $X_k$ iid de loi $\scrN(0,1)$.
            }
%-----------------------
    \end{enumerate}
\end{exo}
\end{document}

