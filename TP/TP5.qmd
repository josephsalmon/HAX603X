---
title: "TP5: Méthode de Monte-Carlo"
bibliography: ../Courses/references.bib
format:
  html:
    out.width: 50%
correction: true
filters:
  - shinylive
---

::: {.content-visible when-format="html"}

::: {.hidden}
{{
\newcommand{\bbE}{\mathbb{E}}
\usepackage{dsfont}
\newcommand{\1}{{1\hspace{-3.8pt} 1}}


}}
:::

:::

::: {.callout-note}

## Objectifs de ce TP
- Implémenter des méthode de Mont-Carlo simple pour calculer des intégrales
- Calculer des intervales de confiances pour évaluer la précision des estimateurs
:::

## Approxiamtion de $\pi$

Implémenter la méthode de Monte Carlo pour le calcul approché de $\pi$ via les deux intégrales suivantes :

1.  pour $X$ de loi uniforme sur $[0,1]$
	$$
	I_1= 4 \cdot \int_0^2 \sqrt{1-x^2} dx=\pi=4 \cdot \bbE\left[\sqrt{1-X^2} \right],
	$$

::: {.content-hidden unless-meta="correction"}
#### [Solution]{style="color:red;"}
```{python}
import numpy as np
from scipy import stats

n = 10000

x = np.random.uniform(0, 1, n)
y = 4 * np.sqrt(1 - x**2)
in_ = np.mean(y)

q = stats.norm.ppf(0.975)
delta = q * np.sqrt(np.var(y) / n)

ic = (in_ - delta, in_ + delta)

print("IN:", in_)
print("Confidence Interval (IC):", ic)
```
:::

1. pour $(X,Y)$ de loi uniforme sur $[-1,1]^2$
	$$
	I_2= \int_{\mathbb R^2} \1_{\{ x^2+ y^2 \leq 1\}} dx dy=\pi=\bbE[4\1_{\{X^2+Y^2\leq 1\}}].
	$$

::: {.content-hidden unless-meta="correction"}
#### [Solution]{style="color:red;"}
```{python}
import numpy as np

# Define the number of points
n = 10000

# Generate random numbers for X and Y in the range [-1, 1]
x = np.random.uniform(-1, 1, n)
y = np.random.uniform(-1, 1, n)

# Calculate Z based on the condition (X^2 + Y^2 <= 1)
z = 4 * (x**2 + y**2 <= 1)

# Calculate the mean of Z
in_ = np.mean(z)

# Calculate the confidence interval (IC)
q = stats.norm.ppf(0.975)
delta = q * np.sqrt(np.var(z) / n)
ic = (in_ - delta, in_ + delta)

print("IN:", in_)
print("Confidence Interval (IC):", ic)
```
:::

3. Quelle méthode donne la meilleure précision ?

::: {.content-hidden unless-meta="correction"}
#### [Solution]{style="color:red;"}

La première méthode donne une meilleure précision. Mais est-ce si concluant.
Il faut regarder plus en détail les variance pour conclure.


:::

## Loi de Cauchy

On souhaite estimer la probabilité qu'une variable aléatoire  $X \sim \text{Cauchy}(0, 1)$ soit plus grande que 2, *i.e.*,
$$
    I = \mathbb P(X\geq 2) = \int_2^\infty \left\{ \pi (1 + x^2) \right\}^{-1}
    \text{d$x$} = - \frac{\arctan 2}{\pi} + \frac{1}{2}
$$

1. Évaluer avec `Python` la valeur exacte de l'intégrale.

::: {.content-hidden unless-meta="correction"}
#### [Solution]{style="color:red;"}
```{python}
print(-np.arctan(2) / np.pi + 1/2)

```
:::

2. Implémenter l’estimateur de Monte Carlo simple à base de loi de Cauchy pour cette intégrale.

::: {.content-hidden unless-meta="correction"}
#### [Solution]{style="color:red;"}
```{python}
import numpy as np

# Define the number of samples
N = 10000

# Generate samples from the Cauchy distribution
X = np.random.standard_cauchy(N)

# Calculate Y based on the condition (X >= 2)
Y = (X >= 2)

# Calculate the mean of Y
in_ = np.mean(Y)

# Calculate the confidence interval (IC)

q = stats.norm.ppf(0.975)
delta = q * np.sqrt(np.var(Y) / n)
ic = (in_ - delta, in_ + delta)

print("IN:", in_)
print("Confidence Interval (IC):", ic)

```
:::

3. Implémenter l’estimateur de Monte Carlo antithétique basé sur la symétrie de la  loi de Cauchy.

::: {.content-hidden unless-meta="correction"}
#### [Solution]{style="color:red;"}
```{python}
import numpy as np

# Define the number of samples
N = 10000

# Generate samples from the Cauchy distribution
X = np.random.standard_cauchy(N)

# Calculate Z based on the condition ((X >= 2) + (-X >= 2)) / 2
Z = ((X >= 2) + (-X >= 2)) / 2

# Calculate the mean of Z
in_ = np.mean(Z)

# Calculate the confidence interval (IC)
delta = q * np.sqrt(np.var(Z) / n)
ic = (in_ - delta, in_ + delta)

print("IN:", in_)
print("Confidence Interval (IC):", ic)
```

L’intervalle de confiance est un peu plus resserré. La variance a été divisée par

```{python}
np.var(Y) / np.var(Z)
```
:::

4. Implémenter l'estimateur de Monte Carlo à base de loi uniforme sur $[0,2]$ en utilisant la relation
$$
  I =
  \frac{1}{2} - \mathbb{E}\left[ \frac{2}{\pi (1 + Y^2)} \right],
  \quad Y {\sim} U[0,2].
$$

::: {.content-hidden unless-meta="correction"}
#### [Solution]{style="color:red;"}

```{python}
import numpy as np

# Define the number of samples
N = 10000

# Generate samples from the uniform distribution in the range [0, 2]
U = np.random.uniform(0, 2, N)

# Calculate A based on the given formula
A = 1/2 - 2 / (np.pi * (1 + U**2))

# Calculate the mean of A
in_ = np.mean(A)

# Calculate the confidence interval (IC)
q = stats.norm.ppf(0.975)
delta = q * np.sqrt(np.var(A) / n)
ic = (in_ - delta, in_ + delta)

print("IN:", in_)
print("Confidence Interval (IC):", ic)
```
L’intervalle de confiance est encore plus resserré. Par rapport à la méthode initiale,
la variance a été divisée par

```{python}
np.var(Y) / np.var(A)
```
:::

5. Implémenter l'estimateur de Monte Carlo à base de loi uniforme sur $[0,1/2]$ en utilisant la relation
$$
I = 
\mathbb{E}\left[\frac{1}{2\pi (1 + Z^2)} \right],
\quad Z {\sim} U[0,1/2].
$$

::: {.content-hidden unless-meta="correction"}
#### [Solution]{style="color:red;"}

```{python}
import numpy as np

# Define the number of samples
N = 10000

# Generate samples from the uniform distribution in the range [0, 1/2]
U = np.random.uniform(0, 0.5, N)

# Calculate B based on the given formula
B = 1 / (2 * np.pi * (1 + U**2))

# Calculate the mean of B
in_ = np.mean(B)

q = stats.norm.ppf(0.975)
delta = q * np.sqrt(np.var(B) / n)
ic = (in_ - delta, in_ + delta)
# Calculate the confidence interval (IC)

print("IN:", in_)
print("Confidence Interval (IC):", ic)

```
L'intervalle de confiance est encore plus resserré, on arrive presque à une précision de $10^{-3}$. Par rapport à la méthode initiale, la variance a été divisée par
```{python}
np.var(Y) / np.var(B)
```
:::

6. Quelle méthode d’estimation est la plus précise ?

::: {.content-hidden unless-meta="correction"}
#### [Solution]{style="color:red;"}

Celle qui a la plus petite variance, donc la dernière.
:::
