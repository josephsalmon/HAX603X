[
  {
    "objectID": "trash/slides_inv_viz.html#test",
    "href": "trash/slides_inv_viz.html#test",
    "title": "Visualisation: inversion",
    "section": "test",
    "text": "test\n\nviewof dist = Inputs.select(['normal','cauchy','laplace','bimodal'], {value: \"bimodal\", label: \"Loi\"})\nviewof replay = html`&lt;button&gt;Relancer`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ninvcdfboard = require(await FileAttachment(\"../inverse-vizu/dist/invcdfboard.umd.cjs\").url())\n\n{\n  replay\n  const canvas = DOM.canvas(500, 500);\n  const galton = invcdfboard.galton(canvas,dist);\n  \n  return html`${galton.canvas}`\n}"
  },
  {
    "objectID": "TP/TP2.html",
    "href": "TP/TP2.html",
    "title": "TP2: ‚Ä¶",
    "section": "",
    "text": "Objectifs de ce TP\n\n\n\n\nUtiliser les g√©n√©rateurs al√©atoires en Python et numpy, savoir afficher un histogramme, une densit√©, etc.\nComprendre au mieux comment utiliser les fonctions al√©atoires (principalement les g√©n√©rateurs) en numpy.",
    "crumbs": [
      "TP",
      "TP2: ..."
    ]
  },
  {
    "objectID": "TP/TP2.html#loi-uniforme-et-graine.",
    "href": "TP/TP2.html#loi-uniforme-et-graine.",
    "title": "TP2: ‚Ä¶",
    "section": "Loi uniforme et graine.",
    "text": "Loi uniforme et graine.\nAvec numpy, la fonction numpy.random.uniform permet la g√©n√©ration de r√©alisations pseudo-al√©atoires de la loi uniforme sur [0,1].\nOn peut modifier la taille de l‚Äô√©chantillon g√©n√©r√© en modifiant l‚Äôargument de la fonction. Pour obtenir n=4 r√©alisations i.i.d. de loi uniforme, essayez par exemple\n\nimport numpy as np\nnp.random.uniform(size=4)\n\narray([0.98147197, 0.72391549, 0.96063583, 0.03857667])\n\n\nPour rappel, l‚Äôalgorithme de g√©n√©ration de v.a. est r√©cursif et s‚Äôappuie sur une graine. La graine peut √™tre modifi√©e avec la cr√©ation d‚Äôun g√©n√©rateur, et il suffit d‚Äôentrer un nombre en argument pour fixer cette graine.\n\nrng = np.random.default_rng(seed=34)\nprint(rng.uniform())\nrng = np.random.default_rng(34)\nprint(rng.uniform())\n\n0.004028243493043537\n0.004028243493043537\n\n\nChanger les valeurs de seed et v√©rifier que les tirages ont bien chang√©.",
    "crumbs": [
      "TP",
      "TP2: ..."
    ]
  },
  {
    "objectID": "TP/TP2.html#exercice-1-simulation-de-loi-uniforme-et-histogramme",
    "href": "TP/TP2.html#exercice-1-simulation-de-loi-uniforme-et-histogramme",
    "title": "TP2: ‚Ä¶",
    "section": "Exercice 1: Simulation de loi uniforme et histogramme",
    "text": "Exercice 1: Simulation de loi uniforme et histogramme\nCr√©ez un vecteur de taille 1000 compos√© de r√©alisations i.i.d. de v.a.uniformes sur [-1,1]. Dans la suite on supposera que l‚Äôon a charg√© matplotlib pour l‚Äôaffichage graphique avec la commande:\n\nimport matplotlib.pylab as plt\nfrom scipy import stats\n\n√Ä l‚Äôaide de la fonction plt.hist, repr√©sentez l‚Äôhistogramme de cet √©chantillon:\nfig, ax = plt.subplots()\nvect = rng.uniform(-1, 1, 1000)\nax.hist(vect, label=\"Histogramme\");\nplt.legend()\n\n\n\n\n\n\n\nOn utilisera l‚Äôaide de hist de matplotlibs pour pr√©ciser les options graphiques suivantes:\n\nAnalysez en particulier ce que fait l‚Äôoption bins en entrant l‚Äôoption bins=30 et bins=10.\nModifiez √©galement votre histogramme avec l‚Äôoption density=True, de sorte que l‚Äôaire soit de 1 (on repr√©sente donc une densit√© qui est constante par morceaux)\nAjoutez un titre √† l‚Äôhistogramme gr√¢ce √† la commande plt.title (avec une cha√Æne de caract√®res entre guillemets). On peut √©galement ajouter un nom aux axes avec l‚Äôoption plt.xl et plt.xlabel.\nLes options ax.set_xlim et ax.set_ylim permettent de pr√©ciser l‚Äô√©chelle de axes: il faut pr√©ciser un tuple (a,b) o√π a&lt;b sont les deux bornes choisies pour votre axe.\nOn modifiera aussi les options fill et histtype de hist pour obtenir le r√©sultat suivant, en affichant sur un m√™me graphique trois tirages, de tailles 1000, 5000 et 10000.\nLa densit√© de la loi uniforme est obtenue avec la fonction pdf du module scipy.stats. Cr√©er un vecteur √©quir√©parti sur [-2, 2] de longueur 300 √©valuer la fonction sur la m√™me figure: on souhaite superposer cette densit√© √† l‚Äôhistogramme. On utilisera la fonction plot pour tracer la densit√©, et on pourra utiliser l‚Äôoption alpha pour rendre la densit√© plus transparente.\n\nUn exemple de figure de qualit√© acceptable est par exemple celle qui suit:",
    "crumbs": [
      "TP",
      "TP2: ..."
    ]
  },
  {
    "objectID": "TP/TP2.html#exercice-2-fonction-de-r√©partition-de-la-loi-uniforme",
    "href": "TP/TP2.html#exercice-2-fonction-de-r√©partition-de-la-loi-uniforme",
    "title": "TP2: ‚Ä¶",
    "section": "Exercice 2: Fonction de r√©partition de la loi uniforme",
    "text": "Exercice 2: Fonction de r√©partition de la loi uniforme\nLa fonction de r√©partition de la loi uniforme est obtenue via la commande cdf du module scipy.stats.uniform. √Ä l‚Äôaide de la commande plt.plot tracez en bleu la fonction de r√©partition de la loi uniforme sur [-1,1], [-0.7, 0.7] et [-0.5,0.5] et donnez un titre √† votre graphique.\n\nOn contr√¥le avec lw (linewidth) l‚Äô√©paisseur du trait.\nVous pouvez modifier le style et les marqueurs facilement en matplotlib. Une liste exhaustive est donn√©e ici: matplotlib.pyplot.plot.html\nEnfin pour les couleurs on pourra consulter l‚Äôaide en ligne ici: Color tutorial. La mani√®re la plus simple est souvent d‚Äôajouter l‚Äôoption color=nom_couleur dans la fonction plot.\n\nManipulez les diff√©rentes options pour vous familiariser avec les graphes\n\n\n\n\n\n\n\n\n\n\n¬≤ :::{.callout-note}",
    "crumbs": [
      "TP",
      "TP2: ..."
    ]
  },
  {
    "objectID": "TP/TP2.html#pour-aller-plus-loin",
    "href": "TP/TP2.html#pour-aller-plus-loin",
    "title": "TP2: ‚Ä¶",
    "section": "Pour aller plus loin",
    "text": "Pour aller plus loin\nTenter de reproduire la figure suivante\n\n\n\n\n\n\n\n\n\n\n:::",
    "crumbs": [
      "TP",
      "TP2: ..."
    ]
  },
  {
    "objectID": "TP/TP2.html#exercice-3-√©tude-de-la-moyenne-empirique",
    "href": "TP/TP2.html#exercice-3-√©tude-de-la-moyenne-empirique",
    "title": "TP2: ‚Ä¶",
    "section": "Exercice 3: √âtude de la moyenne empirique",
    "text": "Exercice 3: √âtude de la moyenne empirique\nCr√©ez un vecteur de taille 100 compos√© de r√©alisations i.i.d. de variables uniformes sur [0,1]. Calculez dans un vecteur la moyenne cumul√©e des valeurs g√©n√©r√©es. Repr√©senter graphiquement l‚Äô√©volution de ces moyennes. Vers quoi semble converger la moyenne quand la taille de l‚Äô√©chantillon augmente ?\nPour ajouter une droite √† un graphe, on utilise la commande ax.axhline. Ajoutez en rouge la droite d‚Äô√©quation y=1/2 sur le graphe pr√©c√©dent. Refaites cet exercice avec un √©chantillon de taille n=1000 pour observer plus finement la convergence.",
    "crumbs": [
      "TP",
      "TP2: ..."
    ]
  },
  {
    "objectID": "TP/TP2.html#exercice-4-m√©thode-dinversion-loi-exponentielle-et-loi-de-cauchy",
    "href": "TP/TP2.html#exercice-4-m√©thode-dinversion-loi-exponentielle-et-loi-de-cauchy",
    "title": "TP2: ‚Ä¶",
    "section": "Exercice 4: M√©thode d‚Äôinversion, loi exponentielle et loi de Cauchy",
    "text": "Exercice 4: M√©thode d‚Äôinversion, loi exponentielle et loi de Cauchy\n\nRepr√©sentez graphiquement la fonction de r√©partition d‚Äôune loi exponentielle de param√®tre \\lambda=1.\n√âcrivez une fonction dzexpo qui prend en argument une taille d‚Äô√©chantillon n et un param√®tre \\lambda &gt; 0 et qui donne en sortie un √©chantillon de taille n de loi \\mathcal{E}(\\lambda). On utilisera la m√©thode d‚Äôinversion vue en cours et seulement des tirages uniformes sur [0,1]. Attention, le mot clef lambda est un mot r√©serv√© en Python.\nRepr√©sentez graphiquement l‚Äôhistogramme cumul√© (voir l‚Äôoption cumulative de hist) d‚Äôun tel √©chantillon pour n=10^2, n=10^3, puis n=10^4, et pour \\lambda = 1, puis \\lambda = 4. Superposez √† chaque fois le graphe de la densit√© de \\mathcal{E}(\\lambda).\nIllustrez graphiquement la loi des grands nombres avec \\lambda = 1, puis \\lambda = 4. On tracera en particulier la droite d‚Äô√©quation y=\\mathbb{E}[X], o√π X \\sim \\mathcal{E}(\\lambda).\nReprenez les questions pr√©c√©dentes avec la loi de Cauchy (mais repr√©senter la densit√© plut√¥t que les fonctions de r√©partition). Commentez les r√©sultats obtenus.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOn reprend le tout avec la loi de Cauchy:",
    "crumbs": [
      "TP",
      "TP2: ..."
    ]
  },
  {
    "objectID": "TD/TD1.html#test2",
    "href": "TD/TD1.html#test2",
    "title": "TD1:‚Ä¶",
    "section": "test2",
    "text": "test2",
    "crumbs": [
      "TD",
      "TD1:..."
    ]
  },
  {
    "objectID": "Slides/slides_simulation.html#enjeu",
    "href": "Slides/slides_simulation.html#enjeu",
    "title": "Simulation",
    "section": "Enjeu",
    "text": "Enjeu\n \nQuestion: Comment simuler en pratique des variables al√©atoires i.i.d?\n \n\nApproche: Commencer par les v.a. uniformes et en d√©duire les autres lois",
    "crumbs": [
      "Slides",
      "Simulation"
    ]
  },
  {
    "objectID": "Slides/slides_simulation.html#rappel-sur-les-variables-uniformes",
    "href": "Slides/slides_simulation.html#rappel-sur-les-variables-uniformes",
    "title": "Simulation",
    "section": "Rappel sur les variables uniformes",
    "text": "Rappel sur les variables uniformes\nRappel : \\(U\\) suit une loi uniforme sur \\([0,1]\\): \\(U\\sim\\mathcal{U}([0,1])\\) ssi sa fonction de r√©partition \\(F_U\\) vaut \\[\nF_U(x)\n=\n\\begin{cases}\n    0, & \\text{si }x &lt; 0\\,,        \\\\\n    x, & \\text{si }x \\in [0,1]\\,,  \\\\\n    1, & \\text{si }x &gt; 1\\,.\n\\end{cases}\n\\]",
    "crumbs": [
      "Slides",
      "Simulation"
    ]
  },
  {
    "objectID": "Slides/slides_simulation.html#challenges",
    "href": "Slides/slides_simulation.html#challenges",
    "title": "Simulation",
    "section": "Challenges",
    "text": "Challenges\n\nObjectif: simuler sur machine une suite \\(U_1, \\dots, U_n\\) de v.a., i.i.d., de loi \\(\\mathcal{U}([0,1])\\).\n Difficult√©s:\n\n\nUne machine est d√©terministe.\nLes nombres flottants entre \\(0\\) et \\(1\\) donn√©s par la machine sont de la forme \\(k/2^p\\), pour \\(k \\in \\{0, \\ldots, 2^{p-1}\\} \\implies\\) impossibilit√© de g√©n√©rer certains nombres.\nV√©rifier qu‚Äôune suite est bien i.i.d. est un probl√®me difficile.",
    "crumbs": [
      "Slides",
      "Simulation"
    ]
  },
  {
    "objectID": "Slides/slides_simulation.html#g√©n√©rateurs-de-nombres-pseudo-al√©atoires-1",
    "href": "Slides/slides_simulation.html#g√©n√©rateurs-de-nombres-pseudo-al√©atoires-1",
    "title": "Simulation",
    "section": "G√©n√©rateurs de nombres pseudo-al√©atoires",
    "text": "G√©n√©rateurs de nombres pseudo-al√©atoires\n\nD√©finition 1 (G√©n√©rateur de nombres pseudo-al√©atoires) \nUn g√©n√©rateur de nombres pseudo-al√©atoires (üá¨üáß: Pseudo Random Number Generator, PRNG), est un algorithme d√©terministe r√©cursif qui renvoie une suite \\(U_1, \\ldots, U_n\\) dans \\([0,1]\\) qui a un ‚Äúcomportement similaire‚Äù √† une suite i.i.d. de loi \\(\\mathcal{U}([0,1])\\).\n\n\nRemarque: ces nombres sont obtenus depuis des nombres entiers g√©n√©r√©s al√©atoirement et uniform√©ment sur grand interval, puis une transformation simple (normalisation) permet d‚Äôobtenir des nombres flottants (üá¨üáß: floats) entre 0 et 1.",
    "crumbs": [
      "Slides",
      "Simulation"
    ]
  },
  {
    "objectID": "Slides/slides_simulation.html#d√©tails-techniques",
    "href": "Slides/slides_simulation.html#d√©tails-techniques",
    "title": "Simulation",
    "section": "D√©tails techniques",
    "text": "D√©tails techniques\nUn PRNG se construit ainsi :\n\nInitialisation: une graine (üá¨üáß: seed) \\(U_0\\), d√©termine la premi√®re valeur (choix arbitraire)\nOn calcule \\(U_{n+1} = f(U_n)\\), o√π \\(f\\) est une transformation d√©terministe, telle que \\(U_{n+1}\\) est ‚Äúle plus ind√©pendant possible‚Äù de \\(U_1, \\dots, U_n\\).\n\n\n\n\\(f\\) : √† valeur dans un ensemble fini \\(\\implies\\) p√©riodicit√© (contrainte: utiliser la plus grande p√©riode possible)\nL‚Äôalgorithme est d√©terministe (une fois la graine fix√©e). Utilit√© de fixer la graine: r√©p√©ter des simulations dans des conditions identiques et ainsi rep√©rer des erreurs",
    "crumbs": [
      "Slides",
      "Simulation"
    ]
  },
  {
    "objectID": "Slides/slides_simulation.html#le-g√©n√©rateur-congruentiel-lin√©aire",
    "href": "Slides/slides_simulation.html#le-g√©n√©rateur-congruentiel-lin√©aire",
    "title": "Simulation",
    "section": "Le g√©n√©rateur congruentiel lin√©aire",
    "text": "Le g√©n√©rateur congruentiel lin√©aire\nLa plupart des PRNG s‚Äôappuient sur des r√©sultats arithm√©tiques.\n\n\nLe plus c√©l√®bre: G√©n√©rateur Congruentiel Lin√©aire (üá¨üáß Linear Congruential Generator, LCG).\nR√©currence: \\[\nX_{n+1} = a X_n + b \\quad \\text{mod } m \\enspace,\n\\] \\(a,b,m\\), entiers bien choisis pour que la suite obtenue ait de bonnes propri√©t√©s\nNormalisation: \\(X_n/m\\).\nExemple: la fonction rand de scilab utilisait cette congruence avec \\(m=2^{31}\\), \\(a=843\\; 314\\; 861\\), et \\(b=453\\; 816\\; 693\\).",
    "crumbs": [
      "Slides",
      "Simulation"
    ]
  },
  {
    "objectID": "Slides/slides_simulation.html#exemples-de-g√©n√©rateurs-alternatifs",
    "href": "Slides/slides_simulation.html#exemples-de-g√©n√©rateurs-alternatifs",
    "title": "Simulation",
    "section": "Exemples de g√©n√©rateurs alternatifs",
    "text": "Exemples de g√©n√©rateurs alternatifs\n \n\nm√©thode par d√©faut pour Python et R: Mersenne-Twister, s‚Äôappuie sur la multiplication vectorielle (p√©riode du g√©n√©rateur \\(m =2^{19937}-1\\))\n\n\n\nm√©thode par d√©faut pour numpy: PCG64 (cf.¬†documentation de numpy), dispose de meilleures garanties statistiques; voir https://www.pcg-random.org\n\n\n\nOn suppose d√©sormais disposer d‚Äôun g√©n√©rateur pseudo-al√©atoire sur \\([0,1]\\).",
    "crumbs": [
      "Slides",
      "Simulation"
    ]
  },
  {
    "objectID": "Slides/slides_simulation.html#usage-en-numpy",
    "href": "Slides/slides_simulation.html#usage-en-numpy",
    "title": "Simulation",
    "section": "Usage en numpy",
    "text": "Usage en numpy\nEn numpy (version&gt;1.17): utiliser des √©l√©ments al√©atoires est d‚Äôutiliser un g√©n√©rateur\n\n\nseed = 12345                       #  choix de la graine\nrng = np.random.default_rng(seed)  #  g√©n√©rateur\n\n\n\n\nprint(rng.random())                #  un tirage uniforme sur [0,1]\n\n0.22733602246716966\n\n\n\n\n\n\nprint(rng.random(size=5))          #  5 tirages uniformes sur [0,1]\n\n[0.31675834 0.79736546 0.67625467 0.39110955 0.33281393]\n\n\n\n\n\n\nprint(rng.random(size=(3, 2)))     #  matrice 3x2, √† entr√©es unif. sur [0,1]\n\n[[0.59830875 0.18673419]\n [0.67275604 0.94180287]\n [0.24824571 0.94888115]]",
    "crumbs": [
      "Slides",
      "Simulation"
    ]
  },
  {
    "objectID": "Slides/slides_simulation.html#plus-sur-les-lois-al√©atoires-et-python",
    "href": "Slides/slides_simulation.html#plus-sur-les-lois-al√©atoires-et-python",
    "title": "Simulation",
    "section": "Plus sur les lois al√©atoires et Python",
    "text": "Plus sur les lois al√©atoires et Python\n\n\n\nSuite du cours: apprendre √† g√©n√©rer de nombreuses lois √† partir de la loi uniforme\nEn pratique: les logiciels proposent les distributions classiques (gaussiennes, exponentielles, etc.), utiliser plut√¥t ces fonctions que de les impl√©menter soi-m√™me.\nListe exhaustive pour numpy:\nhttps://numpy.org/doc/stable/reference/random/generator.html#distributions\n\n\n\n\n\n\n\n\n\n\nPour aller plus loin\n\n\nUne excellent discussion sur les bonnes pratiques al√©atoires en numpy, et l‚Äôusage de np.random.default_rng est donn√©e dans ce blog post d‚ÄôAlbert Thomas.",
    "crumbs": [
      "Slides",
      "Simulation"
    ]
  },
  {
    "objectID": "Slides/slides_simulation.html#rappel-sur-la-fonction-quantile",
    "href": "Slides/slides_simulation.html#rappel-sur-la-fonction-quantile",
    "title": "Simulation",
    "section": "Rappel sur la fonction quantile",
    "text": "Rappel sur la fonction quantile\nRappel: Pour \\(F\\) une fonction d√©finie sur \\(\\mathbb{R}\\) √† valeurs dans \\([0, 1]\\), croissante, on note\n\\[\n\\forall q \\in ]0,1[, \\quad F^\\leftarrow(q) = \\inf\\{ x \\in \\mathbb{R} : F(x)\\geq q\\}\n\\tag{1}\\]\nNote: Si \\(x_0 \\in \\{ x \\in \\mathbb{R} : F(x)\\geq q\\}\\) alors \\([x_0,+\\infty[ \\subset \\{ x \\in \\mathbb{R} : F(x)\\geq q\\}\\)\n\n\nTh√©or√®me 1 (Carat√©risation des quantiles) Soit \\(F\\) une fonction d√©finie sur \\(\\mathbb{R}\\) √† valeurs dans \\([0, 1]\\), croissante et continue √† droite, alors pour tout \\(q \\in ]0, 1[\\), on a \\[\n\\begin{align}\n   \\{x \\in \\mathbb{R} :  F(x) \\geq q) \\} & =\n   \\{x \\in \\mathbb{R} : x \\geq F^\\leftarrow(q)  \\}\n\\end{align}\n\\]\n\n\n\nCas \\(\\subset\\): Soit \\(x \\in \\mathbb{R}\\) t.q. \\(F(x) \\geq q\\), alors par d√©finition de l‚Äôinf dans √âquation¬†1, \\(x \\geq F^\\leftarrow(q)\\)\n\n\n\nCas \\(\\supset\\): Soient \\(\\epsilon&gt;0\\) et \\(x \\in \\mathbb{R}\\) t.q. \\(x \\geq F^\\leftarrow(q)\\) alors (def. de l‚Äôinf) \\(\\exists x_0 \\in \\{ x \\in \\mathbb{R} : F(x)\\geq q\\}\\), t.q. \\(x + \\epsilon &gt; x_0\\). Ainsi, \\(F(x + \\epsilon) \\geq F(x_0) \\geq q\\); par continuit√© √† droite de \\(F\\), \\(F(x) \\geq q\\)",
    "crumbs": [
      "Slides",
      "Simulation"
    ]
  },
  {
    "objectID": "Slides/slides_simulation.html#m√©thode-dinversion",
    "href": "Slides/slides_simulation.html#m√©thode-dinversion",
    "title": "Simulation",
    "section": "M√©thode d‚Äôinversion",
    "text": "M√©thode d‚Äôinversion\n\nTh√©or√®me 2 (M√©thode d‚Äôinversion) Soit \\(X\\) une v.a r√©elle, et \\(U \\sim\\mathcal{U}([0,1])\\), alors la variable al√©atoire \\(F_X^\\leftarrow(U)\\) a m√™me loi que \\(X\\).\n\n\n\nPreuve: en utilisant le th√©or√®me pr√©c√©dent, on a \\[\n\\forall x\\in\\mathbb{R}, \\quad \\mathbb{P}(x \\geq F_X^\\leftarrow(U)) = \\mathbb{P}(F_X(x) \\geq U)\n\\]\n\n\nPuis, comme \\(U\\) est une loi uniforme sur \\([0,1]\\):\n\\[\n\\mathbb{P}(F_X(x) \\geq U) = F_X(x)\n\\]\n\n\nAinsi, \\(F_X^\\leftarrow(U)\\) et \\(X\\) ont m√™me loi: les deux v.a. ont la m√™me fonction de r√©partition",
    "crumbs": [
      "Slides",
      "Simulation"
    ]
  },
  {
    "objectID": "Slides/slides_simulation.html#sym√©trie-de-la-loi-uniforme",
    "href": "Slides/slides_simulation.html#sym√©trie-de-la-loi-uniforme",
    "title": "Simulation",
    "section": "Sym√©trie de la loi uniforme",
    "text": "Sym√©trie de la loi uniforme\n\nProposition 1 (Sym√©trie de la loi uniforme) Soit \\(U \\sim \\mathcal{U}([0,1])\\) une variable uniforme sur \\([0,1]\\). Alors, \\(1-U\\) suit aussi une loi uniforme sur \\([0,1]\\).\n\n\nPreuve:\nOn va d√©crire la fonction de r√©partition de \\(1-U\\) et montrer qu‚Äôelle est √©gale √† celle d‚Äôune loi uniforme sur \\([0,1]\\).\n\n\nLe r√©sultat est facile pour \\(x \\notin [0,1]\\), on suppose donc \\(x \\in [0,1]\\).\n\n\\[\n\\begin{align*}\n\\class{fragment}{{}\\mathbb{P}(1-U \\leq x)} & \\class{fragment}{{}= \\mathbb{P}(U \\geq 1-x) }\\\\\n                       & \\class{fragment}{{} = 1-(1-x)} \\\\\n                       & \\class{fragment}{{} = x}\n\\end{align*}\n\\]",
    "crumbs": [
      "Slides",
      "Simulation"
    ]
  },
  {
    "objectID": "Slides/slides_simulation.html#exemple-loi-exponentielle",
    "href": "Slides/slides_simulation.html#exemple-loi-exponentielle",
    "title": "Simulation",
    "section": "Exemple : loi exponentielle",
    "text": "Exemple : loi exponentielle\n\n\n\nDensit√© d‚Äôune loi \\(\\mathcal{E}(\\lambda)\\) pour \\(\\lambda &gt; 0\\) : \\(f_{\\lambda}(x) = \\lambda e^{-\\lambda x}{1\\hspace{-3.8pt} 1}_{\\mathbb{R}_+}(x)\\)\nFonction de r√©partition: \\(F_{\\lambda}(x) = (1 - e^{-\\lambda x}) {1\\hspace{-3.8pt} 1}_{\\mathbb{R}_+}(x)\\)\n\\(F_{\\lambda}\\) est bijective (de \\(\\mathbb{R}_+\\) dans \\(]0,1[\\)) et pour tout \\(u \\in ]0,1[\\), \\(F_{\\lambda}^{-1}(u) = -\\frac{1}{\\lambda} \\log(1-u)\\)\n\n\n\n\nAvec le r√©sultat: \\[\nU \\sim \\mathcal{U}([0,1]) \\iff 1-U \\sim \\mathcal{U}([0,1])\\enspace,\n\\]\n\nPour simuler une loi exponentielle: simuler \\(U\\) uniforme et appliquer \\(-\\tfrac{1}{\\lambda} \\log(\\cdot)\\)\n\\[\n\\boxed{-\\tfrac{1}{\\lambda} \\log(U) \\sim \\mathcal{E}(\\lambda)}\n\\]",
    "crumbs": [
      "Slides",
      "Simulation"
    ]
  },
  {
    "objectID": "Slides/slides_simulation.html#visualisation",
    "href": "Slides/slides_simulation.html#visualisation",
    "title": "Simulation",
    "section": "Visualisation",
    "text": "Visualisation\nVoir animation dans la section Cours, section ‚ÄúM√©thode d‚Äôinversion‚Äù.",
    "crumbs": [
      "Slides",
      "Simulation"
    ]
  },
  {
    "objectID": "Slides/slides_simulation.html#m√©thode-de-rejet-contraintes",
    "href": "Slides/slides_simulation.html#m√©thode-de-rejet-contraintes",
    "title": "Simulation",
    "section": "M√©thode de rejet: contraintes",
    "text": "M√©thode de rejet: contraintes\nMotivation: simuler une variable al√©atoire \\(X\\) de densit√© \\(f\\) (loi cible), mais \\(f\\) est trop compliqu√©e pour la m√©thode de l‚Äôinverse.\n\nId√©e: tirer suivant une autre loi \\(g\\) (loi des propositions) et rejeter certains tirages.\n\n\non sait simuler \\(Y\\) de loi \\(g\\),\nil existe \\(m &gt; 0\\) tel que \\(f(x) \\leq m \\cdot g(x)\\) (constante de majoration)\non sait √©valuer le rapport d‚Äôacceptation \\(r(x) = \\frac{f(x)}{m\\cdot g(x)}\\)\n\n\n\n\nRemarque 1: \\(g(x)=0 \\implies f(x)=0\\), ainsi le support de \\(g\\) doit englober celui de \\(f\\)\n\n\nRemarque 2: \\(m \\geq 1\\) car \\(m = m \\displaystyle\\int_\\mathbb{R} g(x)\\, dx \\class{fragment}{{} \\geq \\displaystyle\\int_\\mathbb{R} f(x) dx} \\class{fragment}{{} = 1}\\)",
    "crumbs": [
      "Slides",
      "Simulation"
    ]
  },
  {
    "objectID": "Slides/slides_simulation.html#m√©thode-de-rejet-principe",
    "href": "Slides/slides_simulation.html#m√©thode-de-rejet-principe",
    "title": "Simulation",
    "section": "M√©thode de rejet: principe",
    "text": "M√©thode de rejet: principe\nConsid√©rer deux suites i.i.d. de v.a. ind√©pendantes entre elles:\n\n\\((Y_n)_{n \\geq 1}\\) de loi \\(g\\),\n\\((U_n)_{n \\geq 1}\\) de loi uniforme sur \\([0,1]\\).\n\nEn pratique, \\(Y_n\\) correspond √† une proposition et \\(U_n\\) permettra de d√©cider l‚Äôacceptation/rejet de la proposition:\n\n\nSi oui, alors on conserve \\(Y_n\\)\nSi non, on simule \\(Y_{n+1}\\)\n\nPour simuler \\(X\\) de densit√© \\(f\\), simuler \\(Y_n\\) (suivant \\(g\\)), \\(U_n\\) (suivant \\(\\mathcal{U}[0,1]\\)) et accepter si \\[\nU_n \\leq r(Y_n) = \\frac{f(Y_n)}{m\\cdot g(Y_n)}\n\\]",
    "crumbs": [
      "Slides",
      "Simulation"
    ]
  },
  {
    "objectID": "Slides/slides_simulation.html#exemple-simple",
    "href": "Slides/slides_simulation.html#exemple-simple",
    "title": "Simulation",
    "section": "Exemple simple",
    "text": "Exemple simple\n \n\n\n\\(f(x) = 4x^3 {1\\hspace{-3.8pt} 1}_{[0,1]}(x)\\)\n\\(f\\) est major√©e par \\(4\\) \\(\\implies\\) \\(g = {1\\hspace{-3.8pt} 1}_{[0,1]}\\) et \\(m=4\\) conviennent\n\\(r(x) =f(x) / (m\\cdot g(x)) = x^3\\), pour \\(x \\in [0,1]\\). On simule donc \\((Y_n, U_n)\\) et on teste si \\(4 \\cdot U_1 \\leq 4 Y_1^3\\), etc.\n\n\n\n\n\n\n\n\n\nNote\n\n\nDans la suite on verra qu‚Äôon tire des points \\((Y_n, 4U_n)\\) et qu‚Äôon teste si ils sont dans l‚Äôensemble \\(\\{(x,y) \\in \\mathbb{R}^2: y \\leq f(x) \\}\\)",
    "crumbs": [
      "Slides",
      "Simulation"
    ]
  },
  {
    "objectID": "Slides/slides_simulation.html#code-python",
    "href": "Slides/slides_simulation.html#code-python",
    "title": "Simulation",
    "section": "Code Python",
    "text": "Code Python\n\n\ndef accept_reject(n, f, g, g_sampler, m, rng):\n    \"\"\"\n    n: nombre de simulations\n    f: loi cible\n    g: loi des propositions, g_sampler: simulateur selon g\n    m: constante pour la majoration\n    rng: g√©n√©rateur pseudo-al√©atoire\n    \"\"\"\n    x_samples = np.zeros(n)\n    u_samples = np.zeros(n)\n    accepted = np.zeros(n)\n    for i in range(n):\n        x = g_sampler()\n        u = rng.uniform()\n        alpha = u * m * g(x)\n        u_samples [i] = alpha\n        x_samples[i] = x\n        if  alpha &lt;= f(x):\n            accepted[i] = 1\n    return x_samples, u_samples, accepted",
    "crumbs": [
      "Slides",
      "Simulation"
    ]
  },
  {
    "objectID": "Slides/slides_simulation.html#visualisation-de-lexemple",
    "href": "Slides/slides_simulation.html#visualisation-de-lexemple",
    "title": "Simulation",
    "section": "Visualisation de l‚Äôexemple",
    "text": "Visualisation de l‚Äôexemple",
    "crumbs": [
      "Slides",
      "Simulation"
    ]
  },
  {
    "objectID": "Slides/slides_simulation.html#variante-avec-une-loi-triangulaire",
    "href": "Slides/slides_simulation.html#variante-avec-une-loi-triangulaire",
    "title": "Simulation",
    "section": "Variante avec une loi triangulaire",
    "text": "Variante avec une loi triangulaire\n \nSupposons disposer d‚Äôun g√©n√©rateur de loi triangulaire sur \\([0,1]\\) (cf.¬†np.random.triangular(0, 1, 1))\n\n\n\\(f(x) = 4x^3 {1\\hspace{-3.8pt} 1}_{[0,1]}(x)\\)\n\\(f(x)\\) est major√©e par \\(4 x \\cdot {1\\hspace{-3.8pt} 1}_{[0,1]}(x)\\) \\(\\implies\\) \\(g = 2x \\cdot {1\\hspace{-3.8pt} 1}_{[0,1]}\\) et \\(m=2\\) conviennent\n\\(r(x)=f(x) / (m\\cdot g(x)) = x^3\\), pour \\(x \\in [0,1]\\).",
    "crumbs": [
      "Slides",
      "Simulation"
    ]
  },
  {
    "objectID": "Slides/slides_simulation.html#variante-continu√©e",
    "href": "Slides/slides_simulation.html#variante-continu√©e",
    "title": "Simulation",
    "section": "Variante (continu√©e)",
    "text": "Variante (continu√©e)",
    "crumbs": [
      "Slides",
      "Simulation"
    ]
  },
  {
    "objectID": "Slides/slides_simulation.html#comparaison-des-deux-majorants",
    "href": "Slides/slides_simulation.html#comparaison-des-deux-majorants",
    "title": "Simulation",
    "section": "Comparaison des deux majorants",
    "text": "Comparaison des deux majorants\n\n\n\n\n\n\n\n\n\n\n\nmd`Taux d'acceptation: avec la loi uniforme **${ratio1.toPrecision(5)}**`\n\n\n\n\n\n\n\nmd`Taux d'acceptation: avec la loi triangulaire **${ratio2.toPrecision(5)}**`\n\n\n\n\n\n\n\nConclusion: plus le majorant est proche de la loi cible, plus le taux d‚Äôacceptation est √©lev√©, et moins de simulations sont n√©cessaires\n\n\n\n\n\n\n\n\nNote\n\n\nL‚Äôexemple est pour l‚Äôillustration de la m√©thode, dans le cas pr√©sent m√©thode de l‚Äôinverse fonctionnerait aussi (on peut calculer la fonction quantile explicitement).",
    "crumbs": [
      "Slides",
      "Simulation"
    ]
  },
  {
    "objectID": "Slides/slides_simulation.html#m√©thode-de-rejet-validation-th√©orique",
    "href": "Slides/slides_simulation.html#m√©thode-de-rejet-validation-th√©orique",
    "title": "Simulation",
    "section": "M√©thode de rejet: validation th√©orique",
    "text": "M√©thode de rejet: validation th√©orique\nRappel:\n\nil existe \\(m &gt; 0\\) tel que \\(f(x) \\leq m \\cdot g(x)\\) et \\(r(x) = \\frac{f(x)}{m\\cdot g(x)}\\)\n\\((Y_n)_{n \\geq 1}\\) i.i.d. de loi \\(g\\)\n\\((U_n)_{n \\geq 1}\\) i.i.d. de loi uniforme sur \\([0,1]\\) (ind√©pendamment des \\(Y_n\\))\n\n\n\nTh√©or√®me 3 (M√©thode de rejet) \nSoit \\(T = \\inf \\{n \\geq 1 : U_n \\leq r(Y_n)\\}\\) le premier instant o√π le tirage est accept√©. Alors :\n\n\\(T \\sim \\mathcal{G}(\\frac{1}{m})\\) : loi g√©om√©trique de param√®tre \\(\\frac{1}{m}\\)\n\\(Y_T\\) a pour densit√© \\(f\\) et est ind√©pendante de \\(T\\)",
    "crumbs": [
      "Slides",
      "Simulation"
    ]
  },
  {
    "objectID": "Slides/slides_simulation.html#d√©monstration",
    "href": "Slides/slides_simulation.html#d√©monstration",
    "title": "Simulation",
    "section": "D√©monstration",
    "text": "D√©monstration\nPour \\(x \\in \\mathbb{R}\\) et \\(n \\in \\mathbb{N}^{*}\\), et \\(X = Y_T\\), on √©crit \\(\\mathbb{P}(X \\leq x, T=n)= \\mathbb{P}(U_1 &gt; r(Y_1), \\dots, U_{n-1} &gt; r(Y_{n-1}), U_n \\leq r(Y_n), Y_n \\leq x)\\)\n\n\\[\n    \\mathbb{P}(X \\leq x, T=n) = {\\color{blue}\\mathbb{P}(U_1 &gt; r(Y_1))}^{n-1} \\cdot {\\color{brown}\\mathbb{P}(U_n \\leq r(Y_n), Y_n \\leq x)}, \\quad \\textsf{( tirages i.i.d.)}\n\\]\n\n\nPremier terme: \\(Y_1\\) et \\(U_1\\) sont ind√©pendantes, leur loi jointe correspond au produit des densit√©s :\n\n\n\\[\n{\\color{blue}\n\\begin{align*}\n        \\mathbb{P}(U_1 &gt; r(Y_1))\n         & \\class{fragment}{{} = \\mathbb{P}((U_1, Y_1) \\in \\{(u,y) \\in \\mathbb{R}^2 : u &gt; r(y)\\})}                             \\\\\n         & \\class{fragment}{{} = \\int_{\\mathbb{R}^2} \\left( {1\\hspace{-3.8pt} 1}_{\\{u &gt; r(y)\\}} \\right) \\cdot \\left({1\\hspace{-3.8pt} 1}_{[0,1]}(u) g(y)\\right) \\,  du  dy}  \\\\\n         & \\class{fragment}{{} = \\int_\\mathbb{R} \\left( \\int_0^1 {1\\hspace{-3.8pt} 1}_{\\{u &gt; r(y)\\}} \\,  du\\right) g(y)\\,  d y}\n         \\class{fragment}{{} =  \\int_\\mathbb{R} (1-r(y)) \\, g(y)\\,  d y}\\\\\n         & \\class{fragment}{{} =  \\int_\\mathbb{R} g(y) -  \\int_\\mathbb{R}\\frac{f(y)}{m} d y, \\quad\\quad \\text{car }  r(y) = \\frac{f(y)}{m \\cdot g(y)}}\\\\\n     & \\class{fragment}{{} = 1-\\tfrac{1}{m}}\n\\end{align*}\n}\n\\]",
    "crumbs": [
      "Slides",
      "Simulation"
    ]
  },
  {
    "objectID": "Slides/slides_simulation.html#d√©monstration-suite",
    "href": "Slides/slides_simulation.html#d√©monstration-suite",
    "title": "Simulation",
    "section": "D√©monstration (suite)",
    "text": "D√©monstration (suite)\nSecond terme: \\[\n{\\color{brown}\n\\begin{align*}\n    \\mathbb{P}(U_n \\leq r(Y_n), Y_n \\leq x)\n    & \\class{fragment}{{ = \\int_{\\mathbb{R}^2} {1\\hspace{-3.8pt} 1}_{\\{u \\leq r(y)\\}} {1\\hspace{-3.8pt} 1}_{\\{y \\leq x\\}} ({1\\hspace{-3.8pt} 1}_{[0,1]}(u) g(y)) \\,  du  dy  }} \\\\\n    & \\class{fragment}{{} = \\int_\\mathbb{R} \\left( \\int_0^1  {1\\hspace{-3.8pt} 1}_{\\{u \\leq r(y)\\}} \\,  du\\right) {1\\hspace{-3.8pt} 1}_{\\{y \\leq x\\}}  g(y)\\,  d y} \\\\\n        & \\class{fragment}{{} = \\int_\\mathbb{R} r(y) {1\\hspace{-3.8pt} 1}_{\\{y \\leq x\\}}  g(y)\\,  d y } \\\\\n        & \\class{fragment}{{} = \\int_{-\\infty}^x \\dfrac{f(y)}{m}\\,  d y } \\\\\n        & \\class{fragment}{{} = \\dfrac{F(x)}{m} , \\quad F \\textsf{ fonction de r√©partition associ√©e √†} f}\n\\end{align*}\n}\n\\]",
    "crumbs": [
      "Slides",
      "Simulation"
    ]
  },
  {
    "objectID": "Slides/slides_simulation.html#d√©monstration-suite-1",
    "href": "Slides/slides_simulation.html#d√©monstration-suite-1",
    "title": "Simulation",
    "section": "D√©monstration (suite)",
    "text": "D√©monstration (suite)\n\\[\n    \\mathbb{P}(X \\leq x, T=n)\n    =\n    {\\color{blue}\\left(1 - \\tfrac{1}{m}\\right)^{n-1}} \\cdot {\\color{brown}\\tfrac{F(x)}{m}}\n\\]\n\nOn peut alors obtenir les lois marginales: \\[\n\\begin{align*}\n    \\mathbb{P}(T=n)\n    & = \\lim_{q \\to \\infty} \\mathbb{P}(X \\in ]-\\infty, q], T=n) = \\lim_{q \\to \\infty} \\left(1 - \\tfrac{1}{m}\\right)^{n-1} \\tfrac{F(q)}{m}\\\\\n    & = \\left(1 - \\tfrac{1}{m}\\right)^{n-1} \\tfrac{1}{m}\n\\end{align*}\n\\] Ainsi, \\(T\\) suit une loi g√©om√©trique de param√®tre \\(1/m\\), puis \\(X\\) a pour loi \\(F\\):\n\n\n\\[\n\\begin{align*}\n    \\mathbb{P}(X \\leq x)\n    & = \\mathbb{P}(X \\leq x, T \\in \\mathbb{N}^*)\n      = \\sum_{n=1}^\\infty \\mathbb{P}(X \\leq x, T=n) \\\\\n    & = \\sum_{n=1}^\\infty \\left(1 - \\tfrac{1}{m}\\right)^{n-1} \\tfrac{F(x)}{m}\n      = \\tfrac{1}{1-(1-1/m)} \\tfrac{F(x)}{m} = F(x) \\enspace.\n\\end{align*}\n\\]",
    "crumbs": [
      "Slides",
      "Simulation"
    ]
  },
  {
    "objectID": "Slides/slides_simulation.html#fin-de-la-d√©monstration",
    "href": "Slides/slides_simulation.html#fin-de-la-d√©monstration",
    "title": "Simulation",
    "section": "Fin de la d√©monstration",
    "text": "Fin de la d√©monstration\nOn obtient l‚Äôind√©pendance de \\(T\\) et \\(X\\) car on peut alors √©crire: \\[\n    \\forall x \\in \\mathbb{R}, \\forall n \\in \\mathbb{N}^*, \\quad\n    \\mathbb{P}(X \\leq x, T=n)\n    =\n    \\mathbb{P}(X \\leq x) \\cdot \\mathbb{P}(T=n)\n\\]",
    "crumbs": [
      "Slides",
      "Simulation"
    ]
  },
  {
    "objectID": "Slides/slides_simulation.html#cas-de-densit√©-connue-√†-une-constante-pr√®s-exemple",
    "href": "Slides/slides_simulation.html#cas-de-densit√©-connue-√†-une-constante-pr√®s-exemple",
    "title": "Simulation",
    "section": "Cas de densit√© connue √† une constante pr√®s : exemple",
    "text": "Cas de densit√© connue √† une constante pr√®s : exemple\nLoi de Andrews (densit√© proportionnelle √† \\(\\mathrm{sinc}\\), sinus cardinal): \\[\n\\forall x \\in \\mathbb{R},\\quad f(x) = \\frac{1}{S} \\frac{\\sin(\\pi\\cdot x)}{\\pi \\cdot x}  {1\\hspace{-3.8pt} 1}_{[-1,1]}(x)\n\\] avec \\(S = \\int_{-1}^{1}\\frac{\\sin(\\pi\\cdot x)}{\\pi \\cdot x}dx\\) non explicite. On note parfois: \\(f(x) \\propto \\frac{\\sin(\\pi\\cdot x)}{\\pi \\cdot x} {1\\hspace{-3.8pt} 1}_{[-1,1]}(x)\\)\n\n\nM√©thode du rejet: prendre \\(m=2/S\\) et \\(g(x) = \\frac{1}{2} {1\\hspace{-3.8pt} 1}_{[-1,1]}(x)\\):\n\\[\nu \\leq r(x) = \\frac{f(x)}{m \\cdot g(x)} \\iff u \\leq \\frac{1}{S} \\frac{\\sin(\\pi\\cdot x)}{\\pi \\cdot x}  \\cdot \\frac{1}{\\frac{2}{S} g(x)} = \\frac{\\sin(\\pi\\cdot x)}{\\pi \\cdot x}  \\cdot \\frac{1}{2 \\cdot g(x)}\n\\]\n\n\nAinsi l‚Äô√©valuation de \\(r(x)\\) est possible sans conna√Ætre \\(S\\)!",
    "crumbs": [
      "Slides",
      "Simulation"
    ]
  },
  {
    "objectID": "Slides/slides_simulation.html#loi-de-andrews-visualisation",
    "href": "Slides/slides_simulation.html#loi-de-andrews-visualisation",
    "title": "Simulation",
    "section": "Loi de Andrews: visualisation",
    "text": "Loi de Andrews: visualisation\n\nn = 300\nm = 2\nrng = np.random.default_rng(seed)\ng = lambda x: np.ones_like(x) / 2\ng_sampler = lambda: 2 * rng.uniform() - 1\nx_samples, u_samples, accepted = accept_reject(n, np.sinc, g, g_sampler, m, rng)\n\n\n\n\n\n\n\n\n\n\n\n\n\nmd`Le taux d'acceptation est ici de **${ratio_andrews.toPrecision(3)}**.`\n\n\n\n\n\n\n\n\n\n                                                \n\n\n\n\n\nmd`Pour la visualization, on approxime S=**${s_int.toPrecision(3)}** en utilisant une m√©thode de calcul num√©rique.`",
    "crumbs": [
      "Slides",
      "Simulation"
    ]
  },
  {
    "objectID": "Slides/slides_simulation.html#cas-de-densit√©-connue-√†-une-constante-pr√®s-cas-g√©n√©ral",
    "href": "Slides/slides_simulation.html#cas-de-densit√©-connue-√†-une-constante-pr√®s-cas-g√©n√©ral",
    "title": "Simulation",
    "section": "Cas de densit√© connue √† une constante pr√®s (cas g√©n√©ral)",
    "text": "Cas de densit√© connue √† une constante pr√®s (cas g√©n√©ral)\nSoit \\(\\tilde{f}: \\mathbb{R} \\to [0,+\\infty[\\) connue et \\(S \\triangleq \\int_{\\mathbb{R}} \\tilde{f}(x) \\, d x &lt; + \\infty\\) inconnue (ou dure √† √©valuer)\nDensit√© cible: \\(\\quad f(x) = \\frac{\\tilde{f}(x)}{S}\\)\n\nM√©thode du rejet pour \\(f\\), en utilisant seulement \\(\\tilde{f}\\): soit \\(\\tilde{m}&gt;0\\) un majorant de \\(\\tilde{f}\\) t.q. \\[\n\\begin{align*}\n\\tilde{f}(x) \\leq  \\tilde{m} \\cdot g(x)\n\\end{align*}\n\\]\nApplication avec \\(m=\\tilde{m}/S\\) (sans conna√Ætre \\(S\\)), le test d‚Äôacceptation donne:\n\n\\[\n\\begin{align*}\nU_n & \\leq \\frac{f(Y_n)}{m \\cdot g(Y_n)}\\\\\n  & \\class{fragment}{{}\\leq \\frac{\\frac{\\tilde{f}(Y_n)}{S}}{\\frac{\\tilde{m}}{S} \\cdot g(Y_n)}} \\class{fragment}{{}= \\frac{\\tilde{f}(Y_n)}{\\tilde{m} \\cdot g(Y_n)}}\n\\end{align*}\n\\]",
    "crumbs": [
      "Slides",
      "Simulation"
    ]
  },
  {
    "objectID": "Slides/slides_simulation.html#cas-multidimensionnel",
    "href": "Slides/slides_simulation.html#cas-multidimensionnel",
    "title": "Simulation",
    "section": "Cas multidimensionnel",
    "text": "Cas multidimensionnel\n\n\n\nimpossibilit√© de la m√©thode de l‚Äôinverse: fonction de r√©partition non disponible (en g√©n√©ral)\nla m√©thode de rejet : g√©n√©ralisable au cas multidimensionnel\n\n‚Äúfl√©au de la dimension‚Äù: plus la dimension est grande, plus la m√©thode est inefficace (penser au nombre de points n√©cessaires pour quadriller un hypercube‚Ä¶)\ndifficult√© d‚Äô√©crire une fonction de majoration en toute g√©n√©ralit√©",
    "crumbs": [
      "Slides",
      "Simulation"
    ]
  },
  {
    "objectID": "Slides/slides_simulation.html#exemple-loi-uniforme-sur-le-disque-unit√©-2d",
    "href": "Slides/slides_simulation.html#exemple-loi-uniforme-sur-le-disque-unit√©-2d",
    "title": "Simulation",
    "section": "Exemple: loi uniforme sur le disque unit√© (2D)",
    "text": "Exemple: loi uniforme sur le disque unit√© (2D)\n\nLoi cible: loi uniforme sur le disque unit√©, \\(f(x)\\propto {1\\hspace{-3.8pt} 1}_{x_1^2+x_2^2 \\leq 1}(x)\\) pour \\(x=(x_1,x_2)\\in\\mathbb{R}^2\\)\n\nLoi majorante: loi uniforme sur le carr√© \\([-1,1]^2\\), \\(g(x)\\triangleq \\tfrac{1}{2} {1\\hspace{-3.8pt} 1}_{[-1, 1]^2}(x)\\) et \\(m=2\\)\n\n\n\n\n\n\n\nNote\n\n\nLa loi uniforme sur le carr√© est une loi produit: il suffit de savoir g√©n√©rer une loi uniforme sur un segment 1D pour l‚Äôobtenir",
    "crumbs": [
      "Slides",
      "Simulation"
    ]
  },
  {
    "objectID": "Slides/slides_simulation.html#exemple-loi-uniforme-sur-le-disque-visualisation",
    "href": "Slides/slides_simulation.html#exemple-loi-uniforme-sur-le-disque-visualisation",
    "title": "Simulation",
    "section": "Exemple: loi uniforme sur le disque (visualisation)",
    "text": "Exemple: loi uniforme sur le disque (visualisation)\n\n\n\n\n\n                                                \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmd`Ratio estim√©e: **${ratio.toPrecision(5)}**`\nmd`Aire (bleue) estim√©: **${aire.toPrecision(5)}**`",
    "crumbs": [
      "Slides",
      "Simulation"
    ]
  },
  {
    "objectID": "Slides/slides_simulation.html#exemple-loi-uniforme-sur-une-cardio√Øde-2d",
    "href": "Slides/slides_simulation.html#exemple-loi-uniforme-sur-une-cardio√Øde-2d",
    "title": "Simulation",
    "section": "Exemple: loi uniforme sur une cardio√Øde (2D)",
    "text": "Exemple: loi uniforme sur une cardio√Øde (2D)\n\nLoi cible: loi uniforme sur le disque unit√©, \\(f(x)\\triangleq{1\\hspace{-3.8pt} 1}_{(x_1^2+x_2^2 - x_2)^2 \\leq x_1^2+ x_2^2}(x)\\) pour \\(x=(x_1,x_2)\\in\\mathbb{R}^2\\) \nLoi majorante: loi uniforme sur le rectangle \\([-2,3]\\times [-1.5,1.5]\\), \\(g(x)\\triangleq \\tfrac{1}{15}{1\\hspace{-3.8pt} 1}_{[-2,3]\\times [-1.5,1.5]}(x)\\) et \\(m=15\\)\n\n\n\n\n\n\n\nNote\n\n\nLa loi uniforme sur un rectangle est une loi produit: il suffit de savoir g√©n√©rer une loi uniforme sur un segment 1D pour l‚Äôobtenir",
    "crumbs": [
      "Slides",
      "Simulation"
    ]
  },
  {
    "objectID": "Slides/slides_simulation.html#exemple-loi-uniforme-sur-une-cardio√Øde-2d-1",
    "href": "Slides/slides_simulation.html#exemple-loi-uniforme-sur-une-cardio√Øde-2d-1",
    "title": "Simulation",
    "section": "Exemple: loi uniforme sur une cardio√Øde (2D)",
    "text": "Exemple: loi uniforme sur une cardio√Øde (2D)\n\n\n\n\n\n                                                \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmd`Ratio estim√©e: **${ratio_cardioid.toPrecision(5)}**`\nmd`Aire (bleue) estim√©: **${aire_cardioid.toPrecision(5)}**`",
    "crumbs": [
      "Slides",
      "Simulation"
    ]
  },
  {
    "objectID": "Slides/slides_simulation.html#th√©orie",
    "href": "Slides/slides_simulation.html#th√©orie",
    "title": "Simulation",
    "section": "Th√©orie",
    "text": "Th√©orie\n\nTh√©or√®me 4 (G√©neration uniforme sur un ensemble) Supposons \\(A\\subset B \\subset \\mathbb{R}^d\\), deux ensembles mesureables pour la mesure de Lebesgue. Pour g√©n√©rer selon une loi uniforme sur \\(A\\), connaissant un g√©n√©rateur uniforme sur \\(B\\), on peut utiliser la m√©thode du rejet, en tirant \\(Y_i \\sim \\mathcal{U}(B)\\) (i.i.d) et en ne gardant \\(Y_i\\) que si \\(Y_i \\in A\\).\n\n\nPreuve: On note \\(f\\triangleq \\frac{1}{|A|} {1\\hspace{-3.8pt} 1}_{A}\\), \\(g\\triangleq \\frac{1}{|B|}{1\\hspace{-3.8pt} 1}_{B}\\) et \\(m\\triangleq\\frac{|B|}{|A|}\\). Comme \\(A \\subset B\\), pour tout \\(x\\in \\mathbb{R}^{d}\\): \\[\n\\begin{align*}\n\\class{fragment}{{}f(x)} \\class{fragment}{{}= \\frac{1}{|A|} {1\\hspace{-3.8pt} 1}_{A}(x)} \\class{fragment}{{}\\leq \\frac{1}{|B|} {1\\hspace{-3.8pt} 1}_{B}(x) \\cdot \\frac{|B|}{|A|}} \\class{fragment}{{}\\leq g(x) \\cdot m}\n\\end{align*}\n\\]\n\n\\[\n\\begin{align*}\n&\\class{fragment}{{}Y \\sim \\mathcal{U}(B),} \\quad \\class{fragment}{{} U \\sim \\mathcal{U}([0,1])} \\\\\n&\\class{fragment}{{}r(Y)} \\class{fragment}{{}= \\frac{f(Y)}{m\\cdot g(Y)}}\n\\class{fragment}{{}=\n\\frac{ \\frac{1}{|A|} {1\\hspace{-3.8pt} 1}_{A}(Y)}\n{\\frac{|B|}{|A|}\\cdot \\frac{1}{|B|} {1\\hspace{-3.8pt} 1}_{B}(Y)}}\n\\class{fragment}{{}=\n\\frac{  {1\\hspace{-3.8pt} 1}_{A}(Y)}\n{ {1\\hspace{-3.8pt} 1}_{B}(Y)}}\n\\class{fragment}{{}= {1\\hspace{-3.8pt} 1}_{A}(Y)}\n\\end{align*}\n\\]\n\\[\n\\begin{align*}\n\\class{fragment}{{}\\text{Enfin}, U \\leq {1\\hspace{-3.8pt} 1}_{A}(Y) \\iff {1\\hspace{-3.8pt} 1}_{A}(Y)=1}\\class{fragment}{{}\\iff Y \\in A}\n\\end{align*}\n\\]",
    "crumbs": [
      "Slides",
      "Simulation"
    ]
  },
  {
    "objectID": "Slides/slides_simulation.html#remarque-sur-les-constantes",
    "href": "Slides/slides_simulation.html#remarque-sur-les-constantes",
    "title": "Simulation",
    "section": "Remarque sur les constantes",
    "text": "Remarque sur les constantes\n\nDans l‚Äôexemple pr√©c√©dent, on a pu appliquer la m√©thode de rejet sans la connaissance de \\(m\\)\n\nPoint important: parfois la connaissance de \\(m\\) est difficile √† obtenir, et il est pr√©f√©rable de ne pas l‚Äôutiliser (notamment quand les constantes de normalisation des densit√©s sont difficiles √† calculer).\n Exemples: statistiques bayesiennes, mod√®les graphiques, etc.",
    "crumbs": [
      "Slides",
      "Simulation"
    ]
  },
  {
    "objectID": "Slides/slides_simulation.html#sommes-de-variables-al√©atoires",
    "href": "Slides/slides_simulation.html#sommes-de-variables-al√©atoires",
    "title": "Simulation",
    "section": "Sommes de variables al√©atoires",
    "text": "Sommes de variables al√©atoires\nLoi de Bernouilli: avec \\(U_1, \\ldots, U_n\\) i.i.d uniformes sur \\([0,1]\\) (m√©thode d‚Äôinversion): \\[\nX_i \\triangleq {1\\hspace{-3.8pt} 1}_{\\{U_i \\leq p\\}} \\sim \\mathcal{B}(p)\n\\] Loi binomiale: \\[\n    \\sum_{i=1}^n {1\\hspace{-3.8pt} 1}_{\\{U_i \\leq p\\}} \\sim \\mathcal{B}(n,p)\n\\] en rappelant que \\[\n    X = X_1 + \\cdots + X_n \\sim \\mathcal{B}(n,p)\n\\]\n\n\n\n\n\n\nNote\n\n\nLa m√©thode d‚Äôinversion marche, mais n√©cessite le calcul de l‚Äôinverse g√©n√©ralis√©e de \\(F\\), donc de coefficients binomiaux‚Ä¶",
    "crumbs": [
      "Slides",
      "Simulation"
    ]
  },
  {
    "objectID": "Slides/slides_simulation.html#loi-de-poisson",
    "href": "Slides/slides_simulation.html#loi-de-poisson",
    "title": "Simulation",
    "section": "Loi de Poisson",
    "text": "Loi de Poisson\nRappel: \\(\\quad X \\sim \\mathcal{P}(\\lambda) \\iff \\mathbb{P}(X = k) = e^{-\\lambda} \\dfrac{\\lambda^k}{k!}\\,, \\quad \\forall k \\in \\mathbb{N}^*.\\)\n\nProposition 2 (G√©n√©ration de v.a. de loi de Poisson) \nSoit \\((E_n)_{n \\geq 1}\\) des variables al√©atoires i.i.d. de loi exponentielle de param√®tre \\(\\lambda &gt; 0\\). On pose \\(S_k = E_1 + \\cdots + E_k\\). Alors pour tout \\(n \\in \\mathbb{N}^*\\) \\[\n    \\mathbb{P}(S_n \\leq 1 &lt; S_{n+1}) =  e^{-\\lambda} \\dfrac{\\lambda^n}{n!}\\enspace .\n\\] Ainsi, la variable al√©atoire \\(T\\) d√©finie par \\(T \\triangleq \\sup \\{n \\in \\mathbb{N}^* : S_n \\leq 1\\}\\) suit une loi de Poisson de param√®tre \\(\\lambda\\) : \\(T \\sim \\mathcal{P}(\\lambda)\\).\n\n\n\n\n\n\n\n\nPoint num√©rique\n\n\nC‚Äôest la m√©thode utilis√©e par numpy.random.poisson, cf.¬†code source (GitHub) (en passant √† l‚Äôexponentiel), qui fut propos√©e par D. Knuth; Source: Wikipedia",
    "crumbs": [
      "Slides",
      "Simulation"
    ]
  },
  {
    "objectID": "Slides/slides_simulation.html#loi-de-poisson-suite",
    "href": "Slides/slides_simulation.html#loi-de-poisson-suite",
    "title": "Simulation",
    "section": "Loi de Poisson (suite)",
    "text": "Loi de Poisson (suite)\nLa preuve repose sur le r√©sultat suivant:\n\nLemme 1 (Loi de Erlang) \nSoit \\(n\\) variables al√©atoires \\(E_1, \\dots, E_n\\) i.i.d. de loi exponentielle de param√®tre \\(\\lambda &gt;0\\). La somme \\(E_1+\\dots+E_n\\) suit une loi d‚ÄôErlang de param√®tres \\((n,\\lambda)\\), donn√©e par la fonction de r√©partition \\[\n    F_{n,\\lambda}(t) = 1 - \\sum_{k=0}^{n-1} e^{-\\lambda t} \\frac{(\\lambda t)^k}{k!}\\,.\n\\]",
    "crumbs": [
      "Slides",
      "Simulation"
    ]
  },
  {
    "objectID": "Slides/slides_simulation.html#d√©monstration-avec-le-lemme",
    "href": "Slides/slides_simulation.html#d√©monstration-avec-le-lemme",
    "title": "Simulation",
    "section": "D√©monstration avec le lemme",
    "text": "D√©monstration avec le lemme\nPour \\(n \\in \\mathbb{N}^*\\), on d√©compose la probabilit√© \\(\\mathbb{P}(S_n \\leq 1 &lt; S_{n+1})\\) via \\[\n\\begin{align*}\n    \\mathbb{P}(S_n \\leq 1 &lt; S_{n+1})\n    & = \\mathbb{P}(\\{S_n \\leq 1\\} \\setminus \\{S_{n+1} \\leq 1\\})\\\\\n    & = \\mathbb{P}(S_n \\leq 1) - \\mathbb{P}(S_{n+1} \\leq 1)\\,.\n\\end{align*}\n\\] Le lemme pr√©c√©dent donne \\(\\mathbb{P}(S_n \\leq 1) = 1 - \\sum_{n=0}^{n-1} e^{-\\lambda} \\dfrac{\\lambda^k}{k!}\\) et \\(\\mathbb{P}(S_{n+1} \\leq 1) = 1 - \\sum_{k=0}^{n} e^{-\\lambda} \\dfrac{\\lambda^k}{k!}.\\) On obtient alors le r√©sultat souhait√© : \\[\n    \\mathbb{P}(S_n \\leq 1 &lt; S_{n+1})\n    = e^{-\\lambda} \\dfrac{\\lambda^n}{n!}\\,.\n\\]\nOn conclut la preuve de la proposition en remarquant que \\[\n    \\mathbb{P}(T=n) = \\mathbb{P}(S_n \\leq 1 &lt; S_{n+1})\\,.\n\\]\n\n\nSimulation",
    "crumbs": [
      "Slides",
      "Simulation"
    ]
  },
  {
    "objectID": "Slides/slides-index.html#test",
    "href": "Slides/slides-index.html#test",
    "title": "Visualisation: inversion",
    "section": "test",
    "text": "test\n\nviewof dist = Inputs.select(['normal','cauchy','laplace','bimodal'], {value: \"bimodal\", label: \"Loi\"})\nviewof replay = html`&lt;button&gt;Relancer`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ninvcdfboard = require(await FileAttachment(\"../inverse-vizu/dist/invcdfboard.umd.cjs\").url())\n\n{\n  replay\n  const canvas = DOM.canvas(500, 500);\n  const galton = invcdfboard.galton(canvas,dist);\n  \n  return html`${galton.canvas}`\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVisualisation: inversion"
  },
  {
    "objectID": "Courses/th_asymptotique.html",
    "href": "Courses/th_asymptotique.html",
    "title": "Th√©or√®mes asymptotiques",
    "section": "",
    "text": "Le premier r√©sultat fondamental en probabilit√©s concerne le comportement asymptotique de la moyenne empirique: \n\\bar X_n = \\dfrac{X_1 + \\cdots + X_n}{n} \\enspace.\n quand on observe n variables al√©atoires i.i.d X_1,\\dots,X_n, ayant une esp√©rance finie.\n\nTh√©or√®me 1 (Loi forte des grands nombres) \nSoit (X_n)_{n \\geq 1} une suite de variables al√©atoires ind√©pendantes et identiquement distribu√©es (i.i.d.) dans L^1(\\Omega, \\mathcal{F}, \\mathbb{P}). Notons \\mu = \\mathbb{E}[X_1]. Alors \\bar X_n converge vers \\mu presque s√ªrement : \n\\mathbb{P}\\bigg( \\dfrac{X_1 + \\cdots + X_n}{n} \\underset{n \\to \\infty}{\\longrightarrow} \\mu \\bigg) = 1\\,.\n\n\nInterpr√©tation: Intuitivement, la probabilit√© d‚Äôun √©v√©nement A correspond √† la fr√©quence d‚Äôapparition de A quand on r√©p√®te une exp√©rience qui fait intervenir cet √©v√©nement. Par exemple, si on dispose une pi√®ce truqu√©e, on estimera la probabilit√© d‚Äôapparition du c√¥t√© pile en lan√ßant la pi√®ce un grand nombre de fois et en comptant le nombre de pile obtenu. La loi des grands nombres justifie a posteriori cette intuition : si X_1, \\ldots, X_n sont i.i.d. de loi de Bernoulli de param√®tre p, alors \n    \\dfrac{X_1 + \\cdots + X_n}{n} \\xrightarrow[n \\to \\infty]{p.s.} p \\enspace.\n Le membre de gauche correspond au nombre empirique de pile obtenu, celui de droite √† la valeur th√©orique.\nRemarque: Bien qu‚Äôassez intuitif, ce th√©or√®me est difficile √† d√©montrer, cf.[@Ouvrard08;@Barbe_Ledoux06] ou encore [@Williams91] pour une version de preuve avec des martingales.\n#| standalone: true\n#| viewerHeight: 550\nimport numpy as np\nfrom shiny import ui, render, App, reactive\nfrom shinywidgets import output_widget, register_widget\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objs as go\n\n\nn_init = 42\n\n\napp_ui = ui.page_fluid(\n    ui.tags.head(\n        ui.tags.style(\"\"\"\n            .bslib-sidebar-layout &gt; .sidebar &gt; .sidebar-content {\n            display: flex;\n            flex-direction: column;\n            padding: 0.5rem;\n            }\n            .irs--shiny .irs-line {\n            top: 27px;\n            height: 4px;\n            }\n            .irs.irs--shiny .irs-bar {\n            top: 27px;\n            height: 4px;\n            }\n            .irs--shiny .irs-handle {\n            top: 23px;\n            width: 22px;\n            height: 10px;\n        \"\"\")\n    ),\n    ui.panel_title(\"Loi des grands nombres\"),\n    ui.layout_sidebar(\n        ui.panel_sidebar(\n            ui.input_action_button(\"seed\", \"Nouveau tirage\", class_=\"btn-primary\"),\n            ui.input_slider( \"p\", \"Esp√©rance: p\", 0.01, 0.99, value=0.35, step=0.01, ticks=False,\n            ),\n            ui.input_slider( \"n_samples\", \"√âchantillons: n\", 2, 1000, value=15, step=1, ticks=False),\n        width=3),\n    ui.panel_main(output_widget(\"my_widget\"), width = 9)\n    )\n)\n\n\n\n\ndef server(input, output, session):\n    seed = reactive.Value(42)\n\n    @reactive.Effect\n    @reactive.event(input.seed)\n    def _():\n        seed.set(np.random.randint(0, 1000))\n\n    subplots = make_subplots(\n        rows=2,\n        cols=1,\n        vertical_spacing=0.45,\n        horizontal_spacing=0.04,\n        row_heights=[5, 1],\n        subplot_titles=(\n            f\"Moyenne empirique: loi de Bernoulli\",\n            \"Tirages al√©atoires &lt;span style='color:rgb(66, 139, 202)'&gt;bleu: 0&lt;/span&gt;, &lt;span style='color:rgb(255, 0, 0)'&gt;rouge: 1&lt;/span&gt; (seed=\"\n            + f\"{n_init:03}\"\n            + \")\",\n        ),\n    )\n    fig = go.FigureWidget(subplots) \n    fig.add_trace(\n        go.Scatter(\n            mode=\"lines\",\n            line=dict(color=\"black\", width=3),\n            x=[],\n            y=[],\n            name=r\"Moyenne &lt;br&gt; empirique\",\n        ),\n        row=1,\n        col=1,\n    )\n    fig.add_trace(\n        go.Scatter(\n            mode=\"lines\",\n            line=dict(dash=\"dash\", color=\"black\", width=1),\n            marker={},\n            x=[],\n            y=[],\n            name=r\"p\",\n        ),\n            row=1,\n            col=1,\n    )\n    fig.add_trace(\n        go.Heatmap(\n            x=[],\n            z=[],\n            colorscale=[[0, \"rgb(66, 139, 202)\"], [1, \"rgb(255,0,0)\"]],\n            showscale=False,\n        ),\n        row=2,\n        col=1,\n    )\n\n    fig.update_yaxes(range=[0, 1.1], row=1, col=1)\n    fig.update_xaxes(matches=\"x1\", row=2, col=1)\n    fig.update_yaxes(visible=False, row=2, col=1)\n    fig.update_xaxes(visible=False, row=2, col=1)\n\n    fig.update_layout(\n        template=\"simple_white\",\n        showlegend=True,\n        xaxis_title=\"√âchantillons: n\",\n    )\n    fig.update_layout(autosize=True)\n\n    fig.update_layout(\n        legend=dict(\n            yanchor=\"top\",\n            y=0.99,\n            xanchor=\"left\",\n            x=0.65,\n            bgcolor=\"rgba(0,0,0,0)\",\n        )\n    )\n    fig.update_layout(\n        margin=dict(l=0, r=0, b=10, t=70),\n    )\n\n    register_widget(\"my_widget\", fig)\n\n    @reactive.Effect\n    def _():\n        p = input.p()\n        n_samples = input.n_samples()\n\n        rng = np.random.default_rng(seed())\n        iterations = np.arange(1, n_samples + 1)\n        samples = rng.binomial(1, p, size=n_samples)\n        means_samples = np.cumsum(samples) / np.arange(1, n_samples + 1)\n\n        # Update data in fig:\n        fig.data[0].x = iterations\n        fig.data[0].y = means_samples\n\n        fig.data[1].x = iterations\n        fig.data[1].y = np.full((n_samples), p)\n\n        fig.data[2].x = iterations\n        fig.data[2].z = [samples]\n\n        fig.update_xaxes(range=[1, n_samples + 1])\n\n        # Update the subplot titles:\n        fig.layout.annotations[1].update(\n            text=f\"Tirages al√©atoires (seed=\"\n            + f\"{seed():03}\"\n            + \") &lt;br&gt; &lt;span style='color:rgb(66, 139, 202)'&gt;bleu: 0&lt;/span&gt;, &lt;span style='color:rgb(255, 0, 0)'&gt;rouge: 1&lt;/span&gt; \"\n        )\n\n\napp = App(app_ui, server)\n\nPour aller plus loin:\nQuant p varie, √† n fix√©‚Ä¶les signaux g√©n√©r√©s sont tr√®s tr√®s proches, ce qui ne devrait pas √™tre le cas sans structuration particuli√®re de la g√©n√©ration. L‚Äôal√©a est imparfait!",
    "crumbs": [
      "Cours",
      "Th√©or√®mes asymptotiques"
    ]
  },
  {
    "objectID": "Courses/th_asymptotique.html#loi-des-grands-nombres",
    "href": "Courses/th_asymptotique.html#loi-des-grands-nombres",
    "title": "Th√©or√®mes asymptotiques",
    "section": "",
    "text": "Le premier r√©sultat fondamental en probabilit√©s concerne le comportement asymptotique de la moyenne empirique: \n\\bar X_n = \\dfrac{X_1 + \\cdots + X_n}{n} \\enspace.\n quand on observe n variables al√©atoires i.i.d X_1,\\dots,X_n, ayant une esp√©rance finie.\n\nTh√©or√®me 1 (Loi forte des grands nombres) \nSoit (X_n)_{n \\geq 1} une suite de variables al√©atoires ind√©pendantes et identiquement distribu√©es (i.i.d.) dans L^1(\\Omega, \\mathcal{F}, \\mathbb{P}). Notons \\mu = \\mathbb{E}[X_1]. Alors \\bar X_n converge vers \\mu presque s√ªrement : \n\\mathbb{P}\\bigg( \\dfrac{X_1 + \\cdots + X_n}{n} \\underset{n \\to \\infty}{\\longrightarrow} \\mu \\bigg) = 1\\,.\n\n\nInterpr√©tation: Intuitivement, la probabilit√© d‚Äôun √©v√©nement A correspond √† la fr√©quence d‚Äôapparition de A quand on r√©p√®te une exp√©rience qui fait intervenir cet √©v√©nement. Par exemple, si on dispose une pi√®ce truqu√©e, on estimera la probabilit√© d‚Äôapparition du c√¥t√© pile en lan√ßant la pi√®ce un grand nombre de fois et en comptant le nombre de pile obtenu. La loi des grands nombres justifie a posteriori cette intuition : si X_1, \\ldots, X_n sont i.i.d. de loi de Bernoulli de param√®tre p, alors \n    \\dfrac{X_1 + \\cdots + X_n}{n} \\xrightarrow[n \\to \\infty]{p.s.} p \\enspace.\n Le membre de gauche correspond au nombre empirique de pile obtenu, celui de droite √† la valeur th√©orique.\nRemarque: Bien qu‚Äôassez intuitif, ce th√©or√®me est difficile √† d√©montrer, cf.[@Ouvrard08;@Barbe_Ledoux06] ou encore [@Williams91] pour une version de preuve avec des martingales.\n#| standalone: true\n#| viewerHeight: 550\nimport numpy as np\nfrom shiny import ui, render, App, reactive\nfrom shinywidgets import output_widget, register_widget\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objs as go\n\n\nn_init = 42\n\n\napp_ui = ui.page_fluid(\n    ui.tags.head(\n        ui.tags.style(\"\"\"\n            .bslib-sidebar-layout &gt; .sidebar &gt; .sidebar-content {\n            display: flex;\n            flex-direction: column;\n            padding: 0.5rem;\n            }\n            .irs--shiny .irs-line {\n            top: 27px;\n            height: 4px;\n            }\n            .irs.irs--shiny .irs-bar {\n            top: 27px;\n            height: 4px;\n            }\n            .irs--shiny .irs-handle {\n            top: 23px;\n            width: 22px;\n            height: 10px;\n        \"\"\")\n    ),\n    ui.panel_title(\"Loi des grands nombres\"),\n    ui.layout_sidebar(\n        ui.panel_sidebar(\n            ui.input_action_button(\"seed\", \"Nouveau tirage\", class_=\"btn-primary\"),\n            ui.input_slider( \"p\", \"Esp√©rance: p\", 0.01, 0.99, value=0.35, step=0.01, ticks=False,\n            ),\n            ui.input_slider( \"n_samples\", \"√âchantillons: n\", 2, 1000, value=15, step=1, ticks=False),\n        width=3),\n    ui.panel_main(output_widget(\"my_widget\"), width = 9)\n    )\n)\n\n\n\n\ndef server(input, output, session):\n    seed = reactive.Value(42)\n\n    @reactive.Effect\n    @reactive.event(input.seed)\n    def _():\n        seed.set(np.random.randint(0, 1000))\n\n    subplots = make_subplots(\n        rows=2,\n        cols=1,\n        vertical_spacing=0.45,\n        horizontal_spacing=0.04,\n        row_heights=[5, 1],\n        subplot_titles=(\n            f\"Moyenne empirique: loi de Bernoulli\",\n            \"Tirages al√©atoires &lt;span style='color:rgb(66, 139, 202)'&gt;bleu: 0&lt;/span&gt;, &lt;span style='color:rgb(255, 0, 0)'&gt;rouge: 1&lt;/span&gt; (seed=\"\n            + f\"{n_init:03}\"\n            + \")\",\n        ),\n    )\n    fig = go.FigureWidget(subplots) \n    fig.add_trace(\n        go.Scatter(\n            mode=\"lines\",\n            line=dict(color=\"black\", width=3),\n            x=[],\n            y=[],\n            name=r\"Moyenne &lt;br&gt; empirique\",\n        ),\n        row=1,\n        col=1,\n    )\n    fig.add_trace(\n        go.Scatter(\n            mode=\"lines\",\n            line=dict(dash=\"dash\", color=\"black\", width=1),\n            marker={},\n            x=[],\n            y=[],\n            name=r\"p\",\n        ),\n            row=1,\n            col=1,\n    )\n    fig.add_trace(\n        go.Heatmap(\n            x=[],\n            z=[],\n            colorscale=[[0, \"rgb(66, 139, 202)\"], [1, \"rgb(255,0,0)\"]],\n            showscale=False,\n        ),\n        row=2,\n        col=1,\n    )\n\n    fig.update_yaxes(range=[0, 1.1], row=1, col=1)\n    fig.update_xaxes(matches=\"x1\", row=2, col=1)\n    fig.update_yaxes(visible=False, row=2, col=1)\n    fig.update_xaxes(visible=False, row=2, col=1)\n\n    fig.update_layout(\n        template=\"simple_white\",\n        showlegend=True,\n        xaxis_title=\"√âchantillons: n\",\n    )\n    fig.update_layout(autosize=True)\n\n    fig.update_layout(\n        legend=dict(\n            yanchor=\"top\",\n            y=0.99,\n            xanchor=\"left\",\n            x=0.65,\n            bgcolor=\"rgba(0,0,0,0)\",\n        )\n    )\n    fig.update_layout(\n        margin=dict(l=0, r=0, b=10, t=70),\n    )\n\n    register_widget(\"my_widget\", fig)\n\n    @reactive.Effect\n    def _():\n        p = input.p()\n        n_samples = input.n_samples()\n\n        rng = np.random.default_rng(seed())\n        iterations = np.arange(1, n_samples + 1)\n        samples = rng.binomial(1, p, size=n_samples)\n        means_samples = np.cumsum(samples) / np.arange(1, n_samples + 1)\n\n        # Update data in fig:\n        fig.data[0].x = iterations\n        fig.data[0].y = means_samples\n\n        fig.data[1].x = iterations\n        fig.data[1].y = np.full((n_samples), p)\n\n        fig.data[2].x = iterations\n        fig.data[2].z = [samples]\n\n        fig.update_xaxes(range=[1, n_samples + 1])\n\n        # Update the subplot titles:\n        fig.layout.annotations[1].update(\n            text=f\"Tirages al√©atoires (seed=\"\n            + f\"{seed():03}\"\n            + \") &lt;br&gt; &lt;span style='color:rgb(66, 139, 202)'&gt;bleu: 0&lt;/span&gt;, &lt;span style='color:rgb(255, 0, 0)'&gt;rouge: 1&lt;/span&gt; \"\n        )\n\n\napp = App(app_ui, server)\n\nPour aller plus loin:\nQuant p varie, √† n fix√©‚Ä¶les signaux g√©n√©r√©s sont tr√®s tr√®s proches, ce qui ne devrait pas √™tre le cas sans structuration particuli√®re de la g√©n√©ration. L‚Äôal√©a est imparfait!",
    "crumbs": [
      "Cours",
      "Th√©or√®mes asymptotiques"
    ]
  },
  {
    "objectID": "Courses/th_asymptotique.html#th√©or√®me-central-limite-tcl",
    "href": "Courses/th_asymptotique.html#th√©or√®me-central-limite-tcl",
    "title": "Th√©or√®mes asymptotiques",
    "section": "Th√©or√®me central limite (TCL)",
    "text": "Th√©or√®me central limite (TCL)\nUne fois la loi des grands nombres √©tablie, on peut se demander quel est l‚Äôordre suivant dans le d√©veloppement asymptotique de \\bar X_n - \\mu, ou de mani√®re √©quivalente de S_n - n \\mu, o√π S_n = X_1 + \\cdots + X_n. Le th√©or√®me suivant r√©pond √† cette question, en donnant une convergence en loi d‚Äôune transformation affine de la moyenne empirique:\n\nTh√©or√®me 2 (Th√©or√®me central limite) Soit X_1, \\ldots, X_n une suite de variables al√©atoires i.i.d de variance \\sigma^2 = {\\rm var}(X_1) \\in ]0, \\infty[. On note \\mu = \\mathbb{E}[X_1] leur esp√©rance. Alors \n\\sqrt n \\left(\\tfrac{\\bar X_n - \\mu}{\\sigma} \\right) \\xrightarrow[n \\to +\\infty]{\\mathcal{L}} N\\enspace,\n o√π N suit une loi normale centr√©e r√©duite : N \\sim\\mathcal{N}(0,1).\n\nPreuve: cf.[@Ouvrard08;@Barbe_Ledoux06].\nOn peut interpr√©ter ce th√©or√®me grossi√®rement de la fa√ßon suivante: la moyenne empirique de variables al√©atoires i.i.d de variance \\sigma^2 se comporte asymptotiquement comme une loi normale \\mathcal{N}(\\mu, \\tfrac{\\sigma^2}{n}), ce que l‚Äôon √©crit avec un abus de notation:\n\n\\bar X_n \\approx \\mathcal{N}(\\mu, \\frac{\\sigma^2}{n}) \\enspace.\n\nEn termes de somme cumul√©e empirique, la convergence se r√©√©crit\n\n    \\tfrac{S_n - n \\mu}{\\sqrt n \\sigma} \\xrightarrow[n \\to +\\infty]{\\mathcal{L}} N \\enspace.\n\nLes hypoth√®ses de ce th√©or√®me sont plut√¥t faibles (il suffit de supposer une variance finie). Pourtant, le r√©sultat est universel : la loi de d√©part peut √™tre aussi farfelue que l‚Äôon veut, elle se rapprochera toujours asymptotiquement d‚Äôune loi normale.\nOn rappelle que la convergence en loi est √©quivalente √† la convergence des fonctions de r√©partition en tout point de continuit√© de la limite. Ainsi, le th√©or√®me central limite se r√©√©crit de la mani√®re suivante : pour tout a &lt; b, notons \\alpha_n=\\mathbb{P} \\left(\\bar X_n \\notin [ \\mu + \\tfrac{a \\sigma}{\\sqrt{n}}, \\mu + \\tfrac{ b \\sigma}{\\sqrt{n}}] \\right). Ainsi\n\n\\mathbb{P} \\left(\\tfrac{\\bar X_n - \\mu}{\\sigma}\\in [ \\tfrac{a}{\\sqrt{n}},\\tfrac{b}{\\sqrt{n}}] \\right)\\\\\n\\begin{align}\n    1-\\alpha_n& = \\mathbb{P} \\left(\\bar X_n \\in [ \\mu + \\tfrac{a \\sigma}{\\sqrt{n}}, \\mu + \\tfrac{ b \\sigma}{\\sqrt{n}}] \\right)\\nonumber\\\\\n    & =\n    \\mathbb{P} \\left(\\tfrac{\\bar X_n - \\mu}{\\sigma} \\in [ \\tfrac{a}{\\sqrt{n}},\\tfrac{b}{\\sqrt{n}}]\\right) \\nonumber\\\\\n    & =\n    \\mathbb{P} \\bigg( a \\leq \\sqrt n \\left(\\tfrac{\\bar X_n - \\mu}{\\sigma} \\right) \\leq b\\bigg) \\nonumber\\\\\n    & \\underset{n \\to \\infty}{\\longrightarrow}  \\int_a^b \\varphi(x) \\,  dx\\,. \\nonumber\\\\\n\\end{align}\n o√π l‚Äôon note \\varphi (resp. \\Phi) la densit√© (resp. la fonction de r√©partition) d‚Äôune loi normale centr√©e r√©duite, d√©finie pour tout x\\in\\mathbb{R} par \\varphi(x)=\\tfrac{e^{-\\frac{x^2}{2}}}{\\sqrt{2\\pi}} (resp. \\Phi(x)= \\int_{-\\infty}^{x}\\varphi(u) du).\nDans le cas classique d‚Äôun intervalle de confiance √† 95%, c‚Äôest-√†-dire quand \\alpha_n=0.05, et en prenant un intervalle de confiance sym√©trique (alors a=-t et b=q) on obtient 1-\\alpha_n= \\int_{-q}^q \\varphi(x) \\, dx=\\Phi(q)-\\Phi(-q)=2 \\Phi(q)-1 \\implies \\boxed{q=\\Phi^{-1}(1-\\tfrac{\\alpha_n}{2})} et q est donc le quantile de niveau 1-\\tfrac{\\alpha_n}{2} de la loi normale centr√©e r√©duite. Num√©riquement on peut facilement √©valuer q et v√©rifier que q\\approx 1.96 avec scipy:\n\nfrom scipy.stats import norm\nq = norm.ppf(1-0.05/2)\nprint(f\"Gaussienne centr√©e r√©duite,\\nQuantile de niveau (1-Œ±/2):\\nq = {q:.2f}\")\n\nGaussienne centr√©e r√©duite,\nQuantile de niveau (1-Œ±/2):\nq = 1.96\n\n\n\nExemple 1 (Loi de Bernoulli) On consid√®re des variables al√©atoires X_1, \\ldots, X_n i.i.d. suivant une loi de Bernoulli de param√®tre p \\in ]0,1[, dont l‚Äôesp√©rance et la variance sont respectivemenbt p et p(1-p). Le th√©or√®me central limite donne alors \n    \\sqrt n \\left(\\frac{\\bar X_n - p}{p (1-p)} \\right) \\xrightarrow[n \\to +\\infty]{\\mathcal{L}} N\\,,\n avec N \\sim \\mathcal{N}(0,1). Cette convergence est illustr√©e dans le widget ci-dessous. Le contexte est le suivant. On r√©p√®te t fois le processus, qui consiste √† afficher (\\bar{X}_k)_{k \\in [n]}, o√π les n variables al√©atoires sont i.i.d. et suivent une loi de Bernoulli de param√®tre p.\n\n#| standalone: true\n#| viewerHeight: 600\nimport numpy as np\nfrom scipy.stats import norm\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nfrom shiny import ui, render, App, reactive\nfrom shinywidgets import output_widget, render_widget\n\n\napp_ui = ui.page_fluid(\n    ui.tags.head(\n        ui.tags.style(\"\"\"\n            .bslib-sidebar-layout &gt; .sidebar &gt; .sidebar-content {\n            display: flex;\n            flex-direction: column;\n            padding: 0.1rem;\n            }\n            .irs--shiny .irs-line {\n            top: 27px;\n            height: 4px;\n            }\n            .irs.irs.irs--shiny .irs-bar {\n            top: 27px;\n            height: 4px;\n            }\n            .irs--shiny .irs-handle {\n            top: 23px;\n            width: 22px;\n            height: 10px;\n        \"\"\")\n    ),\n    ui.panel_title(\"TCL\"),\n    ui.layout_sidebar(\n        ui.panel_sidebar(\n            ui.input_action_button(\"seed\", \"Nouveau tirage\",class_=\"btn-primary\"),\n            ui.input_slider(\"p\", \"Esp√©rance: p\", 0.01, 0.99, value=0.5, step=0.01, ticks=False),\n            ui.input_slider(\"n_samples\", \"√âchantillons: n\", 1, 200, value=100, step=1, ticks=False),\n            ui.input_slider(\"n_repetitions\", \"R√©p√©titions: t\", 1, 300, value=200, step=1, ticks=False),\n        width=3\n        ),\n    ui.panel_main(output_widget(\"my_widget\"), width = 9)\n    )\n)\n\n\n\ndef server(input, output, session):\n    seed = reactive.Value(42)\n\n\n    @reactive.Effect\n    @reactive.event(input.seed)\n    def _():\n        seed.set(np.random.randint(0, 1000))\n\n\n    @output\n    @render_widget\n    def my_widget():\n\n        rng = np.random.default_rng(seed())\n        p = input.p()\n        n_repetitions = input.n_repetitions()\n        n_samples = input.n_samples()\n        iterations = np.arange(1, n_samples + 1)\n        samples = rng.binomial(1, p, size=(n_repetitions, n_samples))\n        means_samples = np.cumsum(samples, axis=1) / np.arange(1, n_samples + 1)\n        x_hist = np.linspace(0, 1, num=300)\n\n        # Create figure\n        fig = make_subplots(\n                    rows=1,\n                    cols=3,\n                    # vertical_spacing=0.5,\n                    horizontal_spacing=0.02,\n                    column_widths=[20, 2, 3],\n                    subplot_titles=(\"t = \" + str(n_repetitions) + \" r√©p√©titions\",\"\",\"\")\n                )\n\n\n        for i in range(n_repetitions):\n            fig.add_trace(\n                    go.Scatter(\n                        mode='lines',\n                        line=dict(color=\"rgba(0,0,0,0.05)\", width=1),\n                        x=iterations,\n                        y=means_samples[i,:],\n                        ),\n                        row=1, col=1,\n            )\n        fig.add_trace(\n                go.Scatter(\n                    mode='lines',\n                    line=dict(dash=\"dash\", color=\"blue\", width=1),\n                    marker={},\n                    x=iterations,\n                    y=np.full((n_samples), p),\n                    name='p'),\n                    row=1, col=1,\n        )\n        fig.update_layout(\n            template=\"simple_white\",\n            showlegend=False,\n        )\n        fig.add_trace(\n                go.Scatter(\n                    mode='markers',\n                    marker=dict(color=\"rgba(0,0,0,0.05)\", size=4),\n                    x=np.zeros(n_samples),\n                    y=means_samples[:,-1],\n                ),\n                row=1, col=2,\n\n        )\n        y_hist, bins = np.histogram(means_samples[:,-1], bins=int(np.sqrt(n_repetitions)), density=True)\n        fig.add_trace(\n            go.Bar(x=y_hist, y=bins[:-1] + np.diff(bins)/2,\n                    opacity=0.75,\n                    marker_color = 'black',\n                    orientation='h',\n                    width=np.diff(bins),\n                    name=\"Tirages de moyennes empiriques\",\n                    ),\n                row=1, col=3,\n        )\n        fig.add_trace(\n            go.Scatter(x=norm.pdf(x_hist, p, np.sqrt(p*(1-p) / n_samples)),\n                       y=x_hist,\n                       mode='lines',\n                       line=dict(color=\"red\"),\n                       legendgroup='1',\n                       name=\"TCL\"\n                       ),\n                row=1, col=3,\n        )\n\n        fig.update_xaxes(range=[1, n_samples + 1])\n        fig.update_yaxes(range=[-.05, 1.05], row=1, col=1)\n        fig.update_yaxes(matches=\"y1\",visible = False,  row=1, col=2)\n        fig.update_xaxes(range=[-0.2, 0.2], visible = False, row=1, col=2)\n\n        fig.update_yaxes(matches=\"y1\", row=1, col=3, visible=False)\n        fig.update_xaxes(range=[0, 1.1 / np.sqrt(2*np.pi* p*(1-p) / n_samples)], row=1, col=3)\n        fig.update_xaxes(visible=False, row=1, col=3)\n\n\n\n        for trace in fig['data']:\n            print(trace)\n            if (trace['name'] != 'TCL') and (trace['name'] != 'p'):\n                trace['showlegend'] = False\n        fig.update_layout(\n        margin=dict(l=0, r=0, b=10, t=100),\n        )\n        fig.update_layout(\n            title=dict(text=\"Distribution de la moyenne empirique&lt;br&gt; (cas loi de Bernoulli)\", yanchor=\"top\", y=0.95),\n            title_x=0.5,\n            showlegend=True,\n\n        )\n        fig.update_layout(\n            legend=dict(\n                yanchor=\"top\",\n                y=0.99,\n                xanchor=\"left\",\n                x=0.89,\n                bgcolor=\"rgba(0,0,0,0)\",\n            )\n        )\n        fig['layout']['xaxis']['title']='√âchantillons: n'\n\n        fig.update_layout(autosize=True)\n\n        return fig\n\n\napp = App(app_ui, server)\n\nUne autre illustration possible de la convergence donn√©e par le TCL est celle qui correspond au point de vue donn√©e par l‚Äôanalyse. Pour cela supposons que l‚Äôon ait une suite de variables al√©atoires r√©elles X_1, \\dots, X_n, i.i.d. dont la fonction de densit√© commune est not√©e par f.\nOn rappelle quelques √©l√©ments de probabilit√©s concernant les densit√©s. Pour cela on rappelle la d√©finition de la convolution deux fonctions. Pour cela prenons deux fonctions f et g d√©finies sur \\mathbb{R} et qui sont int√©grables au sens de Lebesgue. La convolution de f par g est alors la fonction f*g suivante:\n\n\\begin{align}\n\\mathbb{R} &\\mapsto \\mathbb{R} \\nonumber\\\\\nx &\\to \\int_{-\\infty}^{+\\infty} f(x-y)g(y) dy \\enspace.\\nonumber\n\\end{align}\n\n\n\n\n\n\n\nNote\n\n\n\nOn peut aussi obtenir f*g(x) en calculant \\int_{\\mathbb{R}^2} f(u)g(v) {1\\hspace{-3.8pt} 1}_{u+v=x} du dv.\n\n\n\nTh√©or√®me 3 (Loi de la somme et convolutions) Soient X et Y des v.a. ind√©pendantes de densit√©s f et g respectivement, la loi de X+Y est donn√©e par la convolution f*g.\n\nRappel: pour un scalaire \\alpha\\neq 0, la densit√© de \\alpha X est donn√©e par la fonction x \\mapsto \\frac{1}{|\\alpha|} \\cdot f(\\frac{x}{\\alpha}).\n\nCorollaire 1 (Loi de la moyenne) Soient X_1,\\dots,X_n des v.a. i.i.d. de densit√© f, la densit√© de \\bar{X}_n est donn√©e par la fonction x \\mapsto n \\cdot [f*\\dots*f](n \\cdot x).\n\nDessous, pour X_1, \\dots, X_n, i.i.d., de densit√© f, on affiche la densit√© de la loi de \\bar{X}_n.\n#| standalone: true\n#| viewerHeight: 550\nimport numpy as np\nfrom shiny import ui, render, App, reactive\nfrom shinywidgets import output_widget, register_widget\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objs as go\nfrom scipy import signal\n\n\napp_ui = ui.page_fluid(\n    ui.tags.head(\n        ui.tags.style(\"\"\"\n            .bslib-sidebar-layout &gt; .sidebar &gt; .sidebar-content {\n            display: flex;\n            flex-direction: column;\n            padding: 0.5rem;\n            }\n            .irs--shiny .irs-line {\n            top: 27px;\n            height: 2px;\n            }\n            .irs.irs--shiny .irs-bar {\n            top: 27px;\n            height: 2px;\n            }\n            .irs--shiny .irs-handle {\n            top: 23px;\n            width: 22px;\n            height: 10px;\n        \"\"\")\n    ),\n    ui.panel_title(\"TCL et convolutions\"),\n    ui.layout_sidebar(\n        ui.panel_sidebar(\n            ui.input_select(\n                \"loi\",\n                \"Loi sous-jacente\",\n                {'uniforme': 'Uniforme', 'laplace' : 'Laplace'},\n            ),\n            ui.input_slider(\n                \"n_iter\",\n                \"√âchantillons: n\",\n                1,\n                10,\n                value=1,\n                step=1,\n                ticks=False,\n            ), width = 3\n    ),\n    ui.panel_main(\n        output_widget(\"my_widget\"), width = 9\n    )\n)\n)\n\n\nnnzeros = 10001\nx_min = -20\nx_max = 20\n\nx = np.linspace(x_min, x_max, nnzeros)\ny = np.zeros(nnzeros)\nmask = np.where(np.abs(x) &lt;= 0.5, 1, 0)\ny[mask == 1] = 1\ndelta = (x_max - x_min) / nnzeros\nvar = np.sum(y * x**2 *(delta)) - (np.sum(y * x * delta))**2\n\ndef convolve(signal_in, n_convolutions, delta):\n    output = np.zeros(len(signal_in))\n    if n_convolutions == 0:\n        return output\n    elif n_convolutions == 1:\n        return signal_in\n    else:\n        output = signal_in.copy()\n        for i in range(n_convolutions - 1):\n            output = signal.fftconvolve(\n                output * delta, signal_in, mode=\"same\"\n            )\n        return output\n\n\ndef server(input, output, session):\n\n    fig = go.FigureWidget()\n    fig.add_trace(\n        go.Scatter(\n            mode=\"lines\",\n            line=dict(color=\"black\", width=3),\n            x=[],\n            y=[],\n            name=\"loi de de la moyenne empirique&lt;br&gt;(variance ad√©quate)\",\n        )\n    )\n\n    fig.add_trace(\n        go.Scatter(\n            x=x,\n            y=np.exp(-(x**2) / (2 * var)) / np.sqrt(2 * var * np.pi),\n            mode=\"lines\",\n            line=dict(dash=\"dash\", color=\"red\", width=2),\n            name=f\"Loi normale&lt;br&gt;(variance ad√©quate)\",\n        )\n    )\n    fig.update_xaxes(range=[-3, 3], position=0.)\n    fig.update_yaxes(range=[0, 2 * np.max(y)], position=0.5, showticklabels=False)\n\n\n    fig.update_layout(\n        template=\"simple_white\",\n        showlegend=True,\n        autosize=True,\n        title=dict(text=\"Densit√© : &lt;br&gt; moyenne de n variables al√©atoires\", yanchor=\"top\", y=0.95),\n        title_x=0.5,\n    )\n    fig.update_layout(legend=dict(\n        yanchor=\"top\",\n        y=0.95,\n        xanchor=\"left\",\n        x=0.8,\n        font=dict(size= 18)\n    ))\n    fig.update_layout(\n    height=250,\n    margin=dict(l=0, r=0, b=10, t=100),\n    )\n\n    register_widget(\"my_widget\", fig)\n\n    @reactive.Effect\n    def _():\n        if str(input.loi()) == 'uniforme':\n            y = np.zeros(nnzeros)\n            mask = np.where(np.abs(x) &lt;= 0.5, 1, 0)\n            y[mask == 1] = 1\n\n        else:\n            y=np.exp(-np.abs(x)) / 2\n            y = y / (np.sum(y) * delta)\n\n        var = np.sum(y * x**2 * delta) - (np.sum(y * x * delta))**2\n\n        y_display = convolve(y, input.n_iter(), delta)\n        # Update data in fig:\n        fig.data[0].x = x / np.sqrt(input.n_iter())\n        fig.data[0].y = y_display * np.sqrt(input.n_iter())\n        fig.data[1].y = np.exp(-(x**2) / (2 * var)) / np.sqrt(2 * var * np.pi)\n        fig.update_yaxes(range=[0, 2 * np.max(y)], position=0.5, showticklabels=False)\n\n\napp = App(app_ui, server)\nPour aller plus loin sur les convolutions, voir la vid√©o de 3Blue1Brown √† ce sujet: Convolutions | Why X+Y in probability is a beautiful mess",
    "crumbs": [
      "Cours",
      "Th√©or√®mes asymptotiques"
    ]
  },
  {
    "objectID": "Courses/simulation.html",
    "href": "Courses/simulation.html",
    "title": "Simulation",
    "section": "",
    "text": "Dans ce chapitre on se demande comment simuler en pratique des variables al√©atoires i.i.d. L‚Äôid√©e est de commencer par le cas de variables al√©atoires de loi uniforme et d‚Äôen d√©duire les autres lois.",
    "crumbs": [
      "Cours",
      "Simulation"
    ]
  },
  {
    "objectID": "Courses/simulation.html#variables-al√©atoires-uniformes",
    "href": "Courses/simulation.html#variables-al√©atoires-uniformes",
    "title": "Simulation",
    "section": "Variables al√©atoires uniformes",
    "text": "Variables al√©atoires uniformes\nOn rappelle qu‚Äôune variable al√©atoire U suit une loi uniforme sur [0,1], not√© \\mathcal{U}([0,1]) si sa fonction de r√©partition F_U est donn√©e par \nF_U(x)\n=\n\\begin{cases}\n    0, & \\text{si }x &lt; 0\\,,        \\\\\n    x, & \\text{si }x \\in [0,1]\\,,  \\\\\n    1, & \\text{si }x &gt; 1\\,.        \\\\\n\\end{cases}\n\n\n\n\n\n                                                \n\n\nFigure¬†1: Fonction de r√©partition de la loi uniforme\n\n\n\n\nL‚Äôobjectif est de simuler sur machine une suite U_1, \\ldots, U_n de variables al√©atoires i.i.d. de loi \\mathcal{U}([0,1]). Plusieurs probl√®mes apparaissent alors :\n\nUne machine est d√©terministe.\nLes nombres entre 0 et 1 donn√©s par la machine sont de la forme k/2^p, pour k \\in \\{0, \\ldots, 2^{p-1}\\}. On ne pourra donc jamais g√©n√©rer des nombres qui ne sont pas de cette forme.\nV√©rifier qu‚Äôune suite est bien i.i.d. est un probl√®me difficile.\n\n\nD√©finition 1 (G√©n√©rateur de nombres pseudo-al√©atoires) \nUn g√©n√©rateur de nombres pseudo-al√©atoires (üá¨üáß: Pseudo Random Number Generator, PRNG), est un algorithme d√©terministe r√©cursif qui renvoie une suite U_1, \\ldots, U_n dans [0,1] qui a un ‚Äúcomportement similaire‚Äù √† une suite i.i.d. de loi \\mathcal{U}([0,1]). Pour √™tre plus rigoureux, ces nombres sont en fait des nombres entiers g√©n√©r√©s uniform√©ment sur un certain interval. Dans un second temps, une transformation simple (normalisation) permet d‚Äôobtenir des nombres flottants (üá¨üáß: floats) entre 0 et 1.\n\n\n\n\n\n\n\nPour aller plus loin\n\n\n\nParfois il est utile d‚Äôaller chercher dans le code source certaines information pour savoir comment les fonctions sont cod√©es dans les packages que l‚Äôon utiliser. Par exemple, pour numpy que l‚Äôon utilise fr√©quement, on peut voir l‚Äôop√©ration choisie ici: Random: int -&gt; float en numpy.\n\n\nUn tel algorithme se construit de la mani√®re suivante :\n\nOn part d‚Äôune graine (üá¨üáß: seed) U_0 qui d√©termine la premi√®re valeur de mani√®re la plus arbitraire possible.\nLa proc√©dure r√©cursive s‚Äô√©crit U_{n+1} = f(U_n), o√π f est une transformation d√©terministe, de sorte que U_{n+1} est le plus ind√©pendant possible de U_1, \\dots¬∑, U_n.\n\n\nLa fonction f est d√©terministe et prend ses valeurs dans un ensemble fini, donc l‚Äôalgorithme est p√©riodique. Le but est donc d‚Äôavoir la plus grande p√©riode possible.\nNotons qu‚Äôune fois que la graine est fix√©e, alors l‚Äôalgorithme donne toujours les m√™mes valeurs. Fixer la graine peut donc √™tre tr√®s utile pour r√©p√©ter des simulations dans des conditions identiques et ainsi rep√©rer des erreurs.\n\n\n\n\n\n\n\nExercice: bug ou feature?\n\n\n\nReprendre les widgets du chapitre Th√©or√®mes asymptotiques et faites varier doucement le param√®tre p (de Bernoulli). Que constatez-vous? Proposer une explication potentielle.\n\n\n\nG√©n√©rateur congruentiel lin√©aire\nLa plupart des PRNG s‚Äôappuient sur des r√©sultats arithm√©tiques. Un des plus connus est celui appel√© G√©n√©rateur congruentiel lin√©aire (üá¨üáß Linear congruential generator, LCG). Il est d√©fini comme suit: on construit r√©cursivement une suite d‚Äôentiers X_i via la congruence \n  X_{n+1} = a X_n + b \\quad \\text{mod } m \\enspace,\n o√π a,b,m sont des entiers bien choisis pour que la suite obtenue ait de bonnes propri√©t√©s. Il suffit alors de consid√©rer X_n/m. Par exemple, la fonction rand sur scilab utilise cette congruence avec m=2^{31}, a=843\\; 314\\; 861, et b=453\\; 816\\; 693.\n\n\nG√©n√©rateurs alternatifs\nLes langages Python et R utilisent par d√©faut le g√©n√©rateur Mersenne-Twister qui s‚Äôappuie sur la multiplication vectorielle, mais d‚Äôautres g√©n√©rateurs sont aussi disponibles. Ce g√©n√©rateur a pour p√©riode m =2^{19937}-1, nombre qu‚Äôon peut raisonnablement consid√©rer comme grand.\nPour numpy la m√©thode par d√©faut est PCG64 (cf.¬†documentation de numpy), qui dispose de meilleures garanties statistiques (Voir le site https://www.pcg-random.org pour cela).\n\n\nUsage en numpy\nOn suppose d√©sormais disposer d‚Äôun g√©n√©rateur pseudo-al√©atoire sur [0,1]. En numpy depuis la version 1.17, une bonne mani√®re d‚Äôutiliser des √©l√©ments al√©atoires est d‚Äôutiliser un g√©n√©rateur que l‚Äôon d√©finit soi-m√™me:\n\nseed = 12345  # Toujours √™tre conscient qu'une graine existe\nrng = np.random.default_rng(seed)  #\nprint(rng.random())  ##  un tirage uniforme sur [0,1]\nprint(rng.random(size=5))  ## cinq tirages uniformes sur [0,1]\nprint(rng.random(size=(3, 2)))  ## matrice 3x2, √† entr√©es unif. sur [0,1]\n\n0.22733602246716966\n[0.31675834 0.79736546 0.67625467 0.39110955 0.33281393]\n[[0.59830875 0.18673419]\n [0.67275604 0.94180287]\n [0.24824571 0.94888115]]\n\n\nDans la suite on va voir comment g√©n√©rer d‚Äôautres lois √† partir de la loi uniforme, mais il est clair que les logiciels modernes proposent un large √©ventail de distribution classique (gaussienne, exponentielle, etc.). Une liste exhaustive est donn√©e ici pour numpy.\n\n\n\n\n\n\nPour aller plus loin\n\n\n\nUne excellent discussion sur les bonnes pratiques al√©atoires en numpy, et l‚Äôusage de np.random.default_rng est donn√©e dans ce blog post d‚ÄôAlbert Thomas.\n\n\n\n\nPropri√©t√© de la loi uniforme\nOn verra souvent appara√Ætre la variable al√©atoire 1-U o√π U \\sim \\mathcal{U}([0,1]). Il se trouve que 1-U suit aussi une loi uniforme sur [0,1] comme le montre le calcul de sa fonction de r√©partition. Ainsi pour tout x \\in [0,1] on obtient \n\\begin{align*}\n\\mathbb{P}(1-U \\leq x) & = \\mathbb{P}(U \\geq 1-x),\\\\\n                       & = 1-(1-x), \\\\\n                       & = x\\,.\n\\end{align*}\n On peut d√©montrer facilement la m√™me relation pour x&lt;0 et x&gt;1, d‚Äôo√π le r√©sultat.",
    "crumbs": [
      "Cours",
      "Simulation"
    ]
  },
  {
    "objectID": "Courses/simulation.html#m√©thode-dinversion",
    "href": "Courses/simulation.html#m√©thode-dinversion",
    "title": "Simulation",
    "section": "M√©thode d‚Äôinversion",
    "text": "M√©thode d‚Äôinversion\nL‚Äôid√©e de la m√©thode d‚Äôinversion repose sur le r√©sultat suivant :",
    "crumbs": [
      "Cours",
      "Simulation"
    ]
  },
  {
    "objectID": "Courses/simulation.html#rappel-sur-la-fonction-quantile",
    "href": "Courses/simulation.html#rappel-sur-la-fonction-quantile",
    "title": "Simulation",
    "section": "Rappel sur la fonction quantile",
    "text": "Rappel sur la fonction quantile\nRappel : Pour F une fonction d√©finie sur \\mathbb{R} √† valeurs dans [0, 1], croissante, on note\n\n\\forall q \\in ]0,1[, \\quad F^\\leftarrow(q) = \\inf\\{ x \\in \\mathbb{R} : F(x)\\geq q\\}\n\\tag{1}\n\nTh√©or√®me 1 (Carat√©risation des quantiles) Soit F une fonction d√©finie sur \\mathbb{R} √† valeurs dans [0, 1], croissante et continue √† droite, alors pour tout q \\in ]0, 1[, on a \n\\begin{align}\n   \\{x \\in \\mathbb{R} :  F(x) \\geq q) \\} & =\n   \\{x \\in \\mathbb{R} : x \\geq F^\\leftarrow(q)  \\}\n\\end{align}\n\n\nPreuve\n\nCas \\subset: Soit x \\in \\mathbb{R} t.q. F(x) \\geq q, alors par d√©finition de l‚Äôinf dans √âquation¬†1, x \\geq F^\\leftarrow(q).\nCas \\supset: Soit x \\in \\mathbb{R} t.q. x \\geq F_X^\\leftarrow(q) alors pour tout \\epsilon &gt; 0, x + \\epsilon &gt; F^\\leftarrow(q), donc F(x + \\epsilon) \\geq q. Puis, par continuit√© √† droite de F, F(x) \\geq q.",
    "crumbs": [
      "Cours",
      "Simulation"
    ]
  },
  {
    "objectID": "Courses/simulation.html#m√©thode-dinversion-1",
    "href": "Courses/simulation.html#m√©thode-dinversion-1",
    "title": "Simulation",
    "section": "M√©thode d‚Äôinversion",
    "text": "M√©thode d‚Äôinversion\n\nTh√©or√®me 2 (M√©thode d‚Äôinversion) Soit X une v.a r√©elle, et U \\sim\\mathcal{U}([0,1]), alors la variable al√©atoire F_X^{\\leftarrow}(U) a m√™me loi que X.\n\nPreuve: En utilisant le th√©or√®me pr√©c√©dent, on a \\mathbb{P}(F_X^{\\leftarrow}(U) \\leq x) = \\mathbb{P}(U \\leq F_X(x)) pour tout x\\in\\mathbb{R}. Puis, comme U est une loi uniforme sur [0,1], \\mathbb{P}(U\\leq F_X(x))=F_X(x).\nOn en d√©duit donc que la loi de F_X^{-1}(U) est la m√™me que celle de X, car les deux v.a. ont la m√™me fonction de r√©partition.\n\nExemple 1 (Simulation d‚Äôune loi exponentielle) On rappelle que la loi exponentielle de param√®tre \\lambda &gt; 0 a pour densit√© \nf_{\\lambda}(x) = \\lambda e^{-\\lambda x} {1\\hspace{-3.8pt} 1}_{\\mathbb{R}_+}(x)\\enspace.\n et donc pour fonction de r√©partition \nF_{\\lambda}(x) = 1 - e^{-\\lambda x} {1\\hspace{-3.8pt} 1}_{\\mathbb{R}_+}(x)\\enspace.\n On v√©rifie que F_{\\lambda} est bijective de \\mathbb{R}_+ dans ]0,1[ et que son inverse est donn√©e pour tout u \\in ]0,1[ par \nF_{\\lambda}^{-1}(u) = -\\frac{1}{\\lambda} \\log(1-u)\\enspace.\n\n\nMalheureusement, la fonction F n‚Äôest pas toujours inversible (penser aux lois discr√®tes) c‚Äôest donc pourquoi on utilise l‚Äôinverse l‚Äôinverse g√©n√©ralis√©e ou fonction quantile introduite dans la section Notations: \n\n  F^\\leftarrow(p)=  \\inf\\{ x \\in \\mathbb{R} \\colon F(x)\\geq p\\} \\enspace.\n\nInterpr√©tation: D√©finir l‚Äôinverse d‚Äôune fonction de r√©partition F revient √† r√©soudre l‚Äô√©quation F(x) = \\alpha d‚Äôinconnue x pour un \\alpha fix√©. Si F n‚Äôest pas bijective, deux probl√®mes apparaissent :\n\nl‚Äô√©quation n‚Äôa aucune solution ce qui revient √† dire que F n‚Äôest pas surjectif (graphiquement, F pr√©sente des sauts) ;\nl‚Äô√©quation a plusieurs solutions ce qui revient √† dire que F n‚Äôest pas injective (graphiquement cela se mat√©rialise par un plateau √† la hauteur \\alpha). Un exemple classique est celui o√π F est la fonction de r√©partition d‚Äôune variable al√©atoire discr√®te.\n\nLe passage √† l‚Äôin√©quation F(x) \\geq u permet de contourner la non-surjectivit√© : on ne regarde non plus les droites horizontales y=u mais la r√©gion \\{y \\geq \\alpha\\}. Le choix de l‚Äô\\inf dans la d√©finition de F^{\\leftarrow} permet de contourner la non-injectivit√© : vu qu‚Äôil y a possiblement plusieurs x tels que F(x) \\geq u, on choisit le ‚Äúpremier‚Äù. Ces consid√©rations sont illustr√©es en Figure Figure¬†2.\n\n\n\n\n                                                \n\n\nFigure¬†2\n\n\n\n\nRemarques additionnelles:\n\nLa fonction F √©tant croissante, la quantit√© F^\\leftarrow(u) correspond au premier instant o√π F d√©passe \\alpha. Si F est bijective (ce qui √©quivaut dans ce cas √† strictement croissante et injective), alors F^\\leftarrow = F^{-1}.\nLa fonction F^\\leftarrow n‚Äôest rien d‚Äôautre que la fonction quantile : si 0 &lt; \\alpha &lt; 1, q_{1-\\alpha} = F^\\leftarrow(1-\\alpha) est le quantile d‚Äôordre (1-\\alpha) de F. Par exemple, F^\\leftarrow(1/2) correspond √† la m√©diane.\nNotons que si u=0, on peut alors naturellement poser F^{\\leftarrow}(0) = -\\infty. De m√™me, avec la convention la convention \\inf \\emptyset = +\\infty, on peut alors √©tendre la d√©finition de F^\\leftarrow √† u=1 (mais F^\\leftarrow(1) n‚Äôest pas toujours √©gal √† \\infty, voir les exemples ci-dessous).\n\n\nProposition 1 (Loi √† support fini) \nSoit X une variable al√©atoire discr√®te prenant uniquement les valeurs x_1 &lt; \\dots &lt; x_r (r modalit√© possibles) avec probabilit√© p_1, \\dots, p_r (donc p_1 + \\dots + p_r=1). On v√©rifie que pour tout u \\in ]0,1[, \n        F^\\leftarrow(u) =\n        \\begin{cases}\n            x_1 & \\text{si } 0 &lt; u \\leq p_1\\,,                  \\\\\n            x_2 & \\text{si } p_1 &lt; u \\leq p_1+p_2\\,,            \\\\\n                & \\vdots                                        \\\\\n            x_r & \\text{si }  \\sum_{i=1}^{r-1} p_i &lt; u &lt; 1\\,.\n        \\end{cases}\n\nSur cet exemple, on peut prolonger la d√©finition de F^\\leftarrow √† u=1 en posant F^\\leftarrow(1) = x_r. L‚Äôinverse g√©n√©ralis√©e se r√©√©crit alors sous la forme \n        F^\\leftarrow(u) = \\sum_{k=1}^r x_k {1\\hspace{-3.8pt} 1}_{ \\{  \\sum_{i=1}^{k-1}p_i &lt; u \\leq \\sum_{i=1}^{k}p_i \\} }\\enspace,\n o√π on a pos√© p_0=0.\n\nL‚Äôexpression pr√©c√©dente s‚Äô√©tend directement au cas o√π X prend un nombre (infini) d√©nombrable de valeurs, la somme devenant alors une s√©rie.\nLa m√©thode est illustr√© ci-dessous pour quelques lois int√©ressantes:\n\nviewof dist = Inputs.select(['normal','cauchy','laplace','bimodal'], {value: \"bimodal\", label: \"Loi\"})\nviewof replay = html`&lt;button&gt;Relancer`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ninvcdfboard = require(await FileAttachment(\"../inverse-vizu/dist/invcdfboard.umd.cjs\").url())\n\n{\n  replay\n  const canvas = DOM.canvas(500, 500);\n  const galton = invcdfboard.galton(canvas,dist);\n  \n  return html`${galton.canvas}`\n}",
    "crumbs": [
      "Cours",
      "Simulation"
    ]
  },
  {
    "objectID": "Courses/simulation.html#m√©thode-de-rejet",
    "href": "Courses/simulation.html#m√©thode-de-rejet",
    "title": "Simulation",
    "section": "M√©thode de rejet",
    "text": "M√©thode de rejet\nL‚Äôid√©e de la m√©thode de rejet est la suivante. On souhaite simuler une variable al√©atoire X de densit√© f, appel√©e loi cible, mais f est trop compliqu√©e pour que la simulation puisse se faire directement. On dispose cependant d‚Äôune autre densit√© g poss√©dant les propri√©t√©s suivantes :\n\non sait simuler Y de loi g,\nil existe m &gt; 0 tel que f(x) \\leq m \\cdot g(x),\non sait √©valuer le rapport d‚Äôacceptation r(x) = \\frac{f(x)}{mg(x)}.\n\nRemarquons d‚Äôores et d√©j√† que la constante m est n√©cessairement plus grande que 1 car \n    1 = \\int_\\mathbb{R} f(x) \\, dx \\leq m \\int_\\mathbb{R} g(x)\\, dx = m\\,.\n\nL‚Äôid√©e est alors de consid√©rer deux suites i.i.d. de variables al√©atoires ind√©pendantes entre elles:\n\n(Y_n)_{n \\geq 1} de loi g,\n(U_n)_{n \\geq 1} de loi uniforme sur [0,1].\n\nEn pratique, Y_n correspond √† une proposition et U_n permettra de d√©cider si on accepte la proposition ou non. Si oui, alors on conserve Y_n, sinon on simule Y_{n+1}. Le rapport d‚Äôacceptation, c‚Äôest-√†-dire la proportion de Y_n accept√©es, correspond √† r(x).\nAutrement dit, pour simuler X de densit√© f, il suffit de simuler Y de densit√© g et U uniforme jusqu‚Äô√† ce que U \\leq r(Y). La proposition suivante assure que cette m√©thode donne bien le r√©sultat voulu.\n\nProposition 2 (M√©thode de rejet) \nSoit T = \\inf \\{n \\geq 1 : U_n \\leq r(Y_n)\\} le premier instant o√π le tirage est accept√©. Alors :\n\nT suit une loi g√©om√©trique de param√®tre 1/m,\nla variable al√©atoire X = Y_T a pour densit√© f et est ind√©pendante de T.\n\n\nD√©monstration:\nIl s‚Äôagit d‚Äô√©tudier la loi du couple (X,T). Pour x \\in \\mathbb{R} et n \\in \\mathbb{N}^{*}, on √©crit \\mathbb{P}(X \\leq x, T=n)= \\mathbb{P}(U_1 &gt; r(Y_1), \\dots, U_{n-1} &gt; r(Y_{n-1}), U_n \\leq r(Y_n), Y_n \\leq x). Les n tirages √©tant iid, on obtient \n    \\mathbb{P}(X \\leq x, T=n) = \\mathbb{P}(U_1 &gt; r(Y_1))^{n-1} \\mathbb{P}(U_n \\leq r(Y_n), Y_n \\leq x)\\,.\n\nConcernant le premier terme, les variables al√©atoires Y_1 et U_1 sont ind√©pendantes donc leur loi jointe correspond au produit des densit√©s : \\begin{align*}\n        \\mathbb{P}(U_1 &gt; r(Y_1))\n         & = \\mathbb{P}((U_1, Y_1) \\in \\{(u,y) \\in \\mathbb{R}^2 : u &gt; r(y)\\})                             \\\\\n         & = \\int_{\\mathbb{R}^2} {1\\hspace{-3.8pt} 1}_{\\{u &gt; r(y)\\}} ({1\\hspace{-3.8pt} 1}_{[0,1]}(u) g(y)) \\, du dy  \\\\\n         & = \\int_\\mathbb{R} \\bigg( \\int_0^1 {1\\hspace{-3.8pt} 1}_{\\{u &gt; r(y)\\}} \\, du\\bigg) g(y)\\, d y \\\\\n         & =  \\int_\\mathbb{R} (1-r(y)) \\, g(y)\\, d y\\,,\n    \\end{align*}\n ce qui se r√©√©crit, comme f et g sont des densit√©s et que r(y) = f(y)/(m \\cdot g(y)): \\begin{align*}\n        \\mathbb{P}(U_1 &gt; r(Y_1))\n        & = \\int_\\mathbb{R} g(y)\\, d y - \\int_\\mathbb{R} \\dfrac{f(y)}{m}\\, dy \\\\\n        & = 1 - \\dfrac{1}{m}\\,.\n    \\end{align*}\n Le deuxi√®me terme se calcule de mani√®re analogue : \n\\begin{align*}\n    \\mathbb{P}(U_n \\leq r(Y_n), Y_n \\leq x)\n        & = \\int_{\\mathbb{R}^2} {1\\hspace{-3.8pt} 1}_{\\{u \\leq r(y)\\}} {1\\hspace{-3.8pt} 1}_{\\{y \\leq x\\}} ({1\\hspace{-3.8pt} 1}_{[0,1]}(u) g(y)) \\, du dy       \\\\\n        & = \\int_\\mathbb{R} \\bigg( \\int_0^1  {1\\hspace{-3.8pt} 1}_{\\{u \\leq r(y)\\}} \\, du\\bigg) {1\\hspace{-3.8pt} 1}_{\\{y \\leq x\\}}  g(y)\\, d y\\,,\n\\end{align*}\n c‚Äôest-√†-dire \n\\begin{align*}\n        \\mathbb{P}(U_n \\leq r(Y_n), Y_n \\leq x)\n        & = \\int_\\mathbb{R} r(y) {1\\hspace{-3.8pt} 1}_{\\{y \\leq x\\}}  g(y)\\, d y \\\\\n        & = \\int_{-\\infty}^x \\dfrac{f(y)}{m}\\, d y \\\\\n        & = \\dfrac{F(x)}{m}\\,,\n\\end{align*}\n o√π F est la fonction de r√©partition de la loi de densit√© f. On peut ainsi conclure que \n    \\mathbb{P}(X \\leq x, T=n)\n    =\n    \\bigg(1 - \\dfrac{1}{m}\\bigg)^{n-1} \\dfrac{F(x)}{m}\\,.\n Il ne reste plus qu‚Äô√† √©tudier les lois marginales. D‚Äôune part, par continuit√© monotone croissante, \n    \\mathbb{P}(T=n)\n    = \\lim_{q \\to \\infty} \\mathbb{P}(X \\in ]-\\infty, q], T=n)\\,,\n ce qui donne \n\\begin{align*}\n    \\mathbb{P}(T=n)\n    & = \\lim_{q \\to \\infty} \\bigg(1 - \\dfrac{1}{m}\\bigg)^{n-1} \\dfrac{F(q)}{m}\\\\\n    & = \\bigg(1 - \\dfrac{1}{m}\\bigg)^{n-1} \\dfrac{1}{m}\\,.\n\\end{align*}\n On en d√©duit que T suit une loi g√©om√©trique de param√®tre 1/m. D‚Äôautre part, par \\sigma-additivit√©, \n\\begin{align*}\n    \\mathbb{P}(X \\leq x)\n    & = \\mathbb{P}(X \\leq x, T \\in \\mathbb{N}^*)\\\\\n    & = \\sum_{n=1}^\\infty \\mathbb{P}(X \\leq x, T=n)\\,,\n\\end{align*}\n ce qui donne \n\\begin{align*}\n    \\mathbb{P}(X \\leq x)\n    & = \\sum_{n=1}^\\infty \\bigg(1 - \\dfrac{1}{m}\\bigg)^{n-1} \\dfrac{F(x)}{m}\\\\\n    & = \\dfrac{1}{1-(1-1/m)} \\dfrac{F(x)}{m}\\\\\n    & = F(x)\\,,\n\\end{align*}\n ce qui prouve que X a pour loi F.\nEnfin, la loi du couple (X,T) est √©gale au produit des lois \n\\begin{align*}\n    \\mathbb{P}(X \\leq x, T=n)\n    & = \\bigg(1 - \\dfrac{1}{m}\\bigg)^{n-1} \\dfrac{F(x)}{m}\\\\\n    & = \\mathbb{P}(T=n) \\mathbb{P}(X \\leq x)\\,,\n\\end{align*}\n\n\n\nce qui prouve l‚Äôind√©pendance de X et T.\n\n\n‚ñ°\n\n\n\ndef accept_reject(n, f, g, g_sampler, m):\n    \"\"\"\n    n: nombre de simulations\n    f: densit√© cible\n    g: densit√© des propositions, g_sampler: simulateur selon g\n    m: constante pour la majoration\n    \"\"\"\n    x_samples = np.zeros(n)\n    u_samples = np.zeros(n)\n    accepted = np.zeros(n)\n    n_accepted = 0\n    while n_accepted &lt; n:\n        x = g_sampler()\n        u = np.random.uniform()\n        alpha = u * m * g(x)\n        u_samples [n_accepted] = alpha\n        x_samples[n_accepted] = x\n        if  alpha &lt;= f(x):\n            accepted[n_accepted] = 1\n        n_accepted += 1\n    return x_samples, u_samples, accepted\n\n\n\n\n\n\n\nEn pratique‚Ä¶\n\n\n\nOn simule U_1 et Y_1. Si U_1 \\leq r(Y_1) c‚Äôest gagn√©, on pose X=Y_1. Sinon, on simule U_2 et Y_2 et on teste √† nouveau l‚Äôin√©galit√© U_2 \\leq r(Y_2). Et ainsi de suite. Comme T suit une loi g√©om√©trique de param√®tre 1/m, son esp√©rance vaut m : il faut en moyenne m tentatives pour obtenir une simulation de la loi de densit√© f. L‚Äôobjectif est alors de choisir un couple (g, m) de sorte que m soit le plus proche possible de 1.\n\n\n\nExemple 2 (Rejet d‚Äôune loi polynomiale) Donnons un exemple jouet (on √©tudiera des exemples plus pertinents en TD). On consid√®re la densit√© f(x) = 4x^3 {1\\hspace{-3.8pt} 1}_{[0,1]}(x). Comme f est major√©e par 4, on peut choisir pour g la densit√© de la loi uniforme sur [0,1] et m=4. Alors, r(x) =f(x) / (mg(x)) = x^3, pour x \\in [0,1]. On simule donc (Y_1, U_1) et on teste si U_1 \\leq Y_1^3, etc.\nBien √©videmment, on privil√©giera ici une simulation via F^\\leftarrow qui permet de g√©n√©rer des variables al√©atoires de loi f plus rapidement.\n\n\n\n\n                                                \n\n\nFigure¬†3: Visualisation des zones d‚Äôacceptations/rejet (g uniforme)\n\n\n\n\nNous pouvons facilement am√©liorer la proportion de point accept√©s en proposant par exemple g d√©finie par g(x) = 2x {1\\hspace{-3.8pt} 1}_{[0, 1]}(x), et m=2.\n\n\n\n\n                                                \n\n\nFigure¬†4: Visualisation des zones d‚Äôacceptations/rejet (g triangulaire)\n\n\n\n\n\n\n\n\n\n\n\n\n\nmd`Le taux d'acceptation est pass√© de **${ratio1}** √†\n**${ratio2}** en utilisant une loi triangulaire au lieu d'une loi uniforme.`\n\n\n\n\n\n\n\n\n\nExemple 3 (Rejet d‚Äôune loi de densit√© d‚ÄôAndrews) Consid√©rons la densit√© d‚ÄôAndrews d√©finie par f(x) = \\frac{1}{S} \\frac{\\sin(\\pi\\cdot x)}{\\pi \\cdot x} {1\\hspace{-3.8pt} 1}_{[-1,1]}(x), avec S = \\int_{-1}^{1}\\frac{\\sin(\\pi\\cdot x)}{\\pi \\cdot x}dx. Dans ce contexte, on ne connait pas la valeur exacte de S, et on va donc utiliser la m√©thode de rejet pour simuler des variables al√©atoires de loi f sans cette information. On peut l‚Äôadapter le test de la mani√®re suivante: si l‚Äôon prend m=2/S et g(x) = \\frac{1}{2} {1\\hspace{-3.8pt} 1}_{[-1,1]}(x), on observe que tester u\\leq \\frac{f(x)}{m \\cdot g(x)} est √©quivalent √† tester u \\leq r(x)=\\frac{1}{S} \\frac{\\sin(\\pi\\cdot x)}{\\pi \\cdot x} \\cdot \\frac{1}{\\frac{2}{S} g(x)} = \\frac{\\sin(\\pi\\cdot x)}{\\pi \\cdot x} \\cdot \\frac{1}{2 \\cdot g(x)}, ce qui peut se faire sans connaissance de S. De plus on peut v√©rifier que g(x) = \\frac{1}{2} {1\\hspace{-3.8pt} 1}_{[-1,1]}(x) d√©finit une densit√© et que f(x) \\leq m \\cdot g(x) pour tout x\\in \\mathbb{R}.\n\nn = 10000\ng = lambda x: np.ones_like(x) / 2\ng_sampler = lambda: 2 * np.random.uniform() - 1\nm = 2\n\nx_samples, u_samples, accepted = accept_reject(n, np.sinc, g, g_sampler, m)\nratio = np.sum(accepted) / n\n# Note: https://stackoverflow.com/questions/70804891/how-to-vectorize-a-function-in-python-that-includes-a-limit-e-g-sinx-x\n\n\n\n\n\n\nOn peut approcher num√©riquement la valeur exacte de S en utilisant une m√©thode de calcul approch√©e, ce qui permet de comparer ici notre m√©thode de rejet avec la densit√© sous-jacente:\n\nfrom scipy import integrate\nS = integrate.quad(np.sinc, -1, 1)[0]\nprint(f\"En utilisant la m√©thode de rejet, on trouve que S = {S:.3f}\")\n\nEn utilisant la m√©thode de rejet, on trouve que S = 1.179\n\n\nEnfin, on peut visualiser la qualit√© l‚Äôapproximation de la densit√© par la m√©thode de rejet en comparant la densit√© approch√©e (avec un histogramme) avec la densit√© exacte:\n\nfig = go.Figure()\nfig.add_trace(\n    go.Histogram(\n        x=x_samples[accepted == 1], histnorm=\"probability density\", name=\"√âchantillons\"\n    )\n)\n\n# Plot the density\nx = np.linspace(-1, 1, 100)\nfig.add_trace(\n    go.Scatter(\n        x=x,\n        y=np.sinc(x) / S,\n        mode=\"lines\",\n        line=dict(color=\"black\", dash=\"dash\"),\n        name=\"Densit√©\",\n    )\n)\n\nfig.update_layout(template=\"simple_white\", showlegend=True)\n\n\n\n                                                \n\n\nFigure¬†5: M√©thode de rejet pour simuler une loi de densit√© de type Andrews, sans connaissance de la valeur exacte de la constante de normalisation.\n\n\n\n\n\nmd`Dans cet example, le taux d'acceptation est ici de **${ratio}**.`\n\n\n\n\n\n\n\n\nCas mutlidimensionnel\nCommen√ßons par un cas de dimension deux.\nPour cela on va utiliser la m√©thode de rejet pour simuler une loi de densit√© f sur \\mathbb{R}^2. En particulier, un exemple classique est de tirer des points dans le disque unit√©, c‚Äôest-√†-dire de simuler une loi uniforme sur le disque unit√©. Pour cela, on va utiliser la m√©thode de rejet avec g la densit√© de la loi uniforme sur le carr√© [-1,1]^2.\nMais prenons un autre exemple, √† savoir tirer des points uniform√©ment dans la surface d√©limit√© par une cardio√Øde. Pour cela, on va utiliser la m√©thode de rejet avec g la densit√© de la loi uniforme sur le carr√© [-2,2]^2.\n\n\n\n\n                                                \n\n\nFigure¬†6: M√©thode de rejet pour simuler une loi uniforme sur un disque unit√©.\n\n\n\n\nAire estim√©e: 3.0813333333333333\n\n\n\n\n\n\n                                                \n\n\nFigure¬†7: M√©thode de rejet pour simuler une loi uniforme sur une surface d√©limit√©e par une cardio√Øde.\n\n\n\n\nAire estim√©e: 4.905\n\n\n\n\n\n\n\n\nEXERCISE loi uniforme sur un cylindre\n\n\n\nProposer une m√©thode pour simuler une loi uniforme sur un cylindre de rayon 1 et de hauteur 10.",
    "crumbs": [
      "Cours",
      "Simulation"
    ]
  },
  {
    "objectID": "Courses/simulation.html#autres-m√©thodes",
    "href": "Courses/simulation.html#autres-m√©thodes",
    "title": "Simulation",
    "section": "Autres m√©thodes",
    "text": "Autres m√©thodes\n\nSommation de variables al√©atoires\nPour simuler une variable al√©atoire de loi binomiale \\mathcal{B}(n,p), on peut utiliser la m√©thode d‚Äôinversion. Cependant, cela n√©cessite le calcul de l‚Äôinverse g√©n√©ralis√©e de F, donc de coefficients binomiaux et de puissances de p et 1-p. √Ä la place, on utilisera plut√¥t la relation bien connue suivante : si X_1, \\ldots, X_n est une suite iid de variables al√©atoires de loi de Bernoulli de param√®tre p, alors \n    X = X_1 + \\cdots + X_n \\sim \\mathcal{B}(n,p)\\,.\n\nPour simuler des variables al√©atoires de Bernoulli, on utilise la m√©thode d‚Äôinversion (voir Exemple ). Ainsi, si U_1, \\ldots, U_n sont des variables al√©atoires iid de loi uniforme sur [0,1], alors \n    \\sum_{i=1}^n {1\\hspace{-3.8pt} 1}_{\\{U_i \\leq p\\}} \\sim \\mathcal{B}(n,p)\\,.\n\n\n\nLoi de Poisson\nRappelons qu‚Äôune variable al√©atoire X suit une loi de Poisson de param√®tre \\lambda &gt; 0, not√©e X \\sim \\mathcal{P}(\\lambda) si \n    \\mathbb{P}(X = k) = e^{-\\lambda} \\dfrac{\\lambda^k}{k!}\\,, \\quad k \\in \\mathbb{N}^*\\,.\n Une m√©thode pour simuler une variable al√©atoire de loi de Poisson est donn√©e par la proposition suivante.\n\nProposition 3 (G√©n√©ration de v.a. de loi de Poisson) \nSoit (E_n)_{n \\geq 1} des variables al√©atoires i.i.d. de loi exponentielle de param√®tre \\lambda &gt; 0. On pose S_k = E_1 + \\cdots + E_k. Alors, pour tout n \\in \\mathbb{N}^* \n    \\mathbb{P}(S_n \\leq 1 &lt; S_{n+1}) =  e^{-\\lambda} \\dfrac{\\lambda^n}{n!}\\enspace .\n Ainsi, la variable al√©atoire T d√©finie par \n    T \\triangleq \\sup \\{n \\in \\mathbb{N}^* : S_n \\leq 1\\}\n suit une loi de Poisson de param√®tre \\lambda : T \\sim \\mathcal{P}(\\lambda).\n\nLa preuve repose sur le lemme suivant.\n\nLemme 1 (Loi de Erlang) \nSoit n variables al√©atoires E_1, \\dots, E_n i.i.d. de loi exponentielle de param√®tre \\lambda &gt;0. La somme E_1+\\dots+E_n suit une loi d‚ÄôErlang de param√®tres (n,\\lambda), donn√©e par la fonction de r√©partition \n    F_{n,\\lambda}(t) = 1 - \\sum_{k=0}^{n-1} e^{-\\lambda t} \\frac{(\\lambda t)^k}{k!}\\,.\n\n\nD√©monstration:\nOn montre le r√©sultat pour n=2. La g√©n√©ralisation √† k quelconque se fait par r√©currence. Soit t &gt; 0, et f_{\\lambda}(x)={1\\hspace{-3.8pt} 1}_{\\{x \\geq 0 \\}} \\lambda e^{-\\lambda x} la densit√© d‚Äôune loi exponentielle de param√®tre \\lambda. Les variables al√©atoires E_1 et E_2 √©tant ind√©pendantes et suivant des lois exponentielles de param√®tre \\lambda_1 et \\lambda_2, on a \n\\begin{align*}\n    \\mathbb{P}(E_1+E_2 \\leq t)\n        & = \\int_{\\mathbb{R}^2} {1\\hspace{-3.8pt} 1}_{\\{x_1 + x_2 \\leq t\\}} f_{\\lambda}(x_1) f_{\\lambda}(x_2)\\, d x_1 d x_2 \\\\\n        & = \\int_{\\mathbb{R}^2} {1\\hspace{-3.8pt} 1}_{\\{x_1 + x_2 \\leq t\\}} \\lambda^2 e^{-\\lambda (x_1+x_2)} {1\\hspace{-3.8pt} 1}_{\\{x_1 \\geq 0\\}} {1\\hspace{-3.8pt} 1}_{\\{x_2 \\geq 0\\}}\\, d x_1 d x_2 \\\\\n        & = \\int_{\\mathbb{R}^2} {1\\hspace{-3.8pt} 1}_{\\{0 \\leq x_1 \\leq t\\}} {1\\hspace{-3.8pt} 1}_{\\{0 \\leq x_2 \\leq t-x_1\\}} \\lambda^2 e^{-\\lambda x_1} e^{-\\lambda x_2}\\, d x_1 d x_2             \\\\\n        & = \\int_0^t \\lambda e^{-\\lambda x_1} \\bigg(\\int _0^{t-x_1} \\lambda e^{-\\lambda x_2}\\, d x_2\\bigg)  d x_1\\,.\n\\end{align*}\n La premi√®re int√©grale se calcule alors facilement : \n    \\int _0^{t-x_1} \\lambda e^{-\\lambda x_2}\\, d x_2 = 1 - e^{-\\lambda(t-x_1)}\\,.\n On obtient alors \n\\begin{align*}\n    \\mathbb{P}(E_1+E_2 \\leq t)\n   & = \\int_0^t \\lambda e^{-\\lambda x_1}dx_1 -  \\int_0^t e^{-\\lambda t} d x_1\\\\\n   & = 1 - e^{-\\lambda t} - \\lambda t e^{-\\lambda t}\\,.\n\\end{align*}\n Si t&lt;0, alors comme les E_i ne prennent que des valeurs positives on trouve \\mathbb{P}(E_1 + E_2 \\leq t) = 0. Ceci prouve le r√©sultat pour n=2.\n\n\n\n\n\n‚ñ°\n\n\nOn peut d√©sormais prouver le r√©sultat de la Proposition¬†3.\nD√©monstration:\nPour n \\in \\mathbb{N}^*, on d√©compose la probabilit√© \\mathbb{P}(S_n \\leq 1 &lt; S_{n+1}) via \n\\begin{align*}\n    \\mathbb{P}(S_n \\leq 1 &lt; S_{n+1})\n    & = \\mathbb{P}(\\{S_n \\leq 1\\} \\setminus \\{S_{n+1} \\leq 1\\})\\\\\n    & = \\mathbb{P}(S_n \\leq 1) - \\mathbb{P}(S_{n+1} \\leq 1)\\,.\n\\end{align*}\n Le lemme pr√©c√©dent donne \n    \\mathbb{P}(S_n \\leq 1) = 1 - \\sum_{n=0}^{n-1} e^{-\\lambda} \\dfrac{\\lambda^k}{k!}\n et \n    \\mathbb{P}(S_{n+1} \\leq 1) = 1 - \\sum_{k=0}^{n} e^{-\\lambda} \\dfrac{\\lambda^k}{k!}\\,.\n On obtient alors le r√©sultat souhait√© : \n    \\mathbb{P}(S_n \\leq 1 &lt; S_{n+1})\n    = e^{-\\lambda} \\dfrac{\\lambda^n}{n!}\\,.\n\nOn conclut la preuve de la proposition en remarquant que \n    \\mathbb{P}(T=n) = \\mathbb{P}(S_n \\leq 1 &lt; S_{n+1})\\,.\n\n\n\n\n\n\n‚ñ°\n\n\nLa simulation d‚Äôune variable al√©atoire de Poisson repose donc sur la simulation de lois exponentielles qui se fait via la m√©thode d‚Äôinversion, comme vu dans Exemple¬†1. En pratique, on simule E_1 et on teste si E_1 &gt; 1. Si oui, on pose alors T=0. Si non, on simule E_2 et on teste si E_1 + E_2 &gt; 1. Si oui, on pose T=1. Sinon on continue la proc√©dure.\n\n\nBibliographie et pour aller plus loin\n\nGenerating Random Floating-Point Numbers by Dividing Integers: a Case Study par Fr√©d√©ric Goualard\nGenerating Pseudo-random Floating-Point Values par Allen Downey.",
    "crumbs": [
      "Cours",
      "Simulation"
    ]
  },
  {
    "objectID": "Courses/notations.html",
    "href": "Courses/notations.html",
    "title": "Notations et rappels",
    "section": "",
    "text": "On consid√®re un espace probabilis√© (\\Omega, {\\mathcal{F}}, \\mathbb{P}), compos√© d‚Äôun ensemble \\Omega, d‚Äôune tribu \\mathcal{F}, et d‚Äôune mesure de probabilit√© \\mathbb{P}.\nCette d√©finition permet de transposer l‚Äôal√©a qui provient de \\Omega dans l‚Äôespace E. L‚Äôhypoth√®se \\{X \\in B\\} \\in \\mathcal{F} assure que cet ensemble est bien un √©v√®nement et donc que l‚Äôon peut calculer sa probabilit√©.\nUne fois que l‚Äôal√©a a √©t√© transpos√© de \\Omega vers E, on souhaite √©galement transposer la probabilit√© \\mathbb{P} sur E. Ceci motive l‚Äôintroduction de la notion de loi.\nLes propri√©t√©s de \\mathbb{P} assurent que \\mathbb{P}_X est bien une loi de probabilit√© sur l‚Äôespace mesurable (E, \\mathcal{E}).",
    "crumbs": [
      "Cours",
      "Notations et rappels"
    ]
  },
  {
    "objectID": "Courses/notations.html#loi-discr√®tes",
    "href": "Courses/notations.html#loi-discr√®tes",
    "title": "Notations et rappels",
    "section": "Loi discr√®tes",
    "text": "Loi discr√®tes\nLes variables al√©atoires discr√®tes sont celles √† valeurs dans un ensemble E discret, le plus souvent \\mathbb{N}, muni de la tribu pleine \\mathcal{F} = \\mathcal{P}(E).\n\nExemple 1 (Loi de Bernoulli) La loi la plus simple est la loi de Bernoulli de param√®tre p \\in [0,1], d√©finie sur \\{0,1\\} par \\mathbb{P}(X=1) = 1-\\mathbb{P}(X=0) = p qui mod√©lise une exp√©rience al√©atoire √† deux issues (succ√®s = 1 et √©chec = 0).\n\n\nExemple 2 (Loi binomiale) En sommant des variables al√©atoires ind√©pendantes de loi de Bernoulli on obtient une loi binomiale : \\mathbb{P}(X=k) = \\binom{n}{k} p^k (1-p)^{n-k}, pour k \\in \\{0,\\ldots,n\\}, qui mod√©lise le nombre de succ√®s parmi n lancers.\n\n\nExemple 3 (Loi g√©om√©trique) En observant le nombre d‚Äôexp√©riences n√©cessaires avant d‚Äôobtenir un succ√®s, on obtient une loi g√©om√©trique : \\mathbb{P}(X=k) = p (1-p)^{k-1}, pour k \\geq 1. C‚Äôest une loi de probabilit√© discr√®te qui d√©crit le comportement du nombre d‚Äô√©v√©nements se produisant dans un intervalle de temps fix√©, si ces √©v√©nements se produisent avec une fr√©quence moyenne ou esp√©rance connue, et ind√©pendamment du temps √©coul√© depuis l‚Äô√©v√©nement pr√©c√©dent (e.g., nombre de clients dans une file d‚Äôattente, nombre de mutations dans un g√®ne, etc.).\n\n\nExemple 4 (Loi de Poisson) La loi de Poisson de param√®tre \\lambda &gt; 0 est d√©finie par \\mathbb{P}(X=k) = e^{-\\lambda} \\lambda^k / k!, pour k \\in \\mathbb{N}",
    "crumbs": [
      "Cours",
      "Notations et rappels"
    ]
  },
  {
    "objectID": "Courses/notations.html#lois-continues",
    "href": "Courses/notations.html#lois-continues",
    "title": "Notations et rappels",
    "section": "Lois continues",
    "text": "Lois continues\nParmi les variables al√©atoires r√©elles non discr√®tes, beaucoup peuvent se repr√©senter avec une densit√©, c‚Äôest-√†-dire qu‚Äôil existe une fonction mesurable f : \\mathbb{R} \\to [0, \\infty[ d‚Äôint√©grale 1. La loi d‚Äôune telle variable al√©atoire X est alors donn√©e pour tout A \\in \\mathcal{B}(\\mathbb{R}) par \n    \\mathbb{P}(X \\in A) = \\int_A f(x) \\, \\mathrm d x \\enspace.\n Les propri√©t√©s de l‚Äôint√©grale de Lebesgue assure que cette formule d√©finit bien une loi de probabilit√©.\n\nExemple 5 (Loi uniforme) La loi uniforme sur un ensemble B \\in \\mathcal{B}(\\mathbb{R}), s‚Äôobtient avec la densit√© d√©finie par \nf(x) = {1\\hspace{-3.8pt} 1}_B(x) / \\lambda (B) \\enspace,\n o√π \\lambda (B) repr√©sente la mesure de Lebesgue de l‚Äôensemble B. En particulier pour la loi uniforme sur le segment [0,1] on obtient la fonction suivante: \nf(x) = {1\\hspace{-3.8pt} 1}_{[0,1]}(x)\\enspace.\n Si une variable al√©atoire U suit une telle loi on note U \\sim \\mathcal{U}([0,1]).\n\n\nExemple 6 (Loi exponentielle) La loi exponentielle de param√®tre \\gamma &gt; 0 est obtenue avec la densit√© donn√©e par \nf(x) = \\gamma e^{-\\gamma x} {1\\hspace{-3.8pt} 1}_{\\mathbb{R}_+}(x)\\enspace.\n Si une variable al√©atoire X suit cette loi on note X \\sim \\mathcal{Exp}(\\gamma).\n\n\nExemple 7 (Loi normale/gaussienne univari√©e) On obtient la loi normale de param√®tre \\mu \\in \\mathbb{R} et \\sigma^2 &gt; 0 correspond √† loi dont la densit√© est donn√©e par la fonction r√©elle: \nf(x) = \\frac{1}{\\sqrt{2 \\pi} \\sigma}e^{-\\frac{1}{2 \\sigma^2}(x-\\mu)^2} \\enspace.\n Si une variable al√©atoire X suit une telle loi on note X \\sim \\mathcal{N}(\\mu,\\sigma^2), \\mu correspondant √† l‚Äôesp√©rance de la loi, et \\sigma^2 √† sa variance. On nomme loi normale centr√©e r√©duite le cas correspondant √† \\mu = 0 et \\sigma^2 = 1.\n\n\nExemple 8 (Loi normale multivari√©e) On peut √©tendre les lois normales au cas multi-dimensionnel. Fixons d\\in\\mathbb{N}^* un entier non nul. Pour un vecteur \\mu \\in \\mathbb{R}^d et une matrice sym√©trique-d√©finie positive \\Sigma\\in \\mathbb{R^{d\\times d}}, la densit√© normale mutlivari√©e associ√©e est donn√©e par la fonction: \nf(x) = \\frac{1}{{(2 \\pi)}^{\\frac{d}{2}} {\\rm det}(\\Sigma)} e^{-\\frac{1}{2}(x-\\mu)^\\top \\Sigma ^{-1}(x-\\mu)}\n Notons que \\mu est l‚Äôesp√©rance de la loi et \\Sigma la matrice de variance-covariance.",
    "crumbs": [
      "Cours",
      "Notations et rappels"
    ]
  },
  {
    "objectID": "Courses/notations.html#fonction-de-r√©partition",
    "href": "Courses/notations.html#fonction-de-r√©partition",
    "title": "Notations et rappels",
    "section": "Fonction de r√©partition",
    "text": "Fonction de r√©partition\nLa notion de variable al√©atoire n‚Äôest pas facile √† manipuler puisqu‚Äôelle part d‚Äôun espace \\Omega dont on ne sait rien. On souhaite donc caract√©riser la loi d‚Äôune variable al√©atoire en ne consid√©rant que l‚Äôespace d‚Äôarriv√©e (E, \\mathcal{E}) .\nPlusieurs outils existent : la fonction de r√©partition (pour des variables al√©atoires r√©elles), la fonction caract√©ristique (pour des variables al√©atoires dans \\mathbb{R}^d), la fonction g√©n√©ratrice des moments (pour des variables al√©atoires discr√®tes), etc. On se contente ici de la fonction de r√©partition qui nous sera utile pour simuler des variables al√©atoires, ainsi que son inverse au sens de Levy.\n\nD√©finition 3 (Fonction de r√©partition üá¨üáß: cumulative distribution function) \nSoit X une variable al√©atoire sur (\\mathbb{R}, \\mathcal{B}(\\mathbb{R})). La fonction de r√©partition de X est la fonction F_X d√©finie sur \\mathbb{R} par \n\\begin{align*}\n    F_X(x) & = \\mathbb{P}(X \\leq x)\\\\\n           & = \\mathbb{P}(X \\in ]-\\infty, x]) \\enspace.\n\\end{align*}\n\n\n\nExemple 9 (Cas discret) Soit (x_i)_{i \\in I} une suite ordonn√©e de r√©els, avec I \\subset \\mathbb{N}. Si X est une variable al√©atoire discr√®te prenant les valeurs (x_i)_{i \\in I} et de loi (p_i = \\mathbb{P}(X=x_i))_{i \\in I}, alors \n    F_X(x) = \\sum_{i \\in I} p_i {1\\hspace{-3.8pt} 1}_{[x_i, \\infty[}(x) \\enspace.\n\n\n\nExemple 10 (Cas continu) Si X est une variable al√©atoire de densit√© f, alors \n    F_X(x) = \\int_{-\\infty}^x f(t) \\, \\mathrm dt \\enspace.\n\n\nLe graphe des fonctions de r√©partition des loi de Bernoulli, uniforme et normale sont repr√©sent√©es dans le widget ci-dessous. Notons que la fonction de r√©partition de la loi normale \\mathcal{N}(0,1), souvent not√©e \\Phi, n‚Äôadmet pas d‚Äôexpression explicite autre que \n\\Phi(x) = \\dfrac{1}{\\sqrt{2\\pi}} \\int_{-\\infty}^x e^{-\\frac{t^2}{2}}\\, \\mathrm d t\\enspace,\n Les valeurs num√©riques de \\Phi(x) √©taient autrefois report√©es dans des tables1. Par transformation affine, si X \\sim \\mathcal{N}(\\mu, \\sigma^2) ‚Äî ce que l‚Äôon peut aussi √©crire : X=\\mu + \\sigma Y, avec Y\\sim \\mathcal{N}(0,1) ‚Äî alors sa fonction de r√©partition est donn√©e par F_X(x)=\\Phi((x-\\mu)/\\sigma).\n1¬†Wikipedia: loi normale\nProposition 1 (Propri√©t√©s de la fonction de r√©partition) Soit X une variable al√©atoire de fonction de r√©partition F_X.\n\nF_X est une fonction croissante, de limite 0 en -\\infty et de limite 1 en +\\infty.\nF_X est continue √† droite en tout point.\nPour tout x \\in \\mathbb{R}, on a \\mathbb{P}(X=x) = F_X(x) - F_X(x-), o√π F_X(x-) = \\lim_{\\epsilon \\to 0+} F_X(x- \\epsilon).\nSi X a pour densit√© f, alors F_X est d√©rivable \\lambda-presque partout de d√©riv√©e f.\n\n\nPour les d√©monstrations, voir par exemple [@Barbe_Ledoux06].\nLa propri√©t√© 3. est utile dans le cas discret : les valeurs prises par X correspondent aux points de discontinuit√© de F_X et les probabilit√©s associ√©es correspondent √† la hauteur du saut.\nLa propri√©t√© 4. donne le lien entre la fonction de r√©partition d‚Äôune variable al√©atoire √† densit√© et sa densit√©. On peut donc retrouver la loi de X √† partir de sa fonction de r√©partition. Le th√©or√®me suivant g√©n√©ralise ce r√©sultat √† toute variable al√©atoire r√©elle (pas n√©cessairement discr√®te ou √† densit√©).\n\nTh√©or√®me 1 (Caract√©risation de la loi d‚Äôune variable al√©atoire r√©elle) La fonction de r√©partition d‚Äôune variable al√©atoire caract√©rise sa loi : deux variables al√©atoires ont m√™me loi si et seulement si elles ont m√™me fonction de r√©partition.\n\nD√©monstration: voir Wikipedia\nOn rappelle que la tribu des bor√©liens est engendr√©e par la famille d‚Äôensembles \\{]-\\infty,x], x \\in \\mathbb{R}\\}. Le th√©or√®me pr√©c√©dent assure que si on conna√Æt la mesure \\mathbb{P}_X sur cette famille d‚Äôensembles alors on la conna√Æt partout.\n\nExemple 11 (Loi exponentielle depuis une loi uniforme) On consid√®re une variable al√©atoire U de loi uniforme sur [0,1] et on pose X = -\\ln(1-U). D√©terminons la loi de X en calculant sa fonction de r√©partition. Pour tout x \\in \\mathbb{R}, \n\\begin{align*}\nF_X(x) = & \\mathbb{P}(X \\leq x) \\\\\n       = & \\mathbb{P}(-\\ln(1-U) \\leq x) \\\\\n       = & \\mathbb{P}(U \\leq 1-e^{-x}) \\\\\n       = &\n    \\begin{cases}\n        0           & \\text{ si }x &lt; 0\\,,    \\\\\n        1 - e^{-x} & \\text{ si }x \\geq 0\\,,\n    \\end{cases}\n\\end{align*}\n\no√π on a utilis√© l‚Äô√©galit√© \\mathbb{P}(U \\leq t) = t pour tout t \\in [0,1]. Ainsi la variable al√©atoire X a la m√™me fonction de r√©partition qu‚Äôune loi exponentielle de param√®tre 1. On en conclut que X \\sim \\mathcal{Exp}(1). Notons que l‚Äôon peut aussi montrer que -\\ln(X)\\sim\\mathcal{E}(1), sachant que U et 1-U ont la m√™me loi.",
    "crumbs": [
      "Cours",
      "Notations et rappels"
    ]
  },
  {
    "objectID": "Courses/notations.html#fonction-quantile-inverse-g√©n√©ralis√©e-√†-gauche",
    "href": "Courses/notations.html#fonction-quantile-inverse-g√©n√©ralis√©e-√†-gauche",
    "title": "Notations et rappels",
    "section": "Fonction quantile, inverse g√©n√©ralis√©e √† gauche",
    "text": "Fonction quantile, inverse g√©n√©ralis√©e √† gauche\nLa fonction de r√©partition √©tant une fonction croissante on peut donner un sens √† son inverse g√©n√©ralis√©e de la mani√®re suivante.\n\nD√©finition 4 (Fonction quantile/ inverse g√©n√©ralis√©e üá¨üáß: quantile distribution function) \nSoit X une variable al√©atoire sur (\\mathbb{R}, \\mathcal{B}(\\mathbb{R})) et F_X sa fonction de r√©partition. La fonction quantile associ√©e F_X^\\leftarrow: ]0,1[ \\rightarrow \\mathbb{R} est d√©finie par \n  F_X^\\leftarrow(p)=  \\inf\\{ x \\in \\mathbb{R} \\colon F_X(x)\\geq p\\} \\enspace.\n\n\nOn parle parfois aussi d‚Äôinverse au sens de Levy pour cette inverse g√©n√©ralis√©e.\nDans le cas o√π la fonction de r√©partition F_X est bijective, alors l‚Äôinverse de la fonction de r√©partition coincide avec la fonction quantile.\nLa m√©diane est √©gale √† F_X^\\leftarrow(1/2), les premiers et troisi√®mes quartiles sont √©gaux √† F_X^\\leftarrow(1/4) et F_X^\\leftarrow(3/4). Enfin, les d√©ciles sont les quantiles F_X^\\leftarrow(k/10) pour k=1,\\dots, 9.",
    "crumbs": [
      "Cours",
      "Notations et rappels"
    ]
  },
  {
    "objectID": "Courses/notations.html#visualisation-densit√©-fonction-de-r√©partition-quantiles-etc.",
    "href": "Courses/notations.html#visualisation-densit√©-fonction-de-r√©partition-quantiles-etc.",
    "title": "Notations et rappels",
    "section": "Visualisation: densit√©, fonction de r√©partition, quantiles, etc.",
    "text": "Visualisation: densit√©, fonction de r√©partition, quantiles, etc.\n\nCas des variables continues\n\nObservablePython / Shiny\n\n\n\nPlotly = require('plotly.js-dist');\ndists = require( 'https://cdn.jsdelivr.net/gh/stdlib-js/stats-base-dists@umd/browser.js' );\n// see source here: https://github.com/stdlib-js/stats-base-dists/tree/umd\n// continuous case\njstatPDFs = () =&gt; {\n  const distributions = Object.keys(dists);\n  // Get in continuousDistributions the distributions whose pdf, cdf and quantile are defined\n    const continuousDistributions = distributions.filter(name =&gt; dists[name].pdf && dists[name].cdf && dists[name].quantile);\n  return continuousDistributions\n};\noutput = jstatPDFs();\n\nexcludedPDFs = [];\npdfNames_unsorted = output.filter(name =&gt; !excludedPDFs.includes(name));\npdfNames=pdfNames_unsorted.toSorted();\nviewof inputs = Inputs.form([\n      Inputs.range([-10, 10], {value: 0.1, step: 0.001, label: tex`\\mu`}),\n      Inputs.range([0.01, 5], {value: 1.01, step: 0.001, label: tex`\\sigma`}),\n      Inputs.range([0.001, 0.999], {value: 0.75, step: 0.001, label:tex`\\alpha`}),\n    ]);\n\nviewof distrib_name = Inputs.select(pdfNames, {value: \"normal\", label: \"Distribution\"});\n\n\n{\n  const x = d3.range(-5, 5, 0.01);\n  const pdf = x.map(x =&gt; dists[distrib_name].pdf(x, mu, sigma));\n  const cdf = x.map(x =&gt; dists[distrib_name].cdf(x, mu, sigma));\n  const inv = x.map(x =&gt; dists[distrib_name].quantile(x, mu, sigma));\n  const quantile = dists[distrib_name].quantile(alpha, mu, sigma);\n  const filteredX = x.filter(coord =&gt; coord &lt;= quantile);\n  const filteredPdf = pdf.filter((_, i) =&gt; x[i] &lt;= quantile);\n  const filteredCdf = cdf.filter(coord =&gt; coord &lt;= quantile);\n\n\n{\nvar trace1 = {\n      type: \"scatter\",\n      name: 'Quantile',\n      x : cdf,\n      y : x,\n      line: {color: 'black'},\n      xaxis: 'x1',\n      yaxis: 'y3',\n\n\n};\n\nvar trace12 = {\n        x : [alpha, alpha],\n        y : [x[0], quantile],\n        mode: 'lines',\n        line: {\n            dash: 'dash',\n            width: 1,\n            color: ' #428BCA',\n            marks: {\n                size: 0,\n                color: ' #428BCA',\n            }\n\n          },\n      xaxis: 'x1',\n      yaxis: 'y3',\n\n\n};\n\n\n\nvar trace13 = {\n    x: [0, alpha],\n    y: [quantile, quantile],\n    mode: 'scatter',\n    line: {\n        dash: 'dash',\n        width: 1,\n        color: ' #428BCA',\n        marks: {\n            size: 0,\n        }\n    },\n    xaxis: 'x1',\n    yaxis: 'y3'\n}\n\n\nvar trace2 = {\nx: cdf,\ny: cdf,\ntype: 'scatter',\nname: 'identity',\nline: {color: 'black'},\nxaxis: 'x1',\nyaxis: 'y2'\n\n};\n\n\nvar trace21 = {\n        x : [alpha, 1],\n        y : [alpha, alpha],\n        mode: 'lines',\n        line: {\n            dash: 'dash',\n            width: 1,\n            color: ' #428BCA',\n            marks: {\n                size: 0,\n                color: ' #428BCA',\n            }\n\n          },\n      xaxis: 'x1',\n      yaxis: 'y2'\n}\n\nvar trace22 = {\n        x : [alpha, alpha],\n        y : [alpha, 1],\n        mode: 'lines',\n        line: {\n            dash: 'dash',\n            width: 1,\n            color: ' #428BCA',\n            marks: {\n                size: 0,\n                color: ' #428BCA',\n            }\n\n          },\n      xaxis: 'x1',\n      yaxis: 'y2'\n}\n\n\n\nvar trace23 = {\n    x: [alpha],\n    y: [alpha],\n    mode: 'scatter',\n    line: {\n        dash: 'dash',\n        width: 1,\n        color: ' #428BCA',\n        marks: {\n            size: 0,\n        }\n    },\n    xaxis: 'x1',\n    yaxis: 'y2'\n}\n\n\nvar trace31 = {\n      type: \"scatter\",\n      mode: \"lines\",\n      name: 'PDF2',\n      x: x,\n      y: cdf,\n      line: {color: 'black'},\n      xaxis: 'x2',\n      yaxis: 'y2',\n};\n\nvar trace32 = {\n        x : filteredX,\n        y : filteredX.map(x =&gt; alpha),\n        mode: 'lines',\n        line: {\n            dash: 'dash',\n            width: 1,\n            color: ' #428BCA',\n            marks: {\n                size: 0,\n                color: ' #428BCA',\n            }\n          },\n      xaxis: 'x2',\n      yaxis: 'y2'\n}\n\nvar trace33 = {\n        x : [quantile, quantile],\n        y : [0, alpha],\n        mode: 'lines',\n        line: {\n            dash: 'dash',\n            width: 1,\n            color: ' #428BCA',\n            marks: {\n                size: 0,\n                color: ' #428BCA',\n            }\n          },\n\n      xaxis: 'x2',\n      yaxis: 'y2'\n}\n\nvar trace34 = {\n    x: [quantile],\n    y: [alpha],\n    mode: 'scatter',\n    line: {\n        dash: 'dash',\n        width: 1,\n        color: ' #428BCA',\n        marks: {\n            size: 0,\n        }\n    },\n    xaxis: 'x2',\n    yaxis: 'y2'\n}\n\n\nvar trace41 = {\n\n  type: \"scatter\",\n  name: 'Quantile2',\n  fill: 'tozeroy',\n  x : filteredX,\n  y : filteredPdf,\n  opacity: 0.9,\n  line: {color: ' #428BCA'},\n  xaxis: 'x2',\n  yaxis: 'y1'\n\n};\n\n\nvar trace42 = {\n\n  type: \"scatter\",\n  mode: \"lines\",\n  name: 'PDF2',\n  x: x,\n  y: pdf,\n  line: {color: 'black'},\n  xaxis: 'x2',\n  yaxis: 'y1'\n\n};\n\nvar data = [\n  trace1,\n  trace12, trace13,\n  trace2, trace21, trace22, trace23,\n  trace31, trace32, trace33, trace34,\n  trace41, trace42];\n\n\nvar layout = {\n\n  title: 'Distribution et quantile',\n  xaxis: {\n    domain: [0, 0.32],\n    anchor: 'y1'\n  },\n  yaxis: {\n    domain: [0, 0.24],\n    anchor: 'x1'\n\n  },\n  xaxis2: {\n    domain: [0.35, 1],\n    anchor: 'y'\n  },\n\n  yaxis2: {\n    domain: [0.26, 0.49],\n    anchor: 'x1'\n  },\n\n  yaxis3: {\n    domain: [0.5, 1],\n    anchor: 'x1'\n  },\n\n// legend offset\n  showlegend: false,\n  height: 680,\n  annotations: [\n\n    {\n      x: 1/4,\n      y: quantile - (x[0]-x.slice(-1))/20,\n      xref: 'x1',\n      yref: 'y3',\n      text: 'q= ' + quantile.toFixed(2),\n      font: {\n        size: 12,\n        color: '#428BCA',\n\n      },\n      showarrow: false,\n      arrowhead: 0,\n      ax: 25,\n      ay: -10,\n\n    },\n    {\n      x: 1/4,\n      y: alpha,\n      xref: 'x1',\n      yref: 'y2',\n      text: 'alpha= ' + alpha.toFixed(2),\n      font: {\n        size: 12,\n        color: '#428BCA',\n\n      },\n      showarrow: false,\n      arrowhead: 0,\n      ax: 25,\n      ay: -10,\n\n    },\n    {\n        text: \"Fonction quantile\",\n      font: {\n      size: 15,\n      color: 'black',\n    },\n    showarrow: false,\n    align: 'center',\n    x: -0.01,\n    y: 1.05,\n    xref: 'paper',\n    yref: 'paper',\n    },\n    {\n      text: \"Fonction de r√©partition\",\n      font: {\n      size: 15,\n      color: 'black',\n            },\n    showarrow: false,\n    align: 'center',\n    x: 0.65,\n    y: 0.52,\n    xref: 'paper',\n    yref: 'paper',\n    },\n    {\n      text: \"Fonction de densit√©\",\n      font: {\n      size: 15,\n      color: 'black',\n            },\n    showarrow: false,\n    align: 'center',\n    x: 0.65,\n    y: 0.225,\n    xref: 'paper',\n    yref: 'paper',\n    },\n  ]\n\n};\n// XXX: TODO: put the xticks labels on the middle plot for x and on the right plot for y\n    var config = {responsive: true}\n    const div = DOM.element('div');\n    Plotly.newPlot(div, data, layout, config);\n    return div;\n  }\n\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmu = inputs[0];\nsigma = inputs[1];\nalpha = inputs[2]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#| standalone: true\n#| viewerHeight: 830\nimport numpy as np\nfrom scipy import stats\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nfrom shiny import ui, render, App\nfrom shinywidgets import output_widget, render_widget\n\n\ndef keep_no_param_distribution():\n    distributions = stats._continuous_distns._distn_names\n    distributions_0 = []\n    for _, name in enumerate(distributions):\n        dist = getattr(stats, name)\n        if not dist.shapes or len(dist.shapes) == 0:\n            distributions_0.append(name)\n    distributions_0_val = [\n        getattr(stats.distributions, string) for string in distributions_0\n    ]\n    distributions_0_dict = dict(zip(distributions_0, distributions_0_val))\n    return distributions_0_dict\n\n\ndistributions_0_dict = keep_no_param_distribution()\n\nmu = 0\nsigma = 1\n\napp_ui = ui.page_fluid(\n    ui.div(\n        ui.input_slider(\"alpha\", \"Quantile\", 0.01, 0.99, value=0.5, step=0.01),\n        ui.input_slider(\"xrange\", \"x-range\", -10, 10, value=(-5, 5), step=0.2),\n        ui.input_select(\n            \"distrib\",\n            \"Distribution\",\n            list(distributions_0_dict.keys()),\n            selected='norm'\n        ),\n        class_=\"d-flex gap-3\",\n    ),\n    output_widget(\"my_widget\"),\n)\n\n\ndef server(input, output, session):\n    @output\n    @render_widget\n    def my_widget():\n        fig = make_subplots(\n            rows=3,\n            cols=2,\n            vertical_spacing=0.1,\n            horizontal_spacing=0.15,\n            subplot_titles=(\n                \"Fonction quantile\",\n                \"\",\n                \"\",\n                \"Fonction de r√©partition\",\n                \"\",\n                \"Densit√© et quantile\",\n            ),\n            column_widths=[0.2, 0.5],\n            row_heights=[0.35, 0.17, 0.17],\n        )\n\n        alpha = input.alpha()\n        distribution = distributions_0_dict[input.distrib()]\n        x = np.linspace(input.xrange()[0], input.xrange()[1], num=400)\n        cdf_data = distribution.cdf(x, loc=mu, scale=sigma)\n        pdf_data = distribution.pdf(x, loc=mu, scale=sigma)\n        q_alpha = distribution.ppf(alpha, loc=mu, scale=sigma)\n\n        fig.update_layout(autosize=True, height=700)\n\n        # Quantile plot\n        fig.add_trace(\n            go.Scatter(\n                x=cdf_data, y=x, mode=\"lines\", marker={\"color\": \"black\"}\n            ),\n            row=1,\n            col=1,\n        )\n        # Diagonal\n        fig.add_trace(\n            go.Scatter(\n                x=cdf_data, y=cdf_data, mode=\"lines\", marker={\"color\": \"black\"}\n            ),\n            row=2,\n            col=1,\n        )\n        # Cdf part\n        fig.add_trace(\n            go.Scatter(\n                x=x, y=cdf_data, mode=\"lines\", marker={\"color\": \"black\"}\n            ),\n            row=2,\n            col=2,\n        )\n        # pdf part\n        fig.add_scatter(\n            x=x[x &lt; q_alpha],\n            y=pdf_data[x &lt; q_alpha],\n            fill=\"tozeroy\",\n            mode=\"none\",\n            fillcolor=\"rgb(66, 139, 202)\",\n            row=3,\n            col=2,\n        )\n\n        fig.add_trace(\n            go.Scatter(\n                x=x, y=pdf_data, mode=\"lines\", marker={\"color\": \"black\"}\n            ),\n            row=3,\n            col=2,\n        )\n\n        # Dots\n        fig.add_trace(\n            go.Scatter(\n                x=[alpha],\n                y=[q_alpha],\n                mode=\"markers\",\n                marker={\"color\": \"rgb(66, 139, 202)\"},\n                marker_symbol=\"x\",\n                marker_size=8,\n            ),\n            row=1,\n            col=1,\n        )\n\n        fig.add_trace(\n            go.Scatter(\n                x=[alpha],\n                y=[alpha],\n                mode=\"markers\",\n                marker={\"color\": \"rgb(66, 139, 202)\"},\n                marker_symbol=\"x\",\n                marker_size=8,\n            ),\n            row=2,\n            col=1,\n        )\n\n        fig.add_trace(\n            go.Scatter(\n                x=[q_alpha],\n                y=[alpha],\n                mode=\"markers\",\n                marker={\"color\": \"rgb(66, 139, 202)\"},\n                marker_symbol=\"x\",\n                marker_size=8,\n            ),\n            row=2,\n            col=2,\n        )\n\n        # Lines\n        fig.add_trace(\n            go.Scatter(\n                x=[alpha, alpha],\n                y=[x[0], q_alpha],\n                mode=\"lines\",\n                line=dict(dash=\"dash\", color=\"rgb(66, 139, 202)\")\n            ),\n            row=1,\n            col=1\n        )\n\n        fig.add_trace(\n            go.Scatter(\n                x=[alpha, alpha],\n                y=[alpha, 1.],\n                mode=\"lines\",\n                line=dict(dash=\"dash\", color=\"rgb(66, 139, 202)\")\n            ),\n            row=2,\n            col=1\n        )\n        fig.add_trace(\n            go.Scatter(\n                x=[alpha, 1],\n                y=[alpha, alpha],\n                mode=\"lines\",\n                line=dict(dash=\"dash\", color=\"rgb(66, 139, 202)\")\n            ),\n            row=2,\n            col=1\n        )\n        fig.add_trace(\n            go.Scatter(\n                x=[x[0], q_alpha],\n                y=[alpha, alpha],\n                mode=\"lines\",\n                line=dict(dash=\"dash\", color=\"rgb(66, 139, 202)\")\n            ),\n            row=2,\n            col=2\n        )\n        fig.add_trace(\n            go.Scatter(\n                x=[x[0], q_alpha],\n                y=[x[0], 0],\n                mode=\"lines\",\n                line=dict(dash=\"dash\", color=\"rgb(66, 139, 202)\")\n            ),\n            row=2,\n            col=2\n        )\n        # Axes ranges\n        fig.update_xaxes(range=[0, 1.], row=1, col=1)\n        fig.update_yaxes(matches=\"x6\", row=1, col=1)\n\n        fig.update_yaxes(range=[0, 1.], row=2, col=1)\n        fig.update_xaxes(matches=\"x1\", row=2, col=1)\n\n        fig.update_yaxes(rangemode=\"tozero\", row=3, col=2)\n        fig.update_xaxes(range=[x[0], x[-1]], row=3, col=2)\n\n        fig.update_xaxes(matches=\"x6\", row=2, col=2)\n        fig.update_yaxes(matches=\"y3\", row=2, col=2)\n\n        # Add dropdown\n        fig.update_layout(\n            showlegend=False,\n            template=\"simple_white\",\n        )\n        return fig\n\n\napp = App(app_ui, server)\n\n\n\n\n\n\nCas des variables discr√®tes\n\nObservablePython / Shiny\n\n\n\ndiscretePDFs = () =&gt; {\n  const distributions = Object.keys(dists);\n  // Get in continuousDistributions the distributions whose pdf, cdf and quantile are defined\n    const continuousDistributions = distributions.filter(name =&gt; dists[name].pmf && dists[name].cdf && dists[name].quantile);\n  return continuousDistributions\n};\noutput_discr = discretePDFs();\n\npmfNames_unsorted = output_discr.filter(name =&gt; !excludedPDFs.includes(name));\npmfNames=pmfNames_unsorted.toSorted();\nviewof inputs_disc = Inputs.form([\n      Inputs.range([-10, 10], {value: 0.1, step: 0.001, label: tex`\\mu `}),\n      Inputs.range([0.01, 5], {value: 1.01, step: 0.001, label: tex`\\sigma `}),\n      Inputs.range([0.001, 0.999], {value: 0.75, step: 0.001, label:tex`\\alpha`}),\n    ]);\n\nviewof distrib_name_discr = Inputs.select(pmfNames, {value: \"normal\", label: \"Distribution\"});\n\n\n{\n  const z = d3.range(-5, 5, 0.01);\n  const pmf = z.map(z =&gt; dists[distrib_name_discr].pmf(z, mu_disc, sigma_disc));\n  const cdf = z.map(z =&gt; dists[distrib_name_discr].cdf(z, mu_disc, sigma_disc));\n  const inv = z.map(z =&gt; dists[distrib_name_discr].quantile(z, mu_disc, sigma_disc));\n  const quantile = dists[distrib_name_discr].quantile(alpha_disc, mu_disc, sigma_disc);\n  const filteredX = z.filter(coord =&gt; coord &lt;= quantile);\n  const filteredPmf = pmf.filter((_, i) =&gt; z[i] &lt;= quantile);\n  const filteredCdf = cdf.filter(coord =&gt; coord &lt;= quantile);\n\n\n{\nvar trace1 = {\n      type: \"scatter\",\n      name: 'Quantile',\n      x : cdf,\n      y : z,\n      line: {color: 'black'},\n      xaxis: 'x1',\n      yaxis: 'y3',\n\n\n};\n\nvar trace12 = {\n        x : [alpha_disc, alpha_disc],\n        y : [z[0], quantile],\n        mode: 'lines',\n        line: {\n            dash: 'dash',\n            width: 1,\n            color: ' #428BCA',\n            marks: {\n                size: 0,\n                color: ' #428BCA',\n            }\n\n          },\n      xaxis: 'x1',\n      yaxis: 'y3',\n\n\n};\n\n\n\nvar trace13 = {\n    x: [0, alpha_disc],\n    y: [quantile, quantile],\n    mode: 'scatter',\n    line: {\n        dash: 'dash',\n        width: 1,\n        color: ' #428BCA',\n        marks: {\n            size: 0,\n        }\n    },\n    xaxis: 'x1',\n    yaxis: 'y3'\n}\n\n\nvar trace2 = {\nx: cdf,\ny: cdf,\ntype: 'scatter',\nname: 'identity',\nline: {color: 'black'},\nxaxis: 'x1',\nyaxis: 'y2'\n\n};\n\n\nvar trace21 = {\n        x : [alpha_disc, 1],\n        y : [alpha_disc, alpha_disc],\n        mode: 'lines',\n        line: {\n            dash: 'dash',\n            width: 1,\n            color: ' #428BCA',\n            marks: {\n                size: 0,\n                color: ' #428BCA',\n            }\n\n          },\n      xaxis: 'x1',\n      yaxis: 'y2'\n}\n\nvar trace22 = {\n        x : [alpha_disc, alpha_disc],\n        y : [alpha_disc, 1],\n        mode: 'lines',\n        line: {\n            dash: 'dash',\n            width: 1,\n            color: ' #428BCA',\n            marks: {\n                size: 0,\n                color: ' #428BCA',\n            }\n\n          },\n      xaxis: 'x1',\n      yaxis: 'y2'\n}\n\nvar trace23 = {\n    x: [alpha_disc],\n    y: [alpha_disc],\n    mode: 'scatter',\n    line: {\n        dash: 'dash',\n        width: 1,\n        color: ' #428BCA',\n        marks: {\n            size: 0,\n        }\n    },\n    xaxis: 'x1',\n    yaxis: 'y2'\n}\n\n\nvar trace31 = {\n      type: \"scatter\",\n      mode: \"lines\",\n      name: 'PDF2',\n      x: z,\n      y: cdf,\n      line: {color: 'black'},\n      xaxis: 'x2',\n      yaxis: 'y2',\n};\n\nvar trace32 = {\n        x : filteredX,\n        y : filteredX.map(z =&gt; alpha_disc),\n        mode: 'lines',\n        line: {\n            dash: 'dash',\n            width: 1,\n            color: ' #428BCA',\n            marks: {\n                size: 0,\n                color: ' #428BCA',\n            }\n          },\n      xaxis: 'x2',\n      yaxis: 'y2'\n}\n\nvar trace33 = {\n        x : [quantile, quantile],\n        y : [0, alpha_disc],\n        mode: 'lines',\n        line: {\n            dash: 'dash',\n            width: 1,\n            color: ' #428BCA',\n            marks: {\n                size: 0,\n                color: ' #428BCA',\n            }\n          },\n\n      xaxis: 'x2',\n      yaxis: 'y2'\n}\n\nvar trace34 = {\n    x: [quantile],\n    y: [alpha_disc],\n    mode: 'scatter',\n    line: {\n        dash: 'dash',\n        width: 1,\n        color: ' #428BCA',\n        marks: {\n            size: 0,\n        }\n    },\n    xaxis: 'x2',\n    yaxis: 'y2'\n}\n\n\nvar trace41 = {\n\n  type: \"scatter\",\n  name: 'Quantile2',\n  fill: 'tozeroy',\n  x : filteredX,\n  y : filteredPmf,\n  opacity: 0.9,\n  line: {color: ' #428BCA'},\n  xaxis: 'x2',\n  yaxis: 'y1'\n\n};\n\n\nvar trace42 = {\n\n  type: \"scatter\",\n  mode: \"lines\",\n  name: 'PDF2',\n  x: z,\n  y: pmf,\n  line: {color: 'black'},\n  xaxis: 'x2',\n  yaxis: 'y1'\n\n};\n\nvar data = [\n  trace1,\n  trace12, trace13,\n  trace2, trace21, trace22, trace23,\n  trace31, trace32, trace33, trace34,\n  trace41, trace42];\n\n\nvar layout = {\n\n  title: 'Distribution et quantile',\n  xaxis: {\n    domain: [0, 0.32],\n    anchor: 'y1'\n  },\n  yaxis: {\n    domain: [0, 0.24],\n    anchor: 'x1'\n\n  },\n  xaxis2: {\n    domain: [0.35, 1],\n    anchor: 'y'\n  },\n\n  yaxis2: {\n    domain: [0.26, 0.49],\n    anchor: 'x1'\n  },\n\n\n\n  yaxis3: {\n    domain: [0.5, 1],\n    anchor: 'x1'\n\n  },\n\n  showlegend: false,\n  height: 680,\n\n\n  annotations: [\n\n    {\n      x: 1/4,\n      y: quantile - (z[0]-z.slice(-1))/20,\n      xref: 'x1',\n      yref: 'y3',\n      text: 'q=' + quantile.toFixed(2),\n      font: {\n        size: 12,\n        color: '#428BCA',\n\n      },\n      showarrow: false,\n      arrowhead: 0,\n      ax: 25,\n      ay: -10,\n\n    },\n    {\n      x: 1/4,\n      y: alpha_disc,\n      xref: 'x1',\n      yref: 'y2',\n      text: 'alpha=' + alpha_disc.toFixed(2),\n      font: {\n        size: 12,\n        color: '#428BCA',\n\n      },\n      showarrow: false,\n      arrowhead: 0,\n      ax: 25,\n      ay: -10,\n\n    },\n    {\n        text: \"Fonction quantile\",\n      font: {\n      size: 15,\n      color: 'black',\n    },\n    showarrow: false,\n    align: 'center',\n    x: -0.01,\n    y: 1.05,\n    xref: 'paper',\n    yref: 'paper',\n    },\n    {\n      text: \"Fonction de r√©partition\",\n      font: {\n      size: 15,\n      color: 'black',\n            },\n    showarrow: false,\n    align: 'center',\n    x: 0.65,\n    y: 0.52,\n    xref: 'paper',\n    yref: 'paper',\n    },\n    {\n      text: \"Fonction de densit√©\",\n      font: {\n      size: 15,\n      color: 'black',\n            },\n    showarrow: false,\n    align: 'center',\n    x: 0.65,\n    y: 0.225,\n    xref: 'paper',\n    yref: 'paper',\n    },\n  ]\n\n};\n// XXX: TODO: put the xticks labels on the middle plot for x and on the right plot for y\n\n    var config = {responsive: true}\n    const div = DOM.element('div');\n    Plotly.newPlot(div, data, layout, config);\n    return div;\n  }\n\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmu_disc = inputs_disc[0];\nsigma_disc = inputs_disc[1];\nalpha_disc = inputs_disc[2];\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#| standalone: true\n#| viewerHeight: 830\nimport numpy as np\nfrom scipy import stats\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nfrom shiny import ui, render, App\nfrom shinywidgets import output_widget, render_widget\n\n\ndef keep_no_param_distribution_disc():\n    distributions = stats._discrete_distns._distn_names\n    distributions_0 = [name for name in distributions if not getattr(stats, name).shapes or len(getattr(stats, name).shapes) in [1, 2]]\n    distributions_0_val = [getattr(stats.distributions, string) for string in distributions_0]\n    distributions_0_dict = dict(zip(distributions_0, distributions_0_val))\n    return distributions_0_dict\n\ndef cdf_tool(x, dtype='int64'):\n    y = np.zeros(2*(len(x)), dtype=dtype)\n    y[::2]=x\n    y[1::2]=x\n    return y[1::], y[:-1], y\n\ndef pmf_tool(x, dtype='int64'):\n    y = np.zeros(2*(len(x)), dtype=dtype)\n    y[::2]=x\n    return y[1::], y[:-1], y\n\ndef insert_nones(my_list):\n    for i, val in enumerate(my_list):\n        if i % 3 == 2:\n            my_list.insert(i, None)\n    return my_list\n\ndistributions_0_dict = keep_no_param_distribution_disc()\n\napp_ui = ui.page_fluid(\n    ui.div(\n        ui.input_slider(\"alpha\", \"Quantile\", 0.01, 0.99, value=0.5, step=0.01),\n        ui.input_slider(\"xrange\", \"x-range\", -10, 10, value=(-5.5, 5.5), step=0.2),\n        ui.input_select(\n            \"distrib\",\n            \"Distribution\",\n            list(distributions_0_dict.keys()),\n            selected='poisson'\n        ),\n        class_=\"d-flex gap-3\",\n    ),\n    output_widget(\"my_widget\"),\n)\n\n\ndef server(input, output, session):\n    @output\n    @render_widget\n    def my_widget():\n        fig = make_subplots(\n            rows=3,\n            cols=2,\n            vertical_spacing=0.1,\n            horizontal_spacing=0.15,\n            subplot_titles=(\n                \"Fonction quantile\",\n                \"\",\n                \"\",\n                \"Fonction de r√©partition\",\n                \"\",\n                \"Fonction de masse et quantile\",\n            ),\n            column_widths=[0.2, 0.5],\n            row_heights=[0.35, 0.17, 0.17],\n        )\n\n\n        alpha = input.alpha()\n        # alpha=0.5\n\n        mu = 0.5  # Param needed for some distribution\n        if input.distrib()=='zipf':\n            mu = 2\n        distribution = distributions_0_dict[input.distrib()]\n        # distribution=distributions_0_dict['poisson']\n        x = np.arange(np.floor(input.xrange()[0]), np.ceil(input.xrange()[1]))\n        # x = np.arange(np.floor(-5.5), np.ceil(5.5))\n\n        cdf_data = distribution.cdf(x, mu)\n        pmf_data = distribution.pmf(x, mu)\n        q_alpha = distribution.ppf(alpha, mu)\n        support = pmf_data.nonzero()[0]\n        fig.update_layout(autosize=True, height=700)\n\n        # Quantile plot\n        new_x, new_y, new_z = cdf_tool(support)\n        _, _, new_pmf = pmf_tool(support)\n\n        fig.add_trace(\n            go.Scatter(\n                x=insert_nones(list(np.append(cdf_data[new_y[::-1]], distribution.cdf(x[0], mu)))),\n                y=insert_nones(list(np.append(x[new_x[::-1]], x[new_x[0]]))),\n                mode=\"lines\",\n                line=dict(color=\"black\")\n            ),\n            row=1,\n            col=1,\n        )\n        fig.add_trace(\n             go.Scatter(\n                x=cdf_data[support], y=x[support],\n                mode=\"markers\", marker={\"color\": \"black\"}\n            ),\n            row=1,\n            col=1,\n        )\n        # Diagonal\n        fig.add_trace(\n            go.Scatter(\n                x=cdf_data, y=cdf_data, mode=\"lines\", marker={\"color\": \"black\"}\n            ),\n            row=2,\n            col=1,\n        )\n        # Cdf part\n        fig.add_trace(\n            go.Scatter(\n                x=x[support], y=cdf_data[support],\n                mode=\"markers\", marker={\"color\": \"black\"}\n            ),\n            row=2,\n            col=2,\n        )\n        fig.add_trace(\n            go.Scatter(\n                x=insert_nones(list(np.append(np.insert(x[new_x], 0, [x[0], x[new_x[0]]]),x[-1]))),\n                y=insert_nones(list(np.append(np.insert(cdf_data[new_y], 0, [0,0]), cdf_data[-1]))),\n                mode=\"lines\",\n                line=dict(color=\"black\")\n            ),\n            row=2,\n            col=2\n        )\n\n        fig.add_trace(\n            go.Scatter(\n                x=x, y=pmf_data, mode=\"markers\", marker={\"color\": \"black\"}\n            ),\n            row=3,\n            col=2,\n        )\n        fig.add_trace(\n            go.Scatter(\n                x=x, y=pmf_data, mode=\"markers\", marker={\"color\": \"black\"}\n            ),\n            row=3,\n            col=2,\n        )\n        x_bar = insert_nones(list(x[new_z]))\n        y_bar = insert_nones(list(pmf_data[new_pmf]))\n        fig.add_trace(\n            go.Scatter(\n                x=x_bar,\n                y=y_bar,\n                mode=\"lines\",\n                line=dict(color=\"black\")\n            ),\n            row=3,\n            col=2\n        )\n        _,_, devil_x = cdf_tool(x[x&lt;=q_alpha])\n        _,_, devil_y = cdf_tool(pmf_data[x&lt;q_alpha], dtype='float64')\n\n        x_bar_blue = insert_nones(list(devil_x))\n        y_bar_blue = np.array(insert_nones(list(devil_y)))\n        y_bar_blue[::-3]=0.\n        y_bar_blue = list(y_bar_blue)\n        fig.add_trace(\n            go.Scatter(\n                x=x_bar_blue,\n                y=y_bar_blue,\n                mode=\"lines\",\n                line=dict(color=\"rgb(66, 139, 202)\")\n            ),\n            row=3,\n            col=2\n        )\n        # pdf part\n        fig.add_scatter(\n            x=x[x &lt;= q_alpha],\n            y=pmf_data[x &lt;= q_alpha],\n            mode=\"markers\",\n            marker={\"color\":\"rgb(66, 139, 202)\"},\n            row=3,\n            col=2,\n        )\n\n        # Dots\n        fig.add_trace(\n            go.Scatter(\n                x=[alpha],\n                y=[q_alpha],\n                mode=\"markers\",\n                marker={\"color\": \"rgb(66, 139, 202)\"},\n                marker_symbol=\"x\",\n                marker_size=8,\n            ),\n            row=1,\n            col=1,\n        )\n\n        fig.add_trace(\n            go.Scatter(\n                x=[alpha],\n                y=[alpha],\n                mode=\"markers\",\n                marker={\"color\": \"rgb(66, 139, 202)\"},\n                marker_symbol=\"x\",\n                marker_size=8,\n            ),\n            row=2,\n            col=1,\n        )\n\n        fig.add_trace(\n            go.Scatter(\n                x=[q_alpha],\n                y=[alpha],\n                mode=\"markers\",\n                marker={\"color\": \"rgb(66, 139, 202)\"},\n                marker_symbol=\"x\",\n                marker_size=8,\n            ),\n            row=2,\n            col=2,\n        )\n\n        # Lines\n        fig.add_trace(\n            go.Scatter(\n                x=[alpha, alpha],\n                y=[x[0], q_alpha],\n                mode=\"lines\",\n                line=dict(dash=\"dash\", color=\"rgb(66, 139, 202)\")\n            ),\n            row=1,\n            col=1\n        )\n\n        fig.add_trace(\n            go.Scatter(\n                x=[alpha, alpha],\n                y=[alpha, 1.],\n                mode=\"lines\",\n                line=dict(dash=\"dash\", color=\"rgb(66, 139, 202)\")\n            ),\n            row=2,\n            col=1\n        )\n        fig.add_trace(\n            go.Scatter(\n                x=[alpha, 1],\n                y=[alpha, alpha],\n                mode=\"lines\",\n                line=dict(dash=\"dash\", color=\"rgb(66, 139, 202)\")\n            ),\n            row=2,\n            col=1\n        )\n        fig.add_trace(\n            go.Scatter(\n                x=[x[0], q_alpha],\n                y=[alpha, alpha],\n                mode=\"lines\",\n                line=dict(dash=\"dash\", color=\"rgb(66, 139, 202)\")\n            ),\n            row=2,\n            col=2\n        )\n        fig.add_trace(\n            go.Scatter(\n                x=[x[0], q_alpha],\n                y=[x[0], 0],\n                mode=\"lines\",\n                line=dict(dash=\"dash\", color=\"rgb(66, 139, 202)\")\n            ),\n            row=2,\n            col=2\n        )\n        # Axes ranges\n        fig.update_xaxes(range=[0, 1.05], row=1, col=1)\n        fig.update_yaxes(matches=\"x6\", row=1, col=1)\n\n        fig.update_yaxes(range=[0, 1.05], row=2, col=1)\n        fig.update_xaxes(matches=\"x1\", row=2, col=1)\n\n        fig.update_yaxes(rangemode=\"tozero\", row=3, col=2)\n        fig.update_xaxes(range=[x[0], x[-1]], row=3, col=2)\n\n        fig.update_xaxes(matches=\"x6\", row=2, col=2)\n        fig.update_yaxes(matches=\"y3\", row=2, col=2)\n\n        # Add dropdown\n        fig.update_layout(\n            showlegend=False,\n            template=\"simple_white\",\n        )\n        return fig\n\n\napp = App(app_ui, server)",
    "crumbs": [
      "Cours",
      "Notations et rappels"
    ]
  },
  {
    "objectID": "Courses/loi_normale_multi.html",
    "href": "Courses/loi_normale_multi.html",
    "title": "Loi normale: cas multivari√©",
    "section": "",
    "text": "Loi normale multivari√©e: \\mu \\in \\mathbb{R}^p et \\Sigma \\in \\mathbb{R}^{p \\times p} (sym√©trique et d√©finie positive). \nX \\sim \\mathcal{N}(\\mu,\\Sigma), \\quad \\forall x \\in \\mathbb{R}^p\n\nDensit√© de probabilit√© : \n\\phi(x) = \\frac{1}{ \\sqrt{(2\\pi)^p |\\Sigma|}}  \\exp\\Big( -\\tfrac{1}{2}(x-\\mu)^\\top\\Sigma^{-1}(x - \\mu)   \\Big)\n En dimension p=2, la matrice de covariance \\Sigma peut toujours s‚Äô√©crire comme suit, et la visualisation suivante montre l‚Äôimpact des diff√©rents param√®tres sur la densit√© de probabilit√©. \n\\Sigma =\n\\begin{pmatrix}\\cos(\\theta) & - \\sin(\\theta)\\\\  \\sin(\\theta)& \\cos(\\theta)\n\\end{pmatrix} \\cdot\n\\begin{pmatrix}\\sigma_1 & 0\\\\ 0 & \\sigma_2\n\\end{pmatrix}\\cdot\n\\begin{pmatrix}\n\\cos(\\theta) &\\sin(\\theta)\\\\  -\\sin(\\theta)& \\cos(\\theta)\\end{pmatrix}\n\nPlotly = require('plotly.js-dist');\ndists = require( 'https://cdn.jsdelivr.net/gh/stdlib-js/stats-base-dists@umd/browser.js' );\njstat = require('jstat');\nmath = require(\"mathjs\");\n// see source here: https://github.com/stdlib-js/stats-base-dists/tree/umd\n// continuous case\n\nviewof inputs = Inputs.form([\n  Inputs.range([-1, 1], {label: tex`\\mu_1`, step: 0.1}, {value: 0}),\n  Inputs.range([-1, 1], {label: tex`\\mu_2`, step: 0.1}, {value: 0}),\n  Inputs.range([0.1, 2], {label: tex`\\sigma_1`, step: 0.1, value: 1}),\n  Inputs.range([0.1, 2], {label: tex`\\sigma_2`, step: 0.1, value: 1}),\n  Inputs.range([0, 6.29], {label: tex`\\theta`, step: 0.01, value: 0}),\n  Inputs.range([1, 1000], {label: tex`n`, step: 1}),\n  Inputs.button(\"Re-Tirage\")\n])\n\n\nmu1 = inputs[0];\nmu2 = inputs[1];\nsigma1 = inputs[2];\nsigma2 = inputs[3];\ntheta = inputs[4];\nn_samples = inputs[5];\n\n\nfunction create_sigma(theta, sigma1, sigma2){\n  const mat_rot = math.matrix([[math.cos(theta), -math.sin(theta)], [math.sin(theta), math.cos(theta)]]);\n  const mat_sigma = math.matrix([[sigma1**2, 0], [0, sigma2**2]]);\n  return math.multiply(mat_rot, math.multiply(mat_sigma, math.transpose(mat_rot)));\n}\n\n\nfunction mvnpdf(x, mu, Sigma){\n  const p = 2;\n    return (2*math.pi)**(-p/2)*math.det(Sigma)**(-0.5)*\n      math.exp(-0.5*math.multiply( math.multiply( math.transpose(math.subtract(x,mu)), math.inv(Sigma)), math.subtract(x,mu)));\n}\n\n{\n\n\nfunction normal_rng(mu, Sigma, n=100) {\n    // Compute the Cholesky decomposition of Sigma\n    var cholesky = jstat.cholesky(Sigma);\n\n    // Generate the samples\n    var samples = Array.from({length: n}, () =&gt; {\n        var x = jstat.normal.sample(0, 1);\n        var y = jstat.normal.sample(0, 1);\n        // Transform the standard normal random variables using the Cholesky decomposition\n        var transformedX = mu[0] + cholesky[0][0] * x + cholesky[0][1] * y;\n        var transformedY = mu[1] + cholesky[1][0] * x + cholesky[1][1] * y;\n        return [transformedX, transformedY];\n    });   \n\n    return samples;\n}\nvar Sigma = create_sigma(theta, sigma1, sigma2).toArray();\nvar mu = [mu1, mu2];\n\nvar samples = normal_rng(mu, Sigma, 1000);\nvar npoints=100, mini = -5, maxi=5, x = new Array(npoints), y = new Array(npoints), z = new Array(npoints), i, j;\n\n//  Densit√©:\nfor(var i = 0; i &lt; npoints; i++) {\n    x[i] = mini + i * (maxi - mini) / (npoints - 1);\n    y[i] = mini + i * (maxi - mini) / (npoints - 1);\n    z[i] = new Array(npoints);\n    }\n\nfor(var i = 0; i &lt; npoints; i++) {\n    for(j = 0; j &lt; npoints; j++) {\n\n        z[j][i] = mvnpdf([x[i], x[j]], mu, Sigma);\n    }\n\n}\n  \n\n{\n\nvar trace1 = {\n        x: samples.slice(0, n_samples).map(sample =&gt; sample[0]),\n        y: samples.slice(0, n_samples).map(sample =&gt; sample[1]),\n        mode: 'markers',\n        type: 'scatter',\n        marker: {\n            color: 'rgba(0,0,0,0.5)',\n            size: 5,\n        },\n\n        xaxis: 'x2',\n}\n\nvar trace22 = {\n        x: x,\n        y: y,\n        z: z,\n        type: 'surface',\n        colorscale: 'Oranges',\n        showscale: false,\n        color: {\n            legend: false,\n            label: \"pdf\",\n        },\n\n}\n\nvar trace21 = {\n        x: x,\n        y: y,\n        z: z,\n        type: 'contour',\n        colorscale: 'Oranges',\n        color: {\n            legend: true,\n            label: \"pdf\",\n        },\n        blur: 4,\n        xlim: [-5, 5],\n        ylim: [-5, 5],\n        xaxis: 'x2',\n}\n\n\nvar data = [\n  trace21,\n  trace22,\n  trace1,\n  ];\n\n\n  var layout = {\n       yaxis2: {\n          range: [-5, 5],\n          autorange: false,\n        },\n       xaxis2: {\n          range: [-5, 5],\n          autorange: false,\n        },\n\n      scene: {\n          camera: {\n              eye: {\n                  x: 0.5,\n                  y: 1.2,\n                  z: 1.5,\n              }\n          }\n      },\n    grid: {\n      rows: 1,\n      columns: 2,\n      subplots: [['xy','x2y']],\n    },\n    showlegend: false,\n\n  };\n\n    var config = {responsive: true}\n    const div = DOM.element('div');\n    Plotly.newPlot(div, data, layout, config);\n    return div;\n  }\n\n}",
    "crumbs": [
      "Cours",
      "Loi normale: cas multivari√©"
    ]
  },
  {
    "objectID": "Courses/loi_normale_multi.html#introduction",
    "href": "Courses/loi_normale_multi.html#introduction",
    "title": "Loi normale: cas multivari√©",
    "section": "",
    "text": "Loi normale multivari√©e: \\mu \\in \\mathbb{R}^p et \\Sigma \\in \\mathbb{R}^{p \\times p} (sym√©trique et d√©finie positive). \nX \\sim \\mathcal{N}(\\mu,\\Sigma), \\quad \\forall x \\in \\mathbb{R}^p\n\nDensit√© de probabilit√© : \n\\phi(x) = \\frac{1}{ \\sqrt{(2\\pi)^p |\\Sigma|}}  \\exp\\Big( -\\tfrac{1}{2}(x-\\mu)^\\top\\Sigma^{-1}(x - \\mu)   \\Big)\n En dimension p=2, la matrice de covariance \\Sigma peut toujours s‚Äô√©crire comme suit, et la visualisation suivante montre l‚Äôimpact des diff√©rents param√®tres sur la densit√© de probabilit√©. \n\\Sigma =\n\\begin{pmatrix}\\cos(\\theta) & - \\sin(\\theta)\\\\  \\sin(\\theta)& \\cos(\\theta)\n\\end{pmatrix} \\cdot\n\\begin{pmatrix}\\sigma_1 & 0\\\\ 0 & \\sigma_2\n\\end{pmatrix}\\cdot\n\\begin{pmatrix}\n\\cos(\\theta) &\\sin(\\theta)\\\\  -\\sin(\\theta)& \\cos(\\theta)\\end{pmatrix}\n\nPlotly = require('plotly.js-dist');\ndists = require( 'https://cdn.jsdelivr.net/gh/stdlib-js/stats-base-dists@umd/browser.js' );\njstat = require('jstat');\nmath = require(\"mathjs\");\n// see source here: https://github.com/stdlib-js/stats-base-dists/tree/umd\n// continuous case\n\nviewof inputs = Inputs.form([\n  Inputs.range([-1, 1], {label: tex`\\mu_1`, step: 0.1}, {value: 0}),\n  Inputs.range([-1, 1], {label: tex`\\mu_2`, step: 0.1}, {value: 0}),\n  Inputs.range([0.1, 2], {label: tex`\\sigma_1`, step: 0.1, value: 1}),\n  Inputs.range([0.1, 2], {label: tex`\\sigma_2`, step: 0.1, value: 1}),\n  Inputs.range([0, 6.29], {label: tex`\\theta`, step: 0.01, value: 0}),\n  Inputs.range([1, 1000], {label: tex`n`, step: 1}),\n  Inputs.button(\"Re-Tirage\")\n])\n\n\nmu1 = inputs[0];\nmu2 = inputs[1];\nsigma1 = inputs[2];\nsigma2 = inputs[3];\ntheta = inputs[4];\nn_samples = inputs[5];\n\n\nfunction create_sigma(theta, sigma1, sigma2){\n  const mat_rot = math.matrix([[math.cos(theta), -math.sin(theta)], [math.sin(theta), math.cos(theta)]]);\n  const mat_sigma = math.matrix([[sigma1**2, 0], [0, sigma2**2]]);\n  return math.multiply(mat_rot, math.multiply(mat_sigma, math.transpose(mat_rot)));\n}\n\n\nfunction mvnpdf(x, mu, Sigma){\n  const p = 2;\n    return (2*math.pi)**(-p/2)*math.det(Sigma)**(-0.5)*\n      math.exp(-0.5*math.multiply( math.multiply( math.transpose(math.subtract(x,mu)), math.inv(Sigma)), math.subtract(x,mu)));\n}\n\n{\n\n\nfunction normal_rng(mu, Sigma, n=100) {\n    // Compute the Cholesky decomposition of Sigma\n    var cholesky = jstat.cholesky(Sigma);\n\n    // Generate the samples\n    var samples = Array.from({length: n}, () =&gt; {\n        var x = jstat.normal.sample(0, 1);\n        var y = jstat.normal.sample(0, 1);\n        // Transform the standard normal random variables using the Cholesky decomposition\n        var transformedX = mu[0] + cholesky[0][0] * x + cholesky[0][1] * y;\n        var transformedY = mu[1] + cholesky[1][0] * x + cholesky[1][1] * y;\n        return [transformedX, transformedY];\n    });   \n\n    return samples;\n}\nvar Sigma = create_sigma(theta, sigma1, sigma2).toArray();\nvar mu = [mu1, mu2];\n\nvar samples = normal_rng(mu, Sigma, 1000);\nvar npoints=100, mini = -5, maxi=5, x = new Array(npoints), y = new Array(npoints), z = new Array(npoints), i, j;\n\n//  Densit√©:\nfor(var i = 0; i &lt; npoints; i++) {\n    x[i] = mini + i * (maxi - mini) / (npoints - 1);\n    y[i] = mini + i * (maxi - mini) / (npoints - 1);\n    z[i] = new Array(npoints);\n    }\n\nfor(var i = 0; i &lt; npoints; i++) {\n    for(j = 0; j &lt; npoints; j++) {\n\n        z[j][i] = mvnpdf([x[i], x[j]], mu, Sigma);\n    }\n\n}\n  \n\n{\n\nvar trace1 = {\n        x: samples.slice(0, n_samples).map(sample =&gt; sample[0]),\n        y: samples.slice(0, n_samples).map(sample =&gt; sample[1]),\n        mode: 'markers',\n        type: 'scatter',\n        marker: {\n            color: 'rgba(0,0,0,0.5)',\n            size: 5,\n        },\n\n        xaxis: 'x2',\n}\n\nvar trace22 = {\n        x: x,\n        y: y,\n        z: z,\n        type: 'surface',\n        colorscale: 'Oranges',\n        showscale: false,\n        color: {\n            legend: false,\n            label: \"pdf\",\n        },\n\n}\n\nvar trace21 = {\n        x: x,\n        y: y,\n        z: z,\n        type: 'contour',\n        colorscale: 'Oranges',\n        color: {\n            legend: true,\n            label: \"pdf\",\n        },\n        blur: 4,\n        xlim: [-5, 5],\n        ylim: [-5, 5],\n        xaxis: 'x2',\n}\n\n\nvar data = [\n  trace21,\n  trace22,\n  trace1,\n  ];\n\n\n  var layout = {\n       yaxis2: {\n          range: [-5, 5],\n          autorange: false,\n        },\n       xaxis2: {\n          range: [-5, 5],\n          autorange: false,\n        },\n\n      scene: {\n          camera: {\n              eye: {\n                  x: 0.5,\n                  y: 1.2,\n                  z: 1.5,\n              }\n          }\n      },\n    grid: {\n      rows: 1,\n      columns: 2,\n      subplots: [['xy','x2y']],\n    },\n    showlegend: false,\n\n  };\n\n    var config = {responsive: true}\n    const div = DOM.element('div');\n    Plotly.newPlot(div, data, layout, config);\n    return div;\n  }\n\n}",
    "crumbs": [
      "Cours",
      "Loi normale: cas multivari√©"
    ]
  },
  {
    "objectID": "Courses/loi_normale1D.html",
    "href": "Courses/loi_normale1D.html",
    "title": "Loi normale: cas 1D",
    "section": "",
    "text": "On consid√®re ici \\mathbb{R}^d muni du produit scalaire euclidien \\langle \\cdot, \\cdot \\rangle et de la norme euclidienne \\|\\cdot\\| associ√©e.",
    "crumbs": [
      "Cours",
      "Loi normale: cas 1D"
    ]
  },
  {
    "objectID": "Courses/loi_normale1D.html#la-loi-normale",
    "href": "Courses/loi_normale1D.html#la-loi-normale",
    "title": "Loi normale: cas 1D",
    "section": "La loi normale",
    "text": "La loi normale",
    "crumbs": [
      "Cours",
      "Loi normale: cas 1D"
    ]
  },
  {
    "objectID": "Courses/loi_normale1D.html#d√©finitions-et-propri√©t√©s-de-la-loi-normale",
    "href": "Courses/loi_normale1D.html#d√©finitions-et-propri√©t√©s-de-la-loi-normale",
    "title": "Loi normale: cas 1D",
    "section": "D√©finitions et propri√©t√©s de la loi normale",
    "text": "D√©finitions et propri√©t√©s de la loi normale\nOn rappelle que la loi normale de param√®tres \\mu \\in \\mathbb{R} et \\nu &gt; 0 a une densit√© donn√©e pour tout x \\in \\mathbb{R} par\n\n    \\varphi_{\\mu, \\nu}(x)=\\frac{1}{\\sqrt{2 \\pi \\nu}}\\exp\\Big(-\\frac{(x-\\mu)^2}{2\\nu}\\Big)\\enspace.\n\nOn parle aussi souvent de loi gaussienne, en hommage au math√©maticien Carl Friedrich Gauss, le prince des math√©maticiens1.\n1¬†Carl Friedrich Gauss: (1777-1855) math√©maticien, astronome et physicien n√© √† Brunswick, directeur de l‚Äôobservatoire de G√∂ttingen de 1807 jusqu‚Äô√† sa mort en 1855 On note X \\sim \\mathcal{N}(\\mu, \\nu), si X est une variable al√©atoire ayant pour densit√© \\varphi_{\\mu, \\nu}. Notons que si X \\sim \\mathcal{N}(\\mu,\\nu), alors X a pour esp√©rance \\mu et pour variance \\nu. Le cas particulier \\mu=0 et \\nu=1 correspond √† une variable al√©atoire dite centr√©e r√©duite.\nLa loi normale v√©rifie la propri√©t√© de stabilit√© par transformation affine : si X \\sim \\mathcal{N}(\\mu, \\nu) et si (a,b) \\in \\mathbb{R}^* \\times \\mathbb{R}, alors la variable al√©atoire a X + b suit une loi normale \\mathcal{N}(a\\mu + b, a^2 \\nu). On peut donc facilement passer d‚Äôune loi normale centr√©e r√©duite √† une loi normale quelconque via une transformation affine :\n\nsi X \\sim \\mathcal{N}(0,1), alors \\sqrt{\\nu} X + \\mu \\sim \\mathcal{N}(\\mu, \\nu),\nsi X \\sim \\mathcal{N}(\\mu, \\nu), alors (X-\\mu)/\\sqrt{\\nu} \\sim \\mathcal{N}(0,1).\n\nAinsi, savoir simuler une loi normale centr√©e r√©duite, permet de simuler n‚Äôimporte quelle loi normale.\nRappelons enfin que la fonction caract√©ristique d‚Äôune variable al√©atoire X \\sim \\mathcal{N}(\\mu, \\nu) est donn√©e pour tout t \\in \\mathbb{R} par \n\\begin{align*}\n\\phi_{\\mu, \\nu}(t) & \\triangleq \\mathbb{E}(e^{i t X})  \\\\\n\\phi_X(t) & = \\exp\\Big( i \\mu t - \\frac{\\nu t^2}{2}\\Big)\\enspace.\n\\end{align*}\n\n\nSimulation d‚Äôune loi normale\n\n\n\n\n\n\nUne mauvaise piste pour simuler une loi normale\n\n\n\nOn peut simuler une loi normale √† partir de variables al√©atoires uniformes U_1, \\dots, U_n iid en appliquant le th√©or√®me central limite √† \n    \\frac{U_1 + \\cdots + U_n - n/2}{\\sqrt{n/12}}\\,.\n Cependant, cette m√©thode ne donne qu‚Äôune approximation d‚Äôune loi normale. Par ailleurs, la vitesse de convergence √©tant relativement lente (de l‚Äôordre de \\sqrt n), il faudra simuler beaucoup de variables al√©atoires uniformes pour avoir une approximation correcte, ce qui demande un temps de calcul assez √©lev√©.\n\n\n\nChangement de variables\nLe th√©or√®me suivant permet de passer de la loi d‚Äôun couple (X,Y) √† celle de (U,V) = \\phi(X,Y), o√π \\phi est un C^1-diff√©omorphisme, c‚Äôest-√†-dire une application bijective dont la r√©ciproque est √©galement de classe C^1.\nPour cela rappelons que la jacobienne de \\phi^{-1} correspond √† la matrice (application lin√©aire) des d√©riv√©es partielles. Ainsi, si \\phi(x,y) = (u,v) \\iff (x,y) = \\phi^{-1}(u,v), alors \n\\begin{align*}\n{\\rm{J}}_{\\phi^{-1}}: (u,v) & \\mapsto\n\\begin{pmatrix}\n  \\frac{\\partial x}{\\partial u} & \\frac{\\partial x}{\\partial v}    \\\\\n  \\frac{\\partial y}{\\partial u} & \\frac{\\partial y}{\\partial v}\n\\end{pmatrix} \\enspace.\n\\end{align*}\n\n\nTh√©or√®me 1 (Caract√©risation de la loi d‚Äôune variable al√©atoire r√©elle) Soit (X,Y) un vecteur al√©atoire de densit√© f_{(X,Y)} d√©finie sur l‚Äôouvert A \\subset \\mathbb{R}^2 et \\phi : A \\to B \\subset \\mathbb{R}^2 un C^1-diff√©omorphisme. Le vecteur al√©atoire (U,V)=\\phi(X,Y) admet alors pour densit√© f_{(U,V)} d√©finie sur B pour tout (u,v) \\in \\mathbb{R}^2 par \n\\begin{align*}\n    (u,v) & \\mapsto\n    f_{(X,Y)} (\\phi^{-1}(u,v)) |\\det ({\\rm{J}}_{\\phi^{-1}} (u,v))| {1\\hspace{-3.8pt} 1}_B(u,v)\\enspace.\n\\end{align*}\n\n\nOn a √©nonc√© le r√©sultat en dimension 2 par simplicit√©. Il s‚Äô√©tend bien √©videmment √† une dimension d quelconque. En particulier, pour d=1, on retrouve le changement de variable classique dans le cas de l‚Äôint√©gration d‚Äôune fonction √† valeurs r√©elles.\nD√©monstration.\nOn rappelle que la loi de (U,V) est caract√©ris√©e par les quantit√©s \\mathbb{E}[h(U,V)] pour tout h : \\mathbb{R}^2 \\to \\mathbb{R} mesurable born√©e. On consid√®re donc une telle fonction h et on applique la formule de transfert : \n\\begin{align*}\n  \\mathbb{E}[h(U,V)] &\n  =\\mathbb{E}[h(\\phi(X,Y))]\\\\\n& = \\int_{\\mathbb{R}^2} h(\\phi(x,y)) f_{(X,Y)}(x,y) \\, dx dy \\\\\n& = \\int_{A} h(\\phi(x,y)) f_{(X,Y)}(x,y) \\, d x d y\\enspace.\n\\end{align*}\n On applique alors la formule du changement de variables vu en th√©orie de l‚Äôint√©gration avec (u,v) = \\phi(x,y) \\iff \\phi^{-1}(u,v) = (x,y) : \n\\begin{align*}\n  & \\mathbb{E}[h(U,V)]\\\\\n  & = \\!\\int_{B}  \\!\\!\\! h(u,v) f_{(X,Y)}(\\phi^{-1}(u,v)) |\\det ({\\rm{J}}_{\\phi^{-1}} (u,v))| \\, d u d v\\\\\n  & = \\!\\int_{\\mathbb{R}^2} \\!\\!\\!\\! h(u,v) f_{(X,Y)}(\\phi^{-1}(u,v)) |\\det ({\\rm{J}}_{\\phi^{-1}} (u,v))| {1\\hspace{-3.8pt} 1}_B(u,v)\\, d u d v .\n\\end{align*}\n  ce qui donne le r√©sultat voulu.\n\n\n\n\n\n‚ñ°\n\n\n\n\nExemple 1 (Exemple : loi de \\cos(X), avec X \\sim \\mathcal{U}(]0,\\pi[)) Donnons un exemple dans le cas r√©el. On consid√®re une variable al√©atoire X de loi uniforme sur ]0,\\pi[. Sa densit√© est donn√©e par f_X(x) = {1\\hspace{-3.8pt} 1}_{]0,\\pi[}(x)/\\pi. On pose U = \\cos(X) et on souhaite d√©terminer la loi de U.\nOn applique le th√©or√®me pr√©c√©dent avec la fonction \\phi^{-1}(u) = \\arccos(u) sur ]-1,1[. La densit√© de U est alors donn√©e pour tout u \\in \\mathbb{R} par \n\\begin{align*}\n  f_U(u)\n& = \\frac{{1\\hspace{-3.8pt} 1}_{]0,\\pi[}(\\arccos(u))}{\\pi} \\Big| \\frac{-1}{\\sqrt{1-u^2}} \\Big| {1\\hspace{-3.8pt} 1}_{]-1,1[}(u)\\\\\n& = \\frac{1}{\\pi \\sqrt{1-u^2}} {1\\hspace{-3.8pt} 1}_{]-1,1[}(u)\\enspace.\n\\end{align*}",
    "crumbs": [
      "Cours",
      "Loi normale: cas 1D"
    ]
  },
  {
    "objectID": "Courses/loi_normale1D.html#m√©thode-de-box-m√ºller",
    "href": "Courses/loi_normale1D.html#m√©thode-de-box-m√ºller",
    "title": "Loi normale: cas 1D",
    "section": "M√©thode de Box-M√ºller",
    "text": "M√©thode de Box-M√ºller\nUn cas particulier fondamental de la formule de changement de variables concerne le passage en coordonn√©es polaires. Cette transformation est d√©finie via l‚Äôapplication \n    \\begin{array}{ccccc}\n        \\phi^{-1} & : & ]0, \\infty[ \\times ]0, 2\\pi[ & \\to     & \\mathbb{R}^2 \\setminus ([0,\\infty[ \\times \\{0\\}) \\\\\n                  &   & \\begin{pmatrix} r \\\\ \\theta \\end{pmatrix}                   & \\mapsto & \\begin{pmatrix} r \\cos(\\theta) \\\\ r \\sin(\\theta) \\end{pmatrix}\\,.\n    \\end{array}\n L‚Äôexpression de \\phi ne nous sera pas utile. On peut tout de m√™me la donner au passage :\n\n    \\begin{array}{ccccc}\n        \\phi & : & \\mathbb{R}^2 \\setminus ([0,\\infty[ \\times \\{0\\}) & \\to     & ]0, \\infty[ \\times ]0, 2\\pi[                                                      \\\\\n             &   & \\begin{pmatrix} x \\\\ y \\end{pmatrix}                                            & \\mapsto & \\begin{pmatrix}\\sqrt{x^2+y^2} \\\\ 2 \\arctan \\Big( \\frac{y}{x+\\sqrt{x^2+y^2}} \\Big)\\end{pmatrix}\\,.\n    \\end{array}\n\nIci, le jacobien de \\phi^{-1} est la matrice \n    {\\rm{J}}_{\\phi^{-1}} (r,\\theta)\n    =\n    \\begin{pmatrix}\n        \\cos(\\theta) & -r \\sin(\\theta) \\\\\n        \\sin(\\theta) & r \\cos(\\theta)\n    \\end{pmatrix}\\,,\n qui v√©rifie |\\det({\\rm{J}}_{\\phi^{-1}} (r, \\theta))| = r. Ainsi, si (X,Y) a pour densit√© f_{(X,Y)}, alors (R, \\Theta) = \\phi(X,Y) a pour densit√© \n  f_{(R, \\Theta)} (r, \\theta)\n  = r\\cdot f_{(X,Y)}(r \\cos(\\theta), r \\sin(\\theta)) \\!\\cdot\\! {1\\hspace{-3.8pt} 1}_{]0, \\infty[}(r)  {1\\hspace{-3.8pt} 1}_{]0, 2 \\pi[}(\\theta).\n\nDans le cas o√π X et Y sont des variables al√©atoires gaussiennes ind√©pendantes, on obtient le r√©sultat suivant.\n\nTh√©or√®me 2 (M√©thode de Box-M√ºller) Soit X et Y deux variables al√©atoires ind√©pendantes de loi normales centr√©es r√©duites : X,Y \\sim \\mathcal{N}(0,1). Le couple de variables al√©atoires polaires (R, \\Theta) = \\phi^{-1}(X,Y) a pour densit√© \n            f_{R, \\Theta}(r,\\theta)\n            = \\Big( r \\cdot e^{-\\tfrac{r^2}{2}} {1\\hspace{-3.8pt} 1}_{]0, \\infty[}(r) \\Big) \\bigg(\\frac{{1\\hspace{-3.8pt} 1}_{]0, 2 \\pi[}(\\theta)}{2 \\pi} \\bigg)\\,.\n Autrement dit, elles sont ind√©pendantes, l‚Äôangle \\Theta suit une loi uniforme sur ]0, 2\\pi[ et la distance √† l‚Äôorigine R suit une loi de Rayleigh donn√©e par la densit√© \n    f_R(r) =  r \\cdot e^{-r^2/2} {1\\hspace{-3.8pt} 1}_{]0, \\infty[}(r)\\,, \\quad r &gt; 0\\,.\n\n\nD√©monstration: La densit√© du couple (X,Y) est donn√©e par \n  f_{(X,Y)}(x,y) = \\frac{1}{2\\pi} e^{-\\frac{x^2+y^2}{2}}\\,, \\quad x,y \\in \\mathbb{R}\\,.\n Le th√©or√®me pr√©c√©dent donne alors la densit√© de (R, \\Theta) : \n\\begin{align*}\n  f_{(R, \\Theta)} (r, \\theta) &\n  = r\\cdot f_{(X,Y)}(r \\cos(\\theta), r \\sin(\\theta)) \\!\\cdot\\! {1\\hspace{-3.8pt} 1}_{]0, \\infty[}(r)  {1\\hspace{-3.8pt} 1}_{]0, 2 \\pi[}(\\theta)\\\\\n  &= r \\cdot\\frac{1}{2\\pi} e^{-\\frac{r^2}{2}} \\cdot  {1\\hspace{-3.8pt} 1}_{]0, \\infty[}(r) {1\\hspace{-3.8pt} 1}_{]0, 2 \\pi[}(\\theta)\\,,\n\\end{align*}\n ce qui conclut la preuve.\n\n\n\n\n\n‚ñ°\n\n\nNotons que R^2 est √† valeurs dans ]0,\\infty[ et v√©rifie pour x &gt; 0: \n\\begin{align*}\n    \\mathbb{P}(R^2 &gt; x)\n    &= \\mathbb{P}(R &gt; \\sqrt x)\\\\\n    &= \\int_{\\sqrt{x}}^{\\infty} r e^{-r^2/2}\\, dr\\\\\n    &= \\Big[- e^{-\\tfrac{r^2}{2}} \\Big]_{\\sqrt{x}}^{\\infty}\\\\\n    &= e^{-\\tfrac{x}{2}}\\,.\n\\end{align*}\n On reconna√Æt la fonction de survie d‚Äôune loi exponentielle de param√®tre 1/2. Or, on a vu (cf.¬†m√©thode de l‚Äôinverse) que si U suit une loi uniforme sur [0,1], alors -2 \\ln(U) suit une loi exponentielle de param√®tre 1/2, donc \\sqrt{-2 \\ln(U)} a la m√™me loi que R.\nL‚Äôalgorithme de Box-M√ºller s‚Äôen suit: si U et V sont des v.a. ind√©pendantes de loi uniforme sur [0,1] et qu‚Äôon d√©finit X et Y par \n\\begin{cases}\n  X = \\sqrt{-2 \\ln(U)} \\cos(2\\pi V)\\\\\n  Y = \\sqrt{-2 \\ln(U)} \\sin(2\\pi V)\\,.\n\\end{cases}\n alors X et Y des variables al√©atoires gaussiennes centr√©es r√©duites ind√©pendantes.\n\n\n\n\n\n\nNote\n\n\n\nCet algorithme n‚Äôest en fait pas souvent utilis√© en pratique : il fait appel √† des fonctions dont l‚Äô√©valuation est co√ªteuse (logarithme, cosinus, sinus). Pour s‚Äôaffranchir des fonctions trigonom√©triques, une version modifi√©e de l‚Äôalgorithme de Box-M√ºller a √©t√© propos√©e : la m√©thode de Marsaglia, qui s‚Äôappuie sur des variables al√©atoires uniformes sur le disque unit√© (voir l‚Äôexercice d√©di√© en TD). Une autre alternative est la m√©thode de Ziggurat.\n\n\nPlotly = require('plotly.js-dist');\ndists = require( 'https://cdn.jsdelivr.net/gh/stdlib-js/stats-base-dists@umd/browser.js' );\njstat = require('jstat');\nmath = require(\"mathjs\");\n// see source here: https://github.com/stdlib-js/stats-base-dists/tree/umd\n// continuous case\n\nviewof inputs = Inputs.form([\n  Inputs.range([-2, 2], {label: tex`\\mu`, step: 0.1}, {value: 0}),\n  Inputs.range([0.1, 10], {label: tex`\\nu`, step: 0.1, value: 1}),\n  Inputs.range([1, 1000], {label: tex`n`, step: 1}),\n  Inputs.button(\"Re-Tirage\")\n])\n\n\nmu = inputs[0];\nnu = inputs[1];\nn_samples = inputs[2];\n\n{\n\nfunction mvnpdf(x, mu, nu){\n    return 1.0 / (2*math.pi *nu)**(0.5)* math.exp(-0.5*(x-mu)**2 / nu);\n}\n\n\nfunction normal_rng(mu, nu, n=100){\n    var samples = Array.from({length: n}, () =&gt; {\n        var x = jstat.normal.sample(mu, nu**0.5);\n        return x;\n    });\n\n    return samples;\n}\n\nvar samples = normal_rng(mu, nu, 1000);\nvar samples_jitter = normal_rng(0, 0.03, 1000)\nvar npoints=500, mini = -10, maxi=10, x = new Array(npoints), z = new Array(npoints), i, j;\n\n//  Densit√©:\nfor(var i = 0; i &lt; npoints; i++) {\n    x[i] = mini + i * (maxi - mini) / (npoints - 1);\n    z[i] = mvnpdf(x[i], mu, nu);\n    }\n\n{\nvar trace1 = {\n        x: samples.slice(0, n_samples),\n        y: samples_jitter.slice(0, n_samples),\n        mode: 'markers',\n        type: 'scatter',\n        marker: {\n            color: 'rgba(0,0,0,0.5)',\n            size: 5,\n        },\n\n  }\n\nvar trace22 = {\n        x: x,\n        y: z,\n        type: \"scatter\",\n        mode: \"lines\",\n        name: 'Pdf',\n        line: {color: 'black'},\n        yaxis: 'y2',\n        xaxis: 'x2',\n        }\n\n\nvar data = [\n  trace1,\n  trace22,\n  ];\n\n\nvar layout = {\n\n    yaxis: {domain: [0, 0.2],\n            showticklabels: false,\n            range: [-0.6, 0.6],\n            autorange: false},\n    xaxis2: {matches: 'x',\n              range: [-10, 10],\n          autorange: false},\n    yaxis2: {domain: [0.29, 0.99]},\n\n  showlegend: false,\n\n};\n\n    var config = {responsive: true}\n    const div = DOM.element('div');\n    Plotly.newPlot(div, data, layout, config);\n    return div;\n    }\n}",
    "crumbs": [
      "Cours",
      "Loi normale: cas 1D"
    ]
  },
  {
    "objectID": "Courses/loi_normale1D.html#lois-autour-de-la-loi-normale",
    "href": "Courses/loi_normale1D.html#lois-autour-de-la-loi-normale",
    "title": "Loi normale: cas 1D",
    "section": "Lois autour de la loi normale",
    "text": "Lois autour de la loi normale\n\nLoi du \\chi^2\nConcernant la prononciation, on prononce ‚Äúkhi-deux‚Äù le nom de cette loi.\n\nD√©finition 1 (Loi du \\chi^2) Soit X_1, \\dots, X_k des variables al√©atoires iid de loi normale centr√©e r√©duite. La loi de la variable al√©atoire X = X_1^2 + \\dots + X_k^2 est appel√©e loi du \\chi^2 √† k degr√©s de libert√©. Sa densit√© est donn√©e par \nf(x) = \\frac{1}{2^{\\frac{k}{2}}\\Gamma(\\frac{k}{2})} x^{\\frac{k}{2}-1} e^{-x/2}\\,, \\quad x \\geq 0\\,,\n o√π \\Gamma d√©signe la fonction gamma d‚ÄôEuler : \n\\Gamma(x) = \\int_0^{\\infty} t^{x-1} e^{-t}\\,  dt\\,.\n On note alors X \\sim \\chi^2(k).\n\nAu vu de sa d√©finition, la simulation d‚Äôune loi du \\chi^2 est claire : on simule k variables al√©atoires gaussiennes centr√©es r√©duites ind√©pendantes et on somme leur carr√©s.\nPreuve: Montrons pour k=1 que la densit√© est bien de la forme pr√©c√©dente, c‚Äôest-√†-dire \n    f(x) = \\frac{1}{\\sqrt{2\\pi}} \\frac{e^{-x/2}}{\\sqrt x}\\,, \\quad x \\geq 0\\,,\n o√π on a utilis√© la relation \\Gamma(1/2) = \\sqrt \\pi (int√©grale de Gauss).\nSoit h : \\mathbb{R} \\to \\mathbb{R} une fonction mesurable born√©e. On a \n\\begin{align*}\n    \\mathbb{E}[h(X_1^2)]\n    & = \\int_\\mathbb{R} h(x^2) \\frac{e^{-\\frac{x^2}{2}}}{\\sqrt{2 \\pi}} \\, dx\\\\\n    & = \\int_{-\\infty}^0 h(x^2) \\frac{e^{-\\frac{x^2}{2}}}{\\sqrt{2 \\pi}} \\, dx\n    + \\int_0^{\\infty} h(x^2) \\frac{e^{-\\frac{x^2}{2}}}{\\sqrt{2 \\pi}} \\, dx\\,.\n\\end{align*}\n En effectuant le changement de variable x=-\\sqrt u dans la premi√®re int√©grale et x=\\sqrt u dans la deuxi√®me, on obtient \n    \\mathbb{E}[h(X_1^2)]\n    = \\int_{\\infty}^0 h(u) \\frac{e^{-u/2}}{\\sqrt{2 \\pi}} \\frac{ du}{2 \\sqrt u}\n    + \\int_0^{\\infty} h(u) \\frac{e^{-u/2}}{\\sqrt{2 \\pi}} \\frac {du}{2 \\sqrt u}\\,.\n Les deux int√©grales √©tant √©gales, on conclut que \n    \\mathbb{E}[h(X_1^2)] = \\int_0^{\\infty} h(u) \\frac{e^{-u/2}}{\\sqrt{2 \\pi}\\sqrt u}\\, du\\,,\n ce qui prouve le r√©sultat pour k=1.\nLa g√©n√©ralisation √† k quelconque se fait par r√©currence: on utilise la formule de convolution des la loi pour obtenir la loi pour k+1:\n\n\\begin{align*}\n    X_1^2 + \\dots + X_k^2 + X_{k+1}^2\n    & = (X_1^2 + \\dots + X_k^2) + X_{k+1}^2\\\\\n    & = \\chi^2(k) + X_{k+1}^2\\,.\n\\end{align*}\n Ainsi, \n\\begin{align*}\n    f_{\\chi^2(k+1)}(x)\n    & = \\int_0^{\\infty} f_{\\chi^2(k)}(x-y) f_{X_{k+1}^2}(y) \\, dy\\\\\n    & = \\int_0^x \\frac{1}{2^{\\frac{k}{2}}\\Gamma(\\frac{k}{2})} (x-y)^{\\frac{k}{2}-1} e^{-\\frac{x-y}{2}} \\tfrac{e^{-\\frac{y}{2}}}{\\sqrt{2\\pi y}} \\, dy\\\\\n    & = \\frac{e^{-\\frac{x}{2}}}{2^{\\frac{k+1}{2}} \\Gamma(\\frac{k}{2})  \\Gamma(\\frac{1}{2})}\\int_0^x  (x-y)^{\\frac{k}{2}-1}  \\tfrac{1}{\\sqrt{ y}} \\, dy\\\\\n    & = \\frac{e^{-\\frac{x}{2}}}{2^{\\frac{k+1}{2}} \\Gamma(\\frac{k}{2})  \\Gamma(\\frac{1}{2})} x \\int_0^1  (x-ux)^{\\frac{k}{2}-1}  \\tfrac{1}{\\sqrt{xu}} \\, du\\\\\n\\end{align*}\n avec le changement de variable y=ux. Ensuite, \n\\begin{align*}\n    f_{\\chi^2(k+1)}(x)\n    & = \\frac{x^{\\frac{k+1}{2}} e^{-\\frac{x}{2}}}{2^{\\frac{k+1}{2}} \\Gamma(\\frac{k}{2})  \\Gamma(\\frac{1}{2})}  \\int_0^1  (1-u)^{\\frac{k}{2}-1}  u^{1/2-1} du\\enspace.\n\\end{align*}\n Or rappelons que si \\Beta(a,b) = \\int_0^1 (1-u)^{a-1} u^{b-1} \\, du, alors pour tout a,b \\in [0,+\\infty[, \\Beta(a,b)=\\frac{\\Gamma(a)\\Gamma(b)}{\\Gamma(a+b)}. En effet, en faisant le changement de variable dans l‚Äôint√©grale double qui suit: \n\\begin{array}{ccccc}\n    \\psi & : & \\mathbb{R}^+ \\times \\mathbb{R}^+ & \\to     & \\mathbb{R}^+\\times ]0,1[                 \\\\\n            &   & (s,t)                          & \\mapsto & \\Big(s+t, \\frac{t}{s+t}\\Big)\\,,\n\\end{array}\n c‚Äôest-√†-dire \\psi^{-1}(r,w) = (r(1-w), rw), et le jacobien est donn√© par J_{\\psi^{-1}}(r,w) = \\begin{pmatrix} 1-w & -r \\\\ w & r \\end{pmatrix}, et donc J_{\\psi^{-1}}(r,w)=r, on obtient: \n\\begin{align*}\n\\Gamma(a)\\Gamma(b) & = \\int_0^{\\infty} t^{a-1} e^{-t} \\, dt \\int_0^{\\infty} s^{b-1} e^{-s} \\, ds\\\\\n& = \\int_0^{\\infty} \\int_0^{\\infty} e^{-t-s} t^{a-1} s^{b-1} \\, dt \\, ds\\\\\n& = \\int_0^{1} \\int_0^{\\infty} e^{-r} (rw)^{a-1} (r(1-w))^{b-1} r \\, dr \\, dw\\\\\n& = \\int_0^1 w^{a-1} (1-w)^{b-1} \\int_0^{\\infty} e^{-r} r^{a+b-1} \\, dr \\, dw\\\\\n\\end{align*}\n et donc \\Beta(a,b)=\\frac{\\Gamma(a)\\Gamma(b)}{\\Gamma(a+b)}. En appliquant cette relation pour a=\\frac{k}{2} et b=1/2, on obtient\n\n\\begin{align*}\n    f_{\\chi^2(k+1)}(x)\n    & = \\frac{x^{\\frac{k+1}{2}} e^{-\\frac{x}{2}}}{2^{\\frac{k+1}{2}} \\Gamma(\\frac{k+1}{2})} \\enspace.\n\\end{align*}\n Le r√©sultat est donc prouv√© par r√©currence.\n\n\n\n\n\n‚ñ°\n\n\n\n\n\nLoi de Student\n\nD√©finition 2 (Loi de Student) Soit X \\sim \\mathcal{N}(0,1) et Y \\sim \\chi^2(k) deux variables al√©atoires ind√©pendantes. La loi de la variable al√©atoire V = \\frac{X}{\\sqrt{Y/k}} est appel√©e loi de Student √† k degr√©s de libert√©. Elle admet pour densit√© \n    f_V(t)\n    = \\dfrac{1}{\\sqrt{k \\pi}} \\dfrac{\\Gamma(\\frac{k+1}{2})}{\\Gamma(\\frac{k}{2})} \\Big(1+\\dfrac{t^2}{k}\\Big)^{-\\frac{k+1}{2}}\\,,\n    \\quad t \\in \\mathbb{R}\\,.\n\n\nLa loi de Student correspond donc au ratio d‚Äôune loi normale par la racine carr√©e d‚Äôune loi du \\chi^2(k) normalis√©e. Ce ratio appara√Æt souvent en statistique lors de la construction d‚Äôintervalles de confiance. Cette loi a √©t√© d√©crite en 1908 par William Gosset2.\n2¬†William Gosset: (1876-1937) statisticien et chimiste anglais. Alors qu‚Äôil √©tait employ√© √† la brasserie Guinness √† Dublin. Son employeur lui refusant le droit de publier sous son propre nom, W. Gosset choisit un pseudonyme, Student (üá´üá∑: √©tudiant). Au vu de la proposition pr√©c√©dente, simuler une loi de Student est assez simple : on simule k+1 loi normales ind√©pendantes X_1, \\ldots, X_{k+1} et on consid√®re \n    V = \\dfrac{\\sqrt{k} X_{k+1}}{\\sqrt{X_1^2+\\cdots + X_k^2}}\\,.\n\nPreuve de la formule de la densit√©:\nOn applique pour cela la formule du changement de variables avec la transformation \n    \\begin{array}{ccccc}\n        \\phi & : & \\mathbb{R}^* \\times ]0, \\infty[ & \\to     & \\mathbb{R}^* \\times \\mathbb{R}^*      \\\\\n             &   & (x,y)                           & \\mapsto & \\Big(x, \\dfrac{x}{\\sqrt{y/k}}\\Big)\\,,\n    \\end{array}\n c‚Äôest-√†-dire \n    \\phi^{-1}(u,v) = \\Big(u, k\\dfrac{u^2}{v^2}\\Big)\\,.\n La fonction \\phi^{-1} a pour matrice jacobienne \n    J_{\\phi^{-1}} (u,v)\n    =\n    \\begin{pmatrix}\n        1              & 0                  \\\\\n        \\frac{2k}{v^2} & \\frac{-2ku^2}{v^3}\n    \\end{pmatrix}\\,,\n dont le d√©terminant vaut \\frac{-2ku^2}{v^3}. Par ailleurs, les variables al√©atoires X et Y √©tant ind√©pendantes, la densit√© du couple (X,Y) correspond au produit des densit√©s : \n    f_{(X,Y)}(x,y) = \\dfrac{1}{\\sqrt{2 \\pi}} e^{-\\frac{x^2}{2}} \\dfrac{1}{2^{\\frac{k}{2}2} \\Gamma(\\frac{k}{2})} y^{\\frac{k}{2}-1} e^{-\\frac{y}{2}} {1\\hspace{-3.8pt} 1}_{]0, \\infty[}(y)\\, \\quad x,y \\in \\mathbb{R}\\,.\n Tout est pr√™t pour appliquer la th√©or√®me du changement de variables qui assure que la densit√© du couple (U,V) est donn√©e par \n    f_{(U,V)}(u,v)\n    = \\dfrac{1}{\\sqrt{2 \\pi}} e^{-\\frac{u^2}{2}} \\dfrac{1}{2^{\\frac{k}{2}} \\Gamma(\\frac{k}{2})} \\bigg(\\dfrac{k u^2}{v^2}\\bigg)^{\\frac{k}{2}-1} e^{-\\frac{1}{2} \\frac{k u^2}{v^2}} \\dfrac{2 k u^2}{(v^2)^{\\frac{3}{2}}} {1\\hspace{-3.8pt} 1}_{\\mathbb{R}^*}(v)\\,.\n Il suffit alors de marginaliser pour obtenir la densit√© de V, ce qui s‚Äôeffectue en calculant l‚Äôint√©grale \n    \\int_\\mathbb{R} f_{(U,V)} (u,v) \\, du\\,.\n Les termes en u de l‚Äôexpression pr√©c√©dente s‚Äôint√®gre en \n\\begin{align*}\n    \\int_{-\\infty}^\\infty e^{-\\frac{u^2}{2}(1+\\frac{k}{v^2})} (u^2)^{\\frac{k}{2}} \\, d u\n     & = \\int_0^\\infty e^{-s} \\bigg( \\dfrac{2 s}{1+\\frac{k}{v^2}} \\bigg)^{\\frac{k}{2}} \\sqrt{\\dfrac{2}{1+\\frac{k}{v^2}}} \\dfrac{ds}{2\\sqrt{s}} \\\\\n     & = \\frac{2^{\\frac{k+1}{2}}}{2} \\bigg(1+\\dfrac{k}{v^2}\\bigg)^{- \\frac{k+1}{2}} \\int_0^\\infty e^{-s} s^{\\frac{k}{2} - \\frac{1}{2}} \\, ds\\,,\n\\end{align*}\n o√π la premi√®re √©galit√© r√©sulte du changement de variable \n    s = \\dfrac{u^2}{2} \\bigg( 1 + \\dfrac{k}{v^2} \\bigg) \\iff \\sqrt{\\dfrac{2s }{1+\\frac{k}{v^2}}} = u\\,.\n On reconna√Æt dans l‚Äôint√©grale la valeur de \\Gamma(\\frac{k+1}{2}) ce qui conduit √† \n    f_V(v)\n    =\n    \\dfrac{1}{\\sqrt{2 \\pi}} \\dfrac{1}{2^{\\frac{k}{2}} \\Gamma(\\frac{k}{2})} \\bigg(\\dfrac{k}{v^2}\\bigg)^{\\frac{k}{2}-1} \\dfrac{2 k}{(v^2)^{\\frac{3}{2}}}\n    \\frac{2^{\\frac{k+1}{2}}}{2} \\bigg(1+\\dfrac{k}{v^2}\\bigg)^{- \\frac{k+1}{2}} \\Gamma\\bigg(\\frac{k+1}{2}\\bigg)\\,.\n On r√©√©crit alors les termes en k/v^2 via \n\\begin{align*}\n\\bigg(\\dfrac{k}{v^2}\\bigg)^{\\frac{k}{2}-1} \\dfrac{k}{(v^2)^{\\frac{3}{2}}}\n    \\bigg(1+\\dfrac{k}{v^2}\\bigg)^{- \\frac{k+1}{2}}\n    & =\n    \\bigg(\\dfrac{k}{v^2}\\bigg)^{\\frac{k}{2}-1} \\dfrac{1}{\\sqrt{k}} \\bigg(\\dfrac{k}{v^2}\\bigg)^{\\frac{3}{2}}\n    \\bigg(1+\\dfrac{k}{v^2}\\bigg)^{- \\frac{k+1}{2}}\\\\\n    & = \\dfrac{1}{\\sqrt{k}}\\bigg(1+\\dfrac{v^2}{k}\\bigg)^{- \\frac{k+1}{2}}\\,,\n\\end{align*}\n ce qui permet de conclure : \n    f_V(v)\n    =\n    \\dfrac{1}{\\sqrt{k \\pi}} \\dfrac{\\Gamma(\\frac{k+1}{2})}{\\Gamma(\\frac{k}{2})} \\Big(  1+\\dfrac{v^2}{k}\\Big)^{-\\frac{k+1}{2}}\\,.\n\n\n\n\n\n\n‚ñ°\n\n\n\n\n\nLoi de Cauchy\n\nD√©finition 3 (Loi de Cauchy) Une variable al√©atoire X suit une loi de Cauchy si sa densit√© est donn√©e par \n    f_X(x) = \\dfrac{1}{\\pi(1+x^2)}\\,, \\quad x \\in \\mathbb{R}\\,.\n\n\nLa fonction de r√©partition de X correspond, √† une constante pr√®s, √† la fonction arctangente qui est bijective de \\mathbb{R} sur ]-\\frac{\\pi}{2},\\frac{\\pi}{2}[. La m√©thode d‚Äôinversion permet donc de simuler une variable al√©atoire de loi de Cauchy. La proposition suivante donne un autre moyen.\n\nProposition 1 (Loi de Cauchy et loi normale) Soit X et Y deux variables al√©atoires ind√©pendantes de loi normale centr√©e r√©duite. Alors la variable al√©atoire Y/X suit une loi de Cauchy.\n\nNotons que Y/X est bien d√©finie puisque X est diff√©rent de 0 presque s√ªrement.\nPreuve: Comme pour la loi de Student, on d√©montre ce r√©sultat avec un changement de variables. On consid√®re l‚Äôapplication \n    \\begin{array}{ccccc}\n        \\phi & : & \\mathbb{R}^* \\times \\mathbb{R} & \\to     & \\mathbb{R}^2                 \\\\\n                &   & (x,y)                          & \\mapsto & \\Big(x, \\dfrac{y}{x}\\Big)\\,,\n    \\end{array}\n c‚Äôest-√†-dire \n    \\phi^{-1}(u,v) = (u, uv)\\,.\n La matrice jacobienne de \\phi^{-1} est donn√©e par \n    J_{\\phi^{-1}} (u,v)\n    =\n    \\begin{pmatrix}\n        1 & 0 \\\\\n        v & u\n    \\end{pmatrix}\\,,\n dont le d√©terminant vaut u. Rappelons √©galement que la densit√© du couple (X,Y) vaut \n    f_{(X,Y)}(x,y) = \\dfrac{1}{2 \\pi} e^{- \\frac{x^2 + y^2}{2}}\\,, \\quad x,y \\in \\mathbb{R}\\,.\n La formule du changement de variables donne alors la densit√© de f_{(U,V)} : \n    f_{(U,V)}\n    = \\dfrac{1}{2 \\pi} e^{- \\frac{u^2 + u^2v^2}{2}} |u|\\,.\n On obtient alors la densit√© de V en int√©grant par rapport √† u : \n\\begin{align*}\n    f_V(v)\n    & = \\dfrac{1}{2 \\pi} \\int_\\mathbb{R} e^{- \\frac{u^2 + u^2v^2}{2}} |u| \\, \\mathrm du \\\\\n    & = \\dfrac{1}{\\pi} \\int_0^\\infty e^{- \\frac{u^2(1 + v^2)}{2}} u \\, \\mathrm du\\\\\n    & = \\dfrac{1}{\\pi} \\bigg[ -\\dfrac{e^{- \\frac{u^2(1 + v^2)}{2}}}{1+v^2} \\bigg]_0^\\infty\\\\\n    & = \\dfrac{1}{\\pi(1+v^2)}\\,.\n\\end{align*}\n\n\n\n\n\n\n‚ñ°\n\n\nOn obtient ainsi une autre mani√®re de simuler une loi de Cauchy : on simule deux gaussiennes ind√©pendantes et on prend leur ratio. Cependant, la simulation via la m√©thode d‚Äôinversion peut-√™tre moins co√ªteuse puisqu‚Äôelle ne fait appel qu‚Äô√† une variable al√©atoire uniforme et √† la fonction tangente.",
    "crumbs": [
      "Cours",
      "Loi normale: cas 1D"
    ]
  },
  {
    "objectID": "Courses/matterjs-inverse-vizu.html",
    "href": "Courses/matterjs-inverse-vizu.html",
    "title": "inverse-vizu",
    "section": "",
    "text": "viewof dist = Inputs.select(['normal','cauchy','laplace','bimodal'], {value: \"bimodal\", label: \"Distribution type\"})\nviewof replay = html`&lt;button&gt;replay`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ninvcdfboard = require(await FileAttachment(\"../inverse-vizu/dist/invcdfboard.umd.cjs\").url())\n\n{\n  replay\n  const canvas = DOM.canvas(500, 500);\n  const galton = invcdfboard.galton(canvas,dist);\n  \n  return html`${galton.canvas}`\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Retour au sommet"
  },
  {
    "objectID": "Courses/perspective_historique.html",
    "href": "Courses/perspective_historique.html",
    "title": "Perspectives historiques",
    "section": "",
    "text": "Nous allons pr√©senter ici quelques √©l√©ments historiques sur les m√©thodes de Monte-Carlo, dont les pr√©misses remontent au XVIII√®me si√®cle.",
    "crumbs": [
      "Cours",
      "Perspectives historiques"
    ]
  },
  {
    "objectID": "Courses/perspective_historique.html#laiguille-de-buffon",
    "href": "Courses/perspective_historique.html#laiguille-de-buffon",
    "title": "Perspectives historiques",
    "section": "L‚Äôaiguille de Buffon",
    "text": "L‚Äôaiguille de Buffon\nGeorges-Louis Leclerc, Comte de Buffon1 proposa en 1733 une m√©thode qui s‚Äôav√©ra √™tre utile pour estimer la valeur de \\pi. On d√©signe de nos jours cette exp√©rience sous le nom de l‚Äôaiguille de Buffon. C‚Äôest l‚Äôune des premi√®res m√©thodes de Monte-Carlo r√©f√©renc√©e dans la litt√©rature (la source du texte est disponible ici sur le site de la BNF).\n1¬†Georges-Louis Leclerc, Comte de Buffon: (1707-1788) naturaliste, math√©maticien et industriel fran√ßais du si√®cle des Lumi√®res La question initiale (simplifi√©e ici) pos√©e par Buffon √©tait la suivante: une aiguille de taille 1 tombe sur un parquet compos√© de lattes de largeur 1: quelle est alors la probabilit√© P que l‚Äôaiguille croise une ligne de la trame du parquet ?\nLe contexte original √©tait dans celui d‚Äôun jeu √† deux joueurs: un joueur parie sur le fait que l‚Äôaiguille croise une ligne de la trame du parquet, l‚Äôautre sur le fait que l‚Äôaiguille ne croise pas une ligne de la trame du parquet. L‚Äôenjeu est alors de calculer la probabilit√© de succ√®s de chacun des joueurs, et de voir si le jeu est √©quilibr√© ou non.\nVoil√† bri√®vement la question que s‚Äôest pos√©e Buffon en 1733. La r√©ponse est donn√©e par la formule suivante, qui montre que le jeu qu‚Äôil propose n‚Äôest pas √©quilibr√©:\n\nP = \\frac{2}{\\pi} \\approx 0.6366 \\enspace.\n Une preuve de ce r√©sultat sera donn√©e ci-dessous.\nL‚Äôid√©e sous-jacente de Buffon est que si l‚Äôon r√©p√®te cette exp√©rience un grand nombre de fois, on peut approch√© la quantit√© P num√©riquement, par exemple en proposant un estimateur \\hat{P}_n qui compte la proportion de chevauchement apr√®s avoir fait n r√©p√©tition des lancers. Pour estimer \\pi, il ne restera donc plus qu‚Äô√† √©valuer \\frac{2}{\\hat{P}_n}.\nOn peut faire cette exp√©rience dans le monde r√©elle (c‚Äôest un peu long pour n grand!), mais on peut aussi utiliser une m√©thode num√©rique pour cela. Il s‚Äôagit alors de tirer al√©atoire la position du centre de l‚Äôaiguille, puis de tirer aussi de mani√®re al√©atoire son angle de chute. On teste √† la fin si l‚Äôaiguille croise une ligne de la trame du parquet ou non, et on recommence l‚Äôexp√©rience un grand nombre de fois.\nCette m√©thode est donn√©e ci-dessous, avec un exemple interactif g√©n√©r√© en Python.\n\nCode\nimport numpy as np\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\nrng = np.random.default_rng(44)\n\nn_samples = 200\nxmax = 14.499999\nxmin = -xmax\n\n\n# Create the needles\ncenters_x = rng.uniform(xmin, xmax, n_samples)\nangles = rng.uniform(0, 2 * np.pi, n_samples)\ncenters_y = rng.uniform(-2, 2, n_samples)\n\n# Compute the right borders of the needles\nborders_right = np.zeros((n_samples, 2))\nborders_right[:, 0] = centers_x + np.cos(angles) / 2\nborders_right[:, 1] = centers_y + np.sin(angles) / 2\n\n# Compute the left borders of the needles\nborders_left = np.zeros((n_samples, 2))\nborders_left[:, 0] = centers_x + np.cos(angles + np.pi) / 2\nborders_left[:, 1] = centers_y + np.sin(angles + np.pi) / 2\n\ncenters_x_round = np.round(centers_x)\noverlap = (borders_right[:, 0] - centers_x_round) * (\n    borders_left[:, 0] - centers_x_round\n) &lt; 0\noverlap = np.where(overlap, 1, 0)\nn_overlap = int(np.sum(overlap))\n\n\n# Check if the needles cross a line\nborders_red = np.empty((3 * n_overlap, 2), dtype=object)\nborders_red.fill(None)\nborders_red[::3, :] = borders_right[overlap == 1]\nborders_red[1::3, :] = borders_left[overlap == 1]\n\nborders_blue = np.empty((3 * (n_samples - n_overlap), 2), dtype=object)\nborders_blue.fill(None)\nborders_blue[::3, :] = borders_right[overlap == 0]\nborders_blue[1::3, :] = borders_left[overlap == 0]\n\noverlaps = np.empty((3 * n_samples), dtype=object)\noverlaps.fill(None)\noverlaps[::3] = overlap\noverlaps[1::3] = overlap\noverlaps[2::3] = overlap\n\nidx_red = np.cumsum(overlaps)\nidx_blue = np.cumsum(1 - overlaps)\n\n\n# Create subplots with 2 rows and 1 column with ratio x /  y  of 10\nfig = make_subplots(rows=2, cols=1, vertical_spacing=0.1, row_heights=[2, 1])\n\n# Use a loop to plot vertical lines equation \"y=c\" for integer values c in [-2, -1, 0, 1, 2]\nfor i in range(int(np.round(xmin)), int(np.round(xmax)) + 1):\n    fig.add_shape(\n        type=\"line\",\n        y0=-3,\n        x0=i,\n        y1=3,\n        x1=i,\n        line=dict(\n            color=\"black\",\n            width=2,\n        ),\n        row=1,\n        col=1,\n    )\n\ncolor = np.where(overlaps, 1.0, 0.0)\n\nn_samples_array = np.arange(1, n_samples + 1)\npi_estimate = 2 / (np.cumsum(overlap) / n_samples_array)\nt = n_samples\n\nfig.update_layout(\n    template=\"simple_white\",\n    xaxis=dict(range=[xmin, xmax], constrain=\"domain\", showgrid=False),\n    yaxis_scaleanchor=\"x\",\n    xaxis_visible=False,\n    yaxis_visible=False,\n)\n\nfor i in range(3, t):\n    fig.add_trace(\n        go.Scatter(\n            x=borders_red[: idx_red[3 * i] + 1, 0],\n            y=borders_red[: idx_red[3 * i] + 1, 1],\n            mode=\"lines\",\n            line=dict(width=2),\n            marker=dict(color=\"red\"),\n            name=\"Avec intersection\",\n            visible=False,\n        ),\n        row=1,\n        col=1,\n    )\n    fig.add_trace(\n        go.Scatter(\n            x=borders_blue[: idx_blue[3 * i] + 1, 0],\n            y=borders_blue[: idx_blue[3 * i] + 1, 1],\n            mode=\"lines\",\n            line=dict(width=2),\n            marker=dict(color=\"darkblue\"),\n            name=\"Sans intersection\",\n            visible=False,\n        ),\n        row=1,\n        col=1,\n    )\n\n    fig.add_trace(\n        go.Scatter(\n            x=n_samples_array[:i],\n            y=pi_estimate[:i],\n            mode=\"lines\",\n            line=dict(width=1),\n            marker=dict(color=\"red\"),\n            showlegend=False,\n            visible=False,\n        ),\n        row=2,\n        col=1,\n    )\n\nfig.add_annotation(\n    dict(\n        x=1.25,\n        y=0.14,\n        xref=\"paper\",\n        yref=\"paper\",\n        text=\"Estimation de pi\",\n        showarrow=False,\n        font=dict(color=\"red\"),\n    )\n)\n\nfig.add_annotation(\n    dict(x=-0.04, y=0.19, xref=\"paper\", yref=\"paper\", text=\"pi\", showarrow=False)\n)\n\nfig.update_xaxes(title_text=\"Nombre d'aiguilles tir√©es\", row=2, col=1)\n\nfig.update_layout(\n    template=\"none\",\n    xaxis2=dict(showgrid=True, zeroline=True, zerolinewidth=1, range=[0, n_samples]),\n    yaxis2=dict(showgrid=True, zeroline=True, zerolinewidth=1, range=[0, 6]),\n)\n# plot a dash line at y=pi\nfig.add_shape(\n    type=\"line\",\n    y0=np.pi,\n    x0=0,\n    y1=np.pi,\n    x1=n_samples,\n    line=dict(\n        color=\"black\",\n        width=1,\n        dash=\"dashdot\",\n    ),\n    row=2,\n    col=1,\n)\n\n\nfig.data[10 * 3].visible = True\nfig.data[10 * 3 + 1].visible = True\nfig.data[10 * 3 + 2].visible = True\n\n\nsteps = []\nfor i in range(len(fig.data) // 3):\n    step = dict(\n        label=str(i + 4),\n        method=\"update\",\n        args=[\n            {\"visible\": [False] * len(fig.data)},\n            {\n                \"title\": \"Estimation avec \"\n                + str(i + 4)\n                + f\" aiguilles: pi = {pi_estimate[i]:.4f}\"\n            },\n        ],\n    )\n    step[\"args\"][0][\"visible\"][3 * i] = True\n    step[\"args\"][0][\"visible\"][3 * i + 1] = True\n    step[\"args\"][0][\"visible\"][3 * i + 2] = True\n\n    steps.append(step)\n\nslider = dict(\n    active=0,\n    currentvalue={\"prefix\": \"Nombre d'aiguilles: \"},\n    pad={\"t\": 50},\n    y=-0.32,\n    steps=steps,\n)\n\nfig.update_layout(legend=dict(x=0.5, y=1, xanchor='center', yanchor='bottom'))\nfig.update_layout(sliders=[slider])\nfig.show()\n\n\n\n\n                                                \n\n\n\nOn va fournir ici le calcul de la probabilit√© P. Pour cela on aura besoin de quelques √©l√©ments d√©crits dans le dessin ci-dessous.\n\nx : distance entre le centre de l‚Äôaiguille et la ligne de la trame du parquet la plus proche\n\\theta : angle entre l‚Äôaiguille et la ligne de la trame du parquet la plus proche\n1 : longueur de l‚Äôaiguille (et donc la demi longueur est \\frac{1}{2})\n\\frac{1}{2}\\sin(\\theta) : distance entre l‚Äôextr√©mit√© de l‚Äôaiguille et la ligne de la trame du parquet la plus proche\n\n\n\n\n\n\n\n\nSans croisement\n\n\n\n\n\nAvec croisement\n\n\n\n\n\nFigure¬†1: Configuration sans croisement (√† gauche) et avec croisement (√† droite) de l‚Äôaiguille avec une ligne de la trame du parquet.\n\n\n\nAvec les √©l√©ments ci-dessus, on voit qu‚Äôil y a chevauchement si et seulement si: \\frac{1}{2}\\sin(\\theta) \\geq x.\nMaintenant par des arguments de sym√©trie on voit qu‚Äôon peut se restreindre √† \\theta \\in [0, \\frac{\\pi}{2}], et √† x \\in [0, \\frac{1}{2}]. Les lois de g√©n√©rations des variables al√©atoires X et \\Theta sont les suivantes:\n\nX \\sim \\mathcal{U}([0, \\frac{1}{2}]), de densit√© f_X(x) = 2 {1\\hspace{-3.8pt} 1}_{[0, \\frac{1}{2}]}(x)\n\\Theta \\sim \\mathcal{U}([0, \\frac{\\pi}{2}]) de densit√© f_\\Theta(\\theta) = \\frac{2}{\\pi} {1\\hspace{-3.8pt} 1}_{[0, \\frac{\\pi}{2}]}(\\theta)\n\nDe plus on suppose que X et \\Theta sont ind√©pendantes.\nMaintenant pour calculer la probabilit√© P on proc√®de comme suit: \n\\begin{align*}\nP\n& = \\mathbb{P}\\left(\\frac{1}{2}\\sin(\\Theta) \\geq X\\right) \\\\\n& = \\int_{\\mathbb{R}^2} {1\\hspace{-3.8pt} 1}_{\\{\\frac{1}{2}\\sin(\\theta) \\geq x\\}} f_{\\Theta}(\\theta) f_X(x) d\\theta dx  \\quad (\\text{par ind√©pendance})\\\\\n& = \\int_{\\mathbb{R}} \\int_{\\mathbb{R}}\n{1\\hspace{-3.8pt} 1}_{\\{\\frac{1}{2}\\sin(\\theta) \\geq x\\}} \\frac{2}{\\pi} {1\\hspace{-3.8pt} 1}_{[0, \\frac{\\pi}{2}]}(\\theta) \\cdot 2 {1\\hspace{-3.8pt} 1}_{[0, \\frac{1}{2}]}(x) d\\theta dx \\\\\n& = \\int_{0}^{\\frac{\\pi}{2}} \\int_{0}^{\\frac{1}{2}\\sin(\\theta)} \\frac{4}{\\pi} dx  d\\theta \\\\\n& = \\frac{4}{\\pi} \\int_{0}^{\\frac{\\pi}{2}} {\\frac{1}{2}\\sin(\\theta)}  d\\theta\\\\  \n& = \\frac{2}{\\pi} \\Big[ -\\cos(\\theta)\\Big]_{0}^{\\frac{\\pi}{2}} \\\\\n& = \\frac{2}{\\pi} \\enspace.\n\\end{align*}\n\n\n\n\n\n\n\nExercice: rendre le jeu √©quilibr√©?\n\n\n\nEn reprenant le m√™me type de raisonnement que ci-dessus, trouver la distance entre les lattes du parquet qui rend le jeu √©quilibrer entre les deux joueurs introduit par Buffon (l‚Äôun pariant sur le fait que l‚Äôaiguille croise une ligne de la trame du parquet, l‚Äôautre pariant sur le fait que l‚Äôaiguille ne croise pas une ligne de la trame du parquet).",
    "crumbs": [
      "Cours",
      "Perspectives historiques"
    ]
  },
  {
    "objectID": "Courses/perspective_historique.html#m√©thode-de-monte-carlo",
    "href": "Courses/perspective_historique.html#m√©thode-de-monte-carlo",
    "title": "Perspectives historiques",
    "section": "M√©thode de Monte-Carlo",
    "text": "M√©thode de Monte-Carlo\nLa m√©thode de Monte-Carlo, est une m√©thode de calcul num√©rique qui consiste √† utiliser des nombres al√©atoires pour r√©soudre des probl√®mes d√©terministes. Elle est utilis√©e dans de nombreux domaines, comme la physique, la chimie, la biologie, la finance, ou encore l‚Äôapprentissage automatique. Cette m√©thode bas√©e sur la loi des grands nombres a √©t√© mis au point √† Los Alamos, dans le cadre du projet Manhattan (dont l‚Äôobjectif √©tait le d√©veloppement du nucl√©aire civil et militaire) par un groupe de scientifiques dont les plus connus sont: John von Neumann2, Nicholas Metropolis3 ou encore Stanis≈Çaw Ulam4\n2¬†John von Neumann: (1903-1957) math√©maticien et physicien am√©ricano-hongrois, un des p√®res de l‚Äôinformatique. 3¬†Nicholas Metropolis: (1915-1999), physicien gr√©co-am√©ricain, est des initiateurs de la m√©thode de Monte Carlo et du recuit simul√© 4¬†Stanis≈Çaw Ulam: (1909-1984) Dans le cadre du projet Manhattan, il s‚Äôagissait de calculer des int√©grales de mani√®re num√©rique pour mod√©liser l‚Äô√©volution de particules, en utilisant des nombres al√©atoires.\nEckhardt (1987) donne un bref aper√ßu historique, et mentionne les premi√®res description de la m√©thode du rejet et de la m√©thode de l‚Äôinversion dans des lettres entre Von Neumann et Ulam datant de 1947. Ulam aurait une l‚Äôid√©e d‚Äôutiliser de telles m√©thodes pour r√©soudre le jeu du solitaire lors d‚Äôun s√©jour √† l‚Äôh√¥pital en 1946, et √©viter ainsi de faire des calculs combinatoires fastidieux. Rapidement, la possibilit√© d‚Äôappliquer cette approche pour des calculs en physique math√©matiques (diffusion des neutrons notamment) lui serait apparue prometteuse. Le d√©veloppement de l‚Äôinformatique naissante allait permettre une mise en oeuvre pratique de ces id√©es, et c‚Äôest ainsi que la m√©thode de Monte-Carlo est n√©e. Le nom Monte-Carlo est lui venu du besoin de confidentialit√© du projet, et provient du nom de la ville de Monte-Carlo, connue pour ses jeux de hasard, o√π l‚Äôoncle de Stanis≈Çaw Ulam aimait se rendre pour assouvir sa soif de jeu. Ce serait N. Metropolis qui aurait propos√© ce nom (cf. Metropolis 1987):\n\nEckhardt, R. 1987. ¬´¬†Stan Ulam, John Von Neumann, and the Monte Carlo Method¬†¬ª. Los Alamos Science, n·µí 15: 131‚Äë37.\n\nMetropolis, Nicholas. 1987. ¬´¬†The beginning of the Monte Carlo method¬†¬ª. Los Alamos Science, n·µí 15: 125‚Äë30.\nIt was at that time that I suggested an obvious name for the statistical method‚Äîa suggestion not unrelated to the fact that Stan had an uncle who would borrow money from relatives because he ‚Äújust had to go to Monte Carlo‚Äù.",
    "crumbs": [
      "Cours",
      "Perspectives historiques"
    ]
  },
  {
    "objectID": "Courses/perspective_historique.html#autres-m√©thodes-stochastiques-populaires",
    "href": "Courses/perspective_historique.html#autres-m√©thodes-stochastiques-populaires",
    "title": "Perspectives historiques",
    "section": "Autres m√©thodes stochastiques populaires",
    "text": "Autres m√©thodes stochastiques populaires\n\nM√©thode d‚ÄôHasting-Metropolis\nL‚Äôalgorithme de Hasting-Metropolis est une m√©thode MCMC (üá¨üáß: Monte Carlo Markov Chains) dont le but est d‚Äôobtenir un √©chantillonnage al√©atoire d‚Äôune distribution de probabilit√© quand l‚Äô√©chantillonnage direct en est difficile (en particulier en grande dimension)\nUn avantage est qu‚Äôil ne requiert la connaissance de loi de densit√© qu‚Äô√† constante multiplicative pr√®s.\n\n\nüá¨üáß: Recuit simul√©\nLe recuit simul√© est une m√©thode (empirique) d‚Äôoptimisation, inspir√©e d‚Äôun processus, le recuit, utilis√© en m√©tallurgie. On alterne dans cette derni√®re des cycles de refroidissement lent et de r√©chauffage (recuit) qui ont pour effet de minimiser l‚Äô√©nergie du mat√©riau. Cette m√©thode est transpos√©e en optimisation pour trouver les extrema d‚Äôune fonction.",
    "crumbs": [
      "Cours",
      "Perspectives historiques"
    ]
  },
  {
    "objectID": "Courses/slides.html",
    "href": "Courses/slides.html",
    "title": "Slides: menu principal",
    "section": "",
    "text": "Vous trouverez ci-dessous la listes des slides associ√©s:\nCours introduction, plein √©cran\n\nCours: notations premiers pas, plein √©cran\n\nCours: th√©or√®me asymptotiques, plein √©cran\n\nCours: simulation, m√©thodes classiques\n\n\n\n\n Retour au sommet",
    "crumbs": [
      "Slides",
      "Slides: menu principal"
    ]
  },
  {
    "objectID": "Slides/slides_intro.html#section",
    "href": "Slides/slides_intro.html#section",
    "title": "Introduction",
    "section": "",
    "text": "PS: n‚Äôoubliez pas de mettre [HAX603X] dans le titre de vos mails!",
    "crumbs": [
      "Slides",
      "Introduction"
    ]
  },
  {
    "objectID": "Slides/slides_intro.html#enseignants",
    "href": "Slides/slides_intro.html#enseignants",
    "title": "Introduction",
    "section": "Enseignants",
    "text": "Enseignants\n\n\nJoseph Salmon : CM et TP\n\nSituation actuelle : Professeur √† l‚ÄôUniversit√© de Montpellier\nPr√©c√©demment : Paris Diderot-Paris 7, Duke Univ., T√©l√©com ParisTech, Univ. Washington\nSp√©cialit√©s : statistiques, optimisation, traitement des images, sciences participatives\nBureau : 415, Bat. 9\n\n\n\n\n\n\n\n\nBenjamin Charlier : CM, TD et TP\n\nSituation actuelle : Ma√Ætre de conf√©rences √† l‚ÄôUniversit√© de Montpellier\nPr√©c√©demment : Universit√© Paul Sabatier, ENS Paris-Saclay\nSp√©cialit√©s : traitement des images, statistiques, diff√©rentiation automatique\nBureau : 423, Bat. 9",
    "crumbs": [
      "Slides",
      "Introduction"
    ]
  },
  {
    "objectID": "Slides/slides_intro.html#ressources-en-ligne",
    "href": "Slides/slides_intro.html#ressources-en-ligne",
    "title": "Introduction",
    "section": "Ressources en ligne",
    "text": "Ressources en ligne\n\nInformations principales : site du cours http://josephsalmon.github.io/HAX603X\n\n\n\nSyllabus\nCours (d√©taill√©: site web)\nSlides (r√©sum√©)\nFeuilles de TD\nFeuilles de TP\nRendu TP : Moodle de l‚Äôuniversit√© (https://moodle.umontpellier.fr/course/view.php?id=5558)",
    "crumbs": [
      "Slides",
      "Introduction"
    ]
  },
  {
    "objectID": "Slides/slides_intro.html#validation",
    "href": "Slides/slides_intro.html#validation",
    "title": "Introduction",
    "section": "Validation",
    "text": "Validation\n\nTP not√©s : Rendu = fichier Python .py unique\n\nTP not√© 1 : rendre en fin de session (en S11)\nTP not√© 2 : rendre en fin de session (en S17)\n\nCC : devoir sur table d‚Äôune heure (S18)\n\n\n\nCoefficients:\n\nNote Session 1 : (40% CC + 30% TP 1 + 30% TP 2)\nNote Session 2 : (30% CC + 35% TP 1 + 35% TP 2)\n\n\n\n\n\n\n\n\nImportant\n\n\nLe rendu est individuel pour le TP not√© !!!",
    "crumbs": [
      "Slides",
      "Introduction"
    ]
  },
  {
    "objectID": "Slides/slides_intro.html#notation-pour-les-tps",
    "href": "Slides/slides_intro.html#notation-pour-les-tps",
    "title": "Introduction",
    "section": "Notation pour les TPs",
    "text": "Notation pour les TPs\nRendu : sur Moodle, en d√©posant un fichier nom_prenom.py dans le dossier ad√©quat.\nD√©tails de la notation des TPs :\n\nQualit√© des r√©ponses aux questions\nQualit√© de r√©daction et d‚Äôorthographe\nQualit√© des graphiques (l√©gendes, couleurs)\nQualit√© du code (noms de variables, clairs, commentaires utiles, code synth√©tique, etc.)\nCode reproductible et absence de bug\n\n\n\n\n\n\n\n\nP√©nalit√©s\n\n\n\nEnvoi par mail : z√©ro\nRetard : z√©ro (uploader avant la fin, fermeture automatique de moodle)",
    "crumbs": [
      "Slides",
      "Introduction"
    ]
  },
  {
    "objectID": "Slides/slides_intro.html#pr√©requis---√†-revoir-seul",
    "href": "Slides/slides_intro.html#pr√©requis---√†-revoir-seul",
    "title": "Introduction",
    "section": "Pr√©requis - √† revoir seul",
    "text": "Pr√©requis - √† revoir seul\n\n \n\nBases de probabilit√©s (en particulier ‚ÄúHAX506X- Th√©orie des Probabilit√©s‚Äù): probabilit√©, densit√©, esp√©rance, fonction de r√©partition, mesure, int√©gration, analyse num√©rique √©l√©mentaire, etc. (Foata et Fuchs 1996; Barbe et Ledoux 2006; Ouvrard 2007, 2008)\n\n\n\nProgrammation √©l√©mentaire (en Python): if ‚Ä¶ then‚Ä¶ else ‚Ä¶, for, while, fonctions, etc. HLMA310 - Logiciels scientifiques, (Courant et al. 2013), Cours de Python: Univ. Paris Diderot\n\n\n\nPour aller plus loin: conditionnement, martingales (Williams 1991)",
    "crumbs": [
      "Slides",
      "Introduction"
    ]
  },
  {
    "objectID": "Slides/slides_intro.html#description-du-cours",
    "href": "Slides/slides_intro.html#description-du-cours",
    "title": "Introduction",
    "section": "Description du cours",
    "text": "Description du cours\n\n\nG√©n√©rer l‚Äôal√©a\n\ng√©n√©rateurs pseudo-al√©atoires, simulations de variables al√©atoires¬†(inverse, rejet, etc.)\nillustrations num√©riques et visualisation en Python (loi des grands nombres, TCL)\n\nM√©thode de Monte-Carlo\n\nm√©thode de Monte-Carlo pour le calcul approch√© d‚Äôune int√©grale\nr√©duction de la variance¬†: variables antith√©tiques, variables de contr√¥le, etc.\n\nCompl√©ments\n\nvecteurs gaussiens et lien avec les lois usuelles de la statistique inf√©rentielle (student, chi2)\nconstruction d‚Äôintervalles de confiance.\nmarche al√©atoire simple, etc.",
    "crumbs": [
      "Slides",
      "Introduction"
    ]
  },
  {
    "objectID": "Slides/slides_intro.html#buffon-et-les-pr√©misses-de-la-m√©thode-de-monte-carlo",
    "href": "Slides/slides_intro.html#buffon-et-les-pr√©misses-de-la-m√©thode-de-monte-carlo",
    "title": "Introduction",
    "section": "Buffon et les pr√©misses de la m√©thode de Monte-Carlo",
    "text": "Buffon et les pr√©misses de la m√©thode de Monte-Carlo\n\n\n\n\n1733: l‚Äôaiguille de Buffon, m√©thode d‚Äôestimation de la valeur de \\(\\pi\\).\n\n\n\n\nProbl√®me initial: une aiguille de taille 1 tombe sur un parquet compos√© de lattes de largeur \\(1\\): quelle est alors la probabilit√© \\(P\\) que l‚Äôaiguille croise une ligne de la trame du parquet ?\n\n\n\n\n\n\n                                                \n\n\n\n\n\n\nGeorges-Louis Leclerc, Comte de Buffon(1707-1788) : naturaliste, math√©maticien et industriel fran√ßais du si√®cle des Lumi√®res",
    "crumbs": [
      "Slides",
      "Introduction"
    ]
  },
  {
    "objectID": "Slides/slides_intro.html#laiguille-de-buffon-suite",
    "href": "Slides/slides_intro.html#laiguille-de-buffon-suite",
    "title": "Introduction",
    "section": "L‚Äôaiguille de Buffon (suite)",
    "text": "L‚Äôaiguille de Buffon (suite)\n\nProbl√®me initial: une aiguille de taille 1 tombe sur un parquet compos√© de lattes de largeur \\(1\\): quelle est alors la probabilit√© \\(P\\) que l‚Äôaiguille croise une ligne de la trame du parquet ?\n\n\n\nR√©ponse: \\[\nP = \\frac{2}{\\pi} \\approx 0.6366 \\enspace.\n\\] Une preuve de ce r√©sultat est donn√©e ici.",
    "crumbs": [
      "Slides",
      "Introduction"
    ]
  },
  {
    "objectID": "Slides/slides_intro.html#principe-de-monte-carlo-et-estimation",
    "href": "Slides/slides_intro.html#principe-de-monte-carlo-et-estimation",
    "title": "Introduction",
    "section": "Principe de Monte Carlo et estimation",
    "text": "Principe de Monte Carlo et estimation\nId√©e sous-jacente de Buffon :\nsi l‚Äôon r√©p√®te cette exp√©rience un grand nombre de fois, on peut approch√© la quantit√© \\(P\\) num√©riquement, par exemple en proposant un estimateur \\(\\hat{P}_n\\) qui compte la proportion de chevauchement apr√®s avoir fait \\(n\\) r√©p√©tition des lancers.\n Estimation de \\(\\pi\\):\n\\[\n\\pi \\approx \\frac{2}{\\hat{P}_n}\n\\]",
    "crumbs": [
      "Slides",
      "Introduction"
    ]
  },
  {
    "objectID": "Slides/slides_intro.html#principe-de-monte-carlo-pour-lestimation-suite",
    "href": "Slides/slides_intro.html#principe-de-monte-carlo-pour-lestimation-suite",
    "title": "Introduction",
    "section": "Principe de Monte Carlo pour l‚Äôestimation (suite)",
    "text": "Principe de Monte Carlo pour l‚Äôestimation (suite)",
    "crumbs": [
      "Slides",
      "Introduction"
    ]
  },
  {
    "objectID": "Slides/slides_intro.html#m√©thode-de-monte-carlo",
    "href": "Slides/slides_intro.html#m√©thode-de-monte-carlo",
    "title": "Introduction",
    "section": "M√©thode de Monte-Carlo",
    "text": "M√©thode de Monte-Carlo\nM√©thode de calcul num√©rique qui consiste √† utiliser des nombres al√©atoires pour r√©soudre des probl√®mes d√©terministes.\n\nDomaines d‚Äôapplications:\n\nla physique\nla chimie\nla biologie\nla finance\nl‚Äôapprentissage automatique",
    "crumbs": [
      "Slides",
      "Introduction"
    ]
  },
  {
    "objectID": "Slides/slides_intro.html#contexte-de-la-naissance-de-la-m√©thode-de-monte-carlo",
    "href": "Slides/slides_intro.html#contexte-de-la-naissance-de-la-m√©thode-de-monte-carlo",
    "title": "Introduction",
    "section": "Contexte de la naissance de la m√©thode de Monte Carlo",
    "text": "Contexte de la naissance de la m√©thode de Monte Carlo\n\n\n\n\nLieu: Los Alamos\n√âpoque: seconde guerre mondial\nContexte: Projet Manathan, produire une bombe atomique\nBesoins: mod√©liser les r√©actions nucl√©aires en cha√Æne (combinatoires)\n\n\n\n\n\n\n\n\nJohn von Neumann (1903-1957), math√©maticien et physicien am√©ricano-hongrois, un des p√®res de l‚Äôinformatique.\n\n\n\n\n\n\n\nNicholas Metropolis (1915-1999), physicien gr√©co-am√©ricain, un des initiateurs de la m√©thode de Monte Carlo et du recuit simul√©\n\n\n\n\n\n\n\nStanis≈Çaw Ulam (1909-1984), math√©maticien polono-am√©ricainm, un des initiateurs de la m√©thode de Monte Carlo et de la propulsion nucl√©aire puls√©e\n\n\n\n\n\n\n\n\n\n\nExplosion de Trinity (16 Juillet 1945)",
    "crumbs": [
      "Slides",
      "Introduction"
    ]
  },
  {
    "objectID": "Slides/slides_intro.html#lorigine-du-nom-monte-carlo",
    "href": "Slides/slides_intro.html#lorigine-du-nom-monte-carlo",
    "title": "Introduction",
    "section": "L‚Äôorigine du nom ‚ÄúMonte-Carlo‚Äù",
    "text": "L‚Äôorigine du nom ‚ÄúMonte-Carlo‚Äù\nInitialement: besoin de confidentialit√© du projet Manhattan\n\nMonte-Carlo: connue pour ses jeux de hasard, o√π l‚Äôoncle de Stanis≈Çaw Ulam aimait se rendre pour assouvir sa soif de jeu.\n Ce serait N. Metropolis qui aurait propos√© ce nom, cf. (Metropolis 1987):\nIt was at that time that I suggested an obvious name for the statistical method‚Äîa suggestion not unrelated to the fact that Stan had an uncle who would borrow money from relatives because he ‚Äújust had to go to Monte Carlo‚Äù.",
    "crumbs": [
      "Slides",
      "Introduction"
    ]
  },
  {
    "objectID": "Slides/slides_intro.html#essor-de-la-m√©thode-de-monte-carlo",
    "href": "Slides/slides_intro.html#essor-de-la-m√©thode-de-monte-carlo",
    "title": "Introduction",
    "section": "Essor de la m√©thode de Monte Carlo",
    "text": "Essor de la m√©thode de Monte Carlo\n\n\n\n\nPopularisation croissante:\n\nEssor de l‚Äôinformatique (depuis les ann√©es 80)\nEssor des m√©thodes de calcul parall√®le (GPUs, clusters, etc.)\n\n\n\n\n\nDomaine principaux impact√©s:\n\nfinance : √©valuation des prix de produits d√©riv√©s\napprentissage automatique: utilisation de l‚Äôal√©atoire pour g√©n√©r√© des sc√©narios\nExemples: Alphago (2016), AlphaGeometry (2024)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRecherche arborescente Monte-Carlo (üá¨üáß: Monte Carlo tree search): analyse des sc√©narios les plus prometteurs, en √©largissant l‚Äôarbre de recherche sur la base d‚Äôun √©chantillonnage al√©atoire de l‚Äôespace entier (ingr√©dient important d‚ÄôAlphaGo)",
    "crumbs": [
      "Slides",
      "Introduction"
    ]
  },
  {
    "objectID": "Slides/slides_intro.html#bibliographie",
    "href": "Slides/slides_intro.html#bibliographie",
    "title": "Introduction",
    "section": "Bibliographie",
    "text": "Bibliographie\n\n\nBarbe, Philippe, et Michel Ledoux. 2006. ¬´¬†Cours de Probabilite Pour la Licence (corrig√©e)¬†¬ª.\n\n\nCourant, J., M. de Falco, S. Gonnord, J.-C. Filli√¢tre, S. Conchon, G. Dowek, et B. Wack. 2013. Informatique pour tous en classes pr√©paratoires aux grandes √©coles: Manuel d‚Äôalgorithmique et programmation structur√©e avec Python. Eyrolles.\n\n\nFoata, D., et A. Fuchs. 1996. Calcul des probabilit√©s: cours et exercices corrig√©s. Masson.\n\n\nMetropolis, Nicholas. 1987. ¬´¬†The beginning of the Monte Carlo method¬†¬ª. Los Alamos Science, n·µí 15: 125‚Äë30.\n\n\nOuvrard, J.-Y. 2007. Probabilit√©s : Tome 2, Licence - CAPES. 2·µâ √©d. Enseignement des math√©matiques. Cassini.\n\n\n‚Äî‚Äî‚Äî. 2008. Probabilit√©s : Tome 1, Licence - CAPES. 2·µâ √©d. Enseignement des math√©matiques. Cassini.\n\n\nWilliams, D. 1991. Probability with martingales. Cambridge Mathematical Textbooks. Cambridge: Cambridge University Press.\n\n\n\n\nIntroduction",
    "crumbs": [
      "Slides",
      "Introduction"
    ]
  },
  {
    "objectID": "Slides/slides_notations_premiers_pas.html#notation-et-rappels-de-probabilit√©s",
    "href": "Slides/slides_notations_premiers_pas.html#notation-et-rappels-de-probabilit√©s",
    "title": "Notations et premiers pas",
    "section": "Notation et rappels de probabilit√©s",
    "text": "Notation et rappels de probabilit√©s\n\n\nEspace probabilis√©: \\((\\Omega, {\\mathcal{F}}, \\mathbb{P})\\)\n\ncompos√© d‚Äôun ensemble: \\(\\Omega\\)\nd‚Äôune tribu: \\(\\mathcal{F}\\)\nd‚Äôune mesure de probabilit√©: \\(\\mathbb{P}\\)\n\n\n\n\n\nD√©finition 1 (Variable al√©atoire, v.a.) Soit \\((E, \\mathcal{E})\\) un espace mesurable. Une variable al√©atoire est une application mesurable \\[\n    \\begin{array}{ccccc}\n        X & : & \\Omega & \\to     & E            \\\\\n            &   & \\omega & \\mapsto & X(\\omega)\\,.\n    \\end{array}\n\\] Ainsi \\(\\{\\omega \\in \\Omega : X(\\omega) \\in B\\} = X^{-1}(B) = \\{X \\in B\\} \\in \\mathcal{F}, \\forall B \\in \\mathcal{E}\\)",
    "crumbs": [
      "Slides",
      "Notations et premiers pas"
    ]
  },
  {
    "objectID": "Slides/slides_notations_premiers_pas.html#loi-dune-variable-al√©atoire-unidimensionnelle",
    "href": "Slides/slides_notations_premiers_pas.html#loi-dune-variable-al√©atoire-unidimensionnelle",
    "title": "Notations et premiers pas",
    "section": "Loi d‚Äôune variable al√©atoire (unidimensionnelle)",
    "text": "Loi d‚Äôune variable al√©atoire (unidimensionnelle)\n\nD√©finition 2 (Loi d‚Äôune variable al√©atoire) \nSoit \\(X : (\\Omega, \\mathcal{F}, \\mathbb{P}) \\to (E, \\mathcal{E})\\) une v.a. On appelle loi de \\(X\\) la mesure de probabilit√© sur \\((E, \\mathcal{E})\\) d√©finie par \\[\n        \\begin{array}{ccccc}\n            \\mathbb{P}_X & : & \\mathcal{E} & \\to     & [0,1]          \\\\\n                 &   & B           & \\mapsto & \\mathbb{P}(X \\in B) \\enspace.\n        \\end{array}\n\\]\n\n\n\n\n\n\n\n\nNote\n\n\nLes propri√©t√©s de \\(\\mathbb{P}\\) assurent que \\(\\mathbb{P}_X\\) est bien une mesure de probabilit√© sur l‚Äôespace mesurable \\((E, \\mathcal{E})\\)",
    "crumbs": [
      "Slides",
      "Notations et premiers pas"
    ]
  },
  {
    "objectID": "Slides/slides_notations_premiers_pas.html#lois-discr√®tes",
    "href": "Slides/slides_notations_premiers_pas.html#lois-discr√®tes",
    "title": "Notations et premiers pas",
    "section": "Lois discr√®tes",
    "text": "Lois discr√®tes\nLes variables al√©atoires discr√®tes sont celles √† valeurs dans un ensemble \\(E\\) discret, le plus souvent \\(\\mathbb{N}\\), muni de la tribu pleine \\(\\mathcal{F} = \\mathcal{P}(E)\\).\n\nExemple 1 (Loi de Bernoulli) Soit un param√®tre \\(p \\in [0,1]\\), et \\(E=\\{0,1\\}\\), alors la loi de Bernouilli est donn√©e par \\(\\mathbb{P}(X=1)=1-\\mathbb{P}(X=0) = p\\).\nNotation: \\(\\quad\\) \\(X \\sim \\mathcal{B}(p)\\)\n\nExemple physique:  loi d‚Äôun tirage de pile ou face, de biais \\(p\\)\n\nExemple 2 (Loi binomiale) Soient \\(p \\in [0,1]\\) (biais) et \\(n \\in \\mathbb{N}^*\\) (nombre de tirages) alors la loi Binomiale est donn√©e par \\(\\mathbb{P}(X=k) = \\binom{n}{k} p^k (1-p)^{n-k}\\), pour \\(k \\in E=\\{0,\\dots,n\\}\\).\nNotation: \\(\\quad\\) \\(X \\sim \\mathcal{B}(n,p)\\)\n\nExemple physique:      loi du nombre de succ√®s obtenus lors de \\(n\\) r√©p√©titions ind√©pendantes d‚Äôune exp√©rience al√©atoire de Bernoulli de param√®tre \\(p\\)",
    "crumbs": [
      "Slides",
      "Notations et premiers pas"
    ]
  },
  {
    "objectID": "Slides/slides_notations_premiers_pas.html#lois-discr√®tes-ii",
    "href": "Slides/slides_notations_premiers_pas.html#lois-discr√®tes-ii",
    "title": "Notations et premiers pas",
    "section": "Lois discr√®tes (II)",
    "text": "Lois discr√®tes (II)\n\nExemple 3 (Loi g√©om√©trique) Soient \\(p \\in [0,1]\\) (biais), alors la loi g√©om√©trique est donn√©e par \\(\\mathbb{P}(X=k) = p (1-p)^{k-1}\\), pour \\(k \\in E=\\mathbb{N}^*\\).\nNotation: \\(\\quad\\) \\(X \\sim \\mathcal{G}(p)\\)\n\n\nExemple physique:     \n\nloi du nombre tirage n√©cessaire avant d‚Äôobtenir un succ√®s obtenus en r√©p√©tant ind√©pendamment des exp√©riences al√©atoires de Bernoulli de param√®tre \\(p\\)\n\n\n\n\nExemple 4 (Loi de Poisson) Pour \\(\\lambda&gt;0\\), la loi de Poisson de param√®tre \\(\\lambda\\) est d√©finie par \\(\\mathbb{P}(X=k) = e^{-\\lambda} \\lambda^k / k!\\), pour tout \\(k \\in E=\\mathbb{N}\\).\nNotation: \\(\\quad\\) \\(X \\sim \\mathcal{P}(\\lambda)\\)\n\nExemple physique: comportement du nombre d‚Äô√©v√©nements se produisant avec une fr√©quence connue, et ind√©pendamment du temps √©coul√© depuis l‚Äô√©v√©nement pr√©c√©dent (e.g., nombre de clients dans une file d‚Äôattente, nombre de mutations dans un g√®ne, etc.)",
    "crumbs": [
      "Slides",
      "Notations et premiers pas"
    ]
  },
  {
    "objectID": "Slides/slides_notations_premiers_pas.html#lois-continues",
    "href": "Slides/slides_notations_premiers_pas.html#lois-continues",
    "title": "Notations et premiers pas",
    "section": "Lois continues",
    "text": "Lois continues\nLoi d‚Äôune v.a. admettant une fonction de densit√©, c‚Äôest-√†-dire qu‚Äôil existe une fonction mesurable \\(f : \\mathbb{R} \\to [0, \\infty[\\) d‚Äôint√©grale \\(1\\), telle que pour tout \\(A \\in \\mathcal{B}(\\mathbb{R})\\) \\[\n    \\mathbb{P}(X \\in A) = \\int_A f(x) dx \\enspace.\n\\]\n\n\n\n\n\n\nNote\n\n\nLes propri√©t√©s de l‚Äôint√©grale de Lebesgue assurent que cette formule d√©finit bien une loi de probabilit√©.\n\n\n\n\nEsp√©rance: \\(\\mathbb{E}(X) = \\displaystyle\\int_{\\mathbb{R}} x f(x) dx\\)\nVariance: \\(\\mathbb{V}(X) = \\mathbb{E}((X-\\mathbb{E}(X))^2) = \\displaystyle\\int_{\\mathbb{R}} (x-\\mathbb{E}(X))^2 f(x) dx\\)",
    "crumbs": [
      "Slides",
      "Notations et premiers pas"
    ]
  },
  {
    "objectID": "Slides/slides_notations_premiers_pas.html#lois-continues-usuelles",
    "href": "Slides/slides_notations_premiers_pas.html#lois-continues-usuelles",
    "title": "Notations et premiers pas",
    "section": "Lois continues usuelles",
    "text": "Lois continues usuelles\n\nExemple 5 (Loi uniforme) La loi uniforme sur un ensemble \\(B \\in \\mathcal{B}(\\mathbb{R})\\), s‚Äôobtient avec la densit√© d√©finie par \\[\nf(x) = {1\\hspace{-3.8pt} 1}_B(x) / \\lambda (B) \\enspace,\n\\] o√π \\(\\lambda (B)\\) repr√©sente la mesure de Lebesgue de l‚Äôensemble \\(B\\).\n\nCas particulier: pour la loi uniforme sur \\([0,1]\\), on obtient la fonction suivante: \\[\nf(x) = {1\\hspace{-3.8pt} 1}_{[0,1]}(x)\\enspace.\n\\] Notation: \\(\\quad\\) \\(X \\sim \\mathcal{U}([0,1])\\)\n\n\n\n\n\n\nNote\n\n\nUne telle loi est caract√©ris√©e ainsi : tous les intervalles de m√™me longueur inclus dans le support de la loi ont la m√™me probabilit√©.",
    "crumbs": [
      "Slides",
      "Notations et premiers pas"
    ]
  },
  {
    "objectID": "Slides/slides_notations_premiers_pas.html#lois-continues-usuelles-ii",
    "href": "Slides/slides_notations_premiers_pas.html#lois-continues-usuelles-ii",
    "title": "Notations et premiers pas",
    "section": "Lois continues usuelles (II)",
    "text": "Lois continues usuelles (II)\n\n\nExemple 6 (Loi exponentielle) La loi exponentielle de param√®tre \\(\\gamma &gt; 0\\) est obtenue avec la densit√© donn√©e par \\[\nf(x) = \\gamma e^{-\\gamma x} {1\\hspace{-3.8pt} 1}_{\\mathbb{R}_+}(x)\\enspace.\n\\] Notation: \\(\\quad X \\sim \\mathcal{Exp}(\\gamma)\\)\n\n\n\n\nProposition 1 (Absence de m√©moire) La loi exponentielle mod√©lise la dur√©e de vie d‚Äôun ph√©nom√®ne sans m√©moire (ou sans vieillissement), c‚Äôest-√†-dire que pour tout \\(s,t&gt;0\\), on a \\[\n\\mathbb{P}(X&gt;t+s | X&gt;t)=\\mathbb{P}(X&gt;s) \\enspace.\n\\]",
    "crumbs": [
      "Slides",
      "Notations et premiers pas"
    ]
  },
  {
    "objectID": "Slides/slides_notations_premiers_pas.html#lois-continues-usuelles-iii",
    "href": "Slides/slides_notations_premiers_pas.html#lois-continues-usuelles-iii",
    "title": "Notations et premiers pas",
    "section": "Lois continues usuelles (III)",
    "text": "Lois continues usuelles (III)\n\nExemple 7 (Loi normale/gaussienne univari√©e) Pour des param√®tres \\(\\mu \\in \\mathbb{R}\\) (esp√©rance) et \\(\\sigma^2 &gt; 0\\) (variance), la loi normale associ√©e correspond √† la fonction de densit√© : \\[\nf(x) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}}e^{-\\frac{1}{2 \\sigma^2}(x-\\mu)^2} \\enspace.\n\\] Notation: \\(X \\sim \\mathcal{N}(\\mu,\\sigma^2)\\),\n\nOn nomme loi normale centr√©e r√©duite le cas canonique: \\(\\mu = 0, \\sigma^2 = 1\\).\nSi \\(X\\sim \\mathcal{N}(\\mu,\\sigma^2)\\), alors l‚Äôesp√©rance et la variance de \\(X\\) valent \\(\\mathbb{E}(X) = \\mu\\) et \\(\\mathbb{V}(X) = \\sigma^2\\).\n\n\n\n\n\n\n\nNote\n\n\nLes lois normales sont omnipr√©sente gr√¢ce au th√©or√®me central limite.",
    "crumbs": [
      "Slides",
      "Notations et premiers pas"
    ]
  },
  {
    "objectID": "Slides/slides_notations_premiers_pas.html#enjeu-de-la-fonction-de-r√©partition",
    "href": "Slides/slides_notations_premiers_pas.html#enjeu-de-la-fonction-de-r√©partition",
    "title": "Notations et premiers pas",
    "section": "Enjeu de la fonction de r√©partition",
    "text": "Enjeu de la fonction de r√©partition\n\nEnjeux: caract√©riser la loi d‚Äôune v.a. en ne consid√©rant que l‚Äôespace d‚Äôarriv√©e \\((E, \\mathcal{E})\\)\n\n\nOutils:\n\n\nla fonction de r√©partition (v.a. r√©elles),\nla fonction caract√©ristique (v.a. dans \\(\\mathbb{R}^d\\)), en gros la transform√©e de Fourier de la loi!\nla fonction g√©n√©ratrice des moments (v.a. discr√®tes)\netc.",
    "crumbs": [
      "Slides",
      "Notations et premiers pas"
    ]
  },
  {
    "objectID": "Slides/slides_notations_premiers_pas.html#fonction-de-r√©partition",
    "href": "Slides/slides_notations_premiers_pas.html#fonction-de-r√©partition",
    "title": "Notations et premiers pas",
    "section": "Fonction de r√©partition",
    "text": "Fonction de r√©partition\n\nD√©finition 3 (Fonction de r√©partition üá¨üáß: cumulative distribution function) \nSoit \\(X\\) une variable al√©atoire sur \\((\\mathbb{R}, \\mathcal{B}(\\mathbb{R}))\\). La fonction de r√©partition de \\(X\\) est la fonction \\(F_X\\) d√©finie sur \\(\\mathbb{R}\\) par \\[\n\\begin{align*}\n     F_X(x) & = \\mathbb{P}(X \\leq x)\\\\\n     & \\class{fragment}{{} = \\mathbb{P}(X \\in ]-\\infty, x])}\n\\end{align*}\n\\]",
    "crumbs": [
      "Slides",
      "Notations et premiers pas"
    ]
  },
  {
    "objectID": "Slides/slides_notations_premiers_pas.html#propri√©t√©-√©l√©mentaire-de-la-fonction-de-r√©partition",
    "href": "Slides/slides_notations_premiers_pas.html#propri√©t√©-√©l√©mentaire-de-la-fonction-de-r√©partition",
    "title": "Notations et premiers pas",
    "section": "Propri√©t√© √©l√©mentaire de la fonction de r√©partition",
    "text": "Propri√©t√© √©l√©mentaire de la fonction de r√©partition\n\nProposition 2 (Propri√©t√©s √©l√©mentaires) Soit \\(X\\) une v.a. de fonction de r√©partition \\(F_X\\).\n\n\n\\(F_X\\) est une fonction croissante, de limite \\(0\\) en \\(-\\infty\\) et de limite \\(1\\) en \\(+\\infty\\).\n\\(F_X\\) est continue √† droite en tout point.\n\\(\\forall x \\in \\mathbb{R}\\), on a \\(\\mathbb{P}(X=x) = F_X(x) - \\lim_{\\epsilon \\to 0+}F_X(x- \\epsilon)\\).\nSi \\(X\\) a pour densit√© \\(f\\), alors \\(F_X\\) est d√©rivable \\(\\lambda\\)-presque partout de d√©riv√©e \\(f\\).\n\n\n\n\nD√©monstration: voir par exemple (Barbe et Ledoux 2006)\n\n\n\n\n\n\n\n\nNote\n\n\n\nProp. 1. et 2. : \\(F_X\\) est c√†dl√†g (continue √† droite, limite √† gauche).\nProp 3. (cas discret): les valeurs prises par \\(X\\) correspondent aux discontinuit√©s de \\(F_X\\), les probabilit√©s, √† la hauteur du saut.\nProp. 4. (cas continu): le lien entre la fonction de r√©partition et densit√©.",
    "crumbs": [
      "Slides",
      "Notations et premiers pas"
    ]
  },
  {
    "objectID": "Slides/slides_notations_premiers_pas.html#fonction-de-r√©partition-et-caract√©risation-de-la-loi",
    "href": "Slides/slides_notations_premiers_pas.html#fonction-de-r√©partition-et-caract√©risation-de-la-loi",
    "title": "Notations et premiers pas",
    "section": "Fonction de r√©partition et caract√©risation de la loi",
    "text": "Fonction de r√©partition et caract√©risation de la loi\n\nTh√©or√®me 1 (Caract√©risation de la loi d‚Äôune variable al√©atoire r√©elle) \nLa fonction de r√©partition d‚Äôune variable al√©atoire caract√©rise sa loi : deux variables al√©atoires ont m√™me loi si et seulement si elles ont m√™me fonction de r√©partition.\n\n\n\nD√©monstration: voir Wikipedia \nRappel: la tribu des bor√©liens est engendr√©e par la famille d‚Äôensembles \\(\\{]-\\infty,x], x \\in \\mathbb{R}\\}\\) \nInterpr√©tation: conna√Ætre \\(\\mathbb{P}_X\\) sur cette famille d‚Äôensembles \\(\\implies\\) la conna√Ætre partout",
    "crumbs": [
      "Slides",
      "Notations et premiers pas"
    ]
  },
  {
    "objectID": "Slides/slides_notations_premiers_pas.html#fonction-de-r√©partition-cas-discret",
    "href": "Slides/slides_notations_premiers_pas.html#fonction-de-r√©partition-cas-discret",
    "title": "Notations et premiers pas",
    "section": "Fonction de r√©partition: cas discret",
    "text": "Fonction de r√©partition: cas discret\n Dans le cas d‚Äôune loi discr√®te, la fonction de r√©partition est une fonction en escalier, constante par morceaux, et croissante.   \n\nExemple 8 (Cas discret) Soit \\((x_i)_{i \\in I}\\) une suite ordonn√©e de r√©els, avec \\(I \\subset \\mathbb{N}\\). Si \\(X\\) est une variable al√©atoire discr√®te prenant les valeurs \\((x_i)_{i \\in I}\\) et de loi \\((p_i = \\mathbb{P}(X=x_i))_{i \\in I}\\), alors \\[\n\\forall x \\in \\mathbb{R}, \\quad F_X(x) = \\sum_{i \\in I} p_i {1\\hspace{-3.8pt} 1}_{[x_i, \\infty[}(x) \\enspace.\n\\]",
    "crumbs": [
      "Slides",
      "Notations et premiers pas"
    ]
  },
  {
    "objectID": "Slides/slides_notations_premiers_pas.html#fonction-de-r√©partition-cas-continu",
    "href": "Slides/slides_notations_premiers_pas.html#fonction-de-r√©partition-cas-continu",
    "title": "Notations et premiers pas",
    "section": "Fonction de r√©partition: cas continu",
    "text": "Fonction de r√©partition: cas continu\n\n\n\n\nExemple 9 (Cas continu) Si \\(X\\) est une variable al√©atoire de densit√© \\(f\\), alors \\[\n\\forall x \\in \\mathbb{R}, \\quad F_X(x) = \\int_{-\\infty}^x f(t) \\, \\mathrm dt \\enspace.\n\\]\n\nVocabulaire: densit√© (üá¨üáß: probability density function)",
    "crumbs": [
      "Slides",
      "Notations et premiers pas"
    ]
  },
  {
    "objectID": "Slides/slides_notations_premiers_pas.html#loi-normale",
    "href": "Slides/slides_notations_premiers_pas.html#loi-normale",
    "title": "Notations et premiers pas",
    "section": "Loi normale",
    "text": "Loi normale\nCas de la loi normale centr√©e r√©duite, \\(X \\sim \\mathcal{N}(0,1)\\): \\(F_X=\\Phi\\), avec \\(\\Phi\\) d√©finie par \\[\nF_X(x) \\triangleq \\Phi(x) = \\dfrac{1}{\\sqrt{2\\pi}} \\int_{-\\infty}^x e^{-\\frac{t^2}{2}}\\, \\mathrm d t\\enspace,\n\\]\n\n\n\n\n\n\nNote\n\n\nL‚Äôint√©grale ne peut √™tre obtenue √† partir d‚Äôune formule ferm√©e1. Autrefois, les valeurs de \\(\\Phi(x)\\) √©taient report√©es dans des tables2.\n\n\n\n\n\nTransformation affine: si \\(X \\sim \\mathcal{N}(\\mu, \\sigma^2)\\) ‚Äî i.e., \\(X=\\mu + \\sigma Y\\), avec \\(Y\\sim \\mathcal{N}(0,1)\\) ‚Äî alors \\[\nF_X(x)=\\Phi\\left(\\frac{x-\\mu}{\\sigma}\\right)\n\\]\n\n\nWikipedia: Th√©or√®me de LiouvilleWikipedia: loi normale",
    "crumbs": [
      "Slides",
      "Notations et premiers pas"
    ]
  },
  {
    "objectID": "Slides/slides_notations_premiers_pas.html#fonction-quantile-inverse-g√©n√©ralis√©e-√†-gauche",
    "href": "Slides/slides_notations_premiers_pas.html#fonction-quantile-inverse-g√©n√©ralis√©e-√†-gauche",
    "title": "Notations et premiers pas",
    "section": "Fonction quantile, inverse g√©n√©ralis√©e √† gauche",
    "text": "Fonction quantile, inverse g√©n√©ralis√©e √† gauche\n\nD√©finition 4 (Fonction quantile/ inverse g√©n√©ralis√©e üá¨üáß: quantile distribution function) \nSoit \\(X\\) une variable al√©atoire sur \\((\\mathbb{R}, \\mathcal{B}(\\mathbb{R}))\\) et \\(F_X\\) sa fonction de r√©partion. La fonction quantile associ√©e \\(F_X^\\leftarrow: ]0,1[ \\rightarrow \\mathbb{R}\\) est d√©finie par \\[\n  F_X^\\leftarrow(p) = \\inf\\{ x \\in \\mathbb{R} : F_X(x)\\geq p\\} \\enspace.\n\\]\n\n\n\\(F_X\\) est bijective \\(\\implies\\) \\(F^{-1}=F_X^\\leftarrow\\)\n\nVocabulaire:\n\nla fonction quantile s‚Äôappelle aussi inverse de Levy ou inverse g√©n√©ralis√©e (√† gauche)\nm√©diane : \\(F_X^\\leftarrow(1/2)\\)\npremier/troisi√®me quartile: \\(F_X^\\leftarrow(1/4), F_X^\\leftarrow(3/4)\\)\nd√©ciles : \\(F_X^\\leftarrow(k/10)\\) pour \\(k=1,\\dots, 9\\)",
    "crumbs": [
      "Slides",
      "Notations et premiers pas"
    ]
  },
  {
    "objectID": "Slides/slides_notations_premiers_pas.html#quantiles-cas-continu",
    "href": "Slides/slides_notations_premiers_pas.html#quantiles-cas-continu",
    "title": "Notations et premiers pas",
    "section": "Quantiles: cas continu",
    "text": "Quantiles: cas continu\n\n\n\nPlotly = require('plotly.js-dist');\ndists = require( 'https://cdn.jsdelivr.net/gh/stdlib-js/stats-base-dists@umd/browser.js' );\n// see source here: https://github.com/stdlib-js/stats-base-dists/tree/umd\n// continuous case\njstatPDFs = () =&gt; {\n  const distributions = Object.keys(dists);\n  // Get in continuousDistributions the distributions whose pdf, cdf and quantile are defined\n    const continuousDistributions = distributions.filter(name =&gt; dists[name].pdf && dists[name].cdf && dists[name].quantile);\n  return continuousDistributions\n};\noutput = jstatPDFs();\n\nexcludedPDFs = [];\npdfNames_unsorted = output.filter(name =&gt; !excludedPDFs.includes(name));\npdfNames=pdfNames_unsorted.toSorted();\nviewof inputs = Inputs.form([\n      Inputs.range([-10, 10], {value: 0.1, step: 0.001, label: tex`\\mu`, width: 500}),\n      Inputs.range([0.01, 5], {value: 1.01, step: 0.001, label: tex`\\sigma`, width: 500}),\n      Inputs.range([0.001, 0.999], {value: 0.75, step: 0.001, label:tex`\\alpha`, width: 500}),\n    ]);\n\n\nviewof distrib_name = Inputs.select(pdfNames, {value: \"normal\", label: \"Loi\", width: 500});\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n{\n  const x = d3.range(-5, 5, 0.01);\n  const pdf = x.map(x =&gt; dists[distrib_name].pdf(x, mu, sigma));\n  const cdf = x.map(x =&gt; dists[distrib_name].cdf(x, mu, sigma));\n  const inv = x.map(x =&gt; dists[distrib_name].quantile(x, mu, sigma));\n  const quantile = dists[distrib_name].quantile(alpha, mu, sigma);\n  const filteredX = x.filter(coord =&gt; coord &lt; quantile);\n  const filteredPdf = pdf.filter((_, i) =&gt; x[i] &lt;= quantile);\n  const filteredCdf = cdf.filter(coord =&gt; coord &lt;= quantile);\n\n\n{\nvar trace1 = {\n      type: \"scatter\",\n      name: 'Quantile',\n      x: cdf,\n      y: x,\n      line: {color: 'black'},\n      xaxis: 'x1',\n      yaxis: 'y3',\n\n\n};\n\nvar trace12 = {\n        x : [alpha, alpha],\n        y : [x[0], quantile],\n        mode: 'lines',\n        line: {\n            dash: 'dash',\n            width: 1,\n            color: ' #428BCA',\n            marks: {\n                size: 0,\n                color: ' #428BCA',\n            }\n\n          },\n      xaxis: 'x1',\n      yaxis: 'y3',\n\n\n};\n\n\n\nvar trace13 = {\n    x: [0, alpha],\n    y: [quantile, quantile],\n    mode: 'scatter',\n    line: {\n        dash: 'dash',\n        width: 1,\n        color: ' #428BCA',\n        marks: {\n            size: 0,\n        }\n    },\n    xaxis: 'x1',\n    yaxis: 'y3'\n}\n\n\nvar trace2 = {\nx: cdf,\ny: cdf,\ntype: 'scatter',\nname: 'identity',\nline: {color: 'black'},\nxaxis: 'x1',\nyaxis: 'y2'\n\n};\n\n\nvar trace21 = {\n        x : [alpha, 1],\n        y : [alpha, alpha],\n        mode: 'lines',\n        line: {\n            dash: 'dash',\n            width: 1,\n            color: ' #428BCA',\n            marks: {\n                size: 0,\n                color: ' #428BCA',\n            }\n\n          },\n      xaxis: 'x1',\n      yaxis: 'y2'\n}\n\nvar trace22 = {\n        x : [alpha, alpha],\n        y : [alpha, 1],\n        mode: 'lines',\n        line: {\n            dash: 'dash',\n            width: 1,\n            color: ' #428BCA',\n            marks: {\n                size: 0,\n                color: ' #428BCA',\n            }\n\n          },\n      xaxis: 'x1',\n      yaxis: 'y2'\n}\n\n\n\nvar trace23 = {\n    x: [alpha],\n    y: [alpha],\n    mode: 'scatter',\n    line: {\n        dash: 'dash',\n        width: 1,\n        color: ' #428BCA',\n        marks: {\n            size: 0,\n        }\n    },\n    xaxis: 'x1',\n    yaxis: 'y2'\n}\n\n\nvar trace31 = {\n      type: \"scatter\",\n      mode: \"lines\",\n      name: 'PDF2',\n      x: x,\n      y: cdf,\n      line: {color: 'black'},\n      xaxis: 'x2',\n      yaxis: 'y2',\n};\n\nvar trace32 = {\n        x: filteredX,\n        y: filteredX.map(x =&gt; alpha),\n        mode: 'lines',\n        line: {\n            dash: 'dash',\n            width: 1,\n            color: ' #428BCA',\n            marks: {\n                size: 0,\n                color: ' #428BCA',\n            }\n          },\n      xaxis: 'x2',\n      yaxis: 'y2'\n}\n\nvar trace33 = {\n        x : [quantile, quantile],\n        y : [0, alpha],\n        mode: 'lines',\n        line: {\n            dash: 'dash',\n            width: 1,\n            color: ' #428BCA',\n            marks: {\n                size: 0,\n                color: ' #428BCA',\n            }\n          },\n\n      xaxis: 'x2',\n      yaxis: 'y2'\n}\n\nvar trace34 = {\n    x: [quantile],\n    y: [alpha],\n    mode: 'scatter',\n    line: {\n        dash: 'dash',\n        width: 1,\n        color: ' #428BCA',\n        marks: {\n            size: 0,\n        }\n    },\n    xaxis: 'x2',\n    yaxis: 'y2'\n}\n\n\nvar trace41 = {\n\n  type: \"scatter\",\n  name: 'Quantile2',\n  fill: 'tozeroy',\n  x: filteredX,\n  y: filteredPdf,\n  opacity: 0.9,\n  line: {color: ' #428BCA'},\n  xaxis: 'x2',\n  yaxis: 'y1'\n\n};\n\n\nvar trace42 = {\n\n  type: \"scatter\",\n  mode: \"lines\",\n  name: 'PDF2',\n  x: x,\n  y: pdf,\n  line: {color: 'black'},\n  xaxis: 'x2',\n  yaxis: 'y1'\n\n};\n\nvar data = [\n  trace1,\n  trace12, trace13,\n  trace2, trace21, trace22, trace23,\n  trace31, trace32, trace33, trace34,\n  trace41, trace42];\n\n\nvar layout = {\n\n  title: 'Distribution et quantile',\n  xaxis: {\n    domain: [0, 0.32],\n    anchor: 'y1'\n  },\n  yaxis: {\n    domain: [0, 0.24],\n    anchor: 'x1'\n\n  },\n  xaxis2: {\n    domain: [0.35, 1],\n    anchor: 'y'\n  },\n\n  yaxis2: {\n    domain: [0.26, 0.49],\n    anchor: 'x1'\n  },\n\n  yaxis3: {\n    domain: [0.5, 1],\n    anchor: 'x1'\n  },\n\n// legend offset\n  showlegend: false,\n  height: 680,\n  annotations: [\n\n    {\n      x: 1/4,\n      y: quantile - (x[0]-x.slice(-1))/20,\n      xref: 'x1',\n      yref: 'y3',\n      text: 'q= ' + quantile.toFixed(2),\n      font: {\n        size: 12,\n        color: '#428BCA',\n\n      },\n      showarrow: false,\n      arrowhead: 0,\n      ax: 25,\n      ay: -10,\n\n    },\n    {\n      x: 1/4,\n      y: alpha,\n      xref: 'x1',\n      yref: 'y2',\n      text: 'alpha= ' + alpha.toFixed(2),\n      font: {\n        size: 12,\n        color: '#428BCA',\n\n      },\n      showarrow: false,\n      arrowhead: 0,\n      ax: 25,\n      ay: -10,\n\n    },\n    {\n        text: \"Fonction quantile\",\n      font: {\n      size: 15,\n      color: 'black',\n    },\n    showarrow: false,\n    align: 'center',\n    x: -0.01,\n    y: 1.05,\n    xref: 'paper',\n    yref: 'paper',\n    },\n    {\n      text: \"Fonction de r√©partition\",\n      font: {\n      size: 15,\n      color: 'black',\n            },\n    showarrow: false,\n    align: 'center',\n    x: 0.65,\n    y: 0.52,\n    xref: 'paper',\n    yref: 'paper',\n    },\n    {\n      text: \"Fonction de densit√©\",\n      font: {\n      size: 15,\n      color: 'black',\n            },\n    showarrow: false,\n    align: 'center',\n    x: 0.65,\n    y: 0.225,\n    xref: 'paper',\n    yref: 'paper',\n    },\n  ]\n\n};\n// XXX: TODO: put the xticks labels on the middle plot for x and on the right plot for y\n    var config = {responsive: true}\n    const div = DOM.element('div');\n    Plotly.newPlot(div, data, layout, config);\n    return div;\n  }\n\n}\n\n\n\n\n\n\n\nmu = inputs[0];\nsigma = inputs[1];\nalpha = inputs[2]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote: voir aussi Notations et rappels",
    "crumbs": [
      "Slides",
      "Notations et premiers pas"
    ]
  },
  {
    "objectID": "Slides/slides_notations_premiers_pas.html#quantiles-cas-discret",
    "href": "Slides/slides_notations_premiers_pas.html#quantiles-cas-discret",
    "title": "Notations et premiers pas",
    "section": "Quantiles: cas discret",
    "text": "Quantiles: cas discret\n\n\n\ndiscretePDFs = () =&gt; {\n  const distributions = Object.keys(dists);\n  // Get in continuousDistributions the distributions whose pdf, cdf and quantile are defined\n    const continuousDistributions = distributions.filter(name =&gt; dists[name].pmf && dists[name].cdf && dists[name].quantile);\n  return continuousDistributions\n};\noutput_discr = discretePDFs();\n\npmfNames_unsorted = output_discr.filter(name =&gt; !excludedPDFs.includes(name));\npmfNames=pmfNames_unsorted.toSorted();\nviewof inputs_disc = Inputs.form([\n      Inputs.range([-10, 10], {value: 0.1, step: 0.001, label: tex`\\mu `, width: 500}),\n      Inputs.range([0.01, 5], {value: 1.01, step: 0.001, label: tex`\\sigma `, width: 500}),\n      Inputs.range([0.001, 0.999], {value: 0.75, step: 0.001, label: tex`\\alpha`, width: 500}),\n    ]);\n\nviewof distrib_name_discr = Inputs.select(pmfNames, {value: \"normal\", label: \"Loi\", width: 500});\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n{\n  const z = d3.range(-5, 5, 0.01);\n  const pmf = z.map(z =&gt; dists[distrib_name_discr].pmf(z, mu_disc, sigma_disc));\n  const cdf = z.map(z =&gt; dists[distrib_name_discr].cdf(z, mu_disc, sigma_disc));\n  const inv = z.map(z =&gt; dists[distrib_name_discr].quantile(z, mu_disc, sigma_disc));\n  const quantile = dists[distrib_name_discr].quantile(alpha_disc, mu_disc, sigma_disc);\n  const filteredX = z.filter(coord =&gt; coord &lt;= quantile);\n  const filteredPmf = pmf.filter((_, i) =&gt; z[i] &lt;= quantile);\n  const filteredCdf = cdf.filter(coord =&gt; coord &lt;= quantile);\n\n\n{\nvar trace1 = {\n      type: \"scatter\",\n      name: 'Quantile',\n      x: cdf,\n      y: z,\n      line: {color: 'black'},\n      xaxis: 'x1',\n      yaxis: 'y3',\n\n\n};\n\nvar trace12 = {\n        x : [alpha_disc, alpha_disc],\n        y : [z[0], quantile],\n        mode: 'lines',\n        line: {\n            dash: 'dash',\n            width: 1,\n            color: ' #428BCA',\n            marks: {\n                size: 0,\n                color: ' #428BCA',\n            }\n\n          },\n      xaxis: 'x1',\n      yaxis: 'y3',\n\n\n};\n\n\n\nvar trace13 = {\n    x: [0, alpha_disc],\n    y: [quantile, quantile],\n    mode: 'scatter',\n    line: {\n        dash: 'dash',\n        width: 1,\n        color: ' #428BCA',\n        marks: {\n            size: 0,\n        }\n    },\n    xaxis: 'x1',\n    yaxis: 'y3'\n}\n\n\nvar trace2 = {\nx: cdf,\ny: cdf,\ntype: 'scatter',\nname: 'identity',\nline: {color: 'black'},\nxaxis: 'x1',\nyaxis: 'y2'\n\n};\n\n\nvar trace21 = {\n        x : [alpha_disc, 1],\n        y : [alpha_disc, alpha_disc],\n        mode: 'lines',\n        line: {\n            dash: 'dash',\n            width: 1,\n            color: ' #428BCA',\n            marks: {\n                size: 0,\n                color: ' #428BCA',\n            }\n\n          },\n      xaxis: 'x1',\n      yaxis: 'y2'\n}\n\nvar trace22 = {\n        x : [alpha_disc, alpha_disc],\n        y : [alpha_disc, 1],\n        mode: 'lines',\n        line: {\n            dash: 'dash',\n            width: 1,\n            color: ' #428BCA',\n            marks: {\n                size: 0,\n                color: ' #428BCA',\n            }\n\n          },\n      xaxis: 'x1',\n      yaxis: 'y2'\n}\n\nvar trace23 = {\n    x: [alpha_disc],\n    y: [alpha_disc],\n    mode: 'scatter',\n    line: {\n        dash: 'dash',\n        width: 1,\n        color: ' #428BCA',\n        marks: {\n            size: 0,\n        }\n    },\n    xaxis: 'x1',\n    yaxis: 'y2'\n}\n\n\nvar trace31 = {\n      type: \"scatter\",\n      mode: \"lines\",\n      name: 'PDF2',\n      x: z,\n      y: cdf,\n      line: {color: 'black'},\n      xaxis: 'x2',\n      yaxis: 'y2',\n};\n\nvar trace32 = {\n        x: filteredX,\n        y: filteredX.map(z =&gt; alpha_disc),\n        mode: 'lines',\n        line: {\n            dash: 'dash',\n            width: 1,\n            color: ' #428BCA',\n            marks: {\n                size: 0,\n                color: ' #428BCA',\n            }\n          },\n      xaxis: 'x2',\n      yaxis: 'y2'\n}\n\nvar trace33 = {\n        x : [quantile, quantile],\n        y : [0, alpha_disc],\n        mode: 'lines',\n        line: {\n            dash: 'dash',\n            width: 1,\n            color: ' #428BCA',\n            marks: {\n                size: 0,\n                color: ' #428BCA',\n            }\n          },\n\n      xaxis: 'x2',\n      yaxis: 'y2'\n}\n\nvar trace34 = {\n    x: [quantile],\n    y: [alpha_disc],\n    mode: 'scatter',\n    line: {\n        dash: 'dash',\n        width: 1,\n        color: ' #428BCA',\n        marks: {\n            size: 0,\n        }\n    },\n    xaxis: 'x2',\n    yaxis: 'y2'\n}\n\n\nvar trace41 = {\n\n  type: \"scatter\",\n  name: 'Quantile2',\n  fill: 'tozeroy',\n  x : filteredX,\n  y : filteredPmf,\n  opacity: 0.9,\n  line: {color: ' #428BCA'},\n  xaxis: 'x2',\n  yaxis: 'y1'\n\n};\n\n\nvar trace42 = {\n\n  type: \"scatter\",\n  mode: \"lines\",\n  name: 'PDF2',\n  x: z,\n  y: pmf,\n  line: {color: 'black'},\n  xaxis: 'x2',\n  yaxis: 'y1'\n\n};\n\nvar data = [\n  trace1,\n  trace12, trace13,\n  trace2, trace21, trace22, trace23,\n  trace31, trace32, trace33, trace34,\n  trace41, trace42];\n\n\nvar layout = {\n\n  title: 'Distribution et quantile',\n  xaxis: {\n    domain: [0, 0.32],\n    anchor: 'y1'\n  },\n  yaxis: {\n    domain: [0, 0.24],\n    anchor: 'x1'\n\n  },\n  xaxis2: {\n    domain: [0.35, 1],\n    anchor: 'y'\n  },\n\n  yaxis2: {\n    domain: [0.26, 0.49],\n    anchor: 'x1'\n  },\n\n\n\n  yaxis3: {\n    domain: [0.5, 1],\n    anchor: 'x1'\n\n  },\n\n  showlegend: false,\n  height: 680,\n\n\n  annotations: [\n\n    {\n      x: 1/4,\n      y: quantile - (z[0]-z.slice(-1))/20,\n      xref: 'x1',\n      yref: 'y3',\n      text: 'q=' + quantile.toFixed(2),\n      font: {\n        size: 12,\n        color: '#428BCA',\n\n      },\n      showarrow: false,\n      arrowhead: 0,\n      ax: 25,\n      ay: -10,\n\n    },\n    {\n      x: 1/4,\n      y: alpha_disc,\n      xref: 'x1',\n      yref: 'y2',\n      text: 'alpha=' + alpha_disc.toFixed(2),\n      font: {\n        size: 12,\n        color: '#428BCA',\n\n      },\n      showarrow: false,\n      arrowhead: 0,\n      ax: 25,\n      ay: -10,\n\n    },\n    {\n        text: \"Fonction quantile\",\n      font: {\n      size: 15,\n      color: 'black',\n    },\n    showarrow: false,\n    align: 'center',\n    x: -0.01,\n    y: 1.05,\n    xref: 'paper',\n    yref: 'paper',\n    },\n    {\n      text: \"Fonction de r√©partition\",\n      font: {\n      size: 15,\n      color: 'black',\n            },\n    showarrow: false,\n    align: 'center',\n    x: 0.65,\n    y: 0.52,\n    xref: 'paper',\n    yref: 'paper',\n    },\n    {\n      text: \"Fonction de densit√©\",\n      font: {\n      size: 15,\n      color: 'black',\n            },\n    showarrow: false,\n    align: 'center',\n    x: 0.65,\n    y: 0.225,\n    xref: 'paper',\n    yref: 'paper',\n    },\n  ]\n\n};\n// XXX: TODO: put the xticks labels on the middle plot for x and on the right plot for y\n\n    var config = {responsive: true}\n    const div = DOM.element('div');\n    Plotly.newPlot(div, data, layout, config);\n    return div;\n  }\n\n}\n\n\n\n\n\n\n\nmu_disc = inputs_disc[0];\nsigma_disc = inputs_disc[1];\nalpha_disc = inputs_disc[2];\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote: voir aussi Notations et rappels",
    "crumbs": [
      "Slides",
      "Notations et premiers pas"
    ]
  },
  {
    "objectID": "Slides/slides_notations_premiers_pas.html#bibliographie",
    "href": "Slides/slides_notations_premiers_pas.html#bibliographie",
    "title": "Notations et premiers pas",
    "section": "Bibliographie",
    "text": "Bibliographie\n\n\nBarbe, Philippe, et Michel Ledoux. 2006. ¬´¬†Cours de Probabilite Pour la Licence (corrig√©e)¬†¬ª.\n\n\n\n\nNotations et premiers pas",
    "crumbs": [
      "Slides",
      "Notations et premiers pas"
    ]
  },
  {
    "objectID": "Slides/slides_th_asymptotique.html#loi-forte-des-grands-nombres",
    "href": "Slides/slides_th_asymptotique.html#loi-forte-des-grands-nombres",
    "title": "Th√©or√®mes asymptotiques",
    "section": "Loi forte des grands nombres",
    "text": "Loi forte des grands nombres\n\nR√©sultat fondamental: concerne le comportement asymptotique de la moyenne empirique: \\[\n\\bar X_n = \\dfrac{X_1 + \\cdots + X_n}{n} \\enspace.\n\\] quand on observe \\(n\\) variables al√©atoires i.i.d \\(X_1,\\dots,X_n\\), ayant une esp√©rance finie.\n\n\nTh√©or√®me 1 (Loi forte des grands nombres) \nSoit \\((X_n)_{n \\geq 1}\\) une suite de variables al√©atoires ind√©pendantes et identiquement distribu√©es (i.i.d.) dans \\(L^1(\\Omega, \\mathcal{F}, \\mathbb{P})\\). Notons \\(\\mu = \\mathbb{E}[X_1]\\). Alors \\(\\bar X_n\\) converge vers \\(\\mu\\) presque s√ªrement : \\[\n\\mathbb{P}\\bigg( \\dfrac{X_1 + \\cdots + X_n}{n} \\underset{n \\to \\infty}{\\longrightarrow} \\mu \\bigg) = 1\\,.\n\\]",
    "crumbs": [
      "Slides",
      "Th√©or√®mes asymptotiques"
    ]
  },
  {
    "objectID": "Slides/slides_th_asymptotique.html#interpr√©tation",
    "href": "Slides/slides_th_asymptotique.html#interpr√©tation",
    "title": "Th√©or√®mes asymptotiques",
    "section": "Interpr√©tation",
    "text": "Interpr√©tation\nIntuitivement, la probabilit√© d‚Äôun √©v√©nement \\(A\\) correspond √† la fr√©quence d‚Äôapparition de \\(A\\) quand on r√©p√®te une exp√©rience qui fait intervenir cet √©v√©nement.\n\nExemple 1 (Cas Bernouilli: pile ou face) La probabilit√© d‚Äôapparition du c√¥t√© pile (not√© \\(p\\)) peut-√™tre estim√©e en lan√ßant la pi√®ce un grand nombre de fois et en comptant le nombre de pile obtenu.\nLa loi des grands nombres justifie cette intuition : si \\(X_1, \\ldots, X_n\\) sont i.i.d. de loi de Bernoulli de param√®tre \\(p\\), alors \\[\n    \\dfrac{X_1 + \\cdots + X_n}{n} \\xrightarrow[n \\to \\infty]{p.s.} p =\\mathbb{E}(X_1)\n\\]\n\nMembre de gauche : la fr√©quence empirique de piles\nMembre de droite : la fr√©quence th√©orique de piles",
    "crumbs": [
      "Slides",
      "Th√©or√®mes asymptotiques"
    ]
  },
  {
    "objectID": "Slides/slides_th_asymptotique.html#visualisation-de-lexemple-du-pile-ou-face",
    "href": "Slides/slides_th_asymptotique.html#visualisation-de-lexemple-du-pile-ou-face",
    "title": "Th√©or√®mes asymptotiques",
    "section": "Visualisation de l‚Äôexemple du pile ou face",
    "text": "Visualisation de l‚Äôexemple du pile ou face\n#| echo: false\n#| standalone: true\n#| viewerHeight: 550\nimport numpy as np\nfrom shiny import ui, render, App, reactive\nfrom shinywidgets import output_widget, register_widget\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objs as go\n\n\nn_init = 42\n\n\napp_ui = ui.page_fluid(\n    ui.tags.head(\n        ui.tags.style(\"\"\"\n            .bslib-sidebar-layout &gt; .sidebar &gt; .sidebar-content {\n            display: flex;\n            flex-direction: column;\n            padding: 0.5rem;\n            }\n            .irs--shiny .irs-line {\n            top: 27px;\n            height: 4px;\n            }\n            .irs.irs--shiny .irs-bar {\n            top: 27px;\n            height: 4px;\n            }\n            .irs--shiny .irs-handle {\n            top: 23px;\n            width: 22px;\n            height: 10px;\n        \"\"\")\n    ),\n    ui.panel_title(\"Loi des grands nombres\"),\n    ui.layout_sidebar(\n        ui.panel_sidebar(\n            ui.input_action_button(\"seed\", \"Nouveau tirage\", class_=\"btn-primary\"),\n            ui.input_slider( \"p\", \"Esp√©rance: p\", 0.01, 0.99, value=0.35, step=0.01, ticks=False,\n            ),\n            ui.input_slider( \"n_samples\", \"√âchantillons: n\", 2, 1000, value=15, step=1, ticks=False),\n        width=3),\n    ui.panel_main(output_widget(\"my_widget\"), width = 9)\n    )\n)\n\n\n\n\ndef server(input, output, session):\n    seed = reactive.Value(42)\n\n    @reactive.Effect\n    @reactive.event(input.seed)\n    def _():\n        seed.set(np.random.randint(0, 1000))\n\n    subplots = make_subplots(\n        rows=2,\n        cols=1,\n        vertical_spacing=0.45,\n        horizontal_spacing=0.04,\n        row_heights=[5, 1],\n        subplot_titles=(\n            f\"Moyenne empirique: loi de Bernoulli\",\n            \"Tirages al√©atoires &lt;span style='color:rgb(66, 139, 202)'&gt;bleu: 0&lt;/span&gt;, &lt;span style='color:rgb(255, 0, 0)'&gt;rouge: 1&lt;/span&gt; (seed=\"\n            + f\"{n_init:03}\"\n            + \")\",\n        ),\n    )\n    fig = go.FigureWidget(subplots)\n    fig.add_trace(\n        go.Scatter(\n            mode=\"lines\",\n            line=dict(color=\"black\", width=3),\n            x=[],\n            y=[],\n            name=r\"Moyenne &lt;br&gt; empirique\",\n        ),\n        row=1,\n        col=1,\n    )\n    fig.add_trace(\n        go.Scatter(\n            mode=\"lines\",\n            line=dict(dash=\"dash\", color=\"black\", width=1),\n            marker={},\n            x=[],\n            y=[],\n            name=r\"p\",\n        ),\n        row=1,\n        col=1,\n    )\n    fig.add_trace(\n        go.Heatmap(\n            x=[],\n            z=[],\n            colorscale=[[0, \"rgb(66, 139, 202)\"], [1, \"rgb(255,0,0)\"]],\n            showscale=False,\n        ),\n        row=2,\n        col=1,\n    )\n\n    fig.update_yaxes(range=[0, 1.1], row=1, col=1)\n    fig.update_xaxes(matches=\"x1\", row=2, col=1)\n    fig.update_yaxes(visible=False, row=2, col=1)\n    fig.update_xaxes(visible=False, row=2, col=1)\n\n    fig.update_layout(\n        template=\"simple_white\",\n        showlegend=True,\n        xaxis_title=\"√âchantillons: n\",\n    )\n    fig.update_layout(autosize=True)\n\n    fig.update_layout(\n        legend=dict(\n            yanchor=\"top\",\n            y=1.18,\n            x=0.85,\n            bgcolor=\"rgba(0,0,0,0)\",\n        )\n    )\n    fig.update_layout(\n        margin=dict(l=0, r=0, b=10, t=70),\n    )\n\n    register_widget(\"my_widget\", fig)\n\n    @reactive.Effect\n    def _():\n        p = input.p()\n        n_samples = input.n_samples()\n\n        rng = np.random.default_rng(seed())\n        iterations = np.arange(1, n_samples + 1)\n        samples = rng.binomial(1, p, size=n_samples)\n        means_samples = np.cumsum(samples) / np.arange(1, n_samples + 1)\n\n        # Update data in fig:\n        fig.data[0].x = iterations\n        fig.data[0].y = means_samples\n\n        fig.data[1].x = iterations\n        fig.data[1].y = np.full((n_samples), p)\n\n        fig.data[2].x = iterations\n        fig.data[2].z = [samples]\n\n        fig.update_xaxes(range=[1, n_samples + 1])\n\n        # Update the subplot titles:\n        fig.layout.annotations[1].update(\n            text=f\"Tirages al√©atoires (seed=\"\n            + f\"{seed():03}\"\n            + \") &lt;br&gt; &lt;span style='color:rgb(66, 139, 202)'&gt;bleu: 0&lt;/span&gt;, &lt;span style='color:rgb(255, 0, 0)'&gt;rouge: 1&lt;/span&gt; \"\n        )\n\n\napp = App(app_ui, server)\n\nPour aller plus loin:\nQuand \\(p\\) varie (\\(n\\) fix√©), les signaux g√©n√©r√©s sont tr√®s tr√®s proches, ce qui ne devrait pas √™tre le cas sans structuration particuli√®re de la g√©n√©ration. L‚Äôal√©a est imparfait (structure sous-jacente)!",
    "crumbs": [
      "Slides",
      "Th√©or√®mes asymptotiques"
    ]
  },
  {
    "objectID": "Slides/slides_th_asymptotique.html#au-del√†-de-la-loi-des-grands-nombres",
    "href": "Slides/slides_th_asymptotique.html#au-del√†-de-la-loi-des-grands-nombres",
    "title": "Th√©or√®mes asymptotiques",
    "section": "Au del√† de la loi des grands nombres",
    "text": "Au del√† de la loi des grands nombres\n\n1er ordre d‚Äôapproximation de la convergence de \\(\\bar{X}_n\\): loi des grands nombres\n2√®me ordre d‚Äôapproximation: th√©or√®me central limite\n\nEnjeu: quantifier les variations de \\(\\bar X_n - \\mu\\)\nR√©ponse: th√©or√®me central limite (TCL), avec la convergence en loi d‚Äôune transformation affine de la moyenne empirique",
    "crumbs": [
      "Slides",
      "Th√©or√®mes asymptotiques"
    ]
  },
  {
    "objectID": "Slides/slides_th_asymptotique.html#th√©or√®me-central-limite-1",
    "href": "Slides/slides_th_asymptotique.html#th√©or√®me-central-limite-1",
    "title": "Th√©or√®mes asymptotiques",
    "section": "Th√©or√®me central limite",
    "text": "Th√©or√®me central limite\n\nTh√©or√®me 2 (Th√©or√®me central limite) Soit \\(X_1, \\ldots, X_n\\) une suite de variables al√©atoires i.i.d de variance \\(\\sigma^2 = {\\rm var}(X_1) \\in ]0, \\infty[\\). On note \\(\\mu = \\mathbb{E}[X_1]\\) leur esp√©rance. Alors \\[\n\\sqrt n \\left(\\tfrac{\\bar X_n - \\mu}{\\sigma} \\right) \\xrightarrow[n \\to +\\infty]{\\mathcal{L}} N\\enspace,\n\\] o√π \\(N\\) suit une loi normale centr√©e r√©duite : \\(N \\sim\\mathcal{N}(0,1)\\).\n\nInterpr√©tation: la moyenne empirique de v.a. i.i.d de variance \\(\\sigma^2\\) se comporte asymptotiquement comme une loi normale \\(\\mathcal{N}(\\mu, \\tfrac{\\sigma^2}{n})\\): \\(\\quad \\bar X_n \\approx \\mathcal{N}(\\mu, \\frac{\\sigma^2}{n})\\).\n\n\n\n\n\n\nNote\n\n\nHypoth√®ses du th√©or√®me plut√¥t faibles: variance finie uniquement",
    "crumbs": [
      "Slides",
      "Th√©or√®mes asymptotiques"
    ]
  },
  {
    "objectID": "Slides/slides_th_asymptotique.html#formulation-de-la-convergence",
    "href": "Slides/slides_th_asymptotique.html#formulation-de-la-convergence",
    "title": "Th√©or√®mes asymptotiques",
    "section": "Formulation de la convergence",
    "text": "Formulation de la convergence\nConvergence en loi \\(\\iff\\) convergence des fonctions de r√©partition (aux pts de continuit√©)\nNotations:\n\n\\(\\varphi\\) : la densit√© d‚Äôune loi normale centr√©e r√©duite \\(\\varphi(x) = \\tfrac{e^{-\\frac{x^2}{2}}}{\\sqrt{2\\pi}}\\)\n\\(\\Phi\\) : la fonction de r√©partition d‚Äôune loi normale centr√©e r√©duite \\(\\Phi(x) = \\displaystyle \\int_{-\\infty}^{x}\\varphi(u) du\\)\n\n\nR√©-√©criture du TCL: pour tout \\(a &lt; b\\) on a alors\n\\[\n\\begin{align}\n    \\mathbb{P} \\left(\\bar X_n \\in [ \\mu + \\tfrac{a \\sigma}{\\sqrt{n}}, \\mu + \\tfrac{ b \\sigma}{\\sqrt{n}}] \\right)\n    & =\n    \\mathbb{P} \\left(\\tfrac{\\bar X_n - \\mu}{\\sigma} \\in [ \\tfrac{a}{\\sqrt{n}},\\tfrac{b}{\\sqrt{n}}]\\right) \\nonumber\\\\\n    & \\class{fragment}{{} = \\mathbb{P} \\bigg( a \\leq \\sqrt n \\left(\\tfrac{\\bar X_n - \\mu}{\\sigma} \\right) \\leq b\\bigg) \\nonumber}\n    \\\\\n    & \\class{fragment}{{} \\underset{n \\to \\infty}{\\longrightarrow}  \\int_a^b \\varphi(x) \\,  dx = \\Phi(b) - \\Phi(a) \\nonumber}\\\\\n\\end{align}\n\\]",
    "crumbs": [
      "Slides",
      "Th√©or√®mes asymptotiques"
    ]
  },
  {
    "objectID": "Slides/slides_th_asymptotique.html#lien-intervalle-de-confiance-et-tcl",
    "href": "Slides/slides_th_asymptotique.html#lien-intervalle-de-confiance-et-tcl",
    "title": "Th√©or√®mes asymptotiques",
    "section": "Lien intervalle de confiance et TCL",
    "text": "Lien intervalle de confiance et TCL\n\n\nQuestion: comment choisir \\(a\\) et \\(b\\) pour obtenir un intervalle de confiance √† 95% pour \\(\\mu\\)?\nNotation: \\(\\alpha_n=\\mathbb{P} \\left(\\bar X_n \\notin [ \\mu + \\tfrac{a \\sigma}{\\sqrt{n}}, \\mu + \\tfrac{ b \\sigma}{\\sqrt{n}}] \\right)\\), on cherche \\(a, b\\) tels que \\(\\alpha_n \\approx 0.05\\).\nSimplification: choix d‚Äôun intervalle sym√©trique autour de \\(\\mu \\implies q=a=-b\\) \\[\n\\begin{align}\n& 1-\\alpha_n \\approx \\int_{-q}^q \\varphi(x) \\,  dx=\\Phi(q)-\\Phi(-q)=2 \\Phi(q)-1 \\\\\n\\implies & \\boxed{q\\approx\\Phi^{-1}(1-\\tfrac{\\alpha_n}{2})}\n\\end{align}\n\\]\nInterpretation: \\(q\\) est (approx.) le quantile de niveau \\(1-\\tfrac{\\alpha_n}{2}\\) de la loi normale centr√©e r√©duite\nNum√©riquement: on peut facilement √©valuer \\(q\\) et v√©rifier que \\(q\\approx 1.96\\) avec scipy\n\n\n\n\nfrom scipy.stats import norm  # import du module \"norm\" de scipy.stats\nq = norm.ppf((1-0.05/2))      # Calcul du quantile (en: Percent point function) de niveau 1-0.05/2\nprint(f\"{q:.2f}\")             # Affichage √† 2 d√©cimales\n\n1.96",
    "crumbs": [
      "Slides",
      "Th√©or√®mes asymptotiques"
    ]
  },
  {
    "objectID": "Slides/slides_th_asymptotique.html#visualization-du-tcl",
    "href": "Slides/slides_th_asymptotique.html#visualization-du-tcl",
    "title": "Th√©or√®mes asymptotiques",
    "section": "Visualization du TCL",
    "text": "Visualization du TCL\n#| standalone: true\n#| viewerHeight: 600\nimport numpy as np\nfrom scipy.stats import norm\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nfrom shiny import ui, render, App, reactive\nfrom shinywidgets import output_widget, render_widget\n\n\napp_ui = ui.page_fluid(\n    ui.tags.head(\n        ui.tags.style(\"\"\"\n            .bslib-sidebar-layout &gt; .sidebar &gt; .sidebar-content {\n            display: flex;\n            flex-direction: column;\n            padding: 0.1rem;\n            }\n            .irs--shiny .irs-line {\n            top: 27px;\n            height: 4px;\n            }\n            .irs.irs.irs--shiny .irs-bar {\n            top: 27px;\n            height: 4px;\n            }\n            .irs--shiny .irs-handle {\n            top: 23px;\n            width: 22px;\n            height: 10px;\n        \"\"\")\n    ),\n    ui.panel_title(\"TCL\"),\n    ui.layout_sidebar(\n        ui.panel_sidebar(\n            ui.input_action_button(\"seed\", \"Nouveau tirage\",class_=\"btn-primary\"),\n            ui.input_slider(\"p\", \"Esp√©rance: p\", 0.01, 0.99, value=0.5, step=0.01, ticks=False),\n            ui.input_slider(\"n_samples\", \"√âchantillons: n\", 1, 200, value=100, step=1, ticks=False),\n            ui.input_slider(\"n_repetitions\", \"R√©p√©titions: t\", 1, 300, value=200, step=1, ticks=False),\n        width=3\n        ),\n    ui.panel_main(output_widget(\"my_widget\"), width = 9)\n    )\n)\n\n\n\ndef server(input, output, session):\n    seed = reactive.Value(42)\n\n\n    @reactive.Effect\n    @reactive.event(input.seed)\n    def _():\n        seed.set(np.random.randint(0, 1000))\n\n\n    @output\n    @render_widget\n    def my_widget():\n\n        rng = np.random.default_rng(seed())\n        p = input.p()\n        n_repetitions = input.n_repetitions()\n        n_samples = input.n_samples()\n        iterations = np.arange(1, n_samples + 1)\n        samples = rng.binomial(1, p, size=(n_repetitions, n_samples))\n        means_samples = np.cumsum(samples, axis=1) / np.arange(1, n_samples + 1)\n        x_hist = np.linspace(0, 1, num=300)\n\n        # Create figure\n        fig = make_subplots(\n                    rows=1,\n                    cols=3,\n                    # vertical_spacing=0.5,\n                    horizontal_spacing=0.02,\n                    column_widths=[20, 2, 3],\n                    subplot_titles=(\"t = \" + str(n_repetitions) + \" r√©p√©titions\",\"\",\"\")\n                )\n\n\n        for i in range(n_repetitions):\n            fig.add_trace(\n                    go.Scatter(\n                        mode='lines',\n                        line=dict(color=\"rgba(0,0,0,0.05)\", width=1),\n                        x=iterations,\n                        y=means_samples[i,:],\n                        ),\n                        row=1, col=1,\n            )\n        fig.add_trace(\n                go.Scatter(\n                    mode='lines',\n                    line=dict(dash=\"dash\", color=\"blue\", width=1),\n                    marker={},\n                    x=iterations,\n                    y=np.full((n_samples), p),\n                    name='p'),\n                    row=1, col=1,\n        )\n        fig.update_layout(\n            template=\"simple_white\",\n            showlegend=False,\n        )\n        fig.add_trace(\n                go.Scatter(\n                    mode='markers',\n                    marker=dict(color=\"rgba(0,0,0,0.05)\", size=4),\n                    x=np.zeros(n_samples),\n                    y=means_samples[:,-1],\n                ),\n                row=1, col=2,\n\n        )\n        y_hist, bins = np.histogram(means_samples[:,-1], bins=int(np.sqrt(n_repetitions)), density=True)\n        fig.add_trace(\n            go.Bar(x=y_hist, y=bins[:-1] + np.diff(bins)/2,\n                    opacity=0.75,\n                    marker_color = 'black',\n                    orientation='h',\n                    width=np.diff(bins),\n                    name=\"Tirages de moyennes empiriques\",\n                    ),\n                row=1, col=3,\n        )\n        fig.add_trace(\n            go.Scatter(x=norm.pdf(x_hist, p, np.sqrt(p*(1-p) / n_samples)),\n                       y=x_hist,\n                       mode='lines',\n                       line=dict(color=\"red\"),\n                       legendgroup='1',\n                       name=\"TCL\"\n                       ),\n                row=1, col=3,\n        )\n\n        fig.update_xaxes(range=[1, n_samples + 1])\n        fig.update_yaxes(range=[-.05, 1.05], row=1, col=1)\n        fig.update_yaxes(matches=\"y1\",visible = False,  row=1, col=2)\n        fig.update_xaxes(range=[-0.2, 0.2], visible = False, row=1, col=2)\n\n        fig.update_yaxes(matches=\"y1\", row=1, col=3, visible=False)\n        fig.update_xaxes(range=[0, 1.1 / np.sqrt(2*np.pi* p*(1-p) / n_samples)], row=1, col=3)\n        fig.update_xaxes(visible=False, row=1, col=3)\n\n\n\n        for trace in fig['data']:\n            print(trace)\n            if (trace['name'] != 'TCL') and (trace['name'] != 'p'):\n                trace['showlegend'] = False\n        fig.update_layout(\n        margin=dict(l=0, r=0, b=10, t=100),\n        )\n        fig.update_layout(\n            title=dict(text=\"Distribution de la moyenne empirique&lt;br&gt; (cas loi de Bernoulli)\", yanchor=\"top\", y=0.95),\n            title_x=0.5,\n            showlegend=True,\n\n        )\n        fig.update_layout(\n            legend=dict(\n                yanchor=\"top\",\n                y=1.24,\n                xanchor=\"left\",\n                x=0.85,\n                bgcolor=\"rgba(0,0,0,0)\",\n            )\n        )\n        fig['layout']['xaxis']['title']='√âchantillons: n'\n\n        fig.update_layout(autosize=True)\n\n        return fig\n\n\napp = App(app_ui, server)",
    "crumbs": [
      "Slides",
      "Th√©or√®mes asymptotiques"
    ]
  },
  {
    "objectID": "Slides/slides_th_asymptotique.html#pour-aller-plus-loin-vision-des-convolutions",
    "href": "Slides/slides_th_asymptotique.html#pour-aller-plus-loin-vision-des-convolutions",
    "title": "Th√©or√®mes asymptotiques",
    "section": "Pour aller plus loin: vision des convolutions",
    "text": "Pour aller plus loin: vision des convolutions\n\n\nNotation: Soient \\(f\\) et \\(g\\) d√©finies sur \\(\\mathbb{R}\\) (int√©grables au sens de Lebesgue).\n\nD√©finition 1 (Convolution) La convolution de \\(f\\) par \\(g\\) est la fonction \\(f*g\\) suivante: \\[\n\\begin{align}\nf*g:\n\\mathbb{R} &\\mapsto \\mathbb{R} \\nonumber\\\\\nx &\\to \\int_{-\\infty}^{+\\infty} f(x-y)g(y) dy \\enspace.\\nonumber\n\\end{align}\n\\]\n\n\n\n\n\n\n\n\nNote\n\n\nOn peut aussi obtenir \\(f*g(x)\\) en calculant \\(\\int_{\\mathbb{R}^2} f(u)g(v) {1\\hspace{-3.8pt} 1}_{u+v=x} du dv\\).",
    "crumbs": [
      "Slides",
      "Th√©or√®mes asymptotiques"
    ]
  },
  {
    "objectID": "Slides/slides_th_asymptotique.html#somme-et-convolutions",
    "href": "Slides/slides_th_asymptotique.html#somme-et-convolutions",
    "title": "Th√©or√®mes asymptotiques",
    "section": "Somme et convolutions",
    "text": "Somme et convolutions\n\nTh√©or√®me 3 (Loi de la somme et convolutions) Soient \\(X\\) et \\(Y\\) des v.a. ind√©pendantes de densit√©s respectives \\(f\\) et \\(g\\), alors la densit√© de \\(X+Y\\) est donn√©e par la convolution \\(f*g\\).\n\nRappel: pour un scalaire \\(\\alpha\\neq 0\\), la densit√© de \\(\\alpha X\\) est donn√©e par la fonction \\(x \\mapsto \\frac{1}{|\\alpha|} \\cdot f(\\frac{x}{\\alpha})\\).\n\nCorollaire 1 (Loi de la moyenne) Soient \\(X_1,\\dots,X_n\\) des v.a. i.i.d. de densit√© \\(f\\), la densit√© de \\(\\bar{X}_n\\) est donn√©e par la fonction \\(x \\mapsto n \\cdot [f*\\dots*f](n \\cdot x)\\).",
    "crumbs": [
      "Slides",
      "Th√©or√®mes asymptotiques"
    ]
  },
  {
    "objectID": "Slides/slides_th_asymptotique.html#convolution-et-tcl",
    "href": "Slides/slides_th_asymptotique.html#convolution-et-tcl",
    "title": "Th√©or√®mes asymptotiques",
    "section": "Convolution et TCL",
    "text": "Convolution et TCL\nPour \\(X_1, \\dots, X_n\\), i.i.d., de densit√© \\(f\\), on affiche la loi de \\(\\bar{X}_n\\) (√† une constante pr√®s)\n#| standalone: true\n#| viewerHeight: 550\nimport numpy as np\nfrom shiny import ui, render, App, reactive\nfrom shinywidgets import output_widget, register_widget\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objs as go\nfrom scipy import signal\n\n\napp_ui = ui.page_fluid(\n    ui.tags.head(\n        ui.tags.style(\"\"\"\n            .bslib-sidebar-layout &gt; .sidebar &gt; .sidebar-content {\n            display: flex;\n            flex-direction: column;\n            padding: 0.5rem;\n            }\n            .irs--shiny .irs-line {\n            top: 27px;\n            height: 2px;\n            }\n            .irs.irs--shiny .irs-bar {\n            top: 27px;\n            height: 2px;\n            }\n            .irs--shiny .irs-handle {\n            top: 23px;\n            width: 22px;\n            height: 10px;\n        \"\"\")\n    ),\n    ui.panel_title(\"TCL et convolutions\"),\n    ui.layout_sidebar(\n        ui.panel_sidebar(\n            ui.input_select(\n                \"loi\",\n                \"Loi sous-jacente\",\n                {'uniforme': 'Uniforme', 'laplace' : 'Laplace'},\n            ),\n            ui.input_slider(\n                \"n_iter\",\n                \"√âchantillons: n\",\n                1,\n                10,\n                value=1,\n                step=1,\n                ticks=False,\n            ), width = 3\n    ),\n    ui.panel_main(\n        output_widget(\"my_widget\"), width = 9\n    )\n)\n)\n\n\nnnzeros = 10001\nx_min = -20\nx_max = 20\n\nx = np.linspace(x_min, x_max, nnzeros)\ny = np.zeros(nnzeros)\nmask = np.where(np.abs(x) &lt;= 0.5, 1, 0)\ny[mask == 1] = 1\ndelta = (x_max - x_min) / nnzeros\nvar = np.sum(y * x**2 *(delta)) - (np.sum(y * x * delta))**2\n\ndef convolve(signal_in, n_convolutions, delta):\n    output = np.zeros(len(signal_in))\n    if n_convolutions == 0:\n        return output\n    elif n_convolutions == 1:\n        return signal_in\n    else:\n        output = signal_in.copy()\n        for i in range(n_convolutions - 1):\n            output = signal.fftconvolve(\n                output * delta, signal_in, mode=\"same\"\n            )\n        return output\n\n\ndef server(input, output, session):\n\n    fig = go.FigureWidget()\n    fig.add_trace(\n        go.Scatter(\n            mode=\"lines\",\n            line=dict(color=\"black\", width=3),\n            x=[],\n            y=[],\n            name=\"loi de de la moyenne empirique&lt;br&gt;(variance ad√©quate)\",\n        )\n    )\n\n    fig.add_trace(\n        go.Scatter(\n            x=x,\n            y=np.exp(-(x**2) / (2 * var)) / np.sqrt(2 * var * np.pi),\n            mode=\"lines\",\n            line=dict(dash=\"dash\", color=\"red\", width=2),\n            name=f\"Loi normale&lt;br&gt;(variance ad√©quate)\",\n        )\n    )\n    fig.update_xaxes(range=[-3, 3], position=0.)\n    fig.update_yaxes(range=[0, 2 * np.max(y)], position=0.5, showticklabels=False)\n\n\n    fig.update_layout(\n        template=\"simple_white\",\n        showlegend=True,\n        autosize=True,\n        title=dict(text=\"Densit√© : &lt;br&gt; moyenne de n variables al√©atoires\", yanchor=\"top\", y=0.95),\n        title_x=0.5,\n    )\n    fig.update_layout(legend=dict(\n        yanchor=\"top\",\n        y=0.95,\n        xanchor=\"left\",\n        x=0.8,\n        font=dict(size= 18)\n    ))\n    fig.update_layout(\n    height=250,\n    margin=dict(l=0, r=0, b=10, t=100),\n    )\n\n    register_widget(\"my_widget\", fig)\n\n    @reactive.Effect\n    def _():\n        if str(input.loi()) == 'uniforme':\n            y = np.zeros(nnzeros)\n            mask = np.where(np.abs(x) &lt;= 0.5, 1, 0)\n            y[mask == 1] = 1\n\n        else:\n            y=np.exp(-np.abs(x)) / 2\n            y = y / (np.sum(y) * delta)\n\n        var = np.sum(y * x**2 * delta) - (np.sum(y * x * delta))**2\n\n        y_display = convolve(y, input.n_iter(), delta)\n        # Update data in fig:\n        fig.data[0].x = x / np.sqrt(input.n_iter())\n        fig.data[0].y = y_display * np.sqrt(input.n_iter())\n        fig.data[1].y = np.exp(-(x**2) / (2 * var)) / np.sqrt(2 * var * np.pi)\n        fig.update_yaxes(range=[0, 2 * np.max(y)], position=0.5, showticklabels=False)\n\n\napp = App(app_ui, server)\n\n\n\n\n\n\n\nPour aller plus loin\n\n\nPour plus d‚Äôinfo sur les convolutions, voir la vid√©o de 3Blue1Brown : Convolutions | Why X+Y in probability is a beautiful mess, üá¨üáß\n\n\n\n\n\nTh√©or√®mes asymptotiques",
    "crumbs": [
      "Slides",
      "Th√©or√®mes asymptotiques"
    ]
  },
  {
    "objectID": "TP/TP1.html",
    "href": "TP/TP1.html",
    "title": "TP1: Prise en main de Python",
    "section": "",
    "text": "Objectifs de ce TP\n\n\n\n\nUtiliser les op√©rateurs classiques en Python (+,-,*,/,**,@), etc., savoir cr√©er une fonction, g√©n√©rer un graphique clair et lisible\nQu‚Äôest-ce que la pr√©cision de calcul ? Comment utiliser de une visualisation pour mieux comprendre un th√©or√®me ou une fonction ?\nComprendre au mieux comment utiliser les fonctions al√©atoires (principalement les g√©n√©rateurs) en numpy.",
    "crumbs": [
      "TP",
      "TP1: Prise en main de Python"
    ]
  },
  {
    "objectID": "TP/TP1.html#prise-en-main-de-python",
    "href": "TP/TP1.html#prise-en-main-de-python",
    "title": "TP1: Prise en main de Python",
    "section": "Prise en main de Python",
    "text": "Prise en main de Python\nPython est un langage ouvert qui permet de manipuler des donn√©es, faire des analyses statistiques, tracer des graphes, et bien d‚Äôautres choses encore. Il est distribu√© gratuitement et vous pouvez le t√©l√©charger et l‚Äôinstaller sur une machine personnelle. Dans ce premier TP, on pr√©sente les bases de Python.\nPour plus de d√©tails on pourra consulter les ouvrages:\n\nIntroduction √† Python Cours de Python üá´üá∑\nHLMA310 - Logiciels scientifiques üá´üá∑\nManuel d‚Äôalgorithmique en Python (Courant et al. 2013) üá´üá∑\n\n\nCourant, J., M. de Falco, S. Gonnord, J.-C. Filli√¢tre, S. Conchon, G. Dowek, et B. Wack. 2013. Informatique pour tous en classes pr√©paratoires aux grandes √©coles: Manuel d‚Äôalgorithmique et programmation structur√©e avec Python. Eyrolles.\n\nInstallation de Python\nTout est d√©j√† install√© sur les ordinateurs de l‚Äôuniversit√©. Cette section n‚Äôest utile que si vous souhaitez utiliser votre propre machine.\nLe conseil principal, est d‚Äôinstaller VSCode et d‚Äôutiliser l‚Äôextension Python associ√©e. Pour Python, privil√©gier Conda (ou Mamba) pour installer les packages, voir par exemple: installer-anaconda.\n\n\nL‚Äôenvironnement de travail VSCode / VSCodium\nOn travaillera sous VSCodium (une variante de VSCode) sur les machines de l‚ÄôUniversit√©, un √©diteur de texte qui permet de travailler avec Python, mais aussi avec LaTeX, Markdown, R, etc.\n\nLancer l‚Äôapplication VSCodium, par exemple en cliquant sur l‚Äôic√¥ne ‚ÄúApplication Menu‚Äù en haut √† gauche de votre √©cran, puis en tapant ‚ÄúVSCodium‚Äù dans la barre de recherche. La d√©marche √† suivre est visible dans la vid√©o ci-dessous:\n\n\n\n\nSi besoin (√† ne faire qu‚Äôune fois), il vous faut installer l‚Äôextension ‚Äúhttps://open-vsx.org/extension/ms-python/python‚Äù. Pour cela, il y a plusieurs strat√©gies. La plus simple consiste √† cliquer sur le menu d‚Äôinstallation et chercher l‚Äôapplication Python ‚ÄúPython, extension for Visual Studio Code‚Äù, propos√©e par Microsoft, Intellisense, (attention il y a beaucoup, choisir la bonne, avec plusieurs millions d‚Äô√©toiles et de t√©l√©chargements):\n\n\nRemarque: Une alternative est d‚Äôaller dans le menu ‚ÄúView/Command Palette‚Äù (accessible avec ctrl + shift + p), et taper ‚ÄúExtensions : install extensions‚Äù et installer l‚Äôextension ‚ÄúPython, extension for Visual Studio Code‚Äù (propos√©e par Microsoft). Au besoin, il faudra recharger (reload) VSCodium. Si vous avez d√©j√† install√© l‚Äôextension Python sur votre machine personnelle, vous pouvez passer cette √©tape.\nLa m√™me op√©ration devra √™tre faite pour installer l‚Äôextension ‚Äúhttps://open-vsx.org/extension/ms-toolsai/jupyter‚Äù (propos√©e par Microsoft,) qui nous permettra de manipuler des fen√™tres interactives:\n\n\n\n\nPremiers pas\n\nCr√©er un nouveau fichier dans VSCodium intitul√© HAX603X_tp1.py, et sauvegarder le dans un dossier HAX603X.\nDans ce fichier, copier-coller le code de la bo√Æte suivante. On pourra alors lancer des cellules de code en tapant sur shift + enter dans une cellule d√©limit√©e par les symboles \\# \\%\\%. On peut aussi lancer la cellule en cliquant sur le bouton ‚Äúrun cell‚Äù dans VSCodium (ou clique droit puis une option de type ‚Äúrun cell‚Äù ou ‚Äúrun all cell‚Äù).\n\n\n# %%\n# D√©but de cellule\nprint(1 + 3)  # commentaire en ligne\n# %%\n# Une autre cellule\nprint(2**3)  # commentaire en ligne\n\n4\n8\n\n\n\n\n\nCliquer dans VSCodium sur la version de ‚ÄúPython‚Äù en bas de votre √©cran et choisir sur les machines de l‚Äô√©cole l‚Äôenvironnement ‚Äòdatascience‚Äô (version: 3.10.6 au 20/01/2024). Si vous travaillez sur votre machine personnelle, choisissez un environnement de base, ou bien cr√©er un environnement conda qui vous conviendra, par exemple avec Miniconda1.\nV√©rifier que maintenant vous pouvez lancer une cellule, par exemple en tapant crtl + enter, ou bien en cliquant sur le bouton ‚Äúrun cell‚Äù.\n\n1¬†Installer un environnement de d√©veloppement Python avec Conda\n\nL‚Äôenvironnement de travail\nVous voyez appara√Ætre plusieurs fen√™tres :\n\nla console (√† droite), avec les environnement et l‚Äôhistorique (en haut √† droite)\nla fen√™tre de texte (√† gauche)\n\nLa console permet d‚Äôex√©cuter des instructions ou commandes. C‚Äôest ici que vous donnez vos instructions et que s‚Äôaffichent les r√©sultats demand√©s. La fen√™tre d‚Äôenvironnement et d‚Äôhistorique recense l‚Äôhistorique des commandes et les variables qui ont √©t√© d√©finies. Enfin, la fen√™tre de texte permet d‚Äô√©crire du texte, des commentaires, bref les fichiers que vous conserverez.\nUne mani√®re simple de garder traces de vos calculs/instructions est de les √©crire dans un fichier texte (ici HAX603X_tp1.py), et de les d√©limiter par des symboles \\# \\%\\% (voir ci-dessus), et de les lancer en tapant shift + enter dans une cellule d√©limit√©e par les symboles \\# \\%\\%.\nUne premi√®re utilisation basique de Python concerne les calculs. Vous pouvez entrer toutes les op√©rations classiques : addition +, soustraction -, multiplication *, division /, puissance **, etc. Les fonctions usuelles sont √©galement d√©j√† programm√©es en Python, mais n√©cessite le chargement du package numpy : exponentielle, logarithme, fonctions trigonom√©triques, racine carr√©e, etc.\nPour cela il suffit de taper import numpy as np dans une cellule de code, puis d‚Äôutiliser les fonctions de numpy comme suit par exemple:\nimport numpy as np\n\nprint(np.exp(1))\nprint(np.log(2))\nprint(np.sin(np.pi))\n\n\n\n2.718281828459045\n0.6931471805599453\n1.2246467991473532e-16\n\n\n\n\nQuestion : fonctions math√©matiques\nEntrez quelques op√©rations de base pour vous familiariser avec les instructions sur Python. Faire de m√™me avec les fonctions np.exp, np.log, np.sin, np.cos, np.tan, np.sqrt, np.abs,np.round. Entrer les instructions 1/0 et np.sqrt(-2). Que constatez-vous ?\nOn remarquera qu‚Äôon peut utiliser le symbole np.inf pour repr√©senter l‚Äôinfini. Par ailleurs, si un r√©sultat n‚Äôest pas possible (par exemple en tapant np.sqrt(-2) ou np.inf - np.inf), alors on obtient nan qui signifie Not a Number.\nIl faut se souvenir que les calculs num√©riques ne sont pas toujours exacts du fait de la discr√©tisation des nombres sur machine. Taper par exemple np.sin(0), np.sin(2*np.pi) et np.sin(np.pi*10**16). Voir aussi les diff√©rences entre:\n\nprint(0.6, 0.3 + 0.2 + 0.1)\nprint(0.6, 0.1 + 0.2 + 0.3)\n\n0.6 0.6\n0.6 0.6000000000000001\n\n\n\n\n\n\n\n\nPour aller plus loin\n\n\n\nOn pourra consulter https://0.30000000000000004.com/ pour plus de d√©tails sur les ce type de ph√©nom√®nes.\n\n\n\n\n\nAide en Python\nOn peut utiliser l‚Äôaide de base de python avec les commandes help(la-fonction) ou ?la-fonction. L‚Äôaide en ligne est aussi conseill√©e, surtout pour la cr√©ation de graphiques avec matplotlib pour avoir plus de d√©tails et des galleries de visualisation.\n\n\nR√©pertoire de travail\nLe r√©pertoire de travail (üá¨üáß: working directory) est le r√©pertoire par d√©faut, c‚Äôest-√†-dire le r√©pertoire qui s‚Äôouvre quand vous cliquez sur le bouton pour enregistrer un fichier. La commande pour conna√Ætre le r√©pertoire de travail actuel est getcwd du package os:\n\nimport os\nprint(os.getcwd())\n\n/home/jsalmon/Documents/Mes_cours/Montpellier/HAX603X/HAX603X/TP\n\n\nPour changer le r√©pertoire de travail, on pourra utiliser la commande os.chdir avec un nom de r√©pertoire (valide) entre guillemets, par exemple sous Linux la commande suivante permet de remonter d‚Äôun cran dans l‚Äôarborescence des r√©pertoires:\n\nos.chdir('../')\n\nSi l‚Äôon ferme la fen√™tre interactive (√† droite), alors ex√©cuter une cellule lancera une nouvelle fen√™tre interactive dans le r√©pertoire de travail qui correspond au fichier courant que l‚Äôon √©dite (ici le fichier HAX603X_tp1.py).\n\n\nCr√©ation et affectation de variables\nPour cr√©er des objets, il suffit d‚Äôutiliser la commande =.\n\nQuestion : variables\nCr√©er une variable x qui contient la valeur 12. Effectuer des calculs du type x+3, x**4, 4*x pour v√©rifier que tout se passe comme pr√©vu.\nEn pratique on donnera des noms d‚Äôobjets pertinents, par exemple\n\ndistance = 105  # en km\ntemps = 2  # en heures\nvitesse = distance/temps  # en km/h\n\nOn remarquera que lorsque l‚Äôon cr√©e des objets, ils sont stock√©es dans l‚Äôenvironnement de travail (chercher l‚Äôonglet variables de la fen√™tre interactive).",
    "crumbs": [
      "TP",
      "TP1: Prise en main de Python"
    ]
  },
  {
    "objectID": "TP/TP1.html#numpy-et-calcul-scientifique-en-python",
    "href": "TP/TP1.html#numpy-et-calcul-scientifique-en-python",
    "title": "TP1: Prise en main de Python",
    "section": "numpy et calcul scientifique en Python",
    "text": "numpy et calcul scientifique en Python\nnumpy est l‚Äôoutil de base en Python pour faire du calcul vectoriel et matriciel.\n\nVecteurs en numpy\nPour cr√©er un vecteur, la commande de base est la fonction np.array:\n\nv = np.array([1, 2, 3])\nprint(v)\n\n[1 2 3]\n\n\nEnsuite, on peut concat√©ner des vecteurs, les multiplier par une constante, leur ajouter une constante, les √©lever √† une certaine puissance, etc. Si on manipule deux vecteurs, on prendra garde √† leur taille.\n\nQuestion : Vecteurs\nCr√©ez un vecteur v1 compos√© des r√©els 7, 8, 3, un vecteur v2 compos√© des r√©els -0.5, 120, -12, et un vecteur v3 compos√© des r√©els 0, 1, 0, 1. Testez les commandes suivantes : v1-7, v2**4, 10*v3, v1+v2, v1*v2, v1/v2, v1+v3.\nIl existe un grand nombre de fonctions math√©matiques qui s‚Äôappliquent directement sur un vecteur : np.sum(), np.prod(), len(), np.min(), np.max(), np.nanmax(), np.argmax(), np.mean(), np.median(), np.var(), np.std().\nEnfin on peut aussi utiliser des fonctions de tri, partiel ou non: np.sort(), np.argsort(), np.partition(), np.argpartition() on pourra consulter l‚Äôaide en ligne pour plus de d√©tails: https://numpy.org/doc/stable/reference/routines.sort.html, et les teser sur le vecteur v2 par exemple.\n\n\nQuestion : Op√©rations sur les vecteurs\nCr√©ez un vecteur de taille 5 et appliquez-lui les fonctions pr√©c√©dentes. Si vous ne comprenez pas la sortie (utiliser l‚Äôaide avec ? ou la documentation en ligne).\n\n\n\n\n\n\nPour aller plus loin: vectorisation\n\n\n\nVous pourrez consulter les commandes d√©crites visuellement ici pour cr√©er des vecteurs et/ou des matrices classiques.\n\n\n\n\n\nSuites r√©guli√®res\nUne autre mani√®re de cr√©er des vecteurs consiste √† cr√©er des suites r√©guli√®res :\n\nLa commande np.arange(n1, n2) cr√©e un vecteur de r√©els partant de n1 et croissant d‚Äôune unit√© pour arriver √† n2 (exclu). On peut changer le pas en ajoutant un argument optionnel np.arange(n1, n2, step=pas). Ainsi,\n\nnp.arange(0, 10, step=2)\n\narray([0, 2, 4, 6, 8])\n\n\nLa commande np.tile() permet de r√©p√©ter un vecteur un nombre de fois fix√©.\n\n\nQuestion: arange et tile\nEx√©cutez les commandes suivantes et essayer d‚Äôanalyser les sorties :\n\nnp.arange(9, 13)\nnp.arange(3, -8, step=-1)\nnp.arange(9, 13, step=2)\nnp.arange(9, 13, step=3)\nnp.tile(np.arange(9, 13, step=3), (4, 1))\n\narray([[ 9, 12],\n       [ 9, 12],\n       [ 9, 12],\n       [ 9, 12]])\n\n\nEnfin, pour extraire la valeur d‚Äôindice i d‚Äôun vecteur x, on tapera x[i] (avec la convention que Python commence √† √©num√©rer √† 0). Plus g√©n√©ralement, pour extraire les valeurs associ√©es aux indices 3, 4 et 7, on tapera x[[3,4,7]]. Le v√©rifier sur un vecteur de taille 10. On peut aussi extraire des sous parties de vecteurs, par exemple x[3:7] pour extraire les valeurs d‚Äôindice 3, 4, 5 et 6, ou bien x[3:] pour extraire les valeurs d‚Äôindice 3, 4, 5, etc. jusqu‚Äô√† la fin.\n\n\n\nMatrices en numpy\nLa fonction np.shape permet de conna√Ætre la taille d‚Äôun vecteur ou d‚Äôune matrice. On regardera son comportement sur les vecteurs notamment.\n\nQuestion : op√©rations √©l√©mentaires\nManipulez les op√©rations classiques sur des matrices (arrays) de numpy (si vous √™tes d√©j√† habitu√© √† numpy vous pouvez continuer)\nOp√©rations termes √† termes:\n\n# Somme de deux vecteurs\nA = np.array([1.0, 2, 3])\nB = np.array([-1, -2, -3.0])\n\n# Attribuer √† la variable C la somme de A et B\nsum_A_B = ...  # XXX TODO\n\nnp.testing.assert_allclose(np.zeros((3,)), sum_A_B)\nprint(\"it worked\")\n\n# Le produit terme √† terme avec *\nprod_A_B = ...  # XXX TODO\n\nnp.testing.assert_allclose(np.array([-1.0, -4, -9]), prod_A_B)\nprint(\"it worked\")\n\n# Remarque: la m√™me chose fonctionne terme √† terme avec \\, ** (puissance)\nnp.testing.assert_allclose(np.array([1.0, 4, 9]), A ** 2)\nprint(\"it worked: even for powers\")\n\nLe produit scalaire (ou matriciel) est l‚Äôop√©rateur @. V√©rifiez que pour la matrice J ci-dessous J^3 = Id de deux fa√ßons. Pour cela on pourra aussi utiliser la puissance matricielle avec np.linalg.matrix_power:\n\nJ = np.array([[0, 0, 1.0], [1.0, 0, 0], [0, 1.0, 0]])\n\nI3 = np.eye(3)\n\nnp.testing.assert_allclose(I3, ...)  # XXX TODO\nprint(\"it worked: method 1\")\nnp.testing.assert_allclose(I3, ...)  # XXX TODO\nprint(\"it worked: method 2\")\n\n\n\nQuestion : r√©solution de syst√®mes lin√©aires\nPour r√©soudre le syst√®me de la forme Ax=b en math√©matiques, la formule explicite est x=A^{-1}b (dans le cas o√π A est inversible).\n\n\n\n\n\n\nImportant\n\n\n\nEn pratique vous n‚Äôutiliserez (presque) jamais l‚Äôinversion de matrice ! En effet, on n‚Äôinverse JAMAIS JAMAIS (!) une matrice sans une tr√®s bonne raison. La plupart du temps il existe des m√©thodes plus rapides pour r√©soudre un syst√®me num√©riquement !\n\n\n\nprint(f\"L'inverse de la matrice: \\n {J} \\n est \\n {np.linalg.inv(J)}\")\n\nn = 20  # XXX TODO: tester avec n=100\nJbig = np.roll(np.eye(n), -1, axis=1)  # matrice de permutation de taille n\nprint(Jbig)\n\nb = np.arange(n)\nprint(b)\n\n# on peut transposer une matrice facilement de 2 mani√®res:\nprint(Jbig)\nprint(Jbig.T)\nprint(np.transpose(Jbig))\n\nComparons niveau temps d‚Äôexecution l‚Äôinversion explicite vs.¬†l‚Äôutilisation d‚Äôun solveur de syst√®me lin√©aire tel que np.linalg.solve:\n\nimport time\n# R√©solution de syst√®me par une m√©thode naive: inversion de matrice\nt0 = time.perf_counter()  # XXX TODO\ny1 = ... @ b\ntiming_naive = time.perf_counter() - t0\nprint(\n    f\"Temps pour r√©soudre un syst√®me avec la formule math√©matique: {timing_naive - t0:.4f} s.\"\n)\n\n# R√©solution de syst√®me par une m√©thode adapt√©e: fonctions d√©di√©e de `numpy``\nt0 = time.perf_counter()\ny2 = ...\ntiminig_optimized = time.perf_counter()\nprint(\n    f\"Temps pour r√©soudre un syst√®me avec la formule math√©matique: {timing_optimized:.4f} s.\\nC'est donc {timing_naive / timing_optimized} fois plus rapide d'utiliser la seconde formulation\"\n)\n\nnp.testing.assert_allclose(y1, y2)\nprint(\"Les deux m√©thodes trouvent le m√™me r√©sultat\")\n\n\n\n\n\n\n\nAstuce\n\n\n\nPour des comparaisons d‚Äôefficacit√© temporelle plus pouss√©es on pourra utiliser le package timeit2 ou voir la discussion ici: https://superfastpython.com/time-time-vs-time-perf_counter/.\n\n\n2¬†lien vers la documentation de timeit\n\nQuestion d√©coupage (üá¨üáß: slicing)\nLe d√©coupage permet d‚Äôextraire des √©l√©ments selon un crit√®re (position, condition, etc.). La notation : signifie ‚Äútout le monde‚Äù, et l‚Äôindexation commence en 0. Pour partir de la fin, il est possible de mettre le signe - devant le nombre: ainsi -1 renvoie donc au dernier √©l√©ment. Enfin, on peut extraire des sous suites d‚Äôindices pair ou impair, par exemple x[::2] pour extraire les valeurs d‚Äôindice pair, ou bien x[1::2] pour extraire les valeurs d‚Äôindice impair de x. Enfin on peut aussi utiliser le signe - pour partir de la fin, par exemple x[-1] pour extraire la derni√®re valeur, ou bien x[-2] pour extraire l‚Äôavant-derni√®re valeur.\n\nprint(f\"The first column is {J[:, 0]}\")\n\n# Afficher la deuxi√®me ligne de J\nprint(f\"The second row is {...}\")  # XXX TODO\n\nMettre √† z√©ro une ligne sur 2 de la matrice identit√© de taille 5\\times 5\n\nC = np.eye(5, 5)\nC[...,...] = 0  # mettre √† z√©ro une ligne sur deux. # XXX TODO",
    "crumbs": [
      "TP",
      "TP1: Prise en main de Python"
    ]
  },
  {
    "objectID": "TP/TP1.html#visualisation-dune-figure",
    "href": "TP/TP1.html#visualisation-dune-figure",
    "title": "TP1: Prise en main de Python",
    "section": "Visualisation d‚Äôune figure",
    "text": "Visualisation d‚Äôune figure\nPour lancer une figure on peut utiliser la package matplotlib. Un exemple utilisant le package numpy pour cr√©er une figure simple est donn√© ci-dessous, dans la Figure¬†1.\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = np.cos(2 * np.pi * r)\n\nfig, ax = plt.subplots()\nax.plot(r,theta)\nax.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\nFigure¬†1: Une figure simple.",
    "crumbs": [
      "TP",
      "TP1: Prise en main de Python"
    ]
  },
  {
    "objectID": "TP/TP1.html#aspects-al√©atoires",
    "href": "TP/TP1.html#aspects-al√©atoires",
    "title": "TP1: Prise en main de Python",
    "section": "Aspects al√©atoires",
    "text": "Aspects al√©atoires\nLe module random de numpy permet d‚Äôutiliser l‚Äôal√©atoire et des lois usuelles en Python. On cr√©e d‚Äôabord un g√©n√©rateur qui nous permettra ensuite d‚Äôappeler les lois voulues comme suite:\n\nimport numpy as np  # package de calcul scientifique\nimport matplotlib.pyplot as plt  # package graphique\n\ngenerateur = np.random.default_rng()\ngenerateur.normal()\n\n-0.07033936605617876\n\n\n\nQuestion : Matrices al√©atoires\nCr√©er une matrice de taille 4\\times 5 dont les entr√©es sont i.i.d de loi de Laplace d‚Äôesp√©rance 0 et de variance 2. Lancer plusieurs fois le code et observez les changements. On pourra s‚Äôaider de l‚Äôaide en ligne si besoin.\n\ngenerateur = np.random.default_rng()\nM = ...  # XXX TODO\nprint(M)\n\n\n\nQuestion : Reproduire des r√©sultats\nPour reproduire des r√©sultats ou d√©bugger un code, il est utile de ‚Äúfiger‚Äù l‚Äôal√©atoire. On utilise pour cela une graine (üá´üá∑ seed) dans la cr√©ation du g√©n√©rateur. Fixez la graine √† 0 dans default_rng() et lancez une g√©n√©ration al√©atoire. Commenter.\n\nrng = np.random.default_rng(0)\nrng.normal()\nrng2 = np.random.default_rng(...)\nrng.normal()\n\n\n\nQuestion : afficher un histogramme\nAvec plt.subplot, cr√©er 3 histogrammes de 100 tirages al√©atoires de distributions suivantes:\n\nloi gaussienne (centr√©e-r√©duite)\nloi de Cauchy\nloi de Laplace.\n\nOn utilisera les m√™mes param√®tres de centrage et d‚Äô√©chelle pour les trois lois.\n\nn_samples = 10000\nX = np.empty([n_samples, 3])\nX[:, 0] = ...\nX[:, 1] = ...\nX[:, 2] = ...\n\nlois = [\"Loi de Gauss\", \"Loi de Laplace\", \"Loi de Cauchy\"]\n\nfig_hist, ax = plt.subplots(3, 1, figsize=(3, 3))\n\nfor i, name in enumerate(lois):\n    ax[i].hist(..., bins=100, density=True)\n    ax[i].set_title(name)\n\nplt.tight_layout()\nplt.show()\n\nDe mani√®re compl√©mentaire le module scipy.stats permet d‚Äôutiliser des lois usuelles, et de faire des tests statistiques. On pourra consulter la documentation en ligne pour plus de d√©tails: https://docs.scipy.org/doc/scipy/reference/stats.html.\n\n\n\n\n\n\nPour aller plus loin.\n\n\n\nLa plupart des lois usuelles sont disponibles, cf.¬†la documentation; vous pourrez en manipuler avec des widgets ici.",
    "crumbs": [
      "TP",
      "TP1: Prise en main de Python"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "HAX603X: Mod√©lisation stochastique",
    "section": "",
    "text": "Aspects num√©riques de la mod√©lisation al√©atoire et statistiques (cours de Licence 3). Les sections en haut de page regroupe le contenus p√©dagogique : les slides seront pr√©sent√©s en cours, et pour aller un peu plus loin le ‚Äúpoly‚Äù est disponible en format ‚Äúhtml‚Äù sur la page Cours.\n\n\n\n\n\n\nAvertissement\n\n\n\nSite en construction‚Ä¶(un peu de patience donc)\n\n\n\n\n\nJoseph Salmon: joseph.salmon@umontpellier.fr,\nBenjamin Charlier: benjamin.charlier@umontpellier.fr\n\nCe cours est issu du travail ant√©rieur de la part de:\n\nNicolas Meyer\nBeno√Æte de Saporta et de l‚Äôaide de Fran√ßois-David Collin.\n\n\n\n\n\nBases de probabilit√©s (en particulier ‚ÄúHAX506X- Th√©orie des Probabilit√©s‚Äù): probabilit√©, densit√©, esp√©rance, fonction de r√©partition, mesure, int√©gration, analyse num√©rique √©l√©mentaire, etc. (Foata et Fuchs 1996; Barbe et Ledoux 2006; Ouvrard 2007, 2008)\nProgrammation √©l√©mentaire (en Python): if ‚Ä¶ then‚Ä¶ else ‚Ä¶, for, while, fonctions, etc. HLMA310 - Logiciels scientifiques, (Courant et al. 2013), Cours de Python: Univ. Paris Diderot\n\n\nFoata, D., et A. Fuchs. 1996. Calcul des probabilit√©s: cours et exercices corrig√©s. Masson.\n\nBarbe, Philippe, et Michel Ledoux. 2006. ¬´¬†Cours de Probabilite Pour la Licence (corrig√©e)¬†¬ª.\n\nOuvrard, J.-Y. 2007. Probabilit√©s : Tome 2, Licence - CAPES. 2·µâ √©d. Enseignement des math√©matiques. Cassini.\n\n‚Äî‚Äî‚Äî. 2008. Probabilit√©s : Tome 1, Licence - CAPES. 2·µâ √©d. Enseignement des math√©matiques. Cassini.\n\n\n\n\nG√©n√©rer l‚Äôal√©a\n\ng√©n√©rateurs pseudo-al√©atoires\nillustrations num√©riques et visualisation en Python (loi des grands nombres, th√©or√®me central limite)\nsimulations de variables al√©atoires¬†(m√©thode de l‚Äôinverse, m√©thode du rejet, cas sp√©cifiques, etc.)\n\nM√©thode de Monte-Carlo¬†\n\nm√©thode de Monte-Carlo pour le calcul approch√© d‚Äôune int√©grale\nr√©duction de la variance¬†: variables antith√©tiques, variables de contr√¥le, √©chantillonnage pr√©f√©rentiel.\n\nCompl√©ments\n\nvecteurs gaussiens et lien avec les lois usuelles de la statistique inf√©rentielle (student, chi2)\nconstruction d‚Äôintervalles de confiance.\nmarche al√©atoire simple, etc.\n\n\n\n\n\n\nTP not√©s : Rendu = fichier Python .py unique\n\nTP not√© 1 : rendre en fin de session\nTP not√© 2 : rendre en fin de session\n\nCC : devoir sur table d‚Äôune heure\nCoefficients:\n\nNote Session 1 : (40% CC + 30% TP 1 + 30% TP 2)\nNote Session 2 : (30% CC + 35% TP 1 + 35% TP 2)\n\n\n\n\n\nMoodle: HAX603X Mod√©lisation Stochastique\n\n\n\n\n\n\nIntroduction √† Python Cours de Python üá´üá∑\nHLMA310 - Logiciels scientifiques üá´üá∑\nManuel d‚Äôalgorithmique en Python (Courant et al. 2013) üá´üá∑\nData Science: Python Data Science Handbook, With Application to Understanding Data by J. Van DerPlas, 2016; üá¨üáß  videos: Reproducible Data Analysis in Jupyter\nMath for journalists by Na√´l Shiab üá¨üáß\n\n\nCourant, J., M. de Falco, S. Gonnord, J.-C. Filli√¢tre, S. Conchon, G. Dowek, et B. Wack. 2013. Informatique pour tous en classes pr√©paratoires aux grandes √©coles: Manuel d‚Äôalgorithmique et programmation structur√©e avec Python. Eyrolles.\n\n\n\n\nSoftware Dev. for datascience by J. Salmon and B. Charlier, üá¨üáß\nMonte Carlo Methods and Applications by Keenan Crane üá¨üáß\nCha√Æne de Markov: Markov Chains by Ethan N. Epperly üá¨üáß\nAdvanced Data Analysis from an Elementary Point of View by Cosma Shalizi; üá¨üáß\nMaximum likelihood by numerical optimization üá¨üáß\nConditionnement, martingales et autres preuves de la loi des grands nombres: (Williams 1991) üá¨üáß\n\n\n\n\nWilliams, D. 1991. Probability with martingales. Cambridge Mathematical Textbooks. Cambridge: Cambridge University Press.",
    "crumbs": [
      "HAX603X: Mod√©lisation stochastique"
    ]
  },
  {
    "objectID": "index.html#professeurs",
    "href": "index.html#professeurs",
    "title": "HAX603X: Mod√©lisation stochastique",
    "section": "",
    "text": "Joseph Salmon: joseph.salmon@umontpellier.fr,\nBenjamin Charlier: benjamin.charlier@umontpellier.fr\n\nCe cours est issu du travail ant√©rieur de la part de:\n\nNicolas Meyer\nBeno√Æte de Saporta et de l‚Äôaide de Fran√ßois-David Collin.",
    "crumbs": [
      "HAX603X: Mod√©lisation stochastique"
    ]
  },
  {
    "objectID": "index.html#pr√©requis",
    "href": "index.html#pr√©requis",
    "title": "HAX603X: Mod√©lisation stochastique",
    "section": "",
    "text": "Bases de probabilit√©s (en particulier ‚ÄúHAX506X- Th√©orie des Probabilit√©s‚Äù): probabilit√©, densit√©, esp√©rance, fonction de r√©partition, mesure, int√©gration, analyse num√©rique √©l√©mentaire, etc. (Foata et Fuchs 1996; Barbe et Ledoux 2006; Ouvrard 2007, 2008)\nProgrammation √©l√©mentaire (en Python): if ‚Ä¶ then‚Ä¶ else ‚Ä¶, for, while, fonctions, etc. HLMA310 - Logiciels scientifiques, (Courant et al. 2013), Cours de Python: Univ. Paris Diderot\n\n\nFoata, D., et A. Fuchs. 1996. Calcul des probabilit√©s: cours et exercices corrig√©s. Masson.\n\nBarbe, Philippe, et Michel Ledoux. 2006. ¬´¬†Cours de Probabilite Pour la Licence (corrig√©e)¬†¬ª.\n\nOuvrard, J.-Y. 2007. Probabilit√©s : Tome 2, Licence - CAPES. 2·µâ √©d. Enseignement des math√©matiques. Cassini.\n\n‚Äî‚Äî‚Äî. 2008. Probabilit√©s : Tome 1, Licence - CAPES. 2·µâ √©d. Enseignement des math√©matiques. Cassini.",
    "crumbs": [
      "HAX603X: Mod√©lisation stochastique"
    ]
  },
  {
    "objectID": "index.html#description-du-cours",
    "href": "index.html#description-du-cours",
    "title": "HAX603X: Mod√©lisation stochastique",
    "section": "",
    "text": "G√©n√©rer l‚Äôal√©a\n\ng√©n√©rateurs pseudo-al√©atoires\nillustrations num√©riques et visualisation en Python (loi des grands nombres, th√©or√®me central limite)\nsimulations de variables al√©atoires¬†(m√©thode de l‚Äôinverse, m√©thode du rejet, cas sp√©cifiques, etc.)\n\nM√©thode de Monte-Carlo¬†\n\nm√©thode de Monte-Carlo pour le calcul approch√© d‚Äôune int√©grale\nr√©duction de la variance¬†: variables antith√©tiques, variables de contr√¥le, √©chantillonnage pr√©f√©rentiel.\n\nCompl√©ments\n\nvecteurs gaussiens et lien avec les lois usuelles de la statistique inf√©rentielle (student, chi2)\nconstruction d‚Äôintervalles de confiance.\nmarche al√©atoire simple, etc.",
    "crumbs": [
      "HAX603X: Mod√©lisation stochastique"
    ]
  },
  {
    "objectID": "index.html#modalit√©-de-contr√¥le-des-connaissances",
    "href": "index.html#modalit√©-de-contr√¥le-des-connaissances",
    "title": "HAX603X: Mod√©lisation stochastique",
    "section": "",
    "text": "TP not√©s : Rendu = fichier Python .py unique\n\nTP not√© 1 : rendre en fin de session\nTP not√© 2 : rendre en fin de session\n\nCC : devoir sur table d‚Äôune heure\nCoefficients:\n\nNote Session 1 : (40% CC + 30% TP 1 + 30% TP 2)\nNote Session 2 : (30% CC + 35% TP 1 + 35% TP 2)",
    "crumbs": [
      "HAX603X: Mod√©lisation stochastique"
    ]
  },
  {
    "objectID": "index.html#moodle",
    "href": "index.html#moodle",
    "title": "HAX603X: Mod√©lisation stochastique",
    "section": "",
    "text": "Moodle: HAX603X Mod√©lisation Stochastique",
    "crumbs": [
      "HAX603X: Mod√©lisation stochastique"
    ]
  },
  {
    "objectID": "index.html#ressources-suppl√©mentaires",
    "href": "index.html#ressources-suppl√©mentaires",
    "title": "HAX603X: Mod√©lisation stochastique",
    "section": "",
    "text": "Introduction √† Python Cours de Python üá´üá∑\nHLMA310 - Logiciels scientifiques üá´üá∑\nManuel d‚Äôalgorithmique en Python (Courant et al. 2013) üá´üá∑\nData Science: Python Data Science Handbook, With Application to Understanding Data by J. Van DerPlas, 2016; üá¨üáß  videos: Reproducible Data Analysis in Jupyter\nMath for journalists by Na√´l Shiab üá¨üáß\n\n\nCourant, J., M. de Falco, S. Gonnord, J.-C. Filli√¢tre, S. Conchon, G. Dowek, et B. Wack. 2013. Informatique pour tous en classes pr√©paratoires aux grandes √©coles: Manuel d‚Äôalgorithmique et programmation structur√©e avec Python. Eyrolles.\n\n\n\n\nSoftware Dev. for datascience by J. Salmon and B. Charlier, üá¨üáß\nMonte Carlo Methods and Applications by Keenan Crane üá¨üáß\nCha√Æne de Markov: Markov Chains by Ethan N. Epperly üá¨üáß\nAdvanced Data Analysis from an Elementary Point of View by Cosma Shalizi; üá¨üáß\nMaximum likelihood by numerical optimization üá¨üáß\nConditionnement, martingales et autres preuves de la loi des grands nombres: (Williams 1991) üá¨üáß\n\n\n\n\nWilliams, D. 1991. Probability with martingales. Cambridge Mathematical Textbooks. Cambridge: Cambridge University Press.",
    "crumbs": [
      "HAX603X: Mod√©lisation stochastique"
    ]
  }
]