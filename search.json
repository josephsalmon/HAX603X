[
  {
    "objectID": "trash/slides_inv_viz.html#test",
    "href": "trash/slides_inv_viz.html#test",
    "title": "Visualisation: inversion",
    "section": "test",
    "text": "test\n\nviewof dist = Inputs.select(['normal','cauchy','laplace','bimodal'], {value: \"bimodal\", label: \"Loi\"})\nviewof replay = html`&lt;button&gt;Relancer`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ninvcdfboard = require(await FileAttachment(\"../inverse-vizu/dist/invcdfboard.umd.cjs\").url())\n\n{\n  replay\n  const canvas = DOM.canvas(500, 500);\n  const galton = invcdfboard.galton(canvas,dist);\n  \n  return html`${galton.canvas}`\n}"
  },
  {
    "objectID": "TP/TP2.html",
    "href": "TP/TP2.html",
    "title": "TP2: …",
    "section": "",
    "text": "Objectifs de ce TP\n\n\n\n\nUtiliser les générateurs aléatoires en Python et numpy, savoir afficher un histogramme, une densité, etc.\nComprendre au mieux comment utiliser les fonctions aléatoires (principalement les générateurs) en numpy.",
    "crumbs": [
      "TP",
      "TP2: ..."
    ]
  },
  {
    "objectID": "TP/TP2.html#loi-uniforme-et-graine.",
    "href": "TP/TP2.html#loi-uniforme-et-graine.",
    "title": "TP2: …",
    "section": "Loi uniforme et graine.",
    "text": "Loi uniforme et graine.\nAvec numpy, la fonction numpy.random.uniform permet la génération de réalisations pseudo-aléatoires de la loi uniforme sur [0,1].\nOn peut modifier la taille de l’échantillon généré en modifiant l’argument de la fonction. Pour obtenir n=4 réalisations i.i.d. de loi uniforme, essayez par exemple\n\nimport numpy as np\nnp.random.uniform(size=4)\n\narray([0.98147197, 0.72391549, 0.96063583, 0.03857667])\n\n\nPour rappel, l’algorithme de génération de v.a. est récursif et s’appuie sur une graine. La graine peut être modifiée avec la création d’un générateur, et il suffit d’entrer un nombre en argument pour fixer cette graine.\n\nrng = np.random.default_rng(seed=34)\nprint(rng.uniform())\nrng = np.random.default_rng(34)\nprint(rng.uniform())\n\n0.004028243493043537\n0.004028243493043537\n\n\nChanger les valeurs de seed et vérifier que les tirages ont bien changé.",
    "crumbs": [
      "TP",
      "TP2: ..."
    ]
  },
  {
    "objectID": "TP/TP2.html#exercice-1-simulation-de-loi-uniforme-et-histogramme",
    "href": "TP/TP2.html#exercice-1-simulation-de-loi-uniforme-et-histogramme",
    "title": "TP2: …",
    "section": "Exercice 1: Simulation de loi uniforme et histogramme",
    "text": "Exercice 1: Simulation de loi uniforme et histogramme\nCréez un vecteur de taille 1000 composé de réalisations i.i.d. de v.a.uniformes sur [-1,1]. Dans la suite on supposera que l’on a chargé matplotlib pour l’affichage graphique avec la commande:\n\nimport matplotlib.pylab as plt\nfrom scipy import stats\n\nÀ l’aide de la fonction plt.hist, représentez l’histogramme de cet échantillon:\nfig, ax = plt.subplots()\nvect = rng.uniform(-1, 1, 1000)\nax.hist(vect, label=\"Histogramme\");\nplt.legend()\n\n\n\n\n\n\n\nOn utilisera l’aide de hist de matplotlibs pour préciser les options graphiques suivantes:\n\nAnalysez en particulier ce que fait l’option bins en entrant l’option bins=30 et bins=10.\nModifiez également votre histogramme avec l’option density=True, de sorte que l’aire soit de 1 (on représente donc une densité qui est constante par morceaux)\nAjoutez un titre à l’histogramme grâce à la commande plt.title (avec une chaîne de caractères entre guillemets). On peut également ajouter un nom aux axes avec l’option plt.xl et plt.xlabel.\nLes options ax.set_xlim et ax.set_ylim permettent de préciser l’échelle de axes: il faut préciser un tuple (a,b) où a&lt;b sont les deux bornes choisies pour votre axe.\nOn modifiera aussi les options fill et histtype de hist pour obtenir le résultat suivant, en affichant sur un même graphique trois tirages, de tailles 1000, 5000 et 10000.\nLa densité de la loi uniforme est obtenue avec la fonction pdf du module scipy.stats. Créer un vecteur équiréparti sur [-2, 2] de longueur 300 évaluer la fonction sur la même figure: on souhaite superposer cette densité à l’histogramme. On utilisera la fonction plot pour tracer la densité, et on pourra utiliser l’option alpha pour rendre la densité plus transparente.\n\nUn exemple de figure de qualité acceptable est par exemple celle qui suit:",
    "crumbs": [
      "TP",
      "TP2: ..."
    ]
  },
  {
    "objectID": "TP/TP2.html#exercice-2-fonction-de-répartition-de-la-loi-uniforme",
    "href": "TP/TP2.html#exercice-2-fonction-de-répartition-de-la-loi-uniforme",
    "title": "TP2: …",
    "section": "Exercice 2: Fonction de répartition de la loi uniforme",
    "text": "Exercice 2: Fonction de répartition de la loi uniforme\nLa fonction de répartition de la loi uniforme est obtenue via la commande cdf du module scipy.stats.uniform. À l’aide de la commande plt.plot tracez en bleu la fonction de répartition de la loi uniforme sur [-1,1], [-0.7, 0.7] et [-0.5,0.5] et donnez un titre à votre graphique.\n\nOn contrôle avec lw (linewidth) l’épaisseur du trait.\nVous pouvez modifier le style et les marqueurs facilement en matplotlib. Une liste exhaustive est donnée ici: matplotlib.pyplot.plot.html\nEnfin pour les couleurs on pourra consulter l’aide en ligne ici: Color tutorial. La manière la plus simple est souvent d’ajouter l’option color=nom_couleur dans la fonction plot.\n\nManipulez les différentes options pour vous familiariser avec les graphes\n\n\n\n\n\n\n\n\n\n\n² :::{.callout-note}",
    "crumbs": [
      "TP",
      "TP2: ..."
    ]
  },
  {
    "objectID": "TP/TP2.html#pour-aller-plus-loin",
    "href": "TP/TP2.html#pour-aller-plus-loin",
    "title": "TP2: …",
    "section": "Pour aller plus loin",
    "text": "Pour aller plus loin\nTenter de reproduire la figure suivante\n\n\n\n\n\n\n\n\n\n\n:::",
    "crumbs": [
      "TP",
      "TP2: ..."
    ]
  },
  {
    "objectID": "TP/TP2.html#exercice-3-étude-de-la-moyenne-empirique",
    "href": "TP/TP2.html#exercice-3-étude-de-la-moyenne-empirique",
    "title": "TP2: …",
    "section": "Exercice 3: Étude de la moyenne empirique",
    "text": "Exercice 3: Étude de la moyenne empirique\nCréez un vecteur de taille 100 composé de réalisations i.i.d. de variables uniformes sur [0,1]. Calculez dans un vecteur la moyenne cumulée des valeurs générées. Représenter graphiquement l’évolution de ces moyennes. Vers quoi semble converger la moyenne quand la taille de l’échantillon augmente ?\nPour ajouter une droite à un graphe, on utilise la commande ax.axhline. Ajoutez en rouge la droite d’équation y=1/2 sur le graphe précédent. Refaites cet exercice avec un échantillon de taille n=1000 pour observer plus finement la convergence.",
    "crumbs": [
      "TP",
      "TP2: ..."
    ]
  },
  {
    "objectID": "TP/TP2.html#exercice-4-méthode-dinversion-loi-exponentielle-et-loi-de-cauchy",
    "href": "TP/TP2.html#exercice-4-méthode-dinversion-loi-exponentielle-et-loi-de-cauchy",
    "title": "TP2: …",
    "section": "Exercice 4: Méthode d’inversion, loi exponentielle et loi de Cauchy",
    "text": "Exercice 4: Méthode d’inversion, loi exponentielle et loi de Cauchy\n\nReprésentez graphiquement la fonction de répartition d’une loi exponentielle de paramètre \\lambda=1.\nÉcrivez une fonction dzexpo qui prend en argument une taille d’échantillon n et un paramètre \\lambda &gt; 0 et qui donne en sortie un échantillon de taille n de loi \\mathcal{E}(\\lambda). On utilisera la méthode d’inversion vue en cours et seulement des tirages uniformes sur [0,1]. Attention, le mot clef lambda est un mot réservé en Python.\nReprésentez graphiquement l’histogramme cumulé (voir l’option cumulative de hist) d’un tel échantillon pour n=10^2, n=10^3, puis n=10^4, et pour \\lambda = 1, puis \\lambda = 4. Superposez à chaque fois le graphe de la densité de \\mathcal{E}(\\lambda).\nIllustrez graphiquement la loi des grands nombres avec \\lambda = 1, puis \\lambda = 4. On tracera en particulier la droite d’équation y=\\mathbb{E}[X], où X \\sim \\mathcal{E}(\\lambda).\nReprenez les questions précédentes avec la loi de Cauchy (mais représenter la densité plutôt que les fonctions de répartition). Commentez les résultats obtenus.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOn reprend le tout avec la loi de Cauchy:",
    "crumbs": [
      "TP",
      "TP2: ..."
    ]
  },
  {
    "objectID": "TD/TD1.html#test2",
    "href": "TD/TD1.html#test2",
    "title": "TD1:…",
    "section": "test2",
    "text": "test2",
    "crumbs": [
      "TD",
      "TD1:..."
    ]
  },
  {
    "objectID": "Slides/slides_simulation.html#enjeu",
    "href": "Slides/slides_simulation.html#enjeu",
    "title": "Simulation",
    "section": "Enjeu",
    "text": "Enjeu\n \nQuestion: Comment simuler en pratique des variables aléatoires i.i.d?\n \n\nApproche: Commencer par les v.a. uniformes et en déduire les autres lois",
    "crumbs": [
      "Slides",
      "Simulation"
    ]
  },
  {
    "objectID": "Slides/slides_simulation.html#rappel-sur-les-variables-uniformes",
    "href": "Slides/slides_simulation.html#rappel-sur-les-variables-uniformes",
    "title": "Simulation",
    "section": "Rappel sur les variables uniformes",
    "text": "Rappel sur les variables uniformes\nRappel : \\(U\\) suit une loi uniforme sur \\([0,1]\\): \\(U\\sim\\mathcal{U}([0,1])\\) ssi sa fonction de répartition \\(F_U\\) vaut \\[\nF_U(x)\n=\n\\begin{cases}\n    0, & \\text{si }x &lt; 0\\,,        \\\\\n    x, & \\text{si }x \\in [0,1]\\,,  \\\\\n    1, & \\text{si }x &gt; 1\\,.\n\\end{cases}\n\\]",
    "crumbs": [
      "Slides",
      "Simulation"
    ]
  },
  {
    "objectID": "Slides/slides_simulation.html#challenges",
    "href": "Slides/slides_simulation.html#challenges",
    "title": "Simulation",
    "section": "Challenges",
    "text": "Challenges\n\nObjectif: simuler sur machine une suite \\(U_1, \\dots, U_n\\) de v.a., i.i.d., de loi \\(\\mathcal{U}([0,1])\\).\n Difficultés:\n\n\nUne machine est déterministe.\nLes nombres flottants entre \\(0\\) et \\(1\\) donnés par la machine sont de la forme \\(k/2^p\\), pour \\(k \\in \\{0, \\ldots, 2^{p-1}\\} \\implies\\) impossibilité de générer certains nombres.\nVérifier qu’une suite est bien i.i.d. est un problème difficile.",
    "crumbs": [
      "Slides",
      "Simulation"
    ]
  },
  {
    "objectID": "Slides/slides_simulation.html#générateurs-de-nombres-pseudo-aléatoires-1",
    "href": "Slides/slides_simulation.html#générateurs-de-nombres-pseudo-aléatoires-1",
    "title": "Simulation",
    "section": "Générateurs de nombres pseudo-aléatoires",
    "text": "Générateurs de nombres pseudo-aléatoires\n\nDéfinition 1 (Générateur de nombres pseudo-aléatoires) \nUn générateur de nombres pseudo-aléatoires (🇬🇧: Pseudo Random Number Generator, PRNG), est un algorithme déterministe récursif qui renvoie une suite \\(U_1, \\ldots, U_n\\) dans \\([0,1]\\) qui a un “comportement similaire” à une suite i.i.d. de loi \\(\\mathcal{U}([0,1])\\).\n\n\nRemarque: ces nombres sont obtenus depuis des nombres entiers générés aléatoirement et uniformément sur grand interval, puis une transformation simple (normalisation) permet d’obtenir des nombres flottants (🇬🇧: floats) entre 0 et 1.",
    "crumbs": [
      "Slides",
      "Simulation"
    ]
  },
  {
    "objectID": "Slides/slides_simulation.html#détails-techniques",
    "href": "Slides/slides_simulation.html#détails-techniques",
    "title": "Simulation",
    "section": "Détails techniques",
    "text": "Détails techniques\nUn PRNG se construit ainsi :\n\nInitialisation: une graine (🇬🇧: seed) \\(U_0\\), détermine la première valeur (choix arbitraire)\nOn calcule \\(U_{n+1} = f(U_n)\\), où \\(f\\) est une transformation déterministe, telle que \\(U_{n+1}\\) est “le plus indépendant possible” de \\(U_1, \\dots, U_n\\).\n\n\n\n\\(f\\) : à valeur dans un ensemble fini \\(\\implies\\) périodicité (contrainte: utiliser la plus grande période possible)\nL’algorithme est déterministe (une fois la graine fixée). Utilité de fixer la graine: répéter des simulations dans des conditions identiques et ainsi repérer des erreurs",
    "crumbs": [
      "Slides",
      "Simulation"
    ]
  },
  {
    "objectID": "Slides/slides_simulation.html#le-générateur-congruentiel-linéaire",
    "href": "Slides/slides_simulation.html#le-générateur-congruentiel-linéaire",
    "title": "Simulation",
    "section": "Le générateur congruentiel linéaire",
    "text": "Le générateur congruentiel linéaire\nLa plupart des PRNG s’appuient sur des résultats arithmétiques.\n\n\nLe plus célèbre: Générateur Congruentiel Linéaire (🇬🇧 Linear Congruential Generator, LCG).\nRécurrence: \\[\nX_{n+1} = a X_n + b \\quad \\text{mod } m \\enspace,\n\\] \\(a,b,m\\), entiers bien choisis pour que la suite obtenue ait de bonnes propriétés\nNormalisation: \\(X_n/m\\).\nExemple: la fonction rand de scilab utilisait cette congruence avec \\(m=2^{31}\\), \\(a=843\\; 314\\; 861\\), et \\(b=453\\; 816\\; 693\\).",
    "crumbs": [
      "Slides",
      "Simulation"
    ]
  },
  {
    "objectID": "Slides/slides_simulation.html#exemples-de-générateurs-alternatifs",
    "href": "Slides/slides_simulation.html#exemples-de-générateurs-alternatifs",
    "title": "Simulation",
    "section": "Exemples de générateurs alternatifs",
    "text": "Exemples de générateurs alternatifs\n \n\nméthode par défaut pour Python et R: Mersenne-Twister, s’appuie sur la multiplication vectorielle (période du générateur \\(m =2^{19937}-1\\))\n\n\n\nméthode par défaut pour numpy: PCG64 (cf. documentation de numpy), dispose de meilleures garanties statistiques; voir https://www.pcg-random.org\n\n\n\nOn suppose désormais disposer d’un générateur pseudo-aléatoire sur \\([0,1]\\).",
    "crumbs": [
      "Slides",
      "Simulation"
    ]
  },
  {
    "objectID": "Slides/slides_simulation.html#usage-en-numpy",
    "href": "Slides/slides_simulation.html#usage-en-numpy",
    "title": "Simulation",
    "section": "Usage en numpy",
    "text": "Usage en numpy\nEn numpy (version&gt;1.17): utiliser des éléments aléatoires est d’utiliser un générateur\n\n\nseed = 12345                       #  choix de la graine\nrng = np.random.default_rng(seed)  #  générateur\n\n\n\n\nprint(rng.random())                #  un tirage uniforme sur [0,1]\n\n0.22733602246716966\n\n\n\n\n\n\nprint(rng.random(size=5))          #  5 tirages uniformes sur [0,1]\n\n[0.31675834 0.79736546 0.67625467 0.39110955 0.33281393]\n\n\n\n\n\n\nprint(rng.random(size=(3, 2)))     #  matrice 3x2, à entrées unif. sur [0,1]\n\n[[0.59830875 0.18673419]\n [0.67275604 0.94180287]\n [0.24824571 0.94888115]]",
    "crumbs": [
      "Slides",
      "Simulation"
    ]
  },
  {
    "objectID": "Slides/slides_simulation.html#plus-sur-les-lois-aléatoires-et-python",
    "href": "Slides/slides_simulation.html#plus-sur-les-lois-aléatoires-et-python",
    "title": "Simulation",
    "section": "Plus sur les lois aléatoires et Python",
    "text": "Plus sur les lois aléatoires et Python\n\n\n\nSuite du cours: apprendre à générer de nombreuses lois à partir de la loi uniforme\nEn pratique: les logiciels proposent les distributions classiques (gaussiennes, exponentielles, etc.), utiliser plutôt ces fonctions que de les implémenter soi-même.\nListe exhaustive pour numpy:\nhttps://numpy.org/doc/stable/reference/random/generator.html#distributions\n\n\n\n\n\n\n\n\n\n\nPour aller plus loin\n\n\nUne excellent discussion sur les bonnes pratiques aléatoires en numpy, et l’usage de np.random.default_rng est donnée dans ce blog post d’Albert Thomas.",
    "crumbs": [
      "Slides",
      "Simulation"
    ]
  },
  {
    "objectID": "Slides/slides_simulation.html#rappel-sur-la-fonction-quantile",
    "href": "Slides/slides_simulation.html#rappel-sur-la-fonction-quantile",
    "title": "Simulation",
    "section": "Rappel sur la fonction quantile",
    "text": "Rappel sur la fonction quantile\nRappel: Pour \\(F\\) une fonction définie sur \\(\\mathbb{R}\\) à valeurs dans \\([0, 1]\\), croissante, on note\n\\[\n\\forall q \\in ]0,1[, \\quad F^\\leftarrow(q) = \\inf\\{ x \\in \\mathbb{R} : F(x)\\geq q\\}\n\\tag{1}\\]\nNote: Si \\(x_0 \\in \\{ x \\in \\mathbb{R} : F(x)\\geq q\\}\\) alors \\([x_0,+\\infty[ \\subset \\{ x \\in \\mathbb{R} : F(x)\\geq q\\}\\)\n\n\nThéorème 1 (Caratérisation des quantiles) Soit \\(F\\) une fonction définie sur \\(\\mathbb{R}\\) à valeurs dans \\([0, 1]\\), croissante et continue à droite, alors pour tout \\(q \\in ]0, 1[\\), on a \\[\n\\begin{align}\n   \\{x \\in \\mathbb{R} :  F(x) \\geq q) \\} & =\n   \\{x \\in \\mathbb{R} : x \\geq F^\\leftarrow(q)  \\}\n\\end{align}\n\\]\n\n\n\nCas \\(\\subset\\): Soit \\(x \\in \\mathbb{R}\\) t.q. \\(F(x) \\geq q\\), alors par définition de l’inf dans Équation 1, \\(x \\geq F^\\leftarrow(q)\\)\n\n\n\nCas \\(\\supset\\): Soient \\(\\epsilon&gt;0\\) et \\(x \\in \\mathbb{R}\\) t.q. \\(x \\geq F^\\leftarrow(q)\\) alors (def. de l’inf) \\(\\exists x_0 \\in \\{ x \\in \\mathbb{R} : F(x)\\geq q\\}\\), t.q. \\(x + \\epsilon &gt; x_0\\). Ainsi, \\(F(x + \\epsilon) \\geq F(x_0) \\geq q\\); par continuité à droite de \\(F\\), \\(F(x) \\geq q\\)",
    "crumbs": [
      "Slides",
      "Simulation"
    ]
  },
  {
    "objectID": "Slides/slides_simulation.html#méthode-dinversion",
    "href": "Slides/slides_simulation.html#méthode-dinversion",
    "title": "Simulation",
    "section": "Méthode d’inversion",
    "text": "Méthode d’inversion\n\nThéorème 2 (Méthode d’inversion) Soit \\(X\\) une v.a réelle, et \\(U \\sim\\mathcal{U}([0,1])\\), alors la variable aléatoire \\(F_X^\\leftarrow(U)\\) a même loi que \\(X\\).\n\n\n\nPreuve: en utilisant le théorème précédent, on a \\[\n\\forall x\\in\\mathbb{R}, \\quad \\mathbb{P}(x \\geq F_X^\\leftarrow(U)) = \\mathbb{P}(F_X(x) \\geq U)\n\\]\n\n\nPuis, comme \\(U\\) est une loi uniforme sur \\([0,1]\\):\n\\[\n\\mathbb{P}(F_X(x) \\geq U) = F_X(x)\n\\]\n\n\nAinsi, \\(F_X^\\leftarrow(U)\\) et \\(X\\) ont même loi: les deux v.a. ont la même fonction de répartition",
    "crumbs": [
      "Slides",
      "Simulation"
    ]
  },
  {
    "objectID": "Slides/slides_simulation.html#symétrie-de-la-loi-uniforme",
    "href": "Slides/slides_simulation.html#symétrie-de-la-loi-uniforme",
    "title": "Simulation",
    "section": "Symétrie de la loi uniforme",
    "text": "Symétrie de la loi uniforme\n\nProposition 1 (Symétrie de la loi uniforme) Soit \\(U \\sim \\mathcal{U}([0,1])\\) une variable uniforme sur \\([0,1]\\). Alors, \\(1-U\\) suit aussi une loi uniforme sur \\([0,1]\\).\n\n\nPreuve:\nOn va décrire la fonction de répartition de \\(1-U\\) et montrer qu’elle est égale à celle d’une loi uniforme sur \\([0,1]\\).\n\n\nLe résultat est facile pour \\(x \\notin [0,1]\\), on suppose donc \\(x \\in [0,1]\\).\n\n\\[\n\\begin{align*}\n\\class{fragment}{{}\\mathbb{P}(1-U \\leq x)} & \\class{fragment}{{}= \\mathbb{P}(U \\geq 1-x) }\\\\\n                       & \\class{fragment}{{} = 1-(1-x)} \\\\\n                       & \\class{fragment}{{} = x}\n\\end{align*}\n\\]",
    "crumbs": [
      "Slides",
      "Simulation"
    ]
  },
  {
    "objectID": "Slides/slides_simulation.html#exemple-loi-exponentielle",
    "href": "Slides/slides_simulation.html#exemple-loi-exponentielle",
    "title": "Simulation",
    "section": "Exemple : loi exponentielle",
    "text": "Exemple : loi exponentielle\n\n\n\nDensité d’une loi \\(\\mathcal{E}(\\lambda)\\) pour \\(\\lambda &gt; 0\\) : \\(f_{\\lambda}(x) = \\lambda e^{-\\lambda x}{1\\hspace{-3.8pt} 1}_{\\mathbb{R}_+}(x)\\)\nFonction de répartition: \\(F_{\\lambda}(x) = (1 - e^{-\\lambda x}) {1\\hspace{-3.8pt} 1}_{\\mathbb{R}_+}(x)\\)\n\\(F_{\\lambda}\\) est bijective (de \\(\\mathbb{R}_+\\) dans \\(]0,1[\\)) et pour tout \\(u \\in ]0,1[\\), \\(F_{\\lambda}^{-1}(u) = -\\frac{1}{\\lambda} \\log(1-u)\\)\n\n\n\n\nAvec le résultat: \\[\nU \\sim \\mathcal{U}([0,1]) \\iff 1-U \\sim \\mathcal{U}([0,1])\\enspace,\n\\]\n\nPour simuler une loi exponentielle: simuler \\(U\\) uniforme et appliquer \\(-\\tfrac{1}{\\lambda} \\log(\\cdot)\\)\n\\[\n\\boxed{-\\tfrac{1}{\\lambda} \\log(U) \\sim \\mathcal{E}(\\lambda)}\n\\]",
    "crumbs": [
      "Slides",
      "Simulation"
    ]
  },
  {
    "objectID": "Slides/slides_simulation.html#visualisation",
    "href": "Slides/slides_simulation.html#visualisation",
    "title": "Simulation",
    "section": "Visualisation",
    "text": "Visualisation\nVoir animation dans la section Cours, section “Méthode d’inversion”.",
    "crumbs": [
      "Slides",
      "Simulation"
    ]
  },
  {
    "objectID": "Slides/slides_simulation.html#méthode-de-rejet-contraintes",
    "href": "Slides/slides_simulation.html#méthode-de-rejet-contraintes",
    "title": "Simulation",
    "section": "Méthode de rejet: contraintes",
    "text": "Méthode de rejet: contraintes\nMotivation: simuler une variable aléatoire \\(X\\) de densité \\(f\\) (loi cible), mais \\(f\\) est trop compliquée pour la méthode de l’inverse.\n\nIdée: tirer suivant une autre loi \\(g\\) (loi des propositions) et rejeter certains tirages.\n\n\non sait simuler \\(Y\\) de loi \\(g\\),\nil existe \\(m &gt; 0\\) tel que \\(f(x) \\leq m \\cdot g(x)\\) (constante de majoration)\non sait évaluer le rapport d’acceptation \\(r(x) = \\frac{f(x)}{m\\cdot g(x)}\\)\n\n\n\n\nRemarque 1: \\(g(x)=0 \\implies f(x)=0\\), ainsi le support de \\(g\\) doit englober celui de \\(f\\)\n\n\nRemarque 2: \\(m \\geq 1\\) car \\(m = m \\displaystyle\\int_\\mathbb{R} g(x)\\, dx \\class{fragment}{{} \\geq \\displaystyle\\int_\\mathbb{R} f(x) dx} \\class{fragment}{{} = 1}\\)",
    "crumbs": [
      "Slides",
      "Simulation"
    ]
  },
  {
    "objectID": "Slides/slides_simulation.html#méthode-de-rejet-principe",
    "href": "Slides/slides_simulation.html#méthode-de-rejet-principe",
    "title": "Simulation",
    "section": "Méthode de rejet: principe",
    "text": "Méthode de rejet: principe\nConsidérer deux suites i.i.d. de v.a. indépendantes entre elles:\n\n\\((Y_n)_{n \\geq 1}\\) de loi \\(g\\),\n\\((U_n)_{n \\geq 1}\\) de loi uniforme sur \\([0,1]\\).\n\nEn pratique, \\(Y_n\\) correspond à une proposition et \\(U_n\\) permettra de décider l’acceptation/rejet de la proposition:\n\n\nSi oui, alors on conserve \\(Y_n\\)\nSi non, on simule \\(Y_{n+1}\\)\n\nPour simuler \\(X\\) de densité \\(f\\), simuler \\(Y_n\\) (suivant \\(g\\)), \\(U_n\\) (suivant \\(\\mathcal{U}[0,1]\\)) et accepter si \\[\nU_n \\leq r(Y_n) = \\frac{f(Y_n)}{m\\cdot g(Y_n)}\n\\]",
    "crumbs": [
      "Slides",
      "Simulation"
    ]
  },
  {
    "objectID": "Slides/slides_simulation.html#exemple-simple",
    "href": "Slides/slides_simulation.html#exemple-simple",
    "title": "Simulation",
    "section": "Exemple simple",
    "text": "Exemple simple\n \n\n\n\\(f(x) = 4x^3 {1\\hspace{-3.8pt} 1}_{[0,1]}(x)\\)\n\\(f\\) est majorée par \\(4\\) \\(\\implies\\) \\(g = {1\\hspace{-3.8pt} 1}_{[0,1]}\\) et \\(m=4\\) conviennent\n\\(r(x) =f(x) / (m\\cdot g(x)) = x^3\\), pour \\(x \\in [0,1]\\). On simule donc \\((Y_n, U_n)\\) et on teste si \\(4 \\cdot U_1 \\leq 4 Y_1^3\\), etc.\n\n\n\n\n\n\n\n\n\nNote\n\n\nDans la suite on verra qu’on tire des points \\((Y_n, 4U_n)\\) et qu’on teste si ils sont dans l’ensemble \\(\\{(x,y) \\in \\mathbb{R}^2: y \\leq f(x) \\}\\)",
    "crumbs": [
      "Slides",
      "Simulation"
    ]
  },
  {
    "objectID": "Slides/slides_simulation.html#code-python",
    "href": "Slides/slides_simulation.html#code-python",
    "title": "Simulation",
    "section": "Code Python",
    "text": "Code Python\n\n\ndef accept_reject(n, f, g, g_sampler, m, rng):\n    \"\"\"\n    n: nombre de simulations\n    f: loi cible\n    g: loi des propositions, g_sampler: simulateur selon g\n    m: constante pour la majoration\n    rng: générateur pseudo-aléatoire\n    \"\"\"\n    x_samples = np.zeros(n)\n    u_samples = np.zeros(n)\n    accepted = np.zeros(n)\n    for i in range(n):\n        x = g_sampler()\n        u = rng.uniform()\n        alpha = u * m * g(x)\n        u_samples [i] = alpha\n        x_samples[i] = x\n        if  alpha &lt;= f(x):\n            accepted[i] = 1\n    return x_samples, u_samples, accepted",
    "crumbs": [
      "Slides",
      "Simulation"
    ]
  },
  {
    "objectID": "Slides/slides_simulation.html#visualisation-de-lexemple",
    "href": "Slides/slides_simulation.html#visualisation-de-lexemple",
    "title": "Simulation",
    "section": "Visualisation de l’exemple",
    "text": "Visualisation de l’exemple",
    "crumbs": [
      "Slides",
      "Simulation"
    ]
  },
  {
    "objectID": "Slides/slides_simulation.html#variante-avec-une-loi-triangulaire",
    "href": "Slides/slides_simulation.html#variante-avec-une-loi-triangulaire",
    "title": "Simulation",
    "section": "Variante avec une loi triangulaire",
    "text": "Variante avec une loi triangulaire\n \nSupposons disposer d’un générateur de loi triangulaire sur \\([0,1]\\) (cf. np.random.triangular(0, 1, 1))\n\n\n\\(f(x) = 4x^3 {1\\hspace{-3.8pt} 1}_{[0,1]}(x)\\)\n\\(f(x)\\) est majorée par \\(4 x \\cdot {1\\hspace{-3.8pt} 1}_{[0,1]}(x)\\) \\(\\implies\\) \\(g = 2x \\cdot {1\\hspace{-3.8pt} 1}_{[0,1]}\\) et \\(m=2\\) conviennent\n\\(r(x)=f(x) / (m\\cdot g(x)) = x^3\\), pour \\(x \\in [0,1]\\).",
    "crumbs": [
      "Slides",
      "Simulation"
    ]
  },
  {
    "objectID": "Slides/slides_simulation.html#variante-continuée",
    "href": "Slides/slides_simulation.html#variante-continuée",
    "title": "Simulation",
    "section": "Variante (continuée)",
    "text": "Variante (continuée)",
    "crumbs": [
      "Slides",
      "Simulation"
    ]
  },
  {
    "objectID": "Slides/slides_simulation.html#comparaison-des-deux-majorants",
    "href": "Slides/slides_simulation.html#comparaison-des-deux-majorants",
    "title": "Simulation",
    "section": "Comparaison des deux majorants",
    "text": "Comparaison des deux majorants\n\n\n\n\n\n\n\n\n\n\n\nmd`Taux d'acceptation: avec la loi uniforme **${ratio1.toPrecision(5)}**`\n\n\n\n\n\n\n\nmd`Taux d'acceptation: avec la loi triangulaire **${ratio2.toPrecision(5)}**`\n\n\n\n\n\n\n\nConclusion: plus le majorant est proche de la loi cible, plus le taux d’acceptation est élevé, et moins de simulations sont nécessaires\n\n\n\n\n\n\n\n\nNote\n\n\nL’exemple est pour l’illustration de la méthode, dans le cas présent méthode de l’inverse fonctionnerait aussi (on peut calculer la fonction quantile explicitement).",
    "crumbs": [
      "Slides",
      "Simulation"
    ]
  },
  {
    "objectID": "Slides/slides_simulation.html#méthode-de-rejet-validation-théorique",
    "href": "Slides/slides_simulation.html#méthode-de-rejet-validation-théorique",
    "title": "Simulation",
    "section": "Méthode de rejet: validation théorique",
    "text": "Méthode de rejet: validation théorique\nRappel:\n\nil existe \\(m &gt; 0\\) tel que \\(f(x) \\leq m \\cdot g(x)\\) et \\(r(x) = \\frac{f(x)}{m\\cdot g(x)}\\)\n\\((Y_n)_{n \\geq 1}\\) i.i.d. de loi \\(g\\)\n\\((U_n)_{n \\geq 1}\\) i.i.d. de loi uniforme sur \\([0,1]\\) (indépendamment des \\(Y_n\\))\n\n\n\nThéorème 3 (Méthode de rejet) \nSoit \\(T = \\inf \\{n \\geq 1 : U_n \\leq r(Y_n)\\}\\) le premier instant où le tirage est accepté. Alors :\n\n\\(T \\sim \\mathcal{G}(\\frac{1}{m})\\) : loi géométrique de paramètre \\(\\frac{1}{m}\\)\n\\(Y_T\\) a pour densité \\(f\\) et est indépendante de \\(T\\)",
    "crumbs": [
      "Slides",
      "Simulation"
    ]
  },
  {
    "objectID": "Slides/slides_simulation.html#démonstration",
    "href": "Slides/slides_simulation.html#démonstration",
    "title": "Simulation",
    "section": "Démonstration",
    "text": "Démonstration\nPour \\(x \\in \\mathbb{R}\\) et \\(n \\in \\mathbb{N}^{*}\\), et \\(X = Y_T\\), on écrit \\(\\mathbb{P}(X \\leq x, T=n)= \\mathbb{P}(U_1 &gt; r(Y_1), \\dots, U_{n-1} &gt; r(Y_{n-1}), U_n \\leq r(Y_n), Y_n \\leq x)\\)\n\n\\[\n    \\mathbb{P}(X \\leq x, T=n) = {\\color{blue}\\mathbb{P}(U_1 &gt; r(Y_1))}^{n-1} \\cdot {\\color{brown}\\mathbb{P}(U_n \\leq r(Y_n), Y_n \\leq x)}, \\quad \\textsf{( tirages i.i.d.)}\n\\]\n\n\nPremier terme: \\(Y_1\\) et \\(U_1\\) sont indépendantes, leur loi jointe correspond au produit des densités :\n\n\n\\[\n{\\color{blue}\n\\begin{align*}\n        \\mathbb{P}(U_1 &gt; r(Y_1))\n         & \\class{fragment}{{} = \\mathbb{P}((U_1, Y_1) \\in \\{(u,y) \\in \\mathbb{R}^2 : u &gt; r(y)\\})}                             \\\\\n         & \\class{fragment}{{} = \\int_{\\mathbb{R}^2} \\left( {1\\hspace{-3.8pt} 1}_{\\{u &gt; r(y)\\}} \\right) \\cdot \\left({1\\hspace{-3.8pt} 1}_{[0,1]}(u) g(y)\\right) \\,  du  dy}  \\\\\n         & \\class{fragment}{{} = \\int_\\mathbb{R} \\left( \\int_0^1 {1\\hspace{-3.8pt} 1}_{\\{u &gt; r(y)\\}} \\,  du\\right) g(y)\\,  d y}\n         \\class{fragment}{{} =  \\int_\\mathbb{R} (1-r(y)) \\, g(y)\\,  d y}\\\\\n         & \\class{fragment}{{} =  \\int_\\mathbb{R} g(y) -  \\int_\\mathbb{R}\\frac{f(y)}{m} d y, \\quad\\quad \\text{car }  r(y) = \\frac{f(y)}{m \\cdot g(y)}}\\\\\n     & \\class{fragment}{{} = 1-\\tfrac{1}{m}}\n\\end{align*}\n}\n\\]",
    "crumbs": [
      "Slides",
      "Simulation"
    ]
  },
  {
    "objectID": "Slides/slides_simulation.html#démonstration-suite",
    "href": "Slides/slides_simulation.html#démonstration-suite",
    "title": "Simulation",
    "section": "Démonstration (suite)",
    "text": "Démonstration (suite)\nSecond terme: \\[\n{\\color{brown}\n\\begin{align*}\n    \\mathbb{P}(U_n \\leq r(Y_n), Y_n \\leq x)\n    & \\class{fragment}{{ = \\int_{\\mathbb{R}^2} {1\\hspace{-3.8pt} 1}_{\\{u \\leq r(y)\\}} {1\\hspace{-3.8pt} 1}_{\\{y \\leq x\\}} ({1\\hspace{-3.8pt} 1}_{[0,1]}(u) g(y)) \\,  du  dy  }} \\\\\n    & \\class{fragment}{{} = \\int_\\mathbb{R} \\left( \\int_0^1  {1\\hspace{-3.8pt} 1}_{\\{u \\leq r(y)\\}} \\,  du\\right) {1\\hspace{-3.8pt} 1}_{\\{y \\leq x\\}}  g(y)\\,  d y} \\\\\n        & \\class{fragment}{{} = \\int_\\mathbb{R} r(y) {1\\hspace{-3.8pt} 1}_{\\{y \\leq x\\}}  g(y)\\,  d y } \\\\\n        & \\class{fragment}{{} = \\int_{-\\infty}^x \\dfrac{f(y)}{m}\\,  d y } \\\\\n        & \\class{fragment}{{} = \\dfrac{F(x)}{m} , \\quad F \\textsf{ fonction de répartition associée à} f}\n\\end{align*}\n}\n\\]",
    "crumbs": [
      "Slides",
      "Simulation"
    ]
  },
  {
    "objectID": "Slides/slides_simulation.html#démonstration-suite-1",
    "href": "Slides/slides_simulation.html#démonstration-suite-1",
    "title": "Simulation",
    "section": "Démonstration (suite)",
    "text": "Démonstration (suite)\n\\[\n    \\mathbb{P}(X \\leq x, T=n)\n    =\n    {\\color{blue}\\left(1 - \\tfrac{1}{m}\\right)^{n-1}} \\cdot {\\color{brown}\\tfrac{F(x)}{m}}\n\\]\n\nOn peut alors obtenir les lois marginales: \\[\n\\begin{align*}\n    \\mathbb{P}(T=n)\n    & = \\lim_{q \\to \\infty} \\mathbb{P}(X \\in ]-\\infty, q], T=n) = \\lim_{q \\to \\infty} \\left(1 - \\tfrac{1}{m}\\right)^{n-1} \\tfrac{F(q)}{m}\\\\\n    & = \\left(1 - \\tfrac{1}{m}\\right)^{n-1} \\tfrac{1}{m}\n\\end{align*}\n\\] Ainsi, \\(T\\) suit une loi géométrique de paramètre \\(1/m\\), puis \\(X\\) a pour loi \\(F\\):\n\n\n\\[\n\\begin{align*}\n    \\mathbb{P}(X \\leq x)\n    & = \\mathbb{P}(X \\leq x, T \\in \\mathbb{N}^*)\n      = \\sum_{n=1}^\\infty \\mathbb{P}(X \\leq x, T=n) \\\\\n    & = \\sum_{n=1}^\\infty \\left(1 - \\tfrac{1}{m}\\right)^{n-1} \\tfrac{F(x)}{m}\n      = \\tfrac{1}{1-(1-1/m)} \\tfrac{F(x)}{m} = F(x) \\enspace.\n\\end{align*}\n\\]",
    "crumbs": [
      "Slides",
      "Simulation"
    ]
  },
  {
    "objectID": "Slides/slides_simulation.html#fin-de-la-démonstration",
    "href": "Slides/slides_simulation.html#fin-de-la-démonstration",
    "title": "Simulation",
    "section": "Fin de la démonstration",
    "text": "Fin de la démonstration\nOn obtient l’indépendance de \\(T\\) et \\(X\\) car on peut alors écrire: \\[\n    \\forall x \\in \\mathbb{R}, \\forall n \\in \\mathbb{N}^*, \\quad\n    \\mathbb{P}(X \\leq x, T=n)\n    =\n    \\mathbb{P}(X \\leq x) \\cdot \\mathbb{P}(T=n)\n\\]",
    "crumbs": [
      "Slides",
      "Simulation"
    ]
  },
  {
    "objectID": "Slides/slides_simulation.html#cas-de-densité-connue-à-une-constante-près-exemple",
    "href": "Slides/slides_simulation.html#cas-de-densité-connue-à-une-constante-près-exemple",
    "title": "Simulation",
    "section": "Cas de densité connue à une constante près : exemple",
    "text": "Cas de densité connue à une constante près : exemple\nLoi de Andrews (densité proportionnelle à \\(\\mathrm{sinc}\\), sinus cardinal): \\[\n\\forall x \\in \\mathbb{R},\\quad f(x) = \\frac{1}{S} \\frac{\\sin(\\pi\\cdot x)}{\\pi \\cdot x}  {1\\hspace{-3.8pt} 1}_{[-1,1]}(x)\n\\] avec \\(S = \\int_{-1}^{1}\\frac{\\sin(\\pi\\cdot x)}{\\pi \\cdot x}dx\\) non explicite. On note parfois: \\(f(x) \\propto \\frac{\\sin(\\pi\\cdot x)}{\\pi \\cdot x} {1\\hspace{-3.8pt} 1}_{[-1,1]}(x)\\)\n\n\nMéthode du rejet: prendre \\(m=2/S\\) et \\(g(x) = \\frac{1}{2} {1\\hspace{-3.8pt} 1}_{[-1,1]}(x)\\):\n\\[\nu \\leq r(x) = \\frac{f(x)}{m \\cdot g(x)} \\iff u \\leq \\frac{1}{S} \\frac{\\sin(\\pi\\cdot x)}{\\pi \\cdot x}  \\cdot \\frac{1}{\\frac{2}{S} g(x)} = \\frac{\\sin(\\pi\\cdot x)}{\\pi \\cdot x}  \\cdot \\frac{1}{2 \\cdot g(x)}\n\\]\n\n\nAinsi l’évaluation de \\(r(x)\\) est possible sans connaître \\(S\\)!",
    "crumbs": [
      "Slides",
      "Simulation"
    ]
  },
  {
    "objectID": "Slides/slides_simulation.html#loi-de-andrews-visualisation",
    "href": "Slides/slides_simulation.html#loi-de-andrews-visualisation",
    "title": "Simulation",
    "section": "Loi de Andrews: visualisation",
    "text": "Loi de Andrews: visualisation\n\nn = 300\nm = 2\nrng = np.random.default_rng(seed)\ng = lambda x: np.ones_like(x) / 2\ng_sampler = lambda: 2 * rng.uniform() - 1\nx_samples, u_samples, accepted = accept_reject(n, np.sinc, g, g_sampler, m, rng)\n\n\n\n\n\n\n\n\n\n\n\n\n\nmd`Le taux d'acceptation est ici de **${ratio_andrews.toPrecision(3)}**.`\n\n\n\n\n\n\n\n\n\n                                                \n\n\n\n\n\nmd`Pour la visualization, on approxime S=**${s_int.toPrecision(3)}** en utilisant une méthode de calcul numérique.`",
    "crumbs": [
      "Slides",
      "Simulation"
    ]
  },
  {
    "objectID": "Slides/slides_simulation.html#cas-de-densité-connue-à-une-constante-près-cas-général",
    "href": "Slides/slides_simulation.html#cas-de-densité-connue-à-une-constante-près-cas-général",
    "title": "Simulation",
    "section": "Cas de densité connue à une constante près (cas général)",
    "text": "Cas de densité connue à une constante près (cas général)\nSoit \\(\\tilde{f}: \\mathbb{R} \\to [0,+\\infty[\\) connue et \\(S \\triangleq \\int_{\\mathbb{R}} \\tilde{f}(x) \\, d x &lt; + \\infty\\) inconnue (ou dure à évaluer)\nDensité cible: \\(\\quad f(x) = \\frac{\\tilde{f}(x)}{S}\\)\n\nMéthode du rejet pour \\(f\\), en utilisant seulement \\(\\tilde{f}\\): soit \\(\\tilde{m}&gt;0\\) un majorant de \\(\\tilde{f}\\) t.q. \\[\n\\begin{align*}\n\\tilde{f}(x) \\leq  \\tilde{m} \\cdot g(x)\n\\end{align*}\n\\]\nApplication avec \\(m=\\tilde{m}/S\\) (sans connaître \\(S\\)), le test d’acceptation donne:\n\n\\[\n\\begin{align*}\nU_n & \\leq \\frac{f(Y_n)}{m \\cdot g(Y_n)}\\\\\n  & \\class{fragment}{{}\\leq \\frac{\\frac{\\tilde{f}(Y_n)}{S}}{\\frac{\\tilde{m}}{S} \\cdot g(Y_n)}} \\class{fragment}{{}= \\frac{\\tilde{f}(Y_n)}{\\tilde{m} \\cdot g(Y_n)}}\n\\end{align*}\n\\]",
    "crumbs": [
      "Slides",
      "Simulation"
    ]
  },
  {
    "objectID": "Slides/slides_simulation.html#cas-multidimensionnel",
    "href": "Slides/slides_simulation.html#cas-multidimensionnel",
    "title": "Simulation",
    "section": "Cas multidimensionnel",
    "text": "Cas multidimensionnel\n\n\n\nimpossibilité de la méthode de l’inverse: fonction de répartition non disponible (en général)\nla méthode de rejet : généralisable au cas multidimensionnel\n\n“fléau de la dimension”: plus la dimension est grande, plus la méthode est inefficace (penser au nombre de points nécessaires pour quadriller un hypercube…)\ndifficulté d’écrire une fonction de majoration en toute généralité",
    "crumbs": [
      "Slides",
      "Simulation"
    ]
  },
  {
    "objectID": "Slides/slides_simulation.html#exemple-loi-uniforme-sur-le-disque-unité-2d",
    "href": "Slides/slides_simulation.html#exemple-loi-uniforme-sur-le-disque-unité-2d",
    "title": "Simulation",
    "section": "Exemple: loi uniforme sur le disque unité (2D)",
    "text": "Exemple: loi uniforme sur le disque unité (2D)\n\nLoi cible: loi uniforme sur le disque unité, \\(f(x)\\propto {1\\hspace{-3.8pt} 1}_{x_1^2+x_2^2 \\leq 1}(x)\\) pour \\(x=(x_1,x_2)\\in\\mathbb{R}^2\\)\n\nLoi majorante: loi uniforme sur le carré \\([-1,1]^2\\), \\(g(x)\\triangleq \\tfrac{1}{2} {1\\hspace{-3.8pt} 1}_{[-1, 1]^2}(x)\\) et \\(m=2\\)\n\n\n\n\n\n\n\nNote\n\n\nLa loi uniforme sur le carré est une loi produit: il suffit de savoir générer une loi uniforme sur un segment 1D pour l’obtenir",
    "crumbs": [
      "Slides",
      "Simulation"
    ]
  },
  {
    "objectID": "Slides/slides_simulation.html#exemple-loi-uniforme-sur-le-disque-visualisation",
    "href": "Slides/slides_simulation.html#exemple-loi-uniforme-sur-le-disque-visualisation",
    "title": "Simulation",
    "section": "Exemple: loi uniforme sur le disque (visualisation)",
    "text": "Exemple: loi uniforme sur le disque (visualisation)\n\n\n\n\n\n                                                \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmd`Ratio estimée: **${ratio.toPrecision(5)}**`\nmd`Aire (bleue) estimé: **${aire.toPrecision(5)}**`",
    "crumbs": [
      "Slides",
      "Simulation"
    ]
  },
  {
    "objectID": "Slides/slides_simulation.html#exemple-loi-uniforme-sur-une-cardioïde-2d",
    "href": "Slides/slides_simulation.html#exemple-loi-uniforme-sur-une-cardioïde-2d",
    "title": "Simulation",
    "section": "Exemple: loi uniforme sur une cardioïde (2D)",
    "text": "Exemple: loi uniforme sur une cardioïde (2D)\n\nLoi cible: loi uniforme sur le disque unité, \\(f(x)\\triangleq{1\\hspace{-3.8pt} 1}_{(x_1^2+x_2^2 - x_2)^2 \\leq x_1^2+ x_2^2}(x)\\) pour \\(x=(x_1,x_2)\\in\\mathbb{R}^2\\) \nLoi majorante: loi uniforme sur le rectangle \\([-2,3]\\times [-1.5,1.5]\\), \\(g(x)\\triangleq \\tfrac{1}{15}{1\\hspace{-3.8pt} 1}_{[-2,3]\\times [-1.5,1.5]}(x)\\) et \\(m=15\\)\n\n\n\n\n\n\n\nNote\n\n\nLa loi uniforme sur un rectangle est une loi produit: il suffit de savoir générer une loi uniforme sur un segment 1D pour l’obtenir",
    "crumbs": [
      "Slides",
      "Simulation"
    ]
  },
  {
    "objectID": "Slides/slides_simulation.html#exemple-loi-uniforme-sur-une-cardioïde-2d-1",
    "href": "Slides/slides_simulation.html#exemple-loi-uniforme-sur-une-cardioïde-2d-1",
    "title": "Simulation",
    "section": "Exemple: loi uniforme sur une cardioïde (2D)",
    "text": "Exemple: loi uniforme sur une cardioïde (2D)\n\n\n\n\n\n                                                \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmd`Ratio estimée: **${ratio_cardioid.toPrecision(5)}**`\nmd`Aire (bleue) estimé: **${aire_cardioid.toPrecision(5)}**`",
    "crumbs": [
      "Slides",
      "Simulation"
    ]
  },
  {
    "objectID": "Slides/slides_simulation.html#théorie",
    "href": "Slides/slides_simulation.html#théorie",
    "title": "Simulation",
    "section": "Théorie",
    "text": "Théorie\n\nThéorème 4 (Géneration uniforme sur un ensemble) Supposons \\(A\\subset B \\subset \\mathbb{R}^d\\), deux ensembles mesureables pour la mesure de Lebesgue. Pour générer selon une loi uniforme sur \\(A\\), connaissant un générateur uniforme sur \\(B\\), on peut utiliser la méthode du rejet, en tirant \\(Y_i \\sim \\mathcal{U}(B)\\) (i.i.d) et en ne gardant \\(Y_i\\) que si \\(Y_i \\in A\\).\n\n\nPreuve: On note \\(f\\triangleq \\frac{1}{|A|} {1\\hspace{-3.8pt} 1}_{A}\\), \\(g\\triangleq \\frac{1}{|B|}{1\\hspace{-3.8pt} 1}_{B}\\) et \\(m\\triangleq\\frac{|B|}{|A|}\\). Comme \\(A \\subset B\\), pour tout \\(x\\in \\mathbb{R}^{d}\\): \\[\n\\begin{align*}\n\\class{fragment}{{}f(x)} \\class{fragment}{{}= \\frac{1}{|A|} {1\\hspace{-3.8pt} 1}_{A}(x)} \\class{fragment}{{}\\leq \\frac{1}{|B|} {1\\hspace{-3.8pt} 1}_{B}(x) \\cdot \\frac{|B|}{|A|}} \\class{fragment}{{}\\leq g(x) \\cdot m}\n\\end{align*}\n\\]\n\n\\[\n\\begin{align*}\n&\\class{fragment}{{}Y \\sim \\mathcal{U}(B),} \\quad \\class{fragment}{{} U \\sim \\mathcal{U}([0,1])} \\\\\n&\\class{fragment}{{}r(Y)} \\class{fragment}{{}= \\frac{f(Y)}{m\\cdot g(Y)}}\n\\class{fragment}{{}=\n\\frac{ \\frac{1}{|A|} {1\\hspace{-3.8pt} 1}_{A}(Y)}\n{\\frac{|B|}{|A|}\\cdot \\frac{1}{|B|} {1\\hspace{-3.8pt} 1}_{B}(Y)}}\n\\class{fragment}{{}=\n\\frac{  {1\\hspace{-3.8pt} 1}_{A}(Y)}\n{ {1\\hspace{-3.8pt} 1}_{B}(Y)}}\n\\class{fragment}{{}= {1\\hspace{-3.8pt} 1}_{A}(Y)}\n\\end{align*}\n\\]\n\\[\n\\begin{align*}\n\\class{fragment}{{}\\text{Enfin}, U \\leq {1\\hspace{-3.8pt} 1}_{A}(Y) \\iff {1\\hspace{-3.8pt} 1}_{A}(Y)=1}\\class{fragment}{{}\\iff Y \\in A}\n\\end{align*}\n\\]",
    "crumbs": [
      "Slides",
      "Simulation"
    ]
  },
  {
    "objectID": "Slides/slides_simulation.html#remarque-sur-les-constantes",
    "href": "Slides/slides_simulation.html#remarque-sur-les-constantes",
    "title": "Simulation",
    "section": "Remarque sur les constantes",
    "text": "Remarque sur les constantes\n\nDans l’exemple précédent, on a pu appliquer la méthode de rejet sans la connaissance de \\(m\\)\n\nPoint important: parfois la connaissance de \\(m\\) est difficile à obtenir, et il est préférable de ne pas l’utiliser (notamment quand les constantes de normalisation des densités sont difficiles à calculer).\n Exemples: statistiques bayesiennes, modèles graphiques, etc.",
    "crumbs": [
      "Slides",
      "Simulation"
    ]
  },
  {
    "objectID": "Slides/slides_simulation.html#sommes-de-variables-aléatoires",
    "href": "Slides/slides_simulation.html#sommes-de-variables-aléatoires",
    "title": "Simulation",
    "section": "Sommes de variables aléatoires",
    "text": "Sommes de variables aléatoires\nLoi de Bernouilli: avec \\(U_1, \\ldots, U_n\\) i.i.d uniformes sur \\([0,1]\\) (méthode d’inversion): \\[\nX_i \\triangleq {1\\hspace{-3.8pt} 1}_{\\{U_i \\leq p\\}} \\sim \\mathcal{B}(p)\n\\] Loi binomiale: \\[\n    \\sum_{i=1}^n {1\\hspace{-3.8pt} 1}_{\\{U_i \\leq p\\}} \\sim \\mathcal{B}(n,p)\n\\] en rappelant que \\[\n    X = X_1 + \\cdots + X_n \\sim \\mathcal{B}(n,p)\n\\]\n\n\n\n\n\n\nNote\n\n\nLa méthode d’inversion marche, mais nécessite le calcul de l’inverse généralisée de \\(F\\), donc de coefficients binomiaux…",
    "crumbs": [
      "Slides",
      "Simulation"
    ]
  },
  {
    "objectID": "Slides/slides_simulation.html#loi-de-poisson",
    "href": "Slides/slides_simulation.html#loi-de-poisson",
    "title": "Simulation",
    "section": "Loi de Poisson",
    "text": "Loi de Poisson\nRappel: \\(\\quad X \\sim \\mathcal{P}(\\lambda) \\iff \\mathbb{P}(X = k) = e^{-\\lambda} \\dfrac{\\lambda^k}{k!}\\,, \\quad \\forall k \\in \\mathbb{N}^*.\\)\n\nProposition 2 (Génération de v.a. de loi de Poisson) \nSoit \\((E_n)_{n \\geq 1}\\) des variables aléatoires i.i.d. de loi exponentielle de paramètre \\(\\lambda &gt; 0\\). On pose \\(S_k = E_1 + \\cdots + E_k\\). Alors pour tout \\(n \\in \\mathbb{N}^*\\) \\[\n    \\mathbb{P}(S_n \\leq 1 &lt; S_{n+1}) =  e^{-\\lambda} \\dfrac{\\lambda^n}{n!}\\enspace .\n\\] Ainsi, la variable aléatoire \\(T\\) définie par \\(T \\triangleq \\sup \\{n \\in \\mathbb{N}^* : S_n \\leq 1\\}\\) suit une loi de Poisson de paramètre \\(\\lambda\\) : \\(T \\sim \\mathcal{P}(\\lambda)\\).\n\n\n\n\n\n\n\n\nPoint numérique\n\n\nC’est la méthode utilisée par numpy.random.poisson, cf. code source (GitHub) (en passant à l’exponentiel), qui fut proposée par D. Knuth; Source: Wikipedia",
    "crumbs": [
      "Slides",
      "Simulation"
    ]
  },
  {
    "objectID": "Slides/slides_simulation.html#loi-de-poisson-suite",
    "href": "Slides/slides_simulation.html#loi-de-poisson-suite",
    "title": "Simulation",
    "section": "Loi de Poisson (suite)",
    "text": "Loi de Poisson (suite)\nLa preuve repose sur le résultat suivant:\n\nLemme 1 (Loi de Erlang) \nSoit \\(n\\) variables aléatoires \\(E_1, \\dots, E_n\\) i.i.d. de loi exponentielle de paramètre \\(\\lambda &gt;0\\). La somme \\(E_1+\\dots+E_n\\) suit une loi d’Erlang de paramètres \\((n,\\lambda)\\), donnée par la fonction de répartition \\[\n    F_{n,\\lambda}(t) = 1 - \\sum_{k=0}^{n-1} e^{-\\lambda t} \\frac{(\\lambda t)^k}{k!}\\,.\n\\]",
    "crumbs": [
      "Slides",
      "Simulation"
    ]
  },
  {
    "objectID": "Slides/slides_simulation.html#démonstration-avec-le-lemme",
    "href": "Slides/slides_simulation.html#démonstration-avec-le-lemme",
    "title": "Simulation",
    "section": "Démonstration avec le lemme",
    "text": "Démonstration avec le lemme\nPour \\(n \\in \\mathbb{N}^*\\), on décompose la probabilité \\(\\mathbb{P}(S_n \\leq 1 &lt; S_{n+1})\\) via \\[\n\\begin{align*}\n    \\mathbb{P}(S_n \\leq 1 &lt; S_{n+1})\n    & = \\mathbb{P}(\\{S_n \\leq 1\\} \\setminus \\{S_{n+1} \\leq 1\\})\\\\\n    & = \\mathbb{P}(S_n \\leq 1) - \\mathbb{P}(S_{n+1} \\leq 1)\\,.\n\\end{align*}\n\\] Le lemme précédent donne \\(\\mathbb{P}(S_n \\leq 1) = 1 - \\sum_{n=0}^{n-1} e^{-\\lambda} \\dfrac{\\lambda^k}{k!}\\) et \\(\\mathbb{P}(S_{n+1} \\leq 1) = 1 - \\sum_{k=0}^{n} e^{-\\lambda} \\dfrac{\\lambda^k}{k!}.\\) On obtient alors le résultat souhaité : \\[\n    \\mathbb{P}(S_n \\leq 1 &lt; S_{n+1})\n    = e^{-\\lambda} \\dfrac{\\lambda^n}{n!}\\,.\n\\]\nOn conclut la preuve de la proposition en remarquant que \\[\n    \\mathbb{P}(T=n) = \\mathbb{P}(S_n \\leq 1 &lt; S_{n+1})\\,.\n\\]\n\n\nSimulation",
    "crumbs": [
      "Slides",
      "Simulation"
    ]
  },
  {
    "objectID": "Slides/slides-index.html#test",
    "href": "Slides/slides-index.html#test",
    "title": "Visualisation: inversion",
    "section": "test",
    "text": "test\n\nviewof dist = Inputs.select(['normal','cauchy','laplace','bimodal'], {value: \"bimodal\", label: \"Loi\"})\nviewof replay = html`&lt;button&gt;Relancer`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ninvcdfboard = require(await FileAttachment(\"../inverse-vizu/dist/invcdfboard.umd.cjs\").url())\n\n{\n  replay\n  const canvas = DOM.canvas(500, 500);\n  const galton = invcdfboard.galton(canvas,dist);\n  \n  return html`${galton.canvas}`\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVisualisation: inversion"
  },
  {
    "objectID": "Courses/th_asymptotique.html",
    "href": "Courses/th_asymptotique.html",
    "title": "Théorèmes asymptotiques",
    "section": "",
    "text": "Le premier résultat fondamental en probabilités concerne le comportement asymptotique de la moyenne empirique: \n\\bar X_n = \\dfrac{X_1 + \\cdots + X_n}{n} \\enspace.\n quand on observe n variables aléatoires i.i.d X_1,\\dots,X_n, ayant une espérance finie.\n\nThéorème 1 (Loi forte des grands nombres) \nSoit (X_n)_{n \\geq 1} une suite de variables aléatoires indépendantes et identiquement distribuées (i.i.d.) dans L^1(\\Omega, \\mathcal{F}, \\mathbb{P}). Notons \\mu = \\mathbb{E}[X_1]. Alors \\bar X_n converge vers \\mu presque sûrement : \n\\mathbb{P}\\bigg( \\dfrac{X_1 + \\cdots + X_n}{n} \\underset{n \\to \\infty}{\\longrightarrow} \\mu \\bigg) = 1\\,.\n\n\nInterprétation: Intuitivement, la probabilité d’un événement A correspond à la fréquence d’apparition de A quand on répète une expérience qui fait intervenir cet événement. Par exemple, si on dispose une pièce truquée, on estimera la probabilité d’apparition du côté pile en lançant la pièce un grand nombre de fois et en comptant le nombre de pile obtenu. La loi des grands nombres justifie a posteriori cette intuition : si X_1, \\ldots, X_n sont i.i.d. de loi de Bernoulli de paramètre p, alors \n    \\dfrac{X_1 + \\cdots + X_n}{n} \\xrightarrow[n \\to \\infty]{p.s.} p \\enspace.\n Le membre de gauche correspond au nombre empirique de pile obtenu, celui de droite à la valeur théorique.\nRemarque: Bien qu’assez intuitif, ce théorème est difficile à démontrer, cf.[@Ouvrard08;@Barbe_Ledoux06] ou encore [@Williams91] pour une version de preuve avec des martingales.\n#| standalone: true\n#| viewerHeight: 550\nimport numpy as np\nfrom shiny import ui, render, App, reactive\nfrom shinywidgets import output_widget, register_widget\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objs as go\n\n\nn_init = 42\n\n\napp_ui = ui.page_fluid(\n    ui.tags.head(\n        ui.tags.style(\"\"\"\n            .bslib-sidebar-layout &gt; .sidebar &gt; .sidebar-content {\n            display: flex;\n            flex-direction: column;\n            padding: 0.5rem;\n            }\n            .irs--shiny .irs-line {\n            top: 27px;\n            height: 4px;\n            }\n            .irs.irs--shiny .irs-bar {\n            top: 27px;\n            height: 4px;\n            }\n            .irs--shiny .irs-handle {\n            top: 23px;\n            width: 22px;\n            height: 10px;\n        \"\"\")\n    ),\n    ui.panel_title(\"Loi des grands nombres\"),\n    ui.layout_sidebar(\n        ui.panel_sidebar(\n            ui.input_action_button(\"seed\", \"Nouveau tirage\", class_=\"btn-primary\"),\n            ui.input_slider( \"p\", \"Espérance: p\", 0.01, 0.99, value=0.35, step=0.01, ticks=False,\n            ),\n            ui.input_slider( \"n_samples\", \"Échantillons: n\", 2, 1000, value=15, step=1, ticks=False),\n        width=3),\n    ui.panel_main(output_widget(\"my_widget\"), width = 9)\n    )\n)\n\n\n\n\ndef server(input, output, session):\n    seed = reactive.Value(42)\n\n    @reactive.Effect\n    @reactive.event(input.seed)\n    def _():\n        seed.set(np.random.randint(0, 1000))\n\n    subplots = make_subplots(\n        rows=2,\n        cols=1,\n        vertical_spacing=0.45,\n        horizontal_spacing=0.04,\n        row_heights=[5, 1],\n        subplot_titles=(\n            f\"Moyenne empirique: loi de Bernoulli\",\n            \"Tirages aléatoires &lt;span style='color:rgb(66, 139, 202)'&gt;bleu: 0&lt;/span&gt;, &lt;span style='color:rgb(255, 0, 0)'&gt;rouge: 1&lt;/span&gt; (seed=\"\n            + f\"{n_init:03}\"\n            + \")\",\n        ),\n    )\n    fig = go.FigureWidget(subplots) \n    fig.add_trace(\n        go.Scatter(\n            mode=\"lines\",\n            line=dict(color=\"black\", width=3),\n            x=[],\n            y=[],\n            name=r\"Moyenne &lt;br&gt; empirique\",\n        ),\n        row=1,\n        col=1,\n    )\n    fig.add_trace(\n        go.Scatter(\n            mode=\"lines\",\n            line=dict(dash=\"dash\", color=\"black\", width=1),\n            marker={},\n            x=[],\n            y=[],\n            name=r\"p\",\n        ),\n            row=1,\n            col=1,\n    )\n    fig.add_trace(\n        go.Heatmap(\n            x=[],\n            z=[],\n            colorscale=[[0, \"rgb(66, 139, 202)\"], [1, \"rgb(255,0,0)\"]],\n            showscale=False,\n        ),\n        row=2,\n        col=1,\n    )\n\n    fig.update_yaxes(range=[0, 1.1], row=1, col=1)\n    fig.update_xaxes(matches=\"x1\", row=2, col=1)\n    fig.update_yaxes(visible=False, row=2, col=1)\n    fig.update_xaxes(visible=False, row=2, col=1)\n\n    fig.update_layout(\n        template=\"simple_white\",\n        showlegend=True,\n        xaxis_title=\"Échantillons: n\",\n    )\n    fig.update_layout(autosize=True)\n\n    fig.update_layout(\n        legend=dict(\n            yanchor=\"top\",\n            y=0.99,\n            xanchor=\"left\",\n            x=0.65,\n            bgcolor=\"rgba(0,0,0,0)\",\n        )\n    )\n    fig.update_layout(\n        margin=dict(l=0, r=0, b=10, t=70),\n    )\n\n    register_widget(\"my_widget\", fig)\n\n    @reactive.Effect\n    def _():\n        p = input.p()\n        n_samples = input.n_samples()\n\n        rng = np.random.default_rng(seed())\n        iterations = np.arange(1, n_samples + 1)\n        samples = rng.binomial(1, p, size=n_samples)\n        means_samples = np.cumsum(samples) / np.arange(1, n_samples + 1)\n\n        # Update data in fig:\n        fig.data[0].x = iterations\n        fig.data[0].y = means_samples\n\n        fig.data[1].x = iterations\n        fig.data[1].y = np.full((n_samples), p)\n\n        fig.data[2].x = iterations\n        fig.data[2].z = [samples]\n\n        fig.update_xaxes(range=[1, n_samples + 1])\n\n        # Update the subplot titles:\n        fig.layout.annotations[1].update(\n            text=f\"Tirages aléatoires (seed=\"\n            + f\"{seed():03}\"\n            + \") &lt;br&gt; &lt;span style='color:rgb(66, 139, 202)'&gt;bleu: 0&lt;/span&gt;, &lt;span style='color:rgb(255, 0, 0)'&gt;rouge: 1&lt;/span&gt; \"\n        )\n\n\napp = App(app_ui, server)\n\nPour aller plus loin:\nQuant p varie, à n fixé…les signaux générés sont très très proches, ce qui ne devrait pas être le cas sans structuration particulière de la génération. L’aléa est imparfait!",
    "crumbs": [
      "Cours",
      "Théorèmes asymptotiques"
    ]
  },
  {
    "objectID": "Courses/th_asymptotique.html#loi-des-grands-nombres",
    "href": "Courses/th_asymptotique.html#loi-des-grands-nombres",
    "title": "Théorèmes asymptotiques",
    "section": "",
    "text": "Le premier résultat fondamental en probabilités concerne le comportement asymptotique de la moyenne empirique: \n\\bar X_n = \\dfrac{X_1 + \\cdots + X_n}{n} \\enspace.\n quand on observe n variables aléatoires i.i.d X_1,\\dots,X_n, ayant une espérance finie.\n\nThéorème 1 (Loi forte des grands nombres) \nSoit (X_n)_{n \\geq 1} une suite de variables aléatoires indépendantes et identiquement distribuées (i.i.d.) dans L^1(\\Omega, \\mathcal{F}, \\mathbb{P}). Notons \\mu = \\mathbb{E}[X_1]. Alors \\bar X_n converge vers \\mu presque sûrement : \n\\mathbb{P}\\bigg( \\dfrac{X_1 + \\cdots + X_n}{n} \\underset{n \\to \\infty}{\\longrightarrow} \\mu \\bigg) = 1\\,.\n\n\nInterprétation: Intuitivement, la probabilité d’un événement A correspond à la fréquence d’apparition de A quand on répète une expérience qui fait intervenir cet événement. Par exemple, si on dispose une pièce truquée, on estimera la probabilité d’apparition du côté pile en lançant la pièce un grand nombre de fois et en comptant le nombre de pile obtenu. La loi des grands nombres justifie a posteriori cette intuition : si X_1, \\ldots, X_n sont i.i.d. de loi de Bernoulli de paramètre p, alors \n    \\dfrac{X_1 + \\cdots + X_n}{n} \\xrightarrow[n \\to \\infty]{p.s.} p \\enspace.\n Le membre de gauche correspond au nombre empirique de pile obtenu, celui de droite à la valeur théorique.\nRemarque: Bien qu’assez intuitif, ce théorème est difficile à démontrer, cf.[@Ouvrard08;@Barbe_Ledoux06] ou encore [@Williams91] pour une version de preuve avec des martingales.\n#| standalone: true\n#| viewerHeight: 550\nimport numpy as np\nfrom shiny import ui, render, App, reactive\nfrom shinywidgets import output_widget, register_widget\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objs as go\n\n\nn_init = 42\n\n\napp_ui = ui.page_fluid(\n    ui.tags.head(\n        ui.tags.style(\"\"\"\n            .bslib-sidebar-layout &gt; .sidebar &gt; .sidebar-content {\n            display: flex;\n            flex-direction: column;\n            padding: 0.5rem;\n            }\n            .irs--shiny .irs-line {\n            top: 27px;\n            height: 4px;\n            }\n            .irs.irs--shiny .irs-bar {\n            top: 27px;\n            height: 4px;\n            }\n            .irs--shiny .irs-handle {\n            top: 23px;\n            width: 22px;\n            height: 10px;\n        \"\"\")\n    ),\n    ui.panel_title(\"Loi des grands nombres\"),\n    ui.layout_sidebar(\n        ui.panel_sidebar(\n            ui.input_action_button(\"seed\", \"Nouveau tirage\", class_=\"btn-primary\"),\n            ui.input_slider( \"p\", \"Espérance: p\", 0.01, 0.99, value=0.35, step=0.01, ticks=False,\n            ),\n            ui.input_slider( \"n_samples\", \"Échantillons: n\", 2, 1000, value=15, step=1, ticks=False),\n        width=3),\n    ui.panel_main(output_widget(\"my_widget\"), width = 9)\n    )\n)\n\n\n\n\ndef server(input, output, session):\n    seed = reactive.Value(42)\n\n    @reactive.Effect\n    @reactive.event(input.seed)\n    def _():\n        seed.set(np.random.randint(0, 1000))\n\n    subplots = make_subplots(\n        rows=2,\n        cols=1,\n        vertical_spacing=0.45,\n        horizontal_spacing=0.04,\n        row_heights=[5, 1],\n        subplot_titles=(\n            f\"Moyenne empirique: loi de Bernoulli\",\n            \"Tirages aléatoires &lt;span style='color:rgb(66, 139, 202)'&gt;bleu: 0&lt;/span&gt;, &lt;span style='color:rgb(255, 0, 0)'&gt;rouge: 1&lt;/span&gt; (seed=\"\n            + f\"{n_init:03}\"\n            + \")\",\n        ),\n    )\n    fig = go.FigureWidget(subplots) \n    fig.add_trace(\n        go.Scatter(\n            mode=\"lines\",\n            line=dict(color=\"black\", width=3),\n            x=[],\n            y=[],\n            name=r\"Moyenne &lt;br&gt; empirique\",\n        ),\n        row=1,\n        col=1,\n    )\n    fig.add_trace(\n        go.Scatter(\n            mode=\"lines\",\n            line=dict(dash=\"dash\", color=\"black\", width=1),\n            marker={},\n            x=[],\n            y=[],\n            name=r\"p\",\n        ),\n            row=1,\n            col=1,\n    )\n    fig.add_trace(\n        go.Heatmap(\n            x=[],\n            z=[],\n            colorscale=[[0, \"rgb(66, 139, 202)\"], [1, \"rgb(255,0,0)\"]],\n            showscale=False,\n        ),\n        row=2,\n        col=1,\n    )\n\n    fig.update_yaxes(range=[0, 1.1], row=1, col=1)\n    fig.update_xaxes(matches=\"x1\", row=2, col=1)\n    fig.update_yaxes(visible=False, row=2, col=1)\n    fig.update_xaxes(visible=False, row=2, col=1)\n\n    fig.update_layout(\n        template=\"simple_white\",\n        showlegend=True,\n        xaxis_title=\"Échantillons: n\",\n    )\n    fig.update_layout(autosize=True)\n\n    fig.update_layout(\n        legend=dict(\n            yanchor=\"top\",\n            y=0.99,\n            xanchor=\"left\",\n            x=0.65,\n            bgcolor=\"rgba(0,0,0,0)\",\n        )\n    )\n    fig.update_layout(\n        margin=dict(l=0, r=0, b=10, t=70),\n    )\n\n    register_widget(\"my_widget\", fig)\n\n    @reactive.Effect\n    def _():\n        p = input.p()\n        n_samples = input.n_samples()\n\n        rng = np.random.default_rng(seed())\n        iterations = np.arange(1, n_samples + 1)\n        samples = rng.binomial(1, p, size=n_samples)\n        means_samples = np.cumsum(samples) / np.arange(1, n_samples + 1)\n\n        # Update data in fig:\n        fig.data[0].x = iterations\n        fig.data[0].y = means_samples\n\n        fig.data[1].x = iterations\n        fig.data[1].y = np.full((n_samples), p)\n\n        fig.data[2].x = iterations\n        fig.data[2].z = [samples]\n\n        fig.update_xaxes(range=[1, n_samples + 1])\n\n        # Update the subplot titles:\n        fig.layout.annotations[1].update(\n            text=f\"Tirages aléatoires (seed=\"\n            + f\"{seed():03}\"\n            + \") &lt;br&gt; &lt;span style='color:rgb(66, 139, 202)'&gt;bleu: 0&lt;/span&gt;, &lt;span style='color:rgb(255, 0, 0)'&gt;rouge: 1&lt;/span&gt; \"\n        )\n\n\napp = App(app_ui, server)\n\nPour aller plus loin:\nQuant p varie, à n fixé…les signaux générés sont très très proches, ce qui ne devrait pas être le cas sans structuration particulière de la génération. L’aléa est imparfait!",
    "crumbs": [
      "Cours",
      "Théorèmes asymptotiques"
    ]
  },
  {
    "objectID": "Courses/th_asymptotique.html#théorème-central-limite-tcl",
    "href": "Courses/th_asymptotique.html#théorème-central-limite-tcl",
    "title": "Théorèmes asymptotiques",
    "section": "Théorème central limite (TCL)",
    "text": "Théorème central limite (TCL)\nUne fois la loi des grands nombres établie, on peut se demander quel est l’ordre suivant dans le développement asymptotique de \\bar X_n - \\mu, ou de manière équivalente de S_n - n \\mu, où S_n = X_1 + \\cdots + X_n. Le théorème suivant répond à cette question, en donnant une convergence en loi d’une transformation affine de la moyenne empirique:\n\nThéorème 2 (Théorème central limite) Soit X_1, \\ldots, X_n une suite de variables aléatoires i.i.d de variance \\sigma^2 = {\\rm var}(X_1) \\in ]0, \\infty[. On note \\mu = \\mathbb{E}[X_1] leur espérance. Alors \n\\sqrt n \\left(\\tfrac{\\bar X_n - \\mu}{\\sigma} \\right) \\xrightarrow[n \\to +\\infty]{\\mathcal{L}} N\\enspace,\n où N suit une loi normale centrée réduite : N \\sim\\mathcal{N}(0,1).\n\nPreuve: cf.[@Ouvrard08;@Barbe_Ledoux06].\nOn peut interpréter ce théorème grossièrement de la façon suivante: la moyenne empirique de variables aléatoires i.i.d de variance \\sigma^2 se comporte asymptotiquement comme une loi normale \\mathcal{N}(\\mu, \\tfrac{\\sigma^2}{n}), ce que l’on écrit avec un abus de notation:\n\n\\bar X_n \\approx \\mathcal{N}(\\mu, \\frac{\\sigma^2}{n}) \\enspace.\n\nEn termes de somme cumulée empirique, la convergence se réécrit\n\n    \\tfrac{S_n - n \\mu}{\\sqrt n \\sigma} \\xrightarrow[n \\to +\\infty]{\\mathcal{L}} N \\enspace.\n\nLes hypothèses de ce théorème sont plutôt faibles (il suffit de supposer une variance finie). Pourtant, le résultat est universel : la loi de départ peut être aussi farfelue que l’on veut, elle se rapprochera toujours asymptotiquement d’une loi normale.\nOn rappelle que la convergence en loi est équivalente à la convergence des fonctions de répartition en tout point de continuité de la limite. Ainsi, le théorème central limite se réécrit de la manière suivante : pour tout a &lt; b, notons \\alpha_n=\\mathbb{P} \\left(\\bar X_n \\notin [ \\mu + \\tfrac{a \\sigma}{\\sqrt{n}}, \\mu + \\tfrac{ b \\sigma}{\\sqrt{n}}] \\right). Ainsi\n\n\\mathbb{P} \\left(\\tfrac{\\bar X_n - \\mu}{\\sigma}\\in [ \\tfrac{a}{\\sqrt{n}},\\tfrac{b}{\\sqrt{n}}] \\right)\\\\\n\\begin{align}\n    1-\\alpha_n& = \\mathbb{P} \\left(\\bar X_n \\in [ \\mu + \\tfrac{a \\sigma}{\\sqrt{n}}, \\mu + \\tfrac{ b \\sigma}{\\sqrt{n}}] \\right)\\nonumber\\\\\n    & =\n    \\mathbb{P} \\left(\\tfrac{\\bar X_n - \\mu}{\\sigma} \\in [ \\tfrac{a}{\\sqrt{n}},\\tfrac{b}{\\sqrt{n}}]\\right) \\nonumber\\\\\n    & =\n    \\mathbb{P} \\bigg( a \\leq \\sqrt n \\left(\\tfrac{\\bar X_n - \\mu}{\\sigma} \\right) \\leq b\\bigg) \\nonumber\\\\\n    & \\underset{n \\to \\infty}{\\longrightarrow}  \\int_a^b \\varphi(x) \\,  dx\\,. \\nonumber\\\\\n\\end{align}\n où l’on note \\varphi (resp. \\Phi) la densité (resp. la fonction de répartition) d’une loi normale centrée réduite, définie pour tout x\\in\\mathbb{R} par \\varphi(x)=\\tfrac{e^{-\\frac{x^2}{2}}}{\\sqrt{2\\pi}} (resp. \\Phi(x)= \\int_{-\\infty}^{x}\\varphi(u) du).\nDans le cas classique d’un intervalle de confiance à 95%, c’est-à-dire quand \\alpha_n=0.05, et en prenant un intervalle de confiance symétrique (alors a=-t et b=q) on obtient 1-\\alpha_n= \\int_{-q}^q \\varphi(x) \\, dx=\\Phi(q)-\\Phi(-q)=2 \\Phi(q)-1 \\implies \\boxed{q=\\Phi^{-1}(1-\\tfrac{\\alpha_n}{2})} et q est donc le quantile de niveau 1-\\tfrac{\\alpha_n}{2} de la loi normale centrée réduite. Numériquement on peut facilement évaluer q et vérifier que q\\approx 1.96 avec scipy:\n\nfrom scipy.stats import norm\nq = norm.ppf(1-0.05/2)\nprint(f\"Gaussienne centrée réduite,\\nQuantile de niveau (1-α/2):\\nq = {q:.2f}\")\n\nGaussienne centrée réduite,\nQuantile de niveau (1-α/2):\nq = 1.96\n\n\n\nExemple 1 (Loi de Bernoulli) On considère des variables aléatoires X_1, \\ldots, X_n i.i.d. suivant une loi de Bernoulli de paramètre p \\in ]0,1[, dont l’espérance et la variance sont respectivemenbt p et p(1-p). Le théorème central limite donne alors \n    \\sqrt n \\left(\\frac{\\bar X_n - p}{p (1-p)} \\right) \\xrightarrow[n \\to +\\infty]{\\mathcal{L}} N\\,,\n avec N \\sim \\mathcal{N}(0,1). Cette convergence est illustrée dans le widget ci-dessous. Le contexte est le suivant. On répète t fois le processus, qui consiste à afficher (\\bar{X}_k)_{k \\in [n]}, où les n variables aléatoires sont i.i.d. et suivent une loi de Bernoulli de paramètre p.\n\n#| standalone: true\n#| viewerHeight: 600\nimport numpy as np\nfrom scipy.stats import norm\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nfrom shiny import ui, render, App, reactive\nfrom shinywidgets import output_widget, render_widget\n\n\napp_ui = ui.page_fluid(\n    ui.tags.head(\n        ui.tags.style(\"\"\"\n            .bslib-sidebar-layout &gt; .sidebar &gt; .sidebar-content {\n            display: flex;\n            flex-direction: column;\n            padding: 0.1rem;\n            }\n            .irs--shiny .irs-line {\n            top: 27px;\n            height: 4px;\n            }\n            .irs.irs.irs--shiny .irs-bar {\n            top: 27px;\n            height: 4px;\n            }\n            .irs--shiny .irs-handle {\n            top: 23px;\n            width: 22px;\n            height: 10px;\n        \"\"\")\n    ),\n    ui.panel_title(\"TCL\"),\n    ui.layout_sidebar(\n        ui.panel_sidebar(\n            ui.input_action_button(\"seed\", \"Nouveau tirage\",class_=\"btn-primary\"),\n            ui.input_slider(\"p\", \"Espérance: p\", 0.01, 0.99, value=0.5, step=0.01, ticks=False),\n            ui.input_slider(\"n_samples\", \"Échantillons: n\", 1, 200, value=100, step=1, ticks=False),\n            ui.input_slider(\"n_repetitions\", \"Répétitions: t\", 1, 300, value=200, step=1, ticks=False),\n        width=3\n        ),\n    ui.panel_main(output_widget(\"my_widget\"), width = 9)\n    )\n)\n\n\n\ndef server(input, output, session):\n    seed = reactive.Value(42)\n\n\n    @reactive.Effect\n    @reactive.event(input.seed)\n    def _():\n        seed.set(np.random.randint(0, 1000))\n\n\n    @output\n    @render_widget\n    def my_widget():\n\n        rng = np.random.default_rng(seed())\n        p = input.p()\n        n_repetitions = input.n_repetitions()\n        n_samples = input.n_samples()\n        iterations = np.arange(1, n_samples + 1)\n        samples = rng.binomial(1, p, size=(n_repetitions, n_samples))\n        means_samples = np.cumsum(samples, axis=1) / np.arange(1, n_samples + 1)\n        x_hist = np.linspace(0, 1, num=300)\n\n        # Create figure\n        fig = make_subplots(\n                    rows=1,\n                    cols=3,\n                    # vertical_spacing=0.5,\n                    horizontal_spacing=0.02,\n                    column_widths=[20, 2, 3],\n                    subplot_titles=(\"t = \" + str(n_repetitions) + \" répétitions\",\"\",\"\")\n                )\n\n\n        for i in range(n_repetitions):\n            fig.add_trace(\n                    go.Scatter(\n                        mode='lines',\n                        line=dict(color=\"rgba(0,0,0,0.05)\", width=1),\n                        x=iterations,\n                        y=means_samples[i,:],\n                        ),\n                        row=1, col=1,\n            )\n        fig.add_trace(\n                go.Scatter(\n                    mode='lines',\n                    line=dict(dash=\"dash\", color=\"blue\", width=1),\n                    marker={},\n                    x=iterations,\n                    y=np.full((n_samples), p),\n                    name='p'),\n                    row=1, col=1,\n        )\n        fig.update_layout(\n            template=\"simple_white\",\n            showlegend=False,\n        )\n        fig.add_trace(\n                go.Scatter(\n                    mode='markers',\n                    marker=dict(color=\"rgba(0,0,0,0.05)\", size=4),\n                    x=np.zeros(n_samples),\n                    y=means_samples[:,-1],\n                ),\n                row=1, col=2,\n\n        )\n        y_hist, bins = np.histogram(means_samples[:,-1], bins=int(np.sqrt(n_repetitions)), density=True)\n        fig.add_trace(\n            go.Bar(x=y_hist, y=bins[:-1] + np.diff(bins)/2,\n                    opacity=0.75,\n                    marker_color = 'black',\n                    orientation='h',\n                    width=np.diff(bins),\n                    name=\"Tirages de moyennes empiriques\",\n                    ),\n                row=1, col=3,\n        )\n        fig.add_trace(\n            go.Scatter(x=norm.pdf(x_hist, p, np.sqrt(p*(1-p) / n_samples)),\n                       y=x_hist,\n                       mode='lines',\n                       line=dict(color=\"red\"),\n                       legendgroup='1',\n                       name=\"TCL\"\n                       ),\n                row=1, col=3,\n        )\n\n        fig.update_xaxes(range=[1, n_samples + 1])\n        fig.update_yaxes(range=[-.05, 1.05], row=1, col=1)\n        fig.update_yaxes(matches=\"y1\",visible = False,  row=1, col=2)\n        fig.update_xaxes(range=[-0.2, 0.2], visible = False, row=1, col=2)\n\n        fig.update_yaxes(matches=\"y1\", row=1, col=3, visible=False)\n        fig.update_xaxes(range=[0, 1.1 / np.sqrt(2*np.pi* p*(1-p) / n_samples)], row=1, col=3)\n        fig.update_xaxes(visible=False, row=1, col=3)\n\n\n\n        for trace in fig['data']:\n            print(trace)\n            if (trace['name'] != 'TCL') and (trace['name'] != 'p'):\n                trace['showlegend'] = False\n        fig.update_layout(\n        margin=dict(l=0, r=0, b=10, t=100),\n        )\n        fig.update_layout(\n            title=dict(text=\"Distribution de la moyenne empirique&lt;br&gt; (cas loi de Bernoulli)\", yanchor=\"top\", y=0.95),\n            title_x=0.5,\n            showlegend=True,\n\n        )\n        fig.update_layout(\n            legend=dict(\n                yanchor=\"top\",\n                y=0.99,\n                xanchor=\"left\",\n                x=0.89,\n                bgcolor=\"rgba(0,0,0,0)\",\n            )\n        )\n        fig['layout']['xaxis']['title']='Échantillons: n'\n\n        fig.update_layout(autosize=True)\n\n        return fig\n\n\napp = App(app_ui, server)\n\nUne autre illustration possible de la convergence donnée par le TCL est celle qui correspond au point de vue donnée par l’analyse. Pour cela supposons que l’on ait une suite de variables aléatoires réelles X_1, \\dots, X_n, i.i.d. dont la fonction de densité commune est notée par f.\nOn rappelle quelques éléments de probabilités concernant les densités. Pour cela on rappelle la définition de la convolution deux fonctions. Pour cela prenons deux fonctions f et g définies sur \\mathbb{R} et qui sont intégrables au sens de Lebesgue. La convolution de f par g est alors la fonction f*g suivante:\n\n\\begin{align}\n\\mathbb{R} &\\mapsto \\mathbb{R} \\nonumber\\\\\nx &\\to \\int_{-\\infty}^{+\\infty} f(x-y)g(y) dy \\enspace.\\nonumber\n\\end{align}\n\n\n\n\n\n\n\nNote\n\n\n\nOn peut aussi obtenir f*g(x) en calculant \\int_{\\mathbb{R}^2} f(u)g(v) {1\\hspace{-3.8pt} 1}_{u+v=x} du dv.\n\n\n\nThéorème 3 (Loi de la somme et convolutions) Soient X et Y des v.a. indépendantes de densités f et g respectivement, la loi de X+Y est donnée par la convolution f*g.\n\nRappel: pour un scalaire \\alpha\\neq 0, la densité de \\alpha X est donnée par la fonction x \\mapsto \\frac{1}{|\\alpha|} \\cdot f(\\frac{x}{\\alpha}).\n\nCorollaire 1 (Loi de la moyenne) Soient X_1,\\dots,X_n des v.a. i.i.d. de densité f, la densité de \\bar{X}_n est donnée par la fonction x \\mapsto n \\cdot [f*\\dots*f](n \\cdot x).\n\nDessous, pour X_1, \\dots, X_n, i.i.d., de densité f, on affiche la densité de la loi de \\bar{X}_n.\n#| standalone: true\n#| viewerHeight: 550\nimport numpy as np\nfrom shiny import ui, render, App, reactive\nfrom shinywidgets import output_widget, register_widget\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objs as go\nfrom scipy import signal\n\n\napp_ui = ui.page_fluid(\n    ui.tags.head(\n        ui.tags.style(\"\"\"\n            .bslib-sidebar-layout &gt; .sidebar &gt; .sidebar-content {\n            display: flex;\n            flex-direction: column;\n            padding: 0.5rem;\n            }\n            .irs--shiny .irs-line {\n            top: 27px;\n            height: 2px;\n            }\n            .irs.irs--shiny .irs-bar {\n            top: 27px;\n            height: 2px;\n            }\n            .irs--shiny .irs-handle {\n            top: 23px;\n            width: 22px;\n            height: 10px;\n        \"\"\")\n    ),\n    ui.panel_title(\"TCL et convolutions\"),\n    ui.layout_sidebar(\n        ui.panel_sidebar(\n            ui.input_select(\n                \"loi\",\n                \"Loi sous-jacente\",\n                {'uniforme': 'Uniforme', 'laplace' : 'Laplace'},\n            ),\n            ui.input_slider(\n                \"n_iter\",\n                \"Échantillons: n\",\n                1,\n                10,\n                value=1,\n                step=1,\n                ticks=False,\n            ), width = 3\n    ),\n    ui.panel_main(\n        output_widget(\"my_widget\"), width = 9\n    )\n)\n)\n\n\nnnzeros = 10001\nx_min = -20\nx_max = 20\n\nx = np.linspace(x_min, x_max, nnzeros)\ny = np.zeros(nnzeros)\nmask = np.where(np.abs(x) &lt;= 0.5, 1, 0)\ny[mask == 1] = 1\ndelta = (x_max - x_min) / nnzeros\nvar = np.sum(y * x**2 *(delta)) - (np.sum(y * x * delta))**2\n\ndef convolve(signal_in, n_convolutions, delta):\n    output = np.zeros(len(signal_in))\n    if n_convolutions == 0:\n        return output\n    elif n_convolutions == 1:\n        return signal_in\n    else:\n        output = signal_in.copy()\n        for i in range(n_convolutions - 1):\n            output = signal.fftconvolve(\n                output * delta, signal_in, mode=\"same\"\n            )\n        return output\n\n\ndef server(input, output, session):\n\n    fig = go.FigureWidget()\n    fig.add_trace(\n        go.Scatter(\n            mode=\"lines\",\n            line=dict(color=\"black\", width=3),\n            x=[],\n            y=[],\n            name=\"loi de de la moyenne empirique&lt;br&gt;(variance adéquate)\",\n        )\n    )\n\n    fig.add_trace(\n        go.Scatter(\n            x=x,\n            y=np.exp(-(x**2) / (2 * var)) / np.sqrt(2 * var * np.pi),\n            mode=\"lines\",\n            line=dict(dash=\"dash\", color=\"red\", width=2),\n            name=f\"Loi normale&lt;br&gt;(variance adéquate)\",\n        )\n    )\n    fig.update_xaxes(range=[-3, 3], position=0.)\n    fig.update_yaxes(range=[0, 2 * np.max(y)], position=0.5, showticklabels=False)\n\n\n    fig.update_layout(\n        template=\"simple_white\",\n        showlegend=True,\n        autosize=True,\n        title=dict(text=\"Densité : &lt;br&gt; moyenne de n variables aléatoires\", yanchor=\"top\", y=0.95),\n        title_x=0.5,\n    )\n    fig.update_layout(legend=dict(\n        yanchor=\"top\",\n        y=0.95,\n        xanchor=\"left\",\n        x=0.8,\n        font=dict(size= 18)\n    ))\n    fig.update_layout(\n    height=250,\n    margin=dict(l=0, r=0, b=10, t=100),\n    )\n\n    register_widget(\"my_widget\", fig)\n\n    @reactive.Effect\n    def _():\n        if str(input.loi()) == 'uniforme':\n            y = np.zeros(nnzeros)\n            mask = np.where(np.abs(x) &lt;= 0.5, 1, 0)\n            y[mask == 1] = 1\n\n        else:\n            y=np.exp(-np.abs(x)) / 2\n            y = y / (np.sum(y) * delta)\n\n        var = np.sum(y * x**2 * delta) - (np.sum(y * x * delta))**2\n\n        y_display = convolve(y, input.n_iter(), delta)\n        # Update data in fig:\n        fig.data[0].x = x / np.sqrt(input.n_iter())\n        fig.data[0].y = y_display * np.sqrt(input.n_iter())\n        fig.data[1].y = np.exp(-(x**2) / (2 * var)) / np.sqrt(2 * var * np.pi)\n        fig.update_yaxes(range=[0, 2 * np.max(y)], position=0.5, showticklabels=False)\n\n\napp = App(app_ui, server)\nPour aller plus loin sur les convolutions, voir la vidéo de 3Blue1Brown à ce sujet: Convolutions | Why X+Y in probability is a beautiful mess",
    "crumbs": [
      "Cours",
      "Théorèmes asymptotiques"
    ]
  },
  {
    "objectID": "Courses/simulation.html",
    "href": "Courses/simulation.html",
    "title": "Simulation",
    "section": "",
    "text": "Dans ce chapitre on se demande comment simuler en pratique des variables aléatoires i.i.d. L’idée est de commencer par le cas de variables aléatoires de loi uniforme et d’en déduire les autres lois.",
    "crumbs": [
      "Cours",
      "Simulation"
    ]
  },
  {
    "objectID": "Courses/simulation.html#variables-aléatoires-uniformes",
    "href": "Courses/simulation.html#variables-aléatoires-uniformes",
    "title": "Simulation",
    "section": "Variables aléatoires uniformes",
    "text": "Variables aléatoires uniformes\nOn rappelle qu’une variable aléatoire U suit une loi uniforme sur [0,1], noté \\mathcal{U}([0,1]) si sa fonction de répartition F_U est donnée par \nF_U(x)\n=\n\\begin{cases}\n    0, & \\text{si }x &lt; 0\\,,        \\\\\n    x, & \\text{si }x \\in [0,1]\\,,  \\\\\n    1, & \\text{si }x &gt; 1\\,.        \\\\\n\\end{cases}\n\n\n\n\n\n                                                \n\n\nFigure 1: Fonction de répartition de la loi uniforme\n\n\n\n\nL’objectif est de simuler sur machine une suite U_1, \\ldots, U_n de variables aléatoires i.i.d. de loi \\mathcal{U}([0,1]). Plusieurs problèmes apparaissent alors :\n\nUne machine est déterministe.\nLes nombres entre 0 et 1 donnés par la machine sont de la forme k/2^p, pour k \\in \\{0, \\ldots, 2^{p-1}\\}. On ne pourra donc jamais générer des nombres qui ne sont pas de cette forme.\nVérifier qu’une suite est bien i.i.d. est un problème difficile.\n\n\nDéfinition 1 (Générateur de nombres pseudo-aléatoires) \nUn générateur de nombres pseudo-aléatoires (🇬🇧: Pseudo Random Number Generator, PRNG), est un algorithme déterministe récursif qui renvoie une suite U_1, \\ldots, U_n dans [0,1] qui a un “comportement similaire” à une suite i.i.d. de loi \\mathcal{U}([0,1]). Pour être plus rigoureux, ces nombres sont en fait des nombres entiers générés uniformément sur un certain interval. Dans un second temps, une transformation simple (normalisation) permet d’obtenir des nombres flottants (🇬🇧: floats) entre 0 et 1.\n\n\n\n\n\n\n\nPour aller plus loin\n\n\n\nParfois il est utile d’aller chercher dans le code source certaines information pour savoir comment les fonctions sont codées dans les packages que l’on utiliser. Par exemple, pour numpy que l’on utilise fréquement, on peut voir l’opération choisie ici: Random: int -&gt; float en numpy.\n\n\nUn tel algorithme se construit de la manière suivante :\n\nOn part d’une graine (🇬🇧: seed) U_0 qui détermine la première valeur de manière la plus arbitraire possible.\nLa procédure récursive s’écrit U_{n+1} = f(U_n), où f est une transformation déterministe, de sorte que U_{n+1} est le plus indépendant possible de U_1, \\dots·, U_n.\n\n\nLa fonction f est déterministe et prend ses valeurs dans un ensemble fini, donc l’algorithme est périodique. Le but est donc d’avoir la plus grande période possible.\nNotons qu’une fois que la graine est fixée, alors l’algorithme donne toujours les mêmes valeurs. Fixer la graine peut donc être très utile pour répéter des simulations dans des conditions identiques et ainsi repérer des erreurs.\n\n\n\n\n\n\n\nExercice: bug ou feature?\n\n\n\nReprendre les widgets du chapitre Théorèmes asymptotiques et faites varier doucement le paramètre p (de Bernoulli). Que constatez-vous? Proposer une explication potentielle.\n\n\n\nGénérateur congruentiel linéaire\nLa plupart des PRNG s’appuient sur des résultats arithmétiques. Un des plus connus est celui appelé Générateur congruentiel linéaire (🇬🇧 Linear congruential generator, LCG). Il est défini comme suit: on construit récursivement une suite d’entiers X_i via la congruence \n  X_{n+1} = a X_n + b \\quad \\text{mod } m \\enspace,\n où a,b,m sont des entiers bien choisis pour que la suite obtenue ait de bonnes propriétés. Il suffit alors de considérer X_n/m. Par exemple, la fonction rand sur scilab utilise cette congruence avec m=2^{31}, a=843\\; 314\\; 861, et b=453\\; 816\\; 693.\n\n\nGénérateurs alternatifs\nLes langages Python et R utilisent par défaut le générateur Mersenne-Twister qui s’appuie sur la multiplication vectorielle, mais d’autres générateurs sont aussi disponibles. Ce générateur a pour période m =2^{19937}-1, nombre qu’on peut raisonnablement considérer comme grand.\nPour numpy la méthode par défaut est PCG64 (cf. documentation de numpy), qui dispose de meilleures garanties statistiques (Voir le site https://www.pcg-random.org pour cela).\n\n\nUsage en numpy\nOn suppose désormais disposer d’un générateur pseudo-aléatoire sur [0,1]. En numpy depuis la version 1.17, une bonne manière d’utiliser des éléments aléatoires est d’utiliser un générateur que l’on définit soi-même:\n\nseed = 12345  # Toujours être conscient qu'une graine existe\nrng = np.random.default_rng(seed)  #\nprint(rng.random())  ##  un tirage uniforme sur [0,1]\nprint(rng.random(size=5))  ## cinq tirages uniformes sur [0,1]\nprint(rng.random(size=(3, 2)))  ## matrice 3x2, à entrées unif. sur [0,1]\n\n0.22733602246716966\n[0.31675834 0.79736546 0.67625467 0.39110955 0.33281393]\n[[0.59830875 0.18673419]\n [0.67275604 0.94180287]\n [0.24824571 0.94888115]]\n\n\nDans la suite on va voir comment générer d’autres lois à partir de la loi uniforme, mais il est clair que les logiciels modernes proposent un large éventail de distribution classique (gaussienne, exponentielle, etc.). Une liste exhaustive est donnée ici pour numpy.\n\n\n\n\n\n\nPour aller plus loin\n\n\n\nUne excellent discussion sur les bonnes pratiques aléatoires en numpy, et l’usage de np.random.default_rng est donnée dans ce blog post d’Albert Thomas.\n\n\n\n\nPropriété de la loi uniforme\nOn verra souvent apparaître la variable aléatoire 1-U où U \\sim \\mathcal{U}([0,1]). Il se trouve que 1-U suit aussi une loi uniforme sur [0,1] comme le montre le calcul de sa fonction de répartition. Ainsi pour tout x \\in [0,1] on obtient \n\\begin{align*}\n\\mathbb{P}(1-U \\leq x) & = \\mathbb{P}(U \\geq 1-x),\\\\\n                       & = 1-(1-x), \\\\\n                       & = x\\,.\n\\end{align*}\n On peut démontrer facilement la même relation pour x&lt;0 et x&gt;1, d’où le résultat.",
    "crumbs": [
      "Cours",
      "Simulation"
    ]
  },
  {
    "objectID": "Courses/simulation.html#méthode-dinversion",
    "href": "Courses/simulation.html#méthode-dinversion",
    "title": "Simulation",
    "section": "Méthode d’inversion",
    "text": "Méthode d’inversion\nL’idée de la méthode d’inversion repose sur le résultat suivant :",
    "crumbs": [
      "Cours",
      "Simulation"
    ]
  },
  {
    "objectID": "Courses/simulation.html#rappel-sur-la-fonction-quantile",
    "href": "Courses/simulation.html#rappel-sur-la-fonction-quantile",
    "title": "Simulation",
    "section": "Rappel sur la fonction quantile",
    "text": "Rappel sur la fonction quantile\nRappel : Pour F une fonction définie sur \\mathbb{R} à valeurs dans [0, 1], croissante, on note\n\n\\forall q \\in ]0,1[, \\quad F^\\leftarrow(q) = \\inf\\{ x \\in \\mathbb{R} : F(x)\\geq q\\}\n\\tag{1}\n\nThéorème 1 (Caratérisation des quantiles) Soit F une fonction définie sur \\mathbb{R} à valeurs dans [0, 1], croissante et continue à droite, alors pour tout q \\in ]0, 1[, on a \n\\begin{align}\n   \\{x \\in \\mathbb{R} :  F(x) \\geq q) \\} & =\n   \\{x \\in \\mathbb{R} : x \\geq F^\\leftarrow(q)  \\}\n\\end{align}\n\n\nPreuve\n\nCas \\subset: Soit x \\in \\mathbb{R} t.q. F(x) \\geq q, alors par définition de l’inf dans Équation 1, x \\geq F^\\leftarrow(q).\nCas \\supset: Soit x \\in \\mathbb{R} t.q. x \\geq F_X^\\leftarrow(q) alors pour tout \\epsilon &gt; 0, x + \\epsilon &gt; F^\\leftarrow(q), donc F(x + \\epsilon) \\geq q. Puis, par continuité à droite de F, F(x) \\geq q.",
    "crumbs": [
      "Cours",
      "Simulation"
    ]
  },
  {
    "objectID": "Courses/simulation.html#méthode-dinversion-1",
    "href": "Courses/simulation.html#méthode-dinversion-1",
    "title": "Simulation",
    "section": "Méthode d’inversion",
    "text": "Méthode d’inversion\n\nThéorème 2 (Méthode d’inversion) Soit X une v.a réelle, et U \\sim\\mathcal{U}([0,1]), alors la variable aléatoire F_X^{\\leftarrow}(U) a même loi que X.\n\nPreuve: En utilisant le théorème précédent, on a \\mathbb{P}(F_X^{\\leftarrow}(U) \\leq x) = \\mathbb{P}(U \\leq F_X(x)) pour tout x\\in\\mathbb{R}. Puis, comme U est une loi uniforme sur [0,1], \\mathbb{P}(U\\leq F_X(x))=F_X(x).\nOn en déduit donc que la loi de F_X^{-1}(U) est la même que celle de X, car les deux v.a. ont la même fonction de répartition.\n\nExemple 1 (Simulation d’une loi exponentielle) On rappelle que la loi exponentielle de paramètre \\lambda &gt; 0 a pour densité \nf_{\\lambda}(x) = \\lambda e^{-\\lambda x} {1\\hspace{-3.8pt} 1}_{\\mathbb{R}_+}(x)\\enspace.\n et donc pour fonction de répartition \nF_{\\lambda}(x) = 1 - e^{-\\lambda x} {1\\hspace{-3.8pt} 1}_{\\mathbb{R}_+}(x)\\enspace.\n On vérifie que F_{\\lambda} est bijective de \\mathbb{R}_+ dans ]0,1[ et que son inverse est donnée pour tout u \\in ]0,1[ par \nF_{\\lambda}^{-1}(u) = -\\frac{1}{\\lambda} \\log(1-u)\\enspace.\n\n\nMalheureusement, la fonction F n’est pas toujours inversible (penser aux lois discrètes) c’est donc pourquoi on utilise l’inverse l’inverse généralisée ou fonction quantile introduite dans la section Notations: \n\n  F^\\leftarrow(p)=  \\inf\\{ x \\in \\mathbb{R} \\colon F(x)\\geq p\\} \\enspace.\n\nInterprétation: Définir l’inverse d’une fonction de répartition F revient à résoudre l’équation F(x) = \\alpha d’inconnue x pour un \\alpha fixé. Si F n’est pas bijective, deux problèmes apparaissent :\n\nl’équation n’a aucune solution ce qui revient à dire que F n’est pas surjectif (graphiquement, F présente des sauts) ;\nl’équation a plusieurs solutions ce qui revient à dire que F n’est pas injective (graphiquement cela se matérialise par un plateau à la hauteur \\alpha). Un exemple classique est celui où F est la fonction de répartition d’une variable aléatoire discrète.\n\nLe passage à l’inéquation F(x) \\geq u permet de contourner la non-surjectivité : on ne regarde non plus les droites horizontales y=u mais la région \\{y \\geq \\alpha\\}. Le choix de l’\\inf dans la définition de F^{\\leftarrow} permet de contourner la non-injectivité : vu qu’il y a possiblement plusieurs x tels que F(x) \\geq u, on choisit le “premier”. Ces considérations sont illustrées en Figure Figure 2.\n\n\n\n\n                                                \n\n\nFigure 2\n\n\n\n\nRemarques additionnelles:\n\nLa fonction F étant croissante, la quantité F^\\leftarrow(u) correspond au premier instant où F dépasse \\alpha. Si F est bijective (ce qui équivaut dans ce cas à strictement croissante et injective), alors F^\\leftarrow = F^{-1}.\nLa fonction F^\\leftarrow n’est rien d’autre que la fonction quantile : si 0 &lt; \\alpha &lt; 1, q_{1-\\alpha} = F^\\leftarrow(1-\\alpha) est le quantile d’ordre (1-\\alpha) de F. Par exemple, F^\\leftarrow(1/2) correspond à la médiane.\nNotons que si u=0, on peut alors naturellement poser F^{\\leftarrow}(0) = -\\infty. De même, avec la convention la convention \\inf \\emptyset = +\\infty, on peut alors étendre la définition de F^\\leftarrow à u=1 (mais F^\\leftarrow(1) n’est pas toujours égal à \\infty, voir les exemples ci-dessous).\n\n\nProposition 1 (Loi à support fini) \nSoit X une variable aléatoire discrète prenant uniquement les valeurs x_1 &lt; \\dots &lt; x_r (r modalité possibles) avec probabilité p_1, \\dots, p_r (donc p_1 + \\dots + p_r=1). On vérifie que pour tout u \\in ]0,1[, \n        F^\\leftarrow(u) =\n        \\begin{cases}\n            x_1 & \\text{si } 0 &lt; u \\leq p_1\\,,                  \\\\\n            x_2 & \\text{si } p_1 &lt; u \\leq p_1+p_2\\,,            \\\\\n                & \\vdots                                        \\\\\n            x_r & \\text{si }  \\sum_{i=1}^{r-1} p_i &lt; u &lt; 1\\,.\n        \\end{cases}\n\nSur cet exemple, on peut prolonger la définition de F^\\leftarrow à u=1 en posant F^\\leftarrow(1) = x_r. L’inverse généralisée se réécrit alors sous la forme \n        F^\\leftarrow(u) = \\sum_{k=1}^r x_k {1\\hspace{-3.8pt} 1}_{ \\{  \\sum_{i=1}^{k-1}p_i &lt; u \\leq \\sum_{i=1}^{k}p_i \\} }\\enspace,\n où on a posé p_0=0.\n\nL’expression précédente s’étend directement au cas où X prend un nombre (infini) dénombrable de valeurs, la somme devenant alors une série.\nLa méthode est illustré ci-dessous pour quelques lois intéressantes:\n\nviewof dist = Inputs.select(['normal','cauchy','laplace','bimodal'], {value: \"bimodal\", label: \"Loi\"})\nviewof replay = html`&lt;button&gt;Relancer`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ninvcdfboard = require(await FileAttachment(\"../inverse-vizu/dist/invcdfboard.umd.cjs\").url())\n\n{\n  replay\n  const canvas = DOM.canvas(500, 500);\n  const galton = invcdfboard.galton(canvas,dist);\n  \n  return html`${galton.canvas}`\n}",
    "crumbs": [
      "Cours",
      "Simulation"
    ]
  },
  {
    "objectID": "Courses/simulation.html#méthode-de-rejet",
    "href": "Courses/simulation.html#méthode-de-rejet",
    "title": "Simulation",
    "section": "Méthode de rejet",
    "text": "Méthode de rejet\nL’idée de la méthode de rejet est la suivante. On souhaite simuler une variable aléatoire X de densité f, appelée loi cible, mais f est trop compliquée pour que la simulation puisse se faire directement. On dispose cependant d’une autre densité g possédant les propriétés suivantes :\n\non sait simuler Y de loi g,\nil existe m &gt; 0 tel que f(x) \\leq m \\cdot g(x),\non sait évaluer le rapport d’acceptation r(x) = \\frac{f(x)}{mg(x)}.\n\nRemarquons d’ores et déjà que la constante m est nécessairement plus grande que 1 car \n    1 = \\int_\\mathbb{R} f(x) \\, dx \\leq m \\int_\\mathbb{R} g(x)\\, dx = m\\,.\n\nL’idée est alors de considérer deux suites i.i.d. de variables aléatoires indépendantes entre elles:\n\n(Y_n)_{n \\geq 1} de loi g,\n(U_n)_{n \\geq 1} de loi uniforme sur [0,1].\n\nEn pratique, Y_n correspond à une proposition et U_n permettra de décider si on accepte la proposition ou non. Si oui, alors on conserve Y_n, sinon on simule Y_{n+1}. Le rapport d’acceptation, c’est-à-dire la proportion de Y_n acceptées, correspond à r(x).\nAutrement dit, pour simuler X de densité f, il suffit de simuler Y de densité g et U uniforme jusqu’à ce que U \\leq r(Y). La proposition suivante assure que cette méthode donne bien le résultat voulu.\n\nProposition 2 (Méthode de rejet) \nSoit T = \\inf \\{n \\geq 1 : U_n \\leq r(Y_n)\\} le premier instant où le tirage est accepté. Alors :\n\nT suit une loi géométrique de paramètre 1/m,\nla variable aléatoire X = Y_T a pour densité f et est indépendante de T.\n\n\nDémonstration:\nIl s’agit d’étudier la loi du couple (X,T). Pour x \\in \\mathbb{R} et n \\in \\mathbb{N}^{*}, on écrit \\mathbb{P}(X \\leq x, T=n)= \\mathbb{P}(U_1 &gt; r(Y_1), \\dots, U_{n-1} &gt; r(Y_{n-1}), U_n \\leq r(Y_n), Y_n \\leq x). Les n tirages étant iid, on obtient \n    \\mathbb{P}(X \\leq x, T=n) = \\mathbb{P}(U_1 &gt; r(Y_1))^{n-1} \\mathbb{P}(U_n \\leq r(Y_n), Y_n \\leq x)\\,.\n\nConcernant le premier terme, les variables aléatoires Y_1 et U_1 sont indépendantes donc leur loi jointe correspond au produit des densités : \\begin{align*}\n        \\mathbb{P}(U_1 &gt; r(Y_1))\n         & = \\mathbb{P}((U_1, Y_1) \\in \\{(u,y) \\in \\mathbb{R}^2 : u &gt; r(y)\\})                             \\\\\n         & = \\int_{\\mathbb{R}^2} {1\\hspace{-3.8pt} 1}_{\\{u &gt; r(y)\\}} ({1\\hspace{-3.8pt} 1}_{[0,1]}(u) g(y)) \\, du dy  \\\\\n         & = \\int_\\mathbb{R} \\bigg( \\int_0^1 {1\\hspace{-3.8pt} 1}_{\\{u &gt; r(y)\\}} \\, du\\bigg) g(y)\\, d y \\\\\n         & =  \\int_\\mathbb{R} (1-r(y)) \\, g(y)\\, d y\\,,\n    \\end{align*}\n ce qui se réécrit, comme f et g sont des densités et que r(y) = f(y)/(m \\cdot g(y)): \\begin{align*}\n        \\mathbb{P}(U_1 &gt; r(Y_1))\n        & = \\int_\\mathbb{R} g(y)\\, d y - \\int_\\mathbb{R} \\dfrac{f(y)}{m}\\, dy \\\\\n        & = 1 - \\dfrac{1}{m}\\,.\n    \\end{align*}\n Le deuxième terme se calcule de manière analogue : \n\\begin{align*}\n    \\mathbb{P}(U_n \\leq r(Y_n), Y_n \\leq x)\n        & = \\int_{\\mathbb{R}^2} {1\\hspace{-3.8pt} 1}_{\\{u \\leq r(y)\\}} {1\\hspace{-3.8pt} 1}_{\\{y \\leq x\\}} ({1\\hspace{-3.8pt} 1}_{[0,1]}(u) g(y)) \\, du dy       \\\\\n        & = \\int_\\mathbb{R} \\bigg( \\int_0^1  {1\\hspace{-3.8pt} 1}_{\\{u \\leq r(y)\\}} \\, du\\bigg) {1\\hspace{-3.8pt} 1}_{\\{y \\leq x\\}}  g(y)\\, d y\\,,\n\\end{align*}\n c’est-à-dire \n\\begin{align*}\n        \\mathbb{P}(U_n \\leq r(Y_n), Y_n \\leq x)\n        & = \\int_\\mathbb{R} r(y) {1\\hspace{-3.8pt} 1}_{\\{y \\leq x\\}}  g(y)\\, d y \\\\\n        & = \\int_{-\\infty}^x \\dfrac{f(y)}{m}\\, d y \\\\\n        & = \\dfrac{F(x)}{m}\\,,\n\\end{align*}\n où F est la fonction de répartition de la loi de densité f. On peut ainsi conclure que \n    \\mathbb{P}(X \\leq x, T=n)\n    =\n    \\bigg(1 - \\dfrac{1}{m}\\bigg)^{n-1} \\dfrac{F(x)}{m}\\,.\n Il ne reste plus qu’à étudier les lois marginales. D’une part, par continuité monotone croissante, \n    \\mathbb{P}(T=n)\n    = \\lim_{q \\to \\infty} \\mathbb{P}(X \\in ]-\\infty, q], T=n)\\,,\n ce qui donne \n\\begin{align*}\n    \\mathbb{P}(T=n)\n    & = \\lim_{q \\to \\infty} \\bigg(1 - \\dfrac{1}{m}\\bigg)^{n-1} \\dfrac{F(q)}{m}\\\\\n    & = \\bigg(1 - \\dfrac{1}{m}\\bigg)^{n-1} \\dfrac{1}{m}\\,.\n\\end{align*}\n On en déduit que T suit une loi géométrique de paramètre 1/m. D’autre part, par \\sigma-additivité, \n\\begin{align*}\n    \\mathbb{P}(X \\leq x)\n    & = \\mathbb{P}(X \\leq x, T \\in \\mathbb{N}^*)\\\\\n    & = \\sum_{n=1}^\\infty \\mathbb{P}(X \\leq x, T=n)\\,,\n\\end{align*}\n ce qui donne \n\\begin{align*}\n    \\mathbb{P}(X \\leq x)\n    & = \\sum_{n=1}^\\infty \\bigg(1 - \\dfrac{1}{m}\\bigg)^{n-1} \\dfrac{F(x)}{m}\\\\\n    & = \\dfrac{1}{1-(1-1/m)} \\dfrac{F(x)}{m}\\\\\n    & = F(x)\\,,\n\\end{align*}\n ce qui prouve que X a pour loi F.\nEnfin, la loi du couple (X,T) est égale au produit des lois \n\\begin{align*}\n    \\mathbb{P}(X \\leq x, T=n)\n    & = \\bigg(1 - \\dfrac{1}{m}\\bigg)^{n-1} \\dfrac{F(x)}{m}\\\\\n    & = \\mathbb{P}(T=n) \\mathbb{P}(X \\leq x)\\,,\n\\end{align*}\n\n\n\nce qui prouve l’indépendance de X et T.\n\n\n□\n\n\n\ndef accept_reject(n, f, g, g_sampler, m):\n    \"\"\"\n    n: nombre de simulations\n    f: densité cible\n    g: densité des propositions, g_sampler: simulateur selon g\n    m: constante pour la majoration\n    \"\"\"\n    x_samples = np.zeros(n)\n    u_samples = np.zeros(n)\n    accepted = np.zeros(n)\n    n_accepted = 0\n    while n_accepted &lt; n:\n        x = g_sampler()\n        u = np.random.uniform()\n        alpha = u * m * g(x)\n        u_samples [n_accepted] = alpha\n        x_samples[n_accepted] = x\n        if  alpha &lt;= f(x):\n            accepted[n_accepted] = 1\n        n_accepted += 1\n    return x_samples, u_samples, accepted\n\n\n\n\n\n\n\nEn pratique…\n\n\n\nOn simule U_1 et Y_1. Si U_1 \\leq r(Y_1) c’est gagné, on pose X=Y_1. Sinon, on simule U_2 et Y_2 et on teste à nouveau l’inégalité U_2 \\leq r(Y_2). Et ainsi de suite. Comme T suit une loi géométrique de paramètre 1/m, son espérance vaut m : il faut en moyenne m tentatives pour obtenir une simulation de la loi de densité f. L’objectif est alors de choisir un couple (g, m) de sorte que m soit le plus proche possible de 1.\n\n\n\nExemple 2 (Rejet d’une loi polynomiale) Donnons un exemple jouet (on étudiera des exemples plus pertinents en TD). On considère la densité f(x) = 4x^3 {1\\hspace{-3.8pt} 1}_{[0,1]}(x). Comme f est majorée par 4, on peut choisir pour g la densité de la loi uniforme sur [0,1] et m=4. Alors, r(x) =f(x) / (mg(x)) = x^3, pour x \\in [0,1]. On simule donc (Y_1, U_1) et on teste si U_1 \\leq Y_1^3, etc.\nBien évidemment, on privilégiera ici une simulation via F^\\leftarrow qui permet de générer des variables aléatoires de loi f plus rapidement.\n\n\n\n\n                                                \n\n\nFigure 3: Visualisation des zones d’acceptations/rejet (g uniforme)\n\n\n\n\nNous pouvons facilement améliorer la proportion de point acceptés en proposant par exemple g définie par g(x) = 2x {1\\hspace{-3.8pt} 1}_{[0, 1]}(x), et m=2.\n\n\n\n\n                                                \n\n\nFigure 4: Visualisation des zones d’acceptations/rejet (g triangulaire)\n\n\n\n\n\n\n\n\n\n\n\n\n\nmd`Le taux d'acceptation est passé de **${ratio1}** à\n**${ratio2}** en utilisant une loi triangulaire au lieu d'une loi uniforme.`\n\n\n\n\n\n\n\n\n\nExemple 3 (Rejet d’une loi de densité d’Andrews) Considérons la densité d’Andrews définie par f(x) = \\frac{1}{S} \\frac{\\sin(\\pi\\cdot x)}{\\pi \\cdot x} {1\\hspace{-3.8pt} 1}_{[-1,1]}(x), avec S = \\int_{-1}^{1}\\frac{\\sin(\\pi\\cdot x)}{\\pi \\cdot x}dx. Dans ce contexte, on ne connait pas la valeur exacte de S, et on va donc utiliser la méthode de rejet pour simuler des variables aléatoires de loi f sans cette information. On peut l’adapter le test de la manière suivante: si l’on prend m=2/S et g(x) = \\frac{1}{2} {1\\hspace{-3.8pt} 1}_{[-1,1]}(x), on observe que tester u\\leq \\frac{f(x)}{m \\cdot g(x)} est équivalent à tester u \\leq r(x)=\\frac{1}{S} \\frac{\\sin(\\pi\\cdot x)}{\\pi \\cdot x} \\cdot \\frac{1}{\\frac{2}{S} g(x)} = \\frac{\\sin(\\pi\\cdot x)}{\\pi \\cdot x} \\cdot \\frac{1}{2 \\cdot g(x)}, ce qui peut se faire sans connaissance de S. De plus on peut vérifier que g(x) = \\frac{1}{2} {1\\hspace{-3.8pt} 1}_{[-1,1]}(x) définit une densité et que f(x) \\leq m \\cdot g(x) pour tout x\\in \\mathbb{R}.\n\nn = 10000\ng = lambda x: np.ones_like(x) / 2\ng_sampler = lambda: 2 * np.random.uniform() - 1\nm = 2\n\nx_samples, u_samples, accepted = accept_reject(n, np.sinc, g, g_sampler, m)\nratio = np.sum(accepted) / n\n# Note: https://stackoverflow.com/questions/70804891/how-to-vectorize-a-function-in-python-that-includes-a-limit-e-g-sinx-x\n\n\n\n\n\n\nOn peut approcher numériquement la valeur exacte de S en utilisant une méthode de calcul approchée, ce qui permet de comparer ici notre méthode de rejet avec la densité sous-jacente:\n\nfrom scipy import integrate\nS = integrate.quad(np.sinc, -1, 1)[0]\nprint(f\"En utilisant la méthode de rejet, on trouve que S = {S:.3f}\")\n\nEn utilisant la méthode de rejet, on trouve que S = 1.179\n\n\nEnfin, on peut visualiser la qualité l’approximation de la densité par la méthode de rejet en comparant la densité approchée (avec un histogramme) avec la densité exacte:\n\nfig = go.Figure()\nfig.add_trace(\n    go.Histogram(\n        x=x_samples[accepted == 1], histnorm=\"probability density\", name=\"Échantillons\"\n    )\n)\n\n# Plot the density\nx = np.linspace(-1, 1, 100)\nfig.add_trace(\n    go.Scatter(\n        x=x,\n        y=np.sinc(x) / S,\n        mode=\"lines\",\n        line=dict(color=\"black\", dash=\"dash\"),\n        name=\"Densité\",\n    )\n)\n\nfig.update_layout(template=\"simple_white\", showlegend=True)\n\n\n\n                                                \n\n\nFigure 5: Méthode de rejet pour simuler une loi de densité de type Andrews, sans connaissance de la valeur exacte de la constante de normalisation.\n\n\n\n\n\nmd`Dans cet example, le taux d'acceptation est ici de **${ratio}**.`\n\n\n\n\n\n\n\n\nCas mutlidimensionnel\nCommençons par un cas de dimension deux.\nPour cela on va utiliser la méthode de rejet pour simuler une loi de densité f sur \\mathbb{R}^2. En particulier, un exemple classique est de tirer des points dans le disque unité, c’est-à-dire de simuler une loi uniforme sur le disque unité. Pour cela, on va utiliser la méthode de rejet avec g la densité de la loi uniforme sur le carré [-1,1]^2.\nMais prenons un autre exemple, à savoir tirer des points uniformément dans la surface délimité par une cardioïde. Pour cela, on va utiliser la méthode de rejet avec g la densité de la loi uniforme sur le carré [-2,2]^2.\n\n\n\n\n                                                \n\n\nFigure 6: Méthode de rejet pour simuler une loi uniforme sur un disque unité.\n\n\n\n\nAire estimée: 3.0813333333333333\n\n\n\n\n\n\n                                                \n\n\nFigure 7: Méthode de rejet pour simuler une loi uniforme sur une surface délimitée par une cardioïde.\n\n\n\n\nAire estimée: 4.905\n\n\n\n\n\n\n\n\nEXERCISE loi uniforme sur un cylindre\n\n\n\nProposer une méthode pour simuler une loi uniforme sur un cylindre de rayon 1 et de hauteur 10.",
    "crumbs": [
      "Cours",
      "Simulation"
    ]
  },
  {
    "objectID": "Courses/simulation.html#autres-méthodes",
    "href": "Courses/simulation.html#autres-méthodes",
    "title": "Simulation",
    "section": "Autres méthodes",
    "text": "Autres méthodes\n\nSommation de variables aléatoires\nPour simuler une variable aléatoire de loi binomiale \\mathcal{B}(n,p), on peut utiliser la méthode d’inversion. Cependant, cela nécessite le calcul de l’inverse généralisée de F, donc de coefficients binomiaux et de puissances de p et 1-p. À la place, on utilisera plutôt la relation bien connue suivante : si X_1, \\ldots, X_n est une suite iid de variables aléatoires de loi de Bernoulli de paramètre p, alors \n    X = X_1 + \\cdots + X_n \\sim \\mathcal{B}(n,p)\\,.\n\nPour simuler des variables aléatoires de Bernoulli, on utilise la méthode d’inversion (voir Exemple ). Ainsi, si U_1, \\ldots, U_n sont des variables aléatoires iid de loi uniforme sur [0,1], alors \n    \\sum_{i=1}^n {1\\hspace{-3.8pt} 1}_{\\{U_i \\leq p\\}} \\sim \\mathcal{B}(n,p)\\,.\n\n\n\nLoi de Poisson\nRappelons qu’une variable aléatoire X suit une loi de Poisson de paramètre \\lambda &gt; 0, notée X \\sim \\mathcal{P}(\\lambda) si \n    \\mathbb{P}(X = k) = e^{-\\lambda} \\dfrac{\\lambda^k}{k!}\\,, \\quad k \\in \\mathbb{N}^*\\,.\n Une méthode pour simuler une variable aléatoire de loi de Poisson est donnée par la proposition suivante.\n\nProposition 3 (Génération de v.a. de loi de Poisson) \nSoit (E_n)_{n \\geq 1} des variables aléatoires i.i.d. de loi exponentielle de paramètre \\lambda &gt; 0. On pose S_k = E_1 + \\cdots + E_k. Alors, pour tout n \\in \\mathbb{N}^* \n    \\mathbb{P}(S_n \\leq 1 &lt; S_{n+1}) =  e^{-\\lambda} \\dfrac{\\lambda^n}{n!}\\enspace .\n Ainsi, la variable aléatoire T définie par \n    T \\triangleq \\sup \\{n \\in \\mathbb{N}^* : S_n \\leq 1\\}\n suit une loi de Poisson de paramètre \\lambda : T \\sim \\mathcal{P}(\\lambda).\n\nLa preuve repose sur le lemme suivant.\n\nLemme 1 (Loi de Erlang) \nSoit n variables aléatoires E_1, \\dots, E_n i.i.d. de loi exponentielle de paramètre \\lambda &gt;0. La somme E_1+\\dots+E_n suit une loi d’Erlang de paramètres (n,\\lambda), donnée par la fonction de répartition \n    F_{n,\\lambda}(t) = 1 - \\sum_{k=0}^{n-1} e^{-\\lambda t} \\frac{(\\lambda t)^k}{k!}\\,.\n\n\nDémonstration:\nOn montre le résultat pour n=2. La généralisation à k quelconque se fait par récurrence. Soit t &gt; 0, et f_{\\lambda}(x)={1\\hspace{-3.8pt} 1}_{\\{x \\geq 0 \\}} \\lambda e^{-\\lambda x} la densité d’une loi exponentielle de paramètre \\lambda. Les variables aléatoires E_1 et E_2 étant indépendantes et suivant des lois exponentielles de paramètre \\lambda_1 et \\lambda_2, on a \n\\begin{align*}\n    \\mathbb{P}(E_1+E_2 \\leq t)\n        & = \\int_{\\mathbb{R}^2} {1\\hspace{-3.8pt} 1}_{\\{x_1 + x_2 \\leq t\\}} f_{\\lambda}(x_1) f_{\\lambda}(x_2)\\, d x_1 d x_2 \\\\\n        & = \\int_{\\mathbb{R}^2} {1\\hspace{-3.8pt} 1}_{\\{x_1 + x_2 \\leq t\\}} \\lambda^2 e^{-\\lambda (x_1+x_2)} {1\\hspace{-3.8pt} 1}_{\\{x_1 \\geq 0\\}} {1\\hspace{-3.8pt} 1}_{\\{x_2 \\geq 0\\}}\\, d x_1 d x_2 \\\\\n        & = \\int_{\\mathbb{R}^2} {1\\hspace{-3.8pt} 1}_{\\{0 \\leq x_1 \\leq t\\}} {1\\hspace{-3.8pt} 1}_{\\{0 \\leq x_2 \\leq t-x_1\\}} \\lambda^2 e^{-\\lambda x_1} e^{-\\lambda x_2}\\, d x_1 d x_2             \\\\\n        & = \\int_0^t \\lambda e^{-\\lambda x_1} \\bigg(\\int _0^{t-x_1} \\lambda e^{-\\lambda x_2}\\, d x_2\\bigg)  d x_1\\,.\n\\end{align*}\n La première intégrale se calcule alors facilement : \n    \\int _0^{t-x_1} \\lambda e^{-\\lambda x_2}\\, d x_2 = 1 - e^{-\\lambda(t-x_1)}\\,.\n On obtient alors \n\\begin{align*}\n    \\mathbb{P}(E_1+E_2 \\leq t)\n   & = \\int_0^t \\lambda e^{-\\lambda x_1}dx_1 -  \\int_0^t e^{-\\lambda t} d x_1\\\\\n   & = 1 - e^{-\\lambda t} - \\lambda t e^{-\\lambda t}\\,.\n\\end{align*}\n Si t&lt;0, alors comme les E_i ne prennent que des valeurs positives on trouve \\mathbb{P}(E_1 + E_2 \\leq t) = 0. Ceci prouve le résultat pour n=2.\n\n\n\n\n\n□\n\n\nOn peut désormais prouver le résultat de la Proposition 3.\nDémonstration:\nPour n \\in \\mathbb{N}^*, on décompose la probabilité \\mathbb{P}(S_n \\leq 1 &lt; S_{n+1}) via \n\\begin{align*}\n    \\mathbb{P}(S_n \\leq 1 &lt; S_{n+1})\n    & = \\mathbb{P}(\\{S_n \\leq 1\\} \\setminus \\{S_{n+1} \\leq 1\\})\\\\\n    & = \\mathbb{P}(S_n \\leq 1) - \\mathbb{P}(S_{n+1} \\leq 1)\\,.\n\\end{align*}\n Le lemme précédent donne \n    \\mathbb{P}(S_n \\leq 1) = 1 - \\sum_{n=0}^{n-1} e^{-\\lambda} \\dfrac{\\lambda^k}{k!}\n et \n    \\mathbb{P}(S_{n+1} \\leq 1) = 1 - \\sum_{k=0}^{n} e^{-\\lambda} \\dfrac{\\lambda^k}{k!}\\,.\n On obtient alors le résultat souhaité : \n    \\mathbb{P}(S_n \\leq 1 &lt; S_{n+1})\n    = e^{-\\lambda} \\dfrac{\\lambda^n}{n!}\\,.\n\nOn conclut la preuve de la proposition en remarquant que \n    \\mathbb{P}(T=n) = \\mathbb{P}(S_n \\leq 1 &lt; S_{n+1})\\,.\n\n\n\n\n\n\n□\n\n\nLa simulation d’une variable aléatoire de Poisson repose donc sur la simulation de lois exponentielles qui se fait via la méthode d’inversion, comme vu dans Exemple 1. En pratique, on simule E_1 et on teste si E_1 &gt; 1. Si oui, on pose alors T=0. Si non, on simule E_2 et on teste si E_1 + E_2 &gt; 1. Si oui, on pose T=1. Sinon on continue la procédure.\n\n\nBibliographie et pour aller plus loin\n\nGenerating Random Floating-Point Numbers by Dividing Integers: a Case Study par Frédéric Goualard\nGenerating Pseudo-random Floating-Point Values par Allen Downey.",
    "crumbs": [
      "Cours",
      "Simulation"
    ]
  },
  {
    "objectID": "Courses/notations.html",
    "href": "Courses/notations.html",
    "title": "Notations et rappels",
    "section": "",
    "text": "On considère un espace probabilisé (\\Omega, {\\mathcal{F}}, \\mathbb{P}), composé d’un ensemble \\Omega, d’une tribu \\mathcal{F}, et d’une mesure de probabilité \\mathbb{P}.\nCette définition permet de transposer l’aléa qui provient de \\Omega dans l’espace E. L’hypothèse \\{X \\in B\\} \\in \\mathcal{F} assure que cet ensemble est bien un évènement et donc que l’on peut calculer sa probabilité.\nUne fois que l’aléa a été transposé de \\Omega vers E, on souhaite également transposer la probabilité \\mathbb{P} sur E. Ceci motive l’introduction de la notion de loi.\nLes propriétés de \\mathbb{P} assurent que \\mathbb{P}_X est bien une loi de probabilité sur l’espace mesurable (E, \\mathcal{E}).",
    "crumbs": [
      "Cours",
      "Notations et rappels"
    ]
  },
  {
    "objectID": "Courses/notations.html#loi-discrètes",
    "href": "Courses/notations.html#loi-discrètes",
    "title": "Notations et rappels",
    "section": "Loi discrètes",
    "text": "Loi discrètes\nLes variables aléatoires discrètes sont celles à valeurs dans un ensemble E discret, le plus souvent \\mathbb{N}, muni de la tribu pleine \\mathcal{F} = \\mathcal{P}(E).\n\nExemple 1 (Loi de Bernoulli) La loi la plus simple est la loi de Bernoulli de paramètre p \\in [0,1], définie sur \\{0,1\\} par \\mathbb{P}(X=1) = 1-\\mathbb{P}(X=0) = p qui modélise une expérience aléatoire à deux issues (succès = 1 et échec = 0).\n\n\nExemple 2 (Loi binomiale) En sommant des variables aléatoires indépendantes de loi de Bernoulli on obtient une loi binomiale : \\mathbb{P}(X=k) = \\binom{n}{k} p^k (1-p)^{n-k}, pour k \\in \\{0,\\ldots,n\\}, qui modélise le nombre de succès parmi n lancers.\n\n\nExemple 3 (Loi géométrique) En observant le nombre d’expériences nécessaires avant d’obtenir un succès, on obtient une loi géométrique : \\mathbb{P}(X=k) = p (1-p)^{k-1}, pour k \\geq 1. C’est une loi de probabilité discrète qui décrit le comportement du nombre d’événements se produisant dans un intervalle de temps fixé, si ces événements se produisent avec une fréquence moyenne ou espérance connue, et indépendamment du temps écoulé depuis l’événement précédent (e.g., nombre de clients dans une file d’attente, nombre de mutations dans un gène, etc.).\n\n\nExemple 4 (Loi de Poisson) La loi de Poisson de paramètre \\lambda &gt; 0 est définie par \\mathbb{P}(X=k) = e^{-\\lambda} \\lambda^k / k!, pour k \\in \\mathbb{N}",
    "crumbs": [
      "Cours",
      "Notations et rappels"
    ]
  },
  {
    "objectID": "Courses/notations.html#lois-continues",
    "href": "Courses/notations.html#lois-continues",
    "title": "Notations et rappels",
    "section": "Lois continues",
    "text": "Lois continues\nParmi les variables aléatoires réelles non discrètes, beaucoup peuvent se représenter avec une densité, c’est-à-dire qu’il existe une fonction mesurable f : \\mathbb{R} \\to [0, \\infty[ d’intégrale 1. La loi d’une telle variable aléatoire X est alors donnée pour tout A \\in \\mathcal{B}(\\mathbb{R}) par \n    \\mathbb{P}(X \\in A) = \\int_A f(x) \\, \\mathrm d x \\enspace.\n Les propriétés de l’intégrale de Lebesgue assure que cette formule définit bien une loi de probabilité.\n\nExemple 5 (Loi uniforme) La loi uniforme sur un ensemble B \\in \\mathcal{B}(\\mathbb{R}), s’obtient avec la densité définie par \nf(x) = {1\\hspace{-3.8pt} 1}_B(x) / \\lambda (B) \\enspace,\n où \\lambda (B) représente la mesure de Lebesgue de l’ensemble B. En particulier pour la loi uniforme sur le segment [0,1] on obtient la fonction suivante: \nf(x) = {1\\hspace{-3.8pt} 1}_{[0,1]}(x)\\enspace.\n Si une variable aléatoire U suit une telle loi on note U \\sim \\mathcal{U}([0,1]).\n\n\nExemple 6 (Loi exponentielle) La loi exponentielle de paramètre \\gamma &gt; 0 est obtenue avec la densité donnée par \nf(x) = \\gamma e^{-\\gamma x} {1\\hspace{-3.8pt} 1}_{\\mathbb{R}_+}(x)\\enspace.\n Si une variable aléatoire X suit cette loi on note X \\sim \\mathcal{Exp}(\\gamma).\n\n\nExemple 7 (Loi normale/gaussienne univariée) On obtient la loi normale de paramètre \\mu \\in \\mathbb{R} et \\sigma^2 &gt; 0 correspond à loi dont la densité est donnée par la fonction réelle: \nf(x) = \\frac{1}{\\sqrt{2 \\pi} \\sigma}e^{-\\frac{1}{2 \\sigma^2}(x-\\mu)^2} \\enspace.\n Si une variable aléatoire X suit une telle loi on note X \\sim \\mathcal{N}(\\mu,\\sigma^2), \\mu correspondant à l’espérance de la loi, et \\sigma^2 à sa variance. On nomme loi normale centrée réduite le cas correspondant à \\mu = 0 et \\sigma^2 = 1.\n\n\nExemple 8 (Loi normale multivariée) On peut étendre les lois normales au cas multi-dimensionnel. Fixons d\\in\\mathbb{N}^* un entier non nul. Pour un vecteur \\mu \\in \\mathbb{R}^d et une matrice symétrique-définie positive \\Sigma\\in \\mathbb{R^{d\\times d}}, la densité normale mutlivariée associée est donnée par la fonction: \nf(x) = \\frac{1}{{(2 \\pi)}^{\\frac{d}{2}} {\\rm det}(\\Sigma)} e^{-\\frac{1}{2}(x-\\mu)^\\top \\Sigma ^{-1}(x-\\mu)}\n Notons que \\mu est l’espérance de la loi et \\Sigma la matrice de variance-covariance.",
    "crumbs": [
      "Cours",
      "Notations et rappels"
    ]
  },
  {
    "objectID": "Courses/notations.html#fonction-de-répartition",
    "href": "Courses/notations.html#fonction-de-répartition",
    "title": "Notations et rappels",
    "section": "Fonction de répartition",
    "text": "Fonction de répartition\nLa notion de variable aléatoire n’est pas facile à manipuler puisqu’elle part d’un espace \\Omega dont on ne sait rien. On souhaite donc caractériser la loi d’une variable aléatoire en ne considérant que l’espace d’arrivée (E, \\mathcal{E}) .\nPlusieurs outils existent : la fonction de répartition (pour des variables aléatoires réelles), la fonction caractéristique (pour des variables aléatoires dans \\mathbb{R}^d), la fonction génératrice des moments (pour des variables aléatoires discrètes), etc. On se contente ici de la fonction de répartition qui nous sera utile pour simuler des variables aléatoires, ainsi que son inverse au sens de Levy.\n\nDéfinition 3 (Fonction de répartition 🇬🇧: cumulative distribution function) \nSoit X une variable aléatoire sur (\\mathbb{R}, \\mathcal{B}(\\mathbb{R})). La fonction de répartition de X est la fonction F_X définie sur \\mathbb{R} par \n\\begin{align*}\n    F_X(x) & = \\mathbb{P}(X \\leq x)\\\\\n           & = \\mathbb{P}(X \\in ]-\\infty, x]) \\enspace.\n\\end{align*}\n\n\n\nExemple 9 (Cas discret) Soit (x_i)_{i \\in I} une suite ordonnée de réels, avec I \\subset \\mathbb{N}. Si X est une variable aléatoire discrète prenant les valeurs (x_i)_{i \\in I} et de loi (p_i = \\mathbb{P}(X=x_i))_{i \\in I}, alors \n    F_X(x) = \\sum_{i \\in I} p_i {1\\hspace{-3.8pt} 1}_{[x_i, \\infty[}(x) \\enspace.\n\n\n\nExemple 10 (Cas continu) Si X est une variable aléatoire de densité f, alors \n    F_X(x) = \\int_{-\\infty}^x f(t) \\, \\mathrm dt \\enspace.\n\n\nLe graphe des fonctions de répartition des loi de Bernoulli, uniforme et normale sont représentées dans le widget ci-dessous. Notons que la fonction de répartition de la loi normale \\mathcal{N}(0,1), souvent notée \\Phi, n’admet pas d’expression explicite autre que \n\\Phi(x) = \\dfrac{1}{\\sqrt{2\\pi}} \\int_{-\\infty}^x e^{-\\frac{t^2}{2}}\\, \\mathrm d t\\enspace,\n Les valeurs numériques de \\Phi(x) étaient autrefois reportées dans des tables1. Par transformation affine, si X \\sim \\mathcal{N}(\\mu, \\sigma^2) — ce que l’on peut aussi écrire : X=\\mu + \\sigma Y, avec Y\\sim \\mathcal{N}(0,1) — alors sa fonction de répartition est donnée par F_X(x)=\\Phi((x-\\mu)/\\sigma).\n1 Wikipedia: loi normale\nProposition 1 (Propriétés de la fonction de répartition) Soit X une variable aléatoire de fonction de répartition F_X.\n\nF_X est une fonction croissante, de limite 0 en -\\infty et de limite 1 en +\\infty.\nF_X est continue à droite en tout point.\nPour tout x \\in \\mathbb{R}, on a \\mathbb{P}(X=x) = F_X(x) - F_X(x-), où F_X(x-) = \\lim_{\\epsilon \\to 0+} F_X(x- \\epsilon).\nSi X a pour densité f, alors F_X est dérivable \\lambda-presque partout de dérivée f.\n\n\nPour les démonstrations, voir par exemple [@Barbe_Ledoux06].\nLa propriété 3. est utile dans le cas discret : les valeurs prises par X correspondent aux points de discontinuité de F_X et les probabilités associées correspondent à la hauteur du saut.\nLa propriété 4. donne le lien entre la fonction de répartition d’une variable aléatoire à densité et sa densité. On peut donc retrouver la loi de X à partir de sa fonction de répartition. Le théorème suivant généralise ce résultat à toute variable aléatoire réelle (pas nécessairement discrète ou à densité).\n\nThéorème 1 (Caractérisation de la loi d’une variable aléatoire réelle) La fonction de répartition d’une variable aléatoire caractérise sa loi : deux variables aléatoires ont même loi si et seulement si elles ont même fonction de répartition.\n\nDémonstration: voir Wikipedia\nOn rappelle que la tribu des boréliens est engendrée par la famille d’ensembles \\{]-\\infty,x], x \\in \\mathbb{R}\\}. Le théorème précédent assure que si on connaît la mesure \\mathbb{P}_X sur cette famille d’ensembles alors on la connaît partout.\n\nExemple 11 (Loi exponentielle depuis une loi uniforme) On considère une variable aléatoire U de loi uniforme sur [0,1] et on pose X = -\\ln(1-U). Déterminons la loi de X en calculant sa fonction de répartition. Pour tout x \\in \\mathbb{R}, \n\\begin{align*}\nF_X(x) = & \\mathbb{P}(X \\leq x) \\\\\n       = & \\mathbb{P}(-\\ln(1-U) \\leq x) \\\\\n       = & \\mathbb{P}(U \\leq 1-e^{-x}) \\\\\n       = &\n    \\begin{cases}\n        0           & \\text{ si }x &lt; 0\\,,    \\\\\n        1 - e^{-x} & \\text{ si }x \\geq 0\\,,\n    \\end{cases}\n\\end{align*}\n\noù on a utilisé l’égalité \\mathbb{P}(U \\leq t) = t pour tout t \\in [0,1]. Ainsi la variable aléatoire X a la même fonction de répartition qu’une loi exponentielle de paramètre 1. On en conclut que X \\sim \\mathcal{Exp}(1). Notons que l’on peut aussi montrer que -\\ln(X)\\sim\\mathcal{E}(1), sachant que U et 1-U ont la même loi.",
    "crumbs": [
      "Cours",
      "Notations et rappels"
    ]
  },
  {
    "objectID": "Courses/notations.html#fonction-quantile-inverse-généralisée-à-gauche",
    "href": "Courses/notations.html#fonction-quantile-inverse-généralisée-à-gauche",
    "title": "Notations et rappels",
    "section": "Fonction quantile, inverse généralisée à gauche",
    "text": "Fonction quantile, inverse généralisée à gauche\nLa fonction de répartition étant une fonction croissante on peut donner un sens à son inverse généralisée de la manière suivante.\n\nDéfinition 4 (Fonction quantile/ inverse généralisée 🇬🇧: quantile distribution function) \nSoit X une variable aléatoire sur (\\mathbb{R}, \\mathcal{B}(\\mathbb{R})) et F_X sa fonction de répartition. La fonction quantile associée F_X^\\leftarrow: ]0,1[ \\rightarrow \\mathbb{R} est définie par \n  F_X^\\leftarrow(p)=  \\inf\\{ x \\in \\mathbb{R} \\colon F_X(x)\\geq p\\} \\enspace.\n\n\nOn parle parfois aussi d’inverse au sens de Levy pour cette inverse généralisée.\nDans le cas où la fonction de répartition F_X est bijective, alors l’inverse de la fonction de répartition coincide avec la fonction quantile.\nLa médiane est égale à F_X^\\leftarrow(1/2), les premiers et troisièmes quartiles sont égaux à F_X^\\leftarrow(1/4) et F_X^\\leftarrow(3/4). Enfin, les déciles sont les quantiles F_X^\\leftarrow(k/10) pour k=1,\\dots, 9.",
    "crumbs": [
      "Cours",
      "Notations et rappels"
    ]
  },
  {
    "objectID": "Courses/notations.html#visualisation-densité-fonction-de-répartition-quantiles-etc.",
    "href": "Courses/notations.html#visualisation-densité-fonction-de-répartition-quantiles-etc.",
    "title": "Notations et rappels",
    "section": "Visualisation: densité, fonction de répartition, quantiles, etc.",
    "text": "Visualisation: densité, fonction de répartition, quantiles, etc.\n\nCas des variables continues\n\nObservablePython / Shiny\n\n\n\nPlotly = require('plotly.js-dist');\ndists = require( 'https://cdn.jsdelivr.net/gh/stdlib-js/stats-base-dists@umd/browser.js' );\n// see source here: https://github.com/stdlib-js/stats-base-dists/tree/umd\n// continuous case\njstatPDFs = () =&gt; {\n  const distributions = Object.keys(dists);\n  // Get in continuousDistributions the distributions whose pdf, cdf and quantile are defined\n    const continuousDistributions = distributions.filter(name =&gt; dists[name].pdf && dists[name].cdf && dists[name].quantile);\n  return continuousDistributions\n};\noutput = jstatPDFs();\n\nexcludedPDFs = [];\npdfNames_unsorted = output.filter(name =&gt; !excludedPDFs.includes(name));\npdfNames=pdfNames_unsorted.toSorted();\nviewof inputs = Inputs.form([\n      Inputs.range([-10, 10], {value: 0.1, step: 0.001, label: tex`\\mu`}),\n      Inputs.range([0.01, 5], {value: 1.01, step: 0.001, label: tex`\\sigma`}),\n      Inputs.range([0.001, 0.999], {value: 0.75, step: 0.001, label:tex`\\alpha`}),\n    ]);\n\nviewof distrib_name = Inputs.select(pdfNames, {value: \"normal\", label: \"Distribution\"});\n\n\n{\n  const x = d3.range(-5, 5, 0.01);\n  const pdf = x.map(x =&gt; dists[distrib_name].pdf(x, mu, sigma));\n  const cdf = x.map(x =&gt; dists[distrib_name].cdf(x, mu, sigma));\n  const inv = x.map(x =&gt; dists[distrib_name].quantile(x, mu, sigma));\n  const quantile = dists[distrib_name].quantile(alpha, mu, sigma);\n  const filteredX = x.filter(coord =&gt; coord &lt;= quantile);\n  const filteredPdf = pdf.filter((_, i) =&gt; x[i] &lt;= quantile);\n  const filteredCdf = cdf.filter(coord =&gt; coord &lt;= quantile);\n\n\n{\nvar trace1 = {\n      type: \"scatter\",\n      name: 'Quantile',\n      x : cdf,\n      y : x,\n      line: {color: 'black'},\n      xaxis: 'x1',\n      yaxis: 'y3',\n\n\n};\n\nvar trace12 = {\n        x : [alpha, alpha],\n        y : [x[0], quantile],\n        mode: 'lines',\n        line: {\n            dash: 'dash',\n            width: 1,\n            color: ' #428BCA',\n            marks: {\n                size: 0,\n                color: ' #428BCA',\n            }\n\n          },\n      xaxis: 'x1',\n      yaxis: 'y3',\n\n\n};\n\n\n\nvar trace13 = {\n    x: [0, alpha],\n    y: [quantile, quantile],\n    mode: 'scatter',\n    line: {\n        dash: 'dash',\n        width: 1,\n        color: ' #428BCA',\n        marks: {\n            size: 0,\n        }\n    },\n    xaxis: 'x1',\n    yaxis: 'y3'\n}\n\n\nvar trace2 = {\nx: cdf,\ny: cdf,\ntype: 'scatter',\nname: 'identity',\nline: {color: 'black'},\nxaxis: 'x1',\nyaxis: 'y2'\n\n};\n\n\nvar trace21 = {\n        x : [alpha, 1],\n        y : [alpha, alpha],\n        mode: 'lines',\n        line: {\n            dash: 'dash',\n            width: 1,\n            color: ' #428BCA',\n            marks: {\n                size: 0,\n                color: ' #428BCA',\n            }\n\n          },\n      xaxis: 'x1',\n      yaxis: 'y2'\n}\n\nvar trace22 = {\n        x : [alpha, alpha],\n        y : [alpha, 1],\n        mode: 'lines',\n        line: {\n            dash: 'dash',\n            width: 1,\n            color: ' #428BCA',\n            marks: {\n                size: 0,\n                color: ' #428BCA',\n            }\n\n          },\n      xaxis: 'x1',\n      yaxis: 'y2'\n}\n\n\n\nvar trace23 = {\n    x: [alpha],\n    y: [alpha],\n    mode: 'scatter',\n    line: {\n        dash: 'dash',\n        width: 1,\n        color: ' #428BCA',\n        marks: {\n            size: 0,\n        }\n    },\n    xaxis: 'x1',\n    yaxis: 'y2'\n}\n\n\nvar trace31 = {\n      type: \"scatter\",\n      mode: \"lines\",\n      name: 'PDF2',\n      x: x,\n      y: cdf,\n      line: {color: 'black'},\n      xaxis: 'x2',\n      yaxis: 'y2',\n};\n\nvar trace32 = {\n        x : filteredX,\n        y : filteredX.map(x =&gt; alpha),\n        mode: 'lines',\n        line: {\n            dash: 'dash',\n            width: 1,\n            color: ' #428BCA',\n            marks: {\n                size: 0,\n                color: ' #428BCA',\n            }\n          },\n      xaxis: 'x2',\n      yaxis: 'y2'\n}\n\nvar trace33 = {\n        x : [quantile, quantile],\n        y : [0, alpha],\n        mode: 'lines',\n        line: {\n            dash: 'dash',\n            width: 1,\n            color: ' #428BCA',\n            marks: {\n                size: 0,\n                color: ' #428BCA',\n            }\n          },\n\n      xaxis: 'x2',\n      yaxis: 'y2'\n}\n\nvar trace34 = {\n    x: [quantile],\n    y: [alpha],\n    mode: 'scatter',\n    line: {\n        dash: 'dash',\n        width: 1,\n        color: ' #428BCA',\n        marks: {\n            size: 0,\n        }\n    },\n    xaxis: 'x2',\n    yaxis: 'y2'\n}\n\n\nvar trace41 = {\n\n  type: \"scatter\",\n  name: 'Quantile2',\n  fill: 'tozeroy',\n  x : filteredX,\n  y : filteredPdf,\n  opacity: 0.9,\n  line: {color: ' #428BCA'},\n  xaxis: 'x2',\n  yaxis: 'y1'\n\n};\n\n\nvar trace42 = {\n\n  type: \"scatter\",\n  mode: \"lines\",\n  name: 'PDF2',\n  x: x,\n  y: pdf,\n  line: {color: 'black'},\n  xaxis: 'x2',\n  yaxis: 'y1'\n\n};\n\nvar data = [\n  trace1,\n  trace12, trace13,\n  trace2, trace21, trace22, trace23,\n  trace31, trace32, trace33, trace34,\n  trace41, trace42];\n\n\nvar layout = {\n\n  title: 'Distribution et quantile',\n  xaxis: {\n    domain: [0, 0.32],\n    anchor: 'y1'\n  },\n  yaxis: {\n    domain: [0, 0.24],\n    anchor: 'x1'\n\n  },\n  xaxis2: {\n    domain: [0.35, 1],\n    anchor: 'y'\n  },\n\n  yaxis2: {\n    domain: [0.26, 0.49],\n    anchor: 'x1'\n  },\n\n  yaxis3: {\n    domain: [0.5, 1],\n    anchor: 'x1'\n  },\n\n// legend offset\n  showlegend: false,\n  height: 680,\n  annotations: [\n\n    {\n      x: 1/4,\n      y: quantile - (x[0]-x.slice(-1))/20,\n      xref: 'x1',\n      yref: 'y3',\n      text: 'q= ' + quantile.toFixed(2),\n      font: {\n        size: 12,\n        color: '#428BCA',\n\n      },\n      showarrow: false,\n      arrowhead: 0,\n      ax: 25,\n      ay: -10,\n\n    },\n    {\n      x: 1/4,\n      y: alpha,\n      xref: 'x1',\n      yref: 'y2',\n      text: 'alpha= ' + alpha.toFixed(2),\n      font: {\n        size: 12,\n        color: '#428BCA',\n\n      },\n      showarrow: false,\n      arrowhead: 0,\n      ax: 25,\n      ay: -10,\n\n    },\n    {\n        text: \"Fonction quantile\",\n      font: {\n      size: 15,\n      color: 'black',\n    },\n    showarrow: false,\n    align: 'center',\n    x: -0.01,\n    y: 1.05,\n    xref: 'paper',\n    yref: 'paper',\n    },\n    {\n      text: \"Fonction de répartition\",\n      font: {\n      size: 15,\n      color: 'black',\n            },\n    showarrow: false,\n    align: 'center',\n    x: 0.65,\n    y: 0.52,\n    xref: 'paper',\n    yref: 'paper',\n    },\n    {\n      text: \"Fonction de densité\",\n      font: {\n      size: 15,\n      color: 'black',\n            },\n    showarrow: false,\n    align: 'center',\n    x: 0.65,\n    y: 0.225,\n    xref: 'paper',\n    yref: 'paper',\n    },\n  ]\n\n};\n// XXX: TODO: put the xticks labels on the middle plot for x and on the right plot for y\n    var config = {responsive: true}\n    const div = DOM.element('div');\n    Plotly.newPlot(div, data, layout, config);\n    return div;\n  }\n\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmu = inputs[0];\nsigma = inputs[1];\nalpha = inputs[2]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#| standalone: true\n#| viewerHeight: 830\nimport numpy as np\nfrom scipy import stats\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nfrom shiny import ui, render, App\nfrom shinywidgets import output_widget, render_widget\n\n\ndef keep_no_param_distribution():\n    distributions = stats._continuous_distns._distn_names\n    distributions_0 = []\n    for _, name in enumerate(distributions):\n        dist = getattr(stats, name)\n        if not dist.shapes or len(dist.shapes) == 0:\n            distributions_0.append(name)\n    distributions_0_val = [\n        getattr(stats.distributions, string) for string in distributions_0\n    ]\n    distributions_0_dict = dict(zip(distributions_0, distributions_0_val))\n    return distributions_0_dict\n\n\ndistributions_0_dict = keep_no_param_distribution()\n\nmu = 0\nsigma = 1\n\napp_ui = ui.page_fluid(\n    ui.div(\n        ui.input_slider(\"alpha\", \"Quantile\", 0.01, 0.99, value=0.5, step=0.01),\n        ui.input_slider(\"xrange\", \"x-range\", -10, 10, value=(-5, 5), step=0.2),\n        ui.input_select(\n            \"distrib\",\n            \"Distribution\",\n            list(distributions_0_dict.keys()),\n            selected='norm'\n        ),\n        class_=\"d-flex gap-3\",\n    ),\n    output_widget(\"my_widget\"),\n)\n\n\ndef server(input, output, session):\n    @output\n    @render_widget\n    def my_widget():\n        fig = make_subplots(\n            rows=3,\n            cols=2,\n            vertical_spacing=0.1,\n            horizontal_spacing=0.15,\n            subplot_titles=(\n                \"Fonction quantile\",\n                \"\",\n                \"\",\n                \"Fonction de répartition\",\n                \"\",\n                \"Densité et quantile\",\n            ),\n            column_widths=[0.2, 0.5],\n            row_heights=[0.35, 0.17, 0.17],\n        )\n\n        alpha = input.alpha()\n        distribution = distributions_0_dict[input.distrib()]\n        x = np.linspace(input.xrange()[0], input.xrange()[1], num=400)\n        cdf_data = distribution.cdf(x, loc=mu, scale=sigma)\n        pdf_data = distribution.pdf(x, loc=mu, scale=sigma)\n        q_alpha = distribution.ppf(alpha, loc=mu, scale=sigma)\n\n        fig.update_layout(autosize=True, height=700)\n\n        # Quantile plot\n        fig.add_trace(\n            go.Scatter(\n                x=cdf_data, y=x, mode=\"lines\", marker={\"color\": \"black\"}\n            ),\n            row=1,\n            col=1,\n        )\n        # Diagonal\n        fig.add_trace(\n            go.Scatter(\n                x=cdf_data, y=cdf_data, mode=\"lines\", marker={\"color\": \"black\"}\n            ),\n            row=2,\n            col=1,\n        )\n        # Cdf part\n        fig.add_trace(\n            go.Scatter(\n                x=x, y=cdf_data, mode=\"lines\", marker={\"color\": \"black\"}\n            ),\n            row=2,\n            col=2,\n        )\n        # pdf part\n        fig.add_scatter(\n            x=x[x &lt; q_alpha],\n            y=pdf_data[x &lt; q_alpha],\n            fill=\"tozeroy\",\n            mode=\"none\",\n            fillcolor=\"rgb(66, 139, 202)\",\n            row=3,\n            col=2,\n        )\n\n        fig.add_trace(\n            go.Scatter(\n                x=x, y=pdf_data, mode=\"lines\", marker={\"color\": \"black\"}\n            ),\n            row=3,\n            col=2,\n        )\n\n        # Dots\n        fig.add_trace(\n            go.Scatter(\n                x=[alpha],\n                y=[q_alpha],\n                mode=\"markers\",\n                marker={\"color\": \"rgb(66, 139, 202)\"},\n                marker_symbol=\"x\",\n                marker_size=8,\n            ),\n            row=1,\n            col=1,\n        )\n\n        fig.add_trace(\n            go.Scatter(\n                x=[alpha],\n                y=[alpha],\n                mode=\"markers\",\n                marker={\"color\": \"rgb(66, 139, 202)\"},\n                marker_symbol=\"x\",\n                marker_size=8,\n            ),\n            row=2,\n            col=1,\n        )\n\n        fig.add_trace(\n            go.Scatter(\n                x=[q_alpha],\n                y=[alpha],\n                mode=\"markers\",\n                marker={\"color\": \"rgb(66, 139, 202)\"},\n                marker_symbol=\"x\",\n                marker_size=8,\n            ),\n            row=2,\n            col=2,\n        )\n\n        # Lines\n        fig.add_trace(\n            go.Scatter(\n                x=[alpha, alpha],\n                y=[x[0], q_alpha],\n                mode=\"lines\",\n                line=dict(dash=\"dash\", color=\"rgb(66, 139, 202)\")\n            ),\n            row=1,\n            col=1\n        )\n\n        fig.add_trace(\n            go.Scatter(\n                x=[alpha, alpha],\n                y=[alpha, 1.],\n                mode=\"lines\",\n                line=dict(dash=\"dash\", color=\"rgb(66, 139, 202)\")\n            ),\n            row=2,\n            col=1\n        )\n        fig.add_trace(\n            go.Scatter(\n                x=[alpha, 1],\n                y=[alpha, alpha],\n                mode=\"lines\",\n                line=dict(dash=\"dash\", color=\"rgb(66, 139, 202)\")\n            ),\n            row=2,\n            col=1\n        )\n        fig.add_trace(\n            go.Scatter(\n                x=[x[0], q_alpha],\n                y=[alpha, alpha],\n                mode=\"lines\",\n                line=dict(dash=\"dash\", color=\"rgb(66, 139, 202)\")\n            ),\n            row=2,\n            col=2\n        )\n        fig.add_trace(\n            go.Scatter(\n                x=[x[0], q_alpha],\n                y=[x[0], 0],\n                mode=\"lines\",\n                line=dict(dash=\"dash\", color=\"rgb(66, 139, 202)\")\n            ),\n            row=2,\n            col=2\n        )\n        # Axes ranges\n        fig.update_xaxes(range=[0, 1.], row=1, col=1)\n        fig.update_yaxes(matches=\"x6\", row=1, col=1)\n\n        fig.update_yaxes(range=[0, 1.], row=2, col=1)\n        fig.update_xaxes(matches=\"x1\", row=2, col=1)\n\n        fig.update_yaxes(rangemode=\"tozero\", row=3, col=2)\n        fig.update_xaxes(range=[x[0], x[-1]], row=3, col=2)\n\n        fig.update_xaxes(matches=\"x6\", row=2, col=2)\n        fig.update_yaxes(matches=\"y3\", row=2, col=2)\n\n        # Add dropdown\n        fig.update_layout(\n            showlegend=False,\n            template=\"simple_white\",\n        )\n        return fig\n\n\napp = App(app_ui, server)\n\n\n\n\n\n\nCas des variables discrètes\n\nObservablePython / Shiny\n\n\n\ndiscretePDFs = () =&gt; {\n  const distributions = Object.keys(dists);\n  // Get in continuousDistributions the distributions whose pdf, cdf and quantile are defined\n    const continuousDistributions = distributions.filter(name =&gt; dists[name].pmf && dists[name].cdf && dists[name].quantile);\n  return continuousDistributions\n};\noutput_discr = discretePDFs();\n\npmfNames_unsorted = output_discr.filter(name =&gt; !excludedPDFs.includes(name));\npmfNames=pmfNames_unsorted.toSorted();\nviewof inputs_disc = Inputs.form([\n      Inputs.range([-10, 10], {value: 0.1, step: 0.001, label: tex`\\mu `}),\n      Inputs.range([0.01, 5], {value: 1.01, step: 0.001, label: tex`\\sigma `}),\n      Inputs.range([0.001, 0.999], {value: 0.75, step: 0.001, label:tex`\\alpha`}),\n    ]);\n\nviewof distrib_name_discr = Inputs.select(pmfNames, {value: \"normal\", label: \"Distribution\"});\n\n\n{\n  const z = d3.range(-5, 5, 0.01);\n  const pmf = z.map(z =&gt; dists[distrib_name_discr].pmf(z, mu_disc, sigma_disc));\n  const cdf = z.map(z =&gt; dists[distrib_name_discr].cdf(z, mu_disc, sigma_disc));\n  const inv = z.map(z =&gt; dists[distrib_name_discr].quantile(z, mu_disc, sigma_disc));\n  const quantile = dists[distrib_name_discr].quantile(alpha_disc, mu_disc, sigma_disc);\n  const filteredX = z.filter(coord =&gt; coord &lt;= quantile);\n  const filteredPmf = pmf.filter((_, i) =&gt; z[i] &lt;= quantile);\n  const filteredCdf = cdf.filter(coord =&gt; coord &lt;= quantile);\n\n\n{\nvar trace1 = {\n      type: \"scatter\",\n      name: 'Quantile',\n      x : cdf,\n      y : z,\n      line: {color: 'black'},\n      xaxis: 'x1',\n      yaxis: 'y3',\n\n\n};\n\nvar trace12 = {\n        x : [alpha_disc, alpha_disc],\n        y : [z[0], quantile],\n        mode: 'lines',\n        line: {\n            dash: 'dash',\n            width: 1,\n            color: ' #428BCA',\n            marks: {\n                size: 0,\n                color: ' #428BCA',\n            }\n\n          },\n      xaxis: 'x1',\n      yaxis: 'y3',\n\n\n};\n\n\n\nvar trace13 = {\n    x: [0, alpha_disc],\n    y: [quantile, quantile],\n    mode: 'scatter',\n    line: {\n        dash: 'dash',\n        width: 1,\n        color: ' #428BCA',\n        marks: {\n            size: 0,\n        }\n    },\n    xaxis: 'x1',\n    yaxis: 'y3'\n}\n\n\nvar trace2 = {\nx: cdf,\ny: cdf,\ntype: 'scatter',\nname: 'identity',\nline: {color: 'black'},\nxaxis: 'x1',\nyaxis: 'y2'\n\n};\n\n\nvar trace21 = {\n        x : [alpha_disc, 1],\n        y : [alpha_disc, alpha_disc],\n        mode: 'lines',\n        line: {\n            dash: 'dash',\n            width: 1,\n            color: ' #428BCA',\n            marks: {\n                size: 0,\n                color: ' #428BCA',\n            }\n\n          },\n      xaxis: 'x1',\n      yaxis: 'y2'\n}\n\nvar trace22 = {\n        x : [alpha_disc, alpha_disc],\n        y : [alpha_disc, 1],\n        mode: 'lines',\n        line: {\n            dash: 'dash',\n            width: 1,\n            color: ' #428BCA',\n            marks: {\n                size: 0,\n                color: ' #428BCA',\n            }\n\n          },\n      xaxis: 'x1',\n      yaxis: 'y2'\n}\n\nvar trace23 = {\n    x: [alpha_disc],\n    y: [alpha_disc],\n    mode: 'scatter',\n    line: {\n        dash: 'dash',\n        width: 1,\n        color: ' #428BCA',\n        marks: {\n            size: 0,\n        }\n    },\n    xaxis: 'x1',\n    yaxis: 'y2'\n}\n\n\nvar trace31 = {\n      type: \"scatter\",\n      mode: \"lines\",\n      name: 'PDF2',\n      x: z,\n      y: cdf,\n      line: {color: 'black'},\n      xaxis: 'x2',\n      yaxis: 'y2',\n};\n\nvar trace32 = {\n        x : filteredX,\n        y : filteredX.map(z =&gt; alpha_disc),\n        mode: 'lines',\n        line: {\n            dash: 'dash',\n            width: 1,\n            color: ' #428BCA',\n            marks: {\n                size: 0,\n                color: ' #428BCA',\n            }\n          },\n      xaxis: 'x2',\n      yaxis: 'y2'\n}\n\nvar trace33 = {\n        x : [quantile, quantile],\n        y : [0, alpha_disc],\n        mode: 'lines',\n        line: {\n            dash: 'dash',\n            width: 1,\n            color: ' #428BCA',\n            marks: {\n                size: 0,\n                color: ' #428BCA',\n            }\n          },\n\n      xaxis: 'x2',\n      yaxis: 'y2'\n}\n\nvar trace34 = {\n    x: [quantile],\n    y: [alpha_disc],\n    mode: 'scatter',\n    line: {\n        dash: 'dash',\n        width: 1,\n        color: ' #428BCA',\n        marks: {\n            size: 0,\n        }\n    },\n    xaxis: 'x2',\n    yaxis: 'y2'\n}\n\n\nvar trace41 = {\n\n  type: \"scatter\",\n  name: 'Quantile2',\n  fill: 'tozeroy',\n  x : filteredX,\n  y : filteredPmf,\n  opacity: 0.9,\n  line: {color: ' #428BCA'},\n  xaxis: 'x2',\n  yaxis: 'y1'\n\n};\n\n\nvar trace42 = {\n\n  type: \"scatter\",\n  mode: \"lines\",\n  name: 'PDF2',\n  x: z,\n  y: pmf,\n  line: {color: 'black'},\n  xaxis: 'x2',\n  yaxis: 'y1'\n\n};\n\nvar data = [\n  trace1,\n  trace12, trace13,\n  trace2, trace21, trace22, trace23,\n  trace31, trace32, trace33, trace34,\n  trace41, trace42];\n\n\nvar layout = {\n\n  title: 'Distribution et quantile',\n  xaxis: {\n    domain: [0, 0.32],\n    anchor: 'y1'\n  },\n  yaxis: {\n    domain: [0, 0.24],\n    anchor: 'x1'\n\n  },\n  xaxis2: {\n    domain: [0.35, 1],\n    anchor: 'y'\n  },\n\n  yaxis2: {\n    domain: [0.26, 0.49],\n    anchor: 'x1'\n  },\n\n\n\n  yaxis3: {\n    domain: [0.5, 1],\n    anchor: 'x1'\n\n  },\n\n  showlegend: false,\n  height: 680,\n\n\n  annotations: [\n\n    {\n      x: 1/4,\n      y: quantile - (z[0]-z.slice(-1))/20,\n      xref: 'x1',\n      yref: 'y3',\n      text: 'q=' + quantile.toFixed(2),\n      font: {\n        size: 12,\n        color: '#428BCA',\n\n      },\n      showarrow: false,\n      arrowhead: 0,\n      ax: 25,\n      ay: -10,\n\n    },\n    {\n      x: 1/4,\n      y: alpha_disc,\n      xref: 'x1',\n      yref: 'y2',\n      text: 'alpha=' + alpha_disc.toFixed(2),\n      font: {\n        size: 12,\n        color: '#428BCA',\n\n      },\n      showarrow: false,\n      arrowhead: 0,\n      ax: 25,\n      ay: -10,\n\n    },\n    {\n        text: \"Fonction quantile\",\n      font: {\n      size: 15,\n      color: 'black',\n    },\n    showarrow: false,\n    align: 'center',\n    x: -0.01,\n    y: 1.05,\n    xref: 'paper',\n    yref: 'paper',\n    },\n    {\n      text: \"Fonction de répartition\",\n      font: {\n      size: 15,\n      color: 'black',\n            },\n    showarrow: false,\n    align: 'center',\n    x: 0.65,\n    y: 0.52,\n    xref: 'paper',\n    yref: 'paper',\n    },\n    {\n      text: \"Fonction de densité\",\n      font: {\n      size: 15,\n      color: 'black',\n            },\n    showarrow: false,\n    align: 'center',\n    x: 0.65,\n    y: 0.225,\n    xref: 'paper',\n    yref: 'paper',\n    },\n  ]\n\n};\n// XXX: TODO: put the xticks labels on the middle plot for x and on the right plot for y\n\n    var config = {responsive: true}\n    const div = DOM.element('div');\n    Plotly.newPlot(div, data, layout, config);\n    return div;\n  }\n\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmu_disc = inputs_disc[0];\nsigma_disc = inputs_disc[1];\nalpha_disc = inputs_disc[2];\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#| standalone: true\n#| viewerHeight: 830\nimport numpy as np\nfrom scipy import stats\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nfrom shiny import ui, render, App\nfrom shinywidgets import output_widget, render_widget\n\n\ndef keep_no_param_distribution_disc():\n    distributions = stats._discrete_distns._distn_names\n    distributions_0 = [name for name in distributions if not getattr(stats, name).shapes or len(getattr(stats, name).shapes) in [1, 2]]\n    distributions_0_val = [getattr(stats.distributions, string) for string in distributions_0]\n    distributions_0_dict = dict(zip(distributions_0, distributions_0_val))\n    return distributions_0_dict\n\ndef cdf_tool(x, dtype='int64'):\n    y = np.zeros(2*(len(x)), dtype=dtype)\n    y[::2]=x\n    y[1::2]=x\n    return y[1::], y[:-1], y\n\ndef pmf_tool(x, dtype='int64'):\n    y = np.zeros(2*(len(x)), dtype=dtype)\n    y[::2]=x\n    return y[1::], y[:-1], y\n\ndef insert_nones(my_list):\n    for i, val in enumerate(my_list):\n        if i % 3 == 2:\n            my_list.insert(i, None)\n    return my_list\n\ndistributions_0_dict = keep_no_param_distribution_disc()\n\napp_ui = ui.page_fluid(\n    ui.div(\n        ui.input_slider(\"alpha\", \"Quantile\", 0.01, 0.99, value=0.5, step=0.01),\n        ui.input_slider(\"xrange\", \"x-range\", -10, 10, value=(-5.5, 5.5), step=0.2),\n        ui.input_select(\n            \"distrib\",\n            \"Distribution\",\n            list(distributions_0_dict.keys()),\n            selected='poisson'\n        ),\n        class_=\"d-flex gap-3\",\n    ),\n    output_widget(\"my_widget\"),\n)\n\n\ndef server(input, output, session):\n    @output\n    @render_widget\n    def my_widget():\n        fig = make_subplots(\n            rows=3,\n            cols=2,\n            vertical_spacing=0.1,\n            horizontal_spacing=0.15,\n            subplot_titles=(\n                \"Fonction quantile\",\n                \"\",\n                \"\",\n                \"Fonction de répartition\",\n                \"\",\n                \"Fonction de masse et quantile\",\n            ),\n            column_widths=[0.2, 0.5],\n            row_heights=[0.35, 0.17, 0.17],\n        )\n\n\n        alpha = input.alpha()\n        # alpha=0.5\n\n        mu = 0.5  # Param needed for some distribution\n        if input.distrib()=='zipf':\n            mu = 2\n        distribution = distributions_0_dict[input.distrib()]\n        # distribution=distributions_0_dict['poisson']\n        x = np.arange(np.floor(input.xrange()[0]), np.ceil(input.xrange()[1]))\n        # x = np.arange(np.floor(-5.5), np.ceil(5.5))\n\n        cdf_data = distribution.cdf(x, mu)\n        pmf_data = distribution.pmf(x, mu)\n        q_alpha = distribution.ppf(alpha, mu)\n        support = pmf_data.nonzero()[0]\n        fig.update_layout(autosize=True, height=700)\n\n        # Quantile plot\n        new_x, new_y, new_z = cdf_tool(support)\n        _, _, new_pmf = pmf_tool(support)\n\n        fig.add_trace(\n            go.Scatter(\n                x=insert_nones(list(np.append(cdf_data[new_y[::-1]], distribution.cdf(x[0], mu)))),\n                y=insert_nones(list(np.append(x[new_x[::-1]], x[new_x[0]]))),\n                mode=\"lines\",\n                line=dict(color=\"black\")\n            ),\n            row=1,\n            col=1,\n        )\n        fig.add_trace(\n             go.Scatter(\n                x=cdf_data[support], y=x[support],\n                mode=\"markers\", marker={\"color\": \"black\"}\n            ),\n            row=1,\n            col=1,\n        )\n        # Diagonal\n        fig.add_trace(\n            go.Scatter(\n                x=cdf_data, y=cdf_data, mode=\"lines\", marker={\"color\": \"black\"}\n            ),\n            row=2,\n            col=1,\n        )\n        # Cdf part\n        fig.add_trace(\n            go.Scatter(\n                x=x[support], y=cdf_data[support],\n                mode=\"markers\", marker={\"color\": \"black\"}\n            ),\n            row=2,\n            col=2,\n        )\n        fig.add_trace(\n            go.Scatter(\n                x=insert_nones(list(np.append(np.insert(x[new_x], 0, [x[0], x[new_x[0]]]),x[-1]))),\n                y=insert_nones(list(np.append(np.insert(cdf_data[new_y], 0, [0,0]), cdf_data[-1]))),\n                mode=\"lines\",\n                line=dict(color=\"black\")\n            ),\n            row=2,\n            col=2\n        )\n\n        fig.add_trace(\n            go.Scatter(\n                x=x, y=pmf_data, mode=\"markers\", marker={\"color\": \"black\"}\n            ),\n            row=3,\n            col=2,\n        )\n        fig.add_trace(\n            go.Scatter(\n                x=x, y=pmf_data, mode=\"markers\", marker={\"color\": \"black\"}\n            ),\n            row=3,\n            col=2,\n        )\n        x_bar = insert_nones(list(x[new_z]))\n        y_bar = insert_nones(list(pmf_data[new_pmf]))\n        fig.add_trace(\n            go.Scatter(\n                x=x_bar,\n                y=y_bar,\n                mode=\"lines\",\n                line=dict(color=\"black\")\n            ),\n            row=3,\n            col=2\n        )\n        _,_, devil_x = cdf_tool(x[x&lt;=q_alpha])\n        _,_, devil_y = cdf_tool(pmf_data[x&lt;q_alpha], dtype='float64')\n\n        x_bar_blue = insert_nones(list(devil_x))\n        y_bar_blue = np.array(insert_nones(list(devil_y)))\n        y_bar_blue[::-3]=0.\n        y_bar_blue = list(y_bar_blue)\n        fig.add_trace(\n            go.Scatter(\n                x=x_bar_blue,\n                y=y_bar_blue,\n                mode=\"lines\",\n                line=dict(color=\"rgb(66, 139, 202)\")\n            ),\n            row=3,\n            col=2\n        )\n        # pdf part\n        fig.add_scatter(\n            x=x[x &lt;= q_alpha],\n            y=pmf_data[x &lt;= q_alpha],\n            mode=\"markers\",\n            marker={\"color\":\"rgb(66, 139, 202)\"},\n            row=3,\n            col=2,\n        )\n\n        # Dots\n        fig.add_trace(\n            go.Scatter(\n                x=[alpha],\n                y=[q_alpha],\n                mode=\"markers\",\n                marker={\"color\": \"rgb(66, 139, 202)\"},\n                marker_symbol=\"x\",\n                marker_size=8,\n            ),\n            row=1,\n            col=1,\n        )\n\n        fig.add_trace(\n            go.Scatter(\n                x=[alpha],\n                y=[alpha],\n                mode=\"markers\",\n                marker={\"color\": \"rgb(66, 139, 202)\"},\n                marker_symbol=\"x\",\n                marker_size=8,\n            ),\n            row=2,\n            col=1,\n        )\n\n        fig.add_trace(\n            go.Scatter(\n                x=[q_alpha],\n                y=[alpha],\n                mode=\"markers\",\n                marker={\"color\": \"rgb(66, 139, 202)\"},\n                marker_symbol=\"x\",\n                marker_size=8,\n            ),\n            row=2,\n            col=2,\n        )\n\n        # Lines\n        fig.add_trace(\n            go.Scatter(\n                x=[alpha, alpha],\n                y=[x[0], q_alpha],\n                mode=\"lines\",\n                line=dict(dash=\"dash\", color=\"rgb(66, 139, 202)\")\n            ),\n            row=1,\n            col=1\n        )\n\n        fig.add_trace(\n            go.Scatter(\n                x=[alpha, alpha],\n                y=[alpha, 1.],\n                mode=\"lines\",\n                line=dict(dash=\"dash\", color=\"rgb(66, 139, 202)\")\n            ),\n            row=2,\n            col=1\n        )\n        fig.add_trace(\n            go.Scatter(\n                x=[alpha, 1],\n                y=[alpha, alpha],\n                mode=\"lines\",\n                line=dict(dash=\"dash\", color=\"rgb(66, 139, 202)\")\n            ),\n            row=2,\n            col=1\n        )\n        fig.add_trace(\n            go.Scatter(\n                x=[x[0], q_alpha],\n                y=[alpha, alpha],\n                mode=\"lines\",\n                line=dict(dash=\"dash\", color=\"rgb(66, 139, 202)\")\n            ),\n            row=2,\n            col=2\n        )\n        fig.add_trace(\n            go.Scatter(\n                x=[x[0], q_alpha],\n                y=[x[0], 0],\n                mode=\"lines\",\n                line=dict(dash=\"dash\", color=\"rgb(66, 139, 202)\")\n            ),\n            row=2,\n            col=2\n        )\n        # Axes ranges\n        fig.update_xaxes(range=[0, 1.05], row=1, col=1)\n        fig.update_yaxes(matches=\"x6\", row=1, col=1)\n\n        fig.update_yaxes(range=[0, 1.05], row=2, col=1)\n        fig.update_xaxes(matches=\"x1\", row=2, col=1)\n\n        fig.update_yaxes(rangemode=\"tozero\", row=3, col=2)\n        fig.update_xaxes(range=[x[0], x[-1]], row=3, col=2)\n\n        fig.update_xaxes(matches=\"x6\", row=2, col=2)\n        fig.update_yaxes(matches=\"y3\", row=2, col=2)\n\n        # Add dropdown\n        fig.update_layout(\n            showlegend=False,\n            template=\"simple_white\",\n        )\n        return fig\n\n\napp = App(app_ui, server)",
    "crumbs": [
      "Cours",
      "Notations et rappels"
    ]
  },
  {
    "objectID": "Courses/loi_normale_multi.html",
    "href": "Courses/loi_normale_multi.html",
    "title": "Loi normale: cas multivarié",
    "section": "",
    "text": "Loi normale multivariée: \\mu \\in \\mathbb{R}^p et \\Sigma \\in \\mathbb{R}^{p \\times p} (symétrique et définie positive). \nX \\sim \\mathcal{N}(\\mu,\\Sigma), \\quad \\forall x \\in \\mathbb{R}^p\n\nDensité de probabilité : \n\\phi(x) = \\frac{1}{ \\sqrt{(2\\pi)^p |\\Sigma|}}  \\exp\\Big( -\\tfrac{1}{2}(x-\\mu)^\\top\\Sigma^{-1}(x - \\mu)   \\Big)\n En dimension p=2, la matrice de covariance \\Sigma peut toujours s’écrire comme suit, et la visualisation suivante montre l’impact des différents paramètres sur la densité de probabilité. \n\\Sigma =\n\\begin{pmatrix}\\cos(\\theta) & - \\sin(\\theta)\\\\  \\sin(\\theta)& \\cos(\\theta)\n\\end{pmatrix} \\cdot\n\\begin{pmatrix}\\sigma_1 & 0\\\\ 0 & \\sigma_2\n\\end{pmatrix}\\cdot\n\\begin{pmatrix}\n\\cos(\\theta) &\\sin(\\theta)\\\\  -\\sin(\\theta)& \\cos(\\theta)\\end{pmatrix}\n\nPlotly = require('plotly.js-dist');\ndists = require( 'https://cdn.jsdelivr.net/gh/stdlib-js/stats-base-dists@umd/browser.js' );\njstat = require('jstat');\nmath = require(\"mathjs\");\n// see source here: https://github.com/stdlib-js/stats-base-dists/tree/umd\n// continuous case\n\nviewof inputs = Inputs.form([\n  Inputs.range([-1, 1], {label: tex`\\mu_1`, step: 0.1}, {value: 0}),\n  Inputs.range([-1, 1], {label: tex`\\mu_2`, step: 0.1}, {value: 0}),\n  Inputs.range([0.1, 2], {label: tex`\\sigma_1`, step: 0.1, value: 1}),\n  Inputs.range([0.1, 2], {label: tex`\\sigma_2`, step: 0.1, value: 1}),\n  Inputs.range([0, 6.29], {label: tex`\\theta`, step: 0.01, value: 0}),\n  Inputs.range([1, 1000], {label: tex`n`, step: 1}),\n  Inputs.button(\"Re-Tirage\")\n])\n\n\nmu1 = inputs[0];\nmu2 = inputs[1];\nsigma1 = inputs[2];\nsigma2 = inputs[3];\ntheta = inputs[4];\nn_samples = inputs[5];\n\n\nfunction create_sigma(theta, sigma1, sigma2){\n  const mat_rot = math.matrix([[math.cos(theta), -math.sin(theta)], [math.sin(theta), math.cos(theta)]]);\n  const mat_sigma = math.matrix([[sigma1**2, 0], [0, sigma2**2]]);\n  return math.multiply(mat_rot, math.multiply(mat_sigma, math.transpose(mat_rot)));\n}\n\n\nfunction mvnpdf(x, mu, Sigma){\n  const p = 2;\n    return (2*math.pi)**(-p/2)*math.det(Sigma)**(-0.5)*\n      math.exp(-0.5*math.multiply( math.multiply( math.transpose(math.subtract(x,mu)), math.inv(Sigma)), math.subtract(x,mu)));\n}\n\n{\n\n\nfunction normal_rng(mu, Sigma, n=100) {\n    // Compute the Cholesky decomposition of Sigma\n    var cholesky = jstat.cholesky(Sigma);\n\n    // Generate the samples\n    var samples = Array.from({length: n}, () =&gt; {\n        var x = jstat.normal.sample(0, 1);\n        var y = jstat.normal.sample(0, 1);\n        // Transform the standard normal random variables using the Cholesky decomposition\n        var transformedX = mu[0] + cholesky[0][0] * x + cholesky[0][1] * y;\n        var transformedY = mu[1] + cholesky[1][0] * x + cholesky[1][1] * y;\n        return [transformedX, transformedY];\n    });   \n\n    return samples;\n}\nvar Sigma = create_sigma(theta, sigma1, sigma2).toArray();\nvar mu = [mu1, mu2];\n\nvar samples = normal_rng(mu, Sigma, 1000);\nvar npoints=100, mini = -5, maxi=5, x = new Array(npoints), y = new Array(npoints), z = new Array(npoints), i, j;\n\n//  Densité:\nfor(var i = 0; i &lt; npoints; i++) {\n    x[i] = mini + i * (maxi - mini) / (npoints - 1);\n    y[i] = mini + i * (maxi - mini) / (npoints - 1);\n    z[i] = new Array(npoints);\n    }\n\nfor(var i = 0; i &lt; npoints; i++) {\n    for(j = 0; j &lt; npoints; j++) {\n\n        z[j][i] = mvnpdf([x[i], x[j]], mu, Sigma);\n    }\n\n}\n  \n\n{\n\nvar trace1 = {\n        x: samples.slice(0, n_samples).map(sample =&gt; sample[0]),\n        y: samples.slice(0, n_samples).map(sample =&gt; sample[1]),\n        mode: 'markers',\n        type: 'scatter',\n        marker: {\n            color: 'rgba(0,0,0,0.5)',\n            size: 5,\n        },\n\n        xaxis: 'x2',\n}\n\nvar trace22 = {\n        x: x,\n        y: y,\n        z: z,\n        type: 'surface',\n        colorscale: 'Oranges',\n        showscale: false,\n        color: {\n            legend: false,\n            label: \"pdf\",\n        },\n\n}\n\nvar trace21 = {\n        x: x,\n        y: y,\n        z: z,\n        type: 'contour',\n        colorscale: 'Oranges',\n        color: {\n            legend: true,\n            label: \"pdf\",\n        },\n        blur: 4,\n        xlim: [-5, 5],\n        ylim: [-5, 5],\n        xaxis: 'x2',\n}\n\n\nvar data = [\n  trace21,\n  trace22,\n  trace1,\n  ];\n\n\n  var layout = {\n       yaxis2: {\n          range: [-5, 5],\n          autorange: false,\n        },\n       xaxis2: {\n          range: [-5, 5],\n          autorange: false,\n        },\n\n      scene: {\n          camera: {\n              eye: {\n                  x: 0.5,\n                  y: 1.2,\n                  z: 1.5,\n              }\n          }\n      },\n    grid: {\n      rows: 1,\n      columns: 2,\n      subplots: [['xy','x2y']],\n    },\n    showlegend: false,\n\n  };\n\n    var config = {responsive: true}\n    const div = DOM.element('div');\n    Plotly.newPlot(div, data, layout, config);\n    return div;\n  }\n\n}",
    "crumbs": [
      "Cours",
      "Loi normale: cas multivarié"
    ]
  },
  {
    "objectID": "Courses/loi_normale_multi.html#introduction",
    "href": "Courses/loi_normale_multi.html#introduction",
    "title": "Loi normale: cas multivarié",
    "section": "",
    "text": "Loi normale multivariée: \\mu \\in \\mathbb{R}^p et \\Sigma \\in \\mathbb{R}^{p \\times p} (symétrique et définie positive). \nX \\sim \\mathcal{N}(\\mu,\\Sigma), \\quad \\forall x \\in \\mathbb{R}^p\n\nDensité de probabilité : \n\\phi(x) = \\frac{1}{ \\sqrt{(2\\pi)^p |\\Sigma|}}  \\exp\\Big( -\\tfrac{1}{2}(x-\\mu)^\\top\\Sigma^{-1}(x - \\mu)   \\Big)\n En dimension p=2, la matrice de covariance \\Sigma peut toujours s’écrire comme suit, et la visualisation suivante montre l’impact des différents paramètres sur la densité de probabilité. \n\\Sigma =\n\\begin{pmatrix}\\cos(\\theta) & - \\sin(\\theta)\\\\  \\sin(\\theta)& \\cos(\\theta)\n\\end{pmatrix} \\cdot\n\\begin{pmatrix}\\sigma_1 & 0\\\\ 0 & \\sigma_2\n\\end{pmatrix}\\cdot\n\\begin{pmatrix}\n\\cos(\\theta) &\\sin(\\theta)\\\\  -\\sin(\\theta)& \\cos(\\theta)\\end{pmatrix}\n\nPlotly = require('plotly.js-dist');\ndists = require( 'https://cdn.jsdelivr.net/gh/stdlib-js/stats-base-dists@umd/browser.js' );\njstat = require('jstat');\nmath = require(\"mathjs\");\n// see source here: https://github.com/stdlib-js/stats-base-dists/tree/umd\n// continuous case\n\nviewof inputs = Inputs.form([\n  Inputs.range([-1, 1], {label: tex`\\mu_1`, step: 0.1}, {value: 0}),\n  Inputs.range([-1, 1], {label: tex`\\mu_2`, step: 0.1}, {value: 0}),\n  Inputs.range([0.1, 2], {label: tex`\\sigma_1`, step: 0.1, value: 1}),\n  Inputs.range([0.1, 2], {label: tex`\\sigma_2`, step: 0.1, value: 1}),\n  Inputs.range([0, 6.29], {label: tex`\\theta`, step: 0.01, value: 0}),\n  Inputs.range([1, 1000], {label: tex`n`, step: 1}),\n  Inputs.button(\"Re-Tirage\")\n])\n\n\nmu1 = inputs[0];\nmu2 = inputs[1];\nsigma1 = inputs[2];\nsigma2 = inputs[3];\ntheta = inputs[4];\nn_samples = inputs[5];\n\n\nfunction create_sigma(theta, sigma1, sigma2){\n  const mat_rot = math.matrix([[math.cos(theta), -math.sin(theta)], [math.sin(theta), math.cos(theta)]]);\n  const mat_sigma = math.matrix([[sigma1**2, 0], [0, sigma2**2]]);\n  return math.multiply(mat_rot, math.multiply(mat_sigma, math.transpose(mat_rot)));\n}\n\n\nfunction mvnpdf(x, mu, Sigma){\n  const p = 2;\n    return (2*math.pi)**(-p/2)*math.det(Sigma)**(-0.5)*\n      math.exp(-0.5*math.multiply( math.multiply( math.transpose(math.subtract(x,mu)), math.inv(Sigma)), math.subtract(x,mu)));\n}\n\n{\n\n\nfunction normal_rng(mu, Sigma, n=100) {\n    // Compute the Cholesky decomposition of Sigma\n    var cholesky = jstat.cholesky(Sigma);\n\n    // Generate the samples\n    var samples = Array.from({length: n}, () =&gt; {\n        var x = jstat.normal.sample(0, 1);\n        var y = jstat.normal.sample(0, 1);\n        // Transform the standard normal random variables using the Cholesky decomposition\n        var transformedX = mu[0] + cholesky[0][0] * x + cholesky[0][1] * y;\n        var transformedY = mu[1] + cholesky[1][0] * x + cholesky[1][1] * y;\n        return [transformedX, transformedY];\n    });   \n\n    return samples;\n}\nvar Sigma = create_sigma(theta, sigma1, sigma2).toArray();\nvar mu = [mu1, mu2];\n\nvar samples = normal_rng(mu, Sigma, 1000);\nvar npoints=100, mini = -5, maxi=5, x = new Array(npoints), y = new Array(npoints), z = new Array(npoints), i, j;\n\n//  Densité:\nfor(var i = 0; i &lt; npoints; i++) {\n    x[i] = mini + i * (maxi - mini) / (npoints - 1);\n    y[i] = mini + i * (maxi - mini) / (npoints - 1);\n    z[i] = new Array(npoints);\n    }\n\nfor(var i = 0; i &lt; npoints; i++) {\n    for(j = 0; j &lt; npoints; j++) {\n\n        z[j][i] = mvnpdf([x[i], x[j]], mu, Sigma);\n    }\n\n}\n  \n\n{\n\nvar trace1 = {\n        x: samples.slice(0, n_samples).map(sample =&gt; sample[0]),\n        y: samples.slice(0, n_samples).map(sample =&gt; sample[1]),\n        mode: 'markers',\n        type: 'scatter',\n        marker: {\n            color: 'rgba(0,0,0,0.5)',\n            size: 5,\n        },\n\n        xaxis: 'x2',\n}\n\nvar trace22 = {\n        x: x,\n        y: y,\n        z: z,\n        type: 'surface',\n        colorscale: 'Oranges',\n        showscale: false,\n        color: {\n            legend: false,\n            label: \"pdf\",\n        },\n\n}\n\nvar trace21 = {\n        x: x,\n        y: y,\n        z: z,\n        type: 'contour',\n        colorscale: 'Oranges',\n        color: {\n            legend: true,\n            label: \"pdf\",\n        },\n        blur: 4,\n        xlim: [-5, 5],\n        ylim: [-5, 5],\n        xaxis: 'x2',\n}\n\n\nvar data = [\n  trace21,\n  trace22,\n  trace1,\n  ];\n\n\n  var layout = {\n       yaxis2: {\n          range: [-5, 5],\n          autorange: false,\n        },\n       xaxis2: {\n          range: [-5, 5],\n          autorange: false,\n        },\n\n      scene: {\n          camera: {\n              eye: {\n                  x: 0.5,\n                  y: 1.2,\n                  z: 1.5,\n              }\n          }\n      },\n    grid: {\n      rows: 1,\n      columns: 2,\n      subplots: [['xy','x2y']],\n    },\n    showlegend: false,\n\n  };\n\n    var config = {responsive: true}\n    const div = DOM.element('div');\n    Plotly.newPlot(div, data, layout, config);\n    return div;\n  }\n\n}",
    "crumbs": [
      "Cours",
      "Loi normale: cas multivarié"
    ]
  },
  {
    "objectID": "Courses/loi_normale1D.html",
    "href": "Courses/loi_normale1D.html",
    "title": "Loi normale: cas 1D",
    "section": "",
    "text": "On considère ici \\mathbb{R}^d muni du produit scalaire euclidien \\langle \\cdot, \\cdot \\rangle et de la norme euclidienne \\|\\cdot\\| associée.",
    "crumbs": [
      "Cours",
      "Loi normale: cas 1D"
    ]
  },
  {
    "objectID": "Courses/loi_normale1D.html#la-loi-normale",
    "href": "Courses/loi_normale1D.html#la-loi-normale",
    "title": "Loi normale: cas 1D",
    "section": "La loi normale",
    "text": "La loi normale",
    "crumbs": [
      "Cours",
      "Loi normale: cas 1D"
    ]
  },
  {
    "objectID": "Courses/loi_normale1D.html#définitions-et-propriétés-de-la-loi-normale",
    "href": "Courses/loi_normale1D.html#définitions-et-propriétés-de-la-loi-normale",
    "title": "Loi normale: cas 1D",
    "section": "Définitions et propriétés de la loi normale",
    "text": "Définitions et propriétés de la loi normale\nOn rappelle que la loi normale de paramètres \\mu \\in \\mathbb{R} et \\nu &gt; 0 a une densité donnée pour tout x \\in \\mathbb{R} par\n\n    \\varphi_{\\mu, \\nu}(x)=\\frac{1}{\\sqrt{2 \\pi \\nu}}\\exp\\Big(-\\frac{(x-\\mu)^2}{2\\nu}\\Big)\\enspace.\n\nOn parle aussi souvent de loi gaussienne, en hommage au mathématicien Carl Friedrich Gauss, le prince des mathématiciens1.\n1 Carl Friedrich Gauss: (1777-1855) mathématicien, astronome et physicien né à Brunswick, directeur de l’observatoire de Göttingen de 1807 jusqu’à sa mort en 1855 On note X \\sim \\mathcal{N}(\\mu, \\nu), si X est une variable aléatoire ayant pour densité \\varphi_{\\mu, \\nu}. Notons que si X \\sim \\mathcal{N}(\\mu,\\nu), alors X a pour espérance \\mu et pour variance \\nu. Le cas particulier \\mu=0 et \\nu=1 correspond à une variable aléatoire dite centrée réduite.\nLa loi normale vérifie la propriété de stabilité par transformation affine : si X \\sim \\mathcal{N}(\\mu, \\nu) et si (a,b) \\in \\mathbb{R}^* \\times \\mathbb{R}, alors la variable aléatoire a X + b suit une loi normale \\mathcal{N}(a\\mu + b, a^2 \\nu). On peut donc facilement passer d’une loi normale centrée réduite à une loi normale quelconque via une transformation affine :\n\nsi X \\sim \\mathcal{N}(0,1), alors \\sqrt{\\nu} X + \\mu \\sim \\mathcal{N}(\\mu, \\nu),\nsi X \\sim \\mathcal{N}(\\mu, \\nu), alors (X-\\mu)/\\sqrt{\\nu} \\sim \\mathcal{N}(0,1).\n\nAinsi, savoir simuler une loi normale centrée réduite, permet de simuler n’importe quelle loi normale.\nRappelons enfin que la fonction caractéristique d’une variable aléatoire X \\sim \\mathcal{N}(\\mu, \\nu) est donnée pour tout t \\in \\mathbb{R} par \n\\begin{align*}\n\\phi_{\\mu, \\nu}(t) & \\triangleq \\mathbb{E}(e^{i t X})  \\\\\n\\phi_X(t) & = \\exp\\Big( i \\mu t - \\frac{\\nu t^2}{2}\\Big)\\enspace.\n\\end{align*}\n\n\nSimulation d’une loi normale\n\n\n\n\n\n\nUne mauvaise piste pour simuler une loi normale\n\n\n\nOn peut simuler une loi normale à partir de variables aléatoires uniformes U_1, \\dots, U_n iid en appliquant le théorème central limite à \n    \\frac{U_1 + \\cdots + U_n - n/2}{\\sqrt{n/12}}\\,.\n Cependant, cette méthode ne donne qu’une approximation d’une loi normale. Par ailleurs, la vitesse de convergence étant relativement lente (de l’ordre de \\sqrt n), il faudra simuler beaucoup de variables aléatoires uniformes pour avoir une approximation correcte, ce qui demande un temps de calcul assez élevé.\n\n\n\nChangement de variables\nLe théorème suivant permet de passer de la loi d’un couple (X,Y) à celle de (U,V) = \\phi(X,Y), où \\phi est un C^1-difféomorphisme, c’est-à-dire une application bijective dont la réciproque est également de classe C^1.\nPour cela rappelons que la jacobienne de \\phi^{-1} correspond à la matrice (application linéaire) des dérivées partielles. Ainsi, si \\phi(x,y) = (u,v) \\iff (x,y) = \\phi^{-1}(u,v), alors \n\\begin{align*}\n{\\rm{J}}_{\\phi^{-1}}: (u,v) & \\mapsto\n\\begin{pmatrix}\n  \\frac{\\partial x}{\\partial u} & \\frac{\\partial x}{\\partial v}    \\\\\n  \\frac{\\partial y}{\\partial u} & \\frac{\\partial y}{\\partial v}\n\\end{pmatrix} \\enspace.\n\\end{align*}\n\n\nThéorème 1 (Caractérisation de la loi d’une variable aléatoire réelle) Soit (X,Y) un vecteur aléatoire de densité f_{(X,Y)} définie sur l’ouvert A \\subset \\mathbb{R}^2 et \\phi : A \\to B \\subset \\mathbb{R}^2 un C^1-difféomorphisme. Le vecteur aléatoire (U,V)=\\phi(X,Y) admet alors pour densité f_{(U,V)} définie sur B pour tout (u,v) \\in \\mathbb{R}^2 par \n\\begin{align*}\n    (u,v) & \\mapsto\n    f_{(X,Y)} (\\phi^{-1}(u,v)) |\\det ({\\rm{J}}_{\\phi^{-1}} (u,v))| {1\\hspace{-3.8pt} 1}_B(u,v)\\enspace.\n\\end{align*}\n\n\nOn a énoncé le résultat en dimension 2 par simplicité. Il s’étend bien évidemment à une dimension d quelconque. En particulier, pour d=1, on retrouve le changement de variable classique dans le cas de l’intégration d’une fonction à valeurs réelles.\nDémonstration.\nOn rappelle que la loi de (U,V) est caractérisée par les quantités \\mathbb{E}[h(U,V)] pour tout h : \\mathbb{R}^2 \\to \\mathbb{R} mesurable bornée. On considère donc une telle fonction h et on applique la formule de transfert : \n\\begin{align*}\n  \\mathbb{E}[h(U,V)] &\n  =\\mathbb{E}[h(\\phi(X,Y))]\\\\\n& = \\int_{\\mathbb{R}^2} h(\\phi(x,y)) f_{(X,Y)}(x,y) \\, dx dy \\\\\n& = \\int_{A} h(\\phi(x,y)) f_{(X,Y)}(x,y) \\, d x d y\\enspace.\n\\end{align*}\n On applique alors la formule du changement de variables vu en théorie de l’intégration avec (u,v) = \\phi(x,y) \\iff \\phi^{-1}(u,v) = (x,y) : \n\\begin{align*}\n  & \\mathbb{E}[h(U,V)]\\\\\n  & = \\!\\int_{B}  \\!\\!\\! h(u,v) f_{(X,Y)}(\\phi^{-1}(u,v)) |\\det ({\\rm{J}}_{\\phi^{-1}} (u,v))| \\, d u d v\\\\\n  & = \\!\\int_{\\mathbb{R}^2} \\!\\!\\!\\! h(u,v) f_{(X,Y)}(\\phi^{-1}(u,v)) |\\det ({\\rm{J}}_{\\phi^{-1}} (u,v))| {1\\hspace{-3.8pt} 1}_B(u,v)\\, d u d v .\n\\end{align*}\n  ce qui donne le résultat voulu.\n\n\n\n\n\n□\n\n\n\n\nExemple 1 (Exemple : loi de \\cos(X), avec X \\sim \\mathcal{U}(]0,\\pi[)) Donnons un exemple dans le cas réel. On considère une variable aléatoire X de loi uniforme sur ]0,\\pi[. Sa densité est donnée par f_X(x) = {1\\hspace{-3.8pt} 1}_{]0,\\pi[}(x)/\\pi. On pose U = \\cos(X) et on souhaite déterminer la loi de U.\nOn applique le théorème précédent avec la fonction \\phi^{-1}(u) = \\arccos(u) sur ]-1,1[. La densité de U est alors donnée pour tout u \\in \\mathbb{R} par \n\\begin{align*}\n  f_U(u)\n& = \\frac{{1\\hspace{-3.8pt} 1}_{]0,\\pi[}(\\arccos(u))}{\\pi} \\Big| \\frac{-1}{\\sqrt{1-u^2}} \\Big| {1\\hspace{-3.8pt} 1}_{]-1,1[}(u)\\\\\n& = \\frac{1}{\\pi \\sqrt{1-u^2}} {1\\hspace{-3.8pt} 1}_{]-1,1[}(u)\\enspace.\n\\end{align*}",
    "crumbs": [
      "Cours",
      "Loi normale: cas 1D"
    ]
  },
  {
    "objectID": "Courses/loi_normale1D.html#méthode-de-box-müller",
    "href": "Courses/loi_normale1D.html#méthode-de-box-müller",
    "title": "Loi normale: cas 1D",
    "section": "Méthode de Box-Müller",
    "text": "Méthode de Box-Müller\nUn cas particulier fondamental de la formule de changement de variables concerne le passage en coordonnées polaires. Cette transformation est définie via l’application \n    \\begin{array}{ccccc}\n        \\phi^{-1} & : & ]0, \\infty[ \\times ]0, 2\\pi[ & \\to     & \\mathbb{R}^2 \\setminus ([0,\\infty[ \\times \\{0\\}) \\\\\n                  &   & \\begin{pmatrix} r \\\\ \\theta \\end{pmatrix}                   & \\mapsto & \\begin{pmatrix} r \\cos(\\theta) \\\\ r \\sin(\\theta) \\end{pmatrix}\\,.\n    \\end{array}\n L’expression de \\phi ne nous sera pas utile. On peut tout de même la donner au passage :\n\n    \\begin{array}{ccccc}\n        \\phi & : & \\mathbb{R}^2 \\setminus ([0,\\infty[ \\times \\{0\\}) & \\to     & ]0, \\infty[ \\times ]0, 2\\pi[                                                      \\\\\n             &   & \\begin{pmatrix} x \\\\ y \\end{pmatrix}                                            & \\mapsto & \\begin{pmatrix}\\sqrt{x^2+y^2} \\\\ 2 \\arctan \\Big( \\frac{y}{x+\\sqrt{x^2+y^2}} \\Big)\\end{pmatrix}\\,.\n    \\end{array}\n\nIci, le jacobien de \\phi^{-1} est la matrice \n    {\\rm{J}}_{\\phi^{-1}} (r,\\theta)\n    =\n    \\begin{pmatrix}\n        \\cos(\\theta) & -r \\sin(\\theta) \\\\\n        \\sin(\\theta) & r \\cos(\\theta)\n    \\end{pmatrix}\\,,\n qui vérifie |\\det({\\rm{J}}_{\\phi^{-1}} (r, \\theta))| = r. Ainsi, si (X,Y) a pour densité f_{(X,Y)}, alors (R, \\Theta) = \\phi(X,Y) a pour densité \n  f_{(R, \\Theta)} (r, \\theta)\n  = r\\cdot f_{(X,Y)}(r \\cos(\\theta), r \\sin(\\theta)) \\!\\cdot\\! {1\\hspace{-3.8pt} 1}_{]0, \\infty[}(r)  {1\\hspace{-3.8pt} 1}_{]0, 2 \\pi[}(\\theta).\n\nDans le cas où X et Y sont des variables aléatoires gaussiennes indépendantes, on obtient le résultat suivant.\n\nThéorème 2 (Méthode de Box-Müller) Soit X et Y deux variables aléatoires indépendantes de loi normales centrées réduites : X,Y \\sim \\mathcal{N}(0,1). Le couple de variables aléatoires polaires (R, \\Theta) = \\phi^{-1}(X,Y) a pour densité \n            f_{R, \\Theta}(r,\\theta)\n            = \\Big( r \\cdot e^{-\\tfrac{r^2}{2}} {1\\hspace{-3.8pt} 1}_{]0, \\infty[}(r) \\Big) \\bigg(\\frac{{1\\hspace{-3.8pt} 1}_{]0, 2 \\pi[}(\\theta)}{2 \\pi} \\bigg)\\,.\n Autrement dit, elles sont indépendantes, l’angle \\Theta suit une loi uniforme sur ]0, 2\\pi[ et la distance à l’origine R suit une loi de Rayleigh donnée par la densité \n    f_R(r) =  r \\cdot e^{-r^2/2} {1\\hspace{-3.8pt} 1}_{]0, \\infty[}(r)\\,, \\quad r &gt; 0\\,.\n\n\nDémonstration: La densité du couple (X,Y) est donnée par \n  f_{(X,Y)}(x,y) = \\frac{1}{2\\pi} e^{-\\frac{x^2+y^2}{2}}\\,, \\quad x,y \\in \\mathbb{R}\\,.\n Le théorème précédent donne alors la densité de (R, \\Theta) : \n\\begin{align*}\n  f_{(R, \\Theta)} (r, \\theta) &\n  = r\\cdot f_{(X,Y)}(r \\cos(\\theta), r \\sin(\\theta)) \\!\\cdot\\! {1\\hspace{-3.8pt} 1}_{]0, \\infty[}(r)  {1\\hspace{-3.8pt} 1}_{]0, 2 \\pi[}(\\theta)\\\\\n  &= r \\cdot\\frac{1}{2\\pi} e^{-\\frac{r^2}{2}} \\cdot  {1\\hspace{-3.8pt} 1}_{]0, \\infty[}(r) {1\\hspace{-3.8pt} 1}_{]0, 2 \\pi[}(\\theta)\\,,\n\\end{align*}\n ce qui conclut la preuve.\n\n\n\n\n\n□\n\n\nNotons que R^2 est à valeurs dans ]0,\\infty[ et vérifie pour x &gt; 0: \n\\begin{align*}\n    \\mathbb{P}(R^2 &gt; x)\n    &= \\mathbb{P}(R &gt; \\sqrt x)\\\\\n    &= \\int_{\\sqrt{x}}^{\\infty} r e^{-r^2/2}\\, dr\\\\\n    &= \\Big[- e^{-\\tfrac{r^2}{2}} \\Big]_{\\sqrt{x}}^{\\infty}\\\\\n    &= e^{-\\tfrac{x}{2}}\\,.\n\\end{align*}\n On reconnaît la fonction de survie d’une loi exponentielle de paramètre 1/2. Or, on a vu (cf. méthode de l’inverse) que si U suit une loi uniforme sur [0,1], alors -2 \\ln(U) suit une loi exponentielle de paramètre 1/2, donc \\sqrt{-2 \\ln(U)} a la même loi que R.\nL’algorithme de Box-Müller s’en suit: si U et V sont des v.a. indépendantes de loi uniforme sur [0,1] et qu’on définit X et Y par \n\\begin{cases}\n  X = \\sqrt{-2 \\ln(U)} \\cos(2\\pi V)\\\\\n  Y = \\sqrt{-2 \\ln(U)} \\sin(2\\pi V)\\,.\n\\end{cases}\n alors X et Y des variables aléatoires gaussiennes centrées réduites indépendantes.\n\n\n\n\n\n\nNote\n\n\n\nCet algorithme n’est en fait pas souvent utilisé en pratique : il fait appel à des fonctions dont l’évaluation est coûteuse (logarithme, cosinus, sinus). Pour s’affranchir des fonctions trigonométriques, une version modifiée de l’algorithme de Box-Müller a été proposée : la méthode de Marsaglia, qui s’appuie sur des variables aléatoires uniformes sur le disque unité (voir l’exercice dédié en TD). Une autre alternative est la méthode de Ziggurat.\n\n\nPlotly = require('plotly.js-dist');\ndists = require( 'https://cdn.jsdelivr.net/gh/stdlib-js/stats-base-dists@umd/browser.js' );\njstat = require('jstat');\nmath = require(\"mathjs\");\n// see source here: https://github.com/stdlib-js/stats-base-dists/tree/umd\n// continuous case\n\nviewof inputs = Inputs.form([\n  Inputs.range([-2, 2], {label: tex`\\mu`, step: 0.1}, {value: 0}),\n  Inputs.range([0.1, 10], {label: tex`\\nu`, step: 0.1, value: 1}),\n  Inputs.range([1, 1000], {label: tex`n`, step: 1}),\n  Inputs.button(\"Re-Tirage\")\n])\n\n\nmu = inputs[0];\nnu = inputs[1];\nn_samples = inputs[2];\n\n{\n\nfunction mvnpdf(x, mu, nu){\n    return 1.0 / (2*math.pi *nu)**(0.5)* math.exp(-0.5*(x-mu)**2 / nu);\n}\n\n\nfunction normal_rng(mu, nu, n=100){\n    var samples = Array.from({length: n}, () =&gt; {\n        var x = jstat.normal.sample(mu, nu**0.5);\n        return x;\n    });\n\n    return samples;\n}\n\nvar samples = normal_rng(mu, nu, 1000);\nvar samples_jitter = normal_rng(0, 0.03, 1000)\nvar npoints=500, mini = -10, maxi=10, x = new Array(npoints), z = new Array(npoints), i, j;\n\n//  Densité:\nfor(var i = 0; i &lt; npoints; i++) {\n    x[i] = mini + i * (maxi - mini) / (npoints - 1);\n    z[i] = mvnpdf(x[i], mu, nu);\n    }\n\n{\nvar trace1 = {\n        x: samples.slice(0, n_samples),\n        y: samples_jitter.slice(0, n_samples),\n        mode: 'markers',\n        type: 'scatter',\n        marker: {\n            color: 'rgba(0,0,0,0.5)',\n            size: 5,\n        },\n\n  }\n\nvar trace22 = {\n        x: x,\n        y: z,\n        type: \"scatter\",\n        mode: \"lines\",\n        name: 'Pdf',\n        line: {color: 'black'},\n        yaxis: 'y2',\n        xaxis: 'x2',\n        }\n\n\nvar data = [\n  trace1,\n  trace22,\n  ];\n\n\nvar layout = {\n\n    yaxis: {domain: [0, 0.2],\n            showticklabels: false,\n            range: [-0.6, 0.6],\n            autorange: false},\n    xaxis2: {matches: 'x',\n              range: [-10, 10],\n          autorange: false},\n    yaxis2: {domain: [0.29, 0.99]},\n\n  showlegend: false,\n\n};\n\n    var config = {responsive: true}\n    const div = DOM.element('div');\n    Plotly.newPlot(div, data, layout, config);\n    return div;\n    }\n}",
    "crumbs": [
      "Cours",
      "Loi normale: cas 1D"
    ]
  },
  {
    "objectID": "Courses/loi_normale1D.html#lois-autour-de-la-loi-normale",
    "href": "Courses/loi_normale1D.html#lois-autour-de-la-loi-normale",
    "title": "Loi normale: cas 1D",
    "section": "Lois autour de la loi normale",
    "text": "Lois autour de la loi normale\n\nLoi du \\chi^2\nConcernant la prononciation, on prononce “khi-deux” le nom de cette loi.\n\nDéfinition 1 (Loi du \\chi^2) Soit X_1, \\dots, X_k des variables aléatoires iid de loi normale centrée réduite. La loi de la variable aléatoire X = X_1^2 + \\dots + X_k^2 est appelée loi du \\chi^2 à k degrés de liberté. Sa densité est donnée par \nf(x) = \\frac{1}{2^{\\frac{k}{2}}\\Gamma(\\frac{k}{2})} x^{\\frac{k}{2}-1} e^{-x/2}\\,, \\quad x \\geq 0\\,,\n où \\Gamma désigne la fonction gamma d’Euler : \n\\Gamma(x) = \\int_0^{\\infty} t^{x-1} e^{-t}\\,  dt\\,.\n On note alors X \\sim \\chi^2(k).\n\nAu vu de sa définition, la simulation d’une loi du \\chi^2 est claire : on simule k variables aléatoires gaussiennes centrées réduites indépendantes et on somme leur carrés.\nPreuve: Montrons pour k=1 que la densité est bien de la forme précédente, c’est-à-dire \n    f(x) = \\frac{1}{\\sqrt{2\\pi}} \\frac{e^{-x/2}}{\\sqrt x}\\,, \\quad x \\geq 0\\,,\n où on a utilisé la relation \\Gamma(1/2) = \\sqrt \\pi (intégrale de Gauss).\nSoit h : \\mathbb{R} \\to \\mathbb{R} une fonction mesurable bornée. On a \n\\begin{align*}\n    \\mathbb{E}[h(X_1^2)]\n    & = \\int_\\mathbb{R} h(x^2) \\frac{e^{-\\frac{x^2}{2}}}{\\sqrt{2 \\pi}} \\, dx\\\\\n    & = \\int_{-\\infty}^0 h(x^2) \\frac{e^{-\\frac{x^2}{2}}}{\\sqrt{2 \\pi}} \\, dx\n    + \\int_0^{\\infty} h(x^2) \\frac{e^{-\\frac{x^2}{2}}}{\\sqrt{2 \\pi}} \\, dx\\,.\n\\end{align*}\n En effectuant le changement de variable x=-\\sqrt u dans la première intégrale et x=\\sqrt u dans la deuxième, on obtient \n    \\mathbb{E}[h(X_1^2)]\n    = \\int_{\\infty}^0 h(u) \\frac{e^{-u/2}}{\\sqrt{2 \\pi}} \\frac{ du}{2 \\sqrt u}\n    + \\int_0^{\\infty} h(u) \\frac{e^{-u/2}}{\\sqrt{2 \\pi}} \\frac {du}{2 \\sqrt u}\\,.\n Les deux intégrales étant égales, on conclut que \n    \\mathbb{E}[h(X_1^2)] = \\int_0^{\\infty} h(u) \\frac{e^{-u/2}}{\\sqrt{2 \\pi}\\sqrt u}\\, du\\,,\n ce qui prouve le résultat pour k=1.\nLa généralisation à k quelconque se fait par récurrence: on utilise la formule de convolution des la loi pour obtenir la loi pour k+1:\n\n\\begin{align*}\n    X_1^2 + \\dots + X_k^2 + X_{k+1}^2\n    & = (X_1^2 + \\dots + X_k^2) + X_{k+1}^2\\\\\n    & = \\chi^2(k) + X_{k+1}^2\\,.\n\\end{align*}\n Ainsi, \n\\begin{align*}\n    f_{\\chi^2(k+1)}(x)\n    & = \\int_0^{\\infty} f_{\\chi^2(k)}(x-y) f_{X_{k+1}^2}(y) \\, dy\\\\\n    & = \\int_0^x \\frac{1}{2^{\\frac{k}{2}}\\Gamma(\\frac{k}{2})} (x-y)^{\\frac{k}{2}-1} e^{-\\frac{x-y}{2}} \\tfrac{e^{-\\frac{y}{2}}}{\\sqrt{2\\pi y}} \\, dy\\\\\n    & = \\frac{e^{-\\frac{x}{2}}}{2^{\\frac{k+1}{2}} \\Gamma(\\frac{k}{2})  \\Gamma(\\frac{1}{2})}\\int_0^x  (x-y)^{\\frac{k}{2}-1}  \\tfrac{1}{\\sqrt{ y}} \\, dy\\\\\n    & = \\frac{e^{-\\frac{x}{2}}}{2^{\\frac{k+1}{2}} \\Gamma(\\frac{k}{2})  \\Gamma(\\frac{1}{2})} x \\int_0^1  (x-ux)^{\\frac{k}{2}-1}  \\tfrac{1}{\\sqrt{xu}} \\, du\\\\\n\\end{align*}\n avec le changement de variable y=ux. Ensuite, \n\\begin{align*}\n    f_{\\chi^2(k+1)}(x)\n    & = \\frac{x^{\\frac{k+1}{2}} e^{-\\frac{x}{2}}}{2^{\\frac{k+1}{2}} \\Gamma(\\frac{k}{2})  \\Gamma(\\frac{1}{2})}  \\int_0^1  (1-u)^{\\frac{k}{2}-1}  u^{1/2-1} du\\enspace.\n\\end{align*}\n Or rappelons que si \\Beta(a,b) = \\int_0^1 (1-u)^{a-1} u^{b-1} \\, du, alors pour tout a,b \\in [0,+\\infty[, \\Beta(a,b)=\\frac{\\Gamma(a)\\Gamma(b)}{\\Gamma(a+b)}. En effet, en faisant le changement de variable dans l’intégrale double qui suit: \n\\begin{array}{ccccc}\n    \\psi & : & \\mathbb{R}^+ \\times \\mathbb{R}^+ & \\to     & \\mathbb{R}^+\\times ]0,1[                 \\\\\n            &   & (s,t)                          & \\mapsto & \\Big(s+t, \\frac{t}{s+t}\\Big)\\,,\n\\end{array}\n c’est-à-dire \\psi^{-1}(r,w) = (r(1-w), rw), et le jacobien est donné par J_{\\psi^{-1}}(r,w) = \\begin{pmatrix} 1-w & -r \\\\ w & r \\end{pmatrix}, et donc J_{\\psi^{-1}}(r,w)=r, on obtient: \n\\begin{align*}\n\\Gamma(a)\\Gamma(b) & = \\int_0^{\\infty} t^{a-1} e^{-t} \\, dt \\int_0^{\\infty} s^{b-1} e^{-s} \\, ds\\\\\n& = \\int_0^{\\infty} \\int_0^{\\infty} e^{-t-s} t^{a-1} s^{b-1} \\, dt \\, ds\\\\\n& = \\int_0^{1} \\int_0^{\\infty} e^{-r} (rw)^{a-1} (r(1-w))^{b-1} r \\, dr \\, dw\\\\\n& = \\int_0^1 w^{a-1} (1-w)^{b-1} \\int_0^{\\infty} e^{-r} r^{a+b-1} \\, dr \\, dw\\\\\n\\end{align*}\n et donc \\Beta(a,b)=\\frac{\\Gamma(a)\\Gamma(b)}{\\Gamma(a+b)}. En appliquant cette relation pour a=\\frac{k}{2} et b=1/2, on obtient\n\n\\begin{align*}\n    f_{\\chi^2(k+1)}(x)\n    & = \\frac{x^{\\frac{k+1}{2}} e^{-\\frac{x}{2}}}{2^{\\frac{k+1}{2}} \\Gamma(\\frac{k+1}{2})} \\enspace.\n\\end{align*}\n Le résultat est donc prouvé par récurrence.\n\n\n\n\n\n□\n\n\n\n\n\nLoi de Student\n\nDéfinition 2 (Loi de Student) Soit X \\sim \\mathcal{N}(0,1) et Y \\sim \\chi^2(k) deux variables aléatoires indépendantes. La loi de la variable aléatoire V = \\frac{X}{\\sqrt{Y/k}} est appelée loi de Student à k degrés de liberté. Elle admet pour densité \n    f_V(t)\n    = \\dfrac{1}{\\sqrt{k \\pi}} \\dfrac{\\Gamma(\\frac{k+1}{2})}{\\Gamma(\\frac{k}{2})} \\Big(1+\\dfrac{t^2}{k}\\Big)^{-\\frac{k+1}{2}}\\,,\n    \\quad t \\in \\mathbb{R}\\,.\n\n\nLa loi de Student correspond donc au ratio d’une loi normale par la racine carrée d’une loi du \\chi^2(k) normalisée. Ce ratio apparaît souvent en statistique lors de la construction d’intervalles de confiance. Cette loi a été décrite en 1908 par William Gosset2.\n2 William Gosset: (1876-1937) statisticien et chimiste anglais. Alors qu’il était employé à la brasserie Guinness à Dublin. Son employeur lui refusant le droit de publier sous son propre nom, W. Gosset choisit un pseudonyme, Student (🇫🇷: étudiant). Au vu de la proposition précédente, simuler une loi de Student est assez simple : on simule k+1 loi normales indépendantes X_1, \\ldots, X_{k+1} et on considère \n    V = \\dfrac{\\sqrt{k} X_{k+1}}{\\sqrt{X_1^2+\\cdots + X_k^2}}\\,.\n\nPreuve de la formule de la densité:\nOn applique pour cela la formule du changement de variables avec la transformation \n    \\begin{array}{ccccc}\n        \\phi & : & \\mathbb{R}^* \\times ]0, \\infty[ & \\to     & \\mathbb{R}^* \\times \\mathbb{R}^*      \\\\\n             &   & (x,y)                           & \\mapsto & \\Big(x, \\dfrac{x}{\\sqrt{y/k}}\\Big)\\,,\n    \\end{array}\n c’est-à-dire \n    \\phi^{-1}(u,v) = \\Big(u, k\\dfrac{u^2}{v^2}\\Big)\\,.\n La fonction \\phi^{-1} a pour matrice jacobienne \n    J_{\\phi^{-1}} (u,v)\n    =\n    \\begin{pmatrix}\n        1              & 0                  \\\\\n        \\frac{2k}{v^2} & \\frac{-2ku^2}{v^3}\n    \\end{pmatrix}\\,,\n dont le déterminant vaut \\frac{-2ku^2}{v^3}. Par ailleurs, les variables aléatoires X et Y étant indépendantes, la densité du couple (X,Y) correspond au produit des densités : \n    f_{(X,Y)}(x,y) = \\dfrac{1}{\\sqrt{2 \\pi}} e^{-\\frac{x^2}{2}} \\dfrac{1}{2^{\\frac{k}{2}2} \\Gamma(\\frac{k}{2})} y^{\\frac{k}{2}-1} e^{-\\frac{y}{2}} {1\\hspace{-3.8pt} 1}_{]0, \\infty[}(y)\\, \\quad x,y \\in \\mathbb{R}\\,.\n Tout est prêt pour appliquer la théorème du changement de variables qui assure que la densité du couple (U,V) est donnée par \n    f_{(U,V)}(u,v)\n    = \\dfrac{1}{\\sqrt{2 \\pi}} e^{-\\frac{u^2}{2}} \\dfrac{1}{2^{\\frac{k}{2}} \\Gamma(\\frac{k}{2})} \\bigg(\\dfrac{k u^2}{v^2}\\bigg)^{\\frac{k}{2}-1} e^{-\\frac{1}{2} \\frac{k u^2}{v^2}} \\dfrac{2 k u^2}{(v^2)^{\\frac{3}{2}}} {1\\hspace{-3.8pt} 1}_{\\mathbb{R}^*}(v)\\,.\n Il suffit alors de marginaliser pour obtenir la densité de V, ce qui s’effectue en calculant l’intégrale \n    \\int_\\mathbb{R} f_{(U,V)} (u,v) \\, du\\,.\n Les termes en u de l’expression précédente s’intègre en \n\\begin{align*}\n    \\int_{-\\infty}^\\infty e^{-\\frac{u^2}{2}(1+\\frac{k}{v^2})} (u^2)^{\\frac{k}{2}} \\, d u\n     & = \\int_0^\\infty e^{-s} \\bigg( \\dfrac{2 s}{1+\\frac{k}{v^2}} \\bigg)^{\\frac{k}{2}} \\sqrt{\\dfrac{2}{1+\\frac{k}{v^2}}} \\dfrac{ds}{2\\sqrt{s}} \\\\\n     & = \\frac{2^{\\frac{k+1}{2}}}{2} \\bigg(1+\\dfrac{k}{v^2}\\bigg)^{- \\frac{k+1}{2}} \\int_0^\\infty e^{-s} s^{\\frac{k}{2} - \\frac{1}{2}} \\, ds\\,,\n\\end{align*}\n où la première égalité résulte du changement de variable \n    s = \\dfrac{u^2}{2} \\bigg( 1 + \\dfrac{k}{v^2} \\bigg) \\iff \\sqrt{\\dfrac{2s }{1+\\frac{k}{v^2}}} = u\\,.\n On reconnaît dans l’intégrale la valeur de \\Gamma(\\frac{k+1}{2}) ce qui conduit à \n    f_V(v)\n    =\n    \\dfrac{1}{\\sqrt{2 \\pi}} \\dfrac{1}{2^{\\frac{k}{2}} \\Gamma(\\frac{k}{2})} \\bigg(\\dfrac{k}{v^2}\\bigg)^{\\frac{k}{2}-1} \\dfrac{2 k}{(v^2)^{\\frac{3}{2}}}\n    \\frac{2^{\\frac{k+1}{2}}}{2} \\bigg(1+\\dfrac{k}{v^2}\\bigg)^{- \\frac{k+1}{2}} \\Gamma\\bigg(\\frac{k+1}{2}\\bigg)\\,.\n On réécrit alors les termes en k/v^2 via \n\\begin{align*}\n\\bigg(\\dfrac{k}{v^2}\\bigg)^{\\frac{k}{2}-1} \\dfrac{k}{(v^2)^{\\frac{3}{2}}}\n    \\bigg(1+\\dfrac{k}{v^2}\\bigg)^{- \\frac{k+1}{2}}\n    & =\n    \\bigg(\\dfrac{k}{v^2}\\bigg)^{\\frac{k}{2}-1} \\dfrac{1}{\\sqrt{k}} \\bigg(\\dfrac{k}{v^2}\\bigg)^{\\frac{3}{2}}\n    \\bigg(1+\\dfrac{k}{v^2}\\bigg)^{- \\frac{k+1}{2}}\\\\\n    & = \\dfrac{1}{\\sqrt{k}}\\bigg(1+\\dfrac{v^2}{k}\\bigg)^{- \\frac{k+1}{2}}\\,,\n\\end{align*}\n ce qui permet de conclure : \n    f_V(v)\n    =\n    \\dfrac{1}{\\sqrt{k \\pi}} \\dfrac{\\Gamma(\\frac{k+1}{2})}{\\Gamma(\\frac{k}{2})} \\Big(  1+\\dfrac{v^2}{k}\\Big)^{-\\frac{k+1}{2}}\\,.\n\n\n\n\n\n\n□\n\n\n\n\n\nLoi de Cauchy\n\nDéfinition 3 (Loi de Cauchy) Une variable aléatoire X suit une loi de Cauchy si sa densité est donnée par \n    f_X(x) = \\dfrac{1}{\\pi(1+x^2)}\\,, \\quad x \\in \\mathbb{R}\\,.\n\n\nLa fonction de répartition de X correspond, à une constante près, à la fonction arctangente qui est bijective de \\mathbb{R} sur ]-\\frac{\\pi}{2},\\frac{\\pi}{2}[. La méthode d’inversion permet donc de simuler une variable aléatoire de loi de Cauchy. La proposition suivante donne un autre moyen.\n\nProposition 1 (Loi de Cauchy et loi normale) Soit X et Y deux variables aléatoires indépendantes de loi normale centrée réduite. Alors la variable aléatoire Y/X suit une loi de Cauchy.\n\nNotons que Y/X est bien définie puisque X est différent de 0 presque sûrement.\nPreuve: Comme pour la loi de Student, on démontre ce résultat avec un changement de variables. On considère l’application \n    \\begin{array}{ccccc}\n        \\phi & : & \\mathbb{R}^* \\times \\mathbb{R} & \\to     & \\mathbb{R}^2                 \\\\\n                &   & (x,y)                          & \\mapsto & \\Big(x, \\dfrac{y}{x}\\Big)\\,,\n    \\end{array}\n c’est-à-dire \n    \\phi^{-1}(u,v) = (u, uv)\\,.\n La matrice jacobienne de \\phi^{-1} est donnée par \n    J_{\\phi^{-1}} (u,v)\n    =\n    \\begin{pmatrix}\n        1 & 0 \\\\\n        v & u\n    \\end{pmatrix}\\,,\n dont le déterminant vaut u. Rappelons également que la densité du couple (X,Y) vaut \n    f_{(X,Y)}(x,y) = \\dfrac{1}{2 \\pi} e^{- \\frac{x^2 + y^2}{2}}\\,, \\quad x,y \\in \\mathbb{R}\\,.\n La formule du changement de variables donne alors la densité de f_{(U,V)} : \n    f_{(U,V)}\n    = \\dfrac{1}{2 \\pi} e^{- \\frac{u^2 + u^2v^2}{2}} |u|\\,.\n On obtient alors la densité de V en intégrant par rapport à u : \n\\begin{align*}\n    f_V(v)\n    & = \\dfrac{1}{2 \\pi} \\int_\\mathbb{R} e^{- \\frac{u^2 + u^2v^2}{2}} |u| \\, \\mathrm du \\\\\n    & = \\dfrac{1}{\\pi} \\int_0^\\infty e^{- \\frac{u^2(1 + v^2)}{2}} u \\, \\mathrm du\\\\\n    & = \\dfrac{1}{\\pi} \\bigg[ -\\dfrac{e^{- \\frac{u^2(1 + v^2)}{2}}}{1+v^2} \\bigg]_0^\\infty\\\\\n    & = \\dfrac{1}{\\pi(1+v^2)}\\,.\n\\end{align*}\n\n\n\n\n\n\n□\n\n\nOn obtient ainsi une autre manière de simuler une loi de Cauchy : on simule deux gaussiennes indépendantes et on prend leur ratio. Cependant, la simulation via la méthode d’inversion peut-être moins coûteuse puisqu’elle ne fait appel qu’à une variable aléatoire uniforme et à la fonction tangente.",
    "crumbs": [
      "Cours",
      "Loi normale: cas 1D"
    ]
  },
  {
    "objectID": "Courses/matterjs-inverse-vizu.html",
    "href": "Courses/matterjs-inverse-vizu.html",
    "title": "inverse-vizu",
    "section": "",
    "text": "viewof dist = Inputs.select(['normal','cauchy','laplace','bimodal'], {value: \"bimodal\", label: \"Distribution type\"})\nviewof replay = html`&lt;button&gt;replay`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ninvcdfboard = require(await FileAttachment(\"../inverse-vizu/dist/invcdfboard.umd.cjs\").url())\n\n{\n  replay\n  const canvas = DOM.canvas(500, 500);\n  const galton = invcdfboard.galton(canvas,dist);\n  \n  return html`${galton.canvas}`\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Retour au sommet"
  },
  {
    "objectID": "Courses/perspective_historique.html",
    "href": "Courses/perspective_historique.html",
    "title": "Perspectives historiques",
    "section": "",
    "text": "Nous allons présenter ici quelques éléments historiques sur les méthodes de Monte-Carlo, dont les prémisses remontent au XVIIIème siècle.",
    "crumbs": [
      "Cours",
      "Perspectives historiques"
    ]
  },
  {
    "objectID": "Courses/perspective_historique.html#laiguille-de-buffon",
    "href": "Courses/perspective_historique.html#laiguille-de-buffon",
    "title": "Perspectives historiques",
    "section": "L’aiguille de Buffon",
    "text": "L’aiguille de Buffon\nGeorges-Louis Leclerc, Comte de Buffon1 proposa en 1733 une méthode qui s’avéra être utile pour estimer la valeur de \\pi. On désigne de nos jours cette expérience sous le nom de l’aiguille de Buffon. C’est l’une des premières méthodes de Monte-Carlo référencée dans la littérature (la source du texte est disponible ici sur le site de la BNF).\n1 Georges-Louis Leclerc, Comte de Buffon: (1707-1788) naturaliste, mathématicien et industriel français du siècle des Lumières La question initiale (simplifiée ici) posée par Buffon était la suivante: une aiguille de taille 1 tombe sur un parquet composé de lattes de largeur 1: quelle est alors la probabilité P que l’aiguille croise une ligne de la trame du parquet ?\nLe contexte original était dans celui d’un jeu à deux joueurs: un joueur parie sur le fait que l’aiguille croise une ligne de la trame du parquet, l’autre sur le fait que l’aiguille ne croise pas une ligne de la trame du parquet. L’enjeu est alors de calculer la probabilité de succès de chacun des joueurs, et de voir si le jeu est équilibré ou non.\nVoilà brièvement la question que s’est posée Buffon en 1733. La réponse est donnée par la formule suivante, qui montre que le jeu qu’il propose n’est pas équilibré:\n\nP = \\frac{2}{\\pi} \\approx 0.6366 \\enspace.\n Une preuve de ce résultat sera donnée ci-dessous.\nL’idée sous-jacente de Buffon est que si l’on répète cette expérience un grand nombre de fois, on peut approché la quantité P numériquement, par exemple en proposant un estimateur \\hat{P}_n qui compte la proportion de chevauchement après avoir fait n répétition des lancers. Pour estimer \\pi, il ne restera donc plus qu’à évaluer \\frac{2}{\\hat{P}_n}.\nOn peut faire cette expérience dans le monde réelle (c’est un peu long pour n grand!), mais on peut aussi utiliser une méthode numérique pour cela. Il s’agit alors de tirer aléatoire la position du centre de l’aiguille, puis de tirer aussi de manière aléatoire son angle de chute. On teste à la fin si l’aiguille croise une ligne de la trame du parquet ou non, et on recommence l’expérience un grand nombre de fois.\nCette méthode est donnée ci-dessous, avec un exemple interactif généré en Python.\n\nCode\nimport numpy as np\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\nrng = np.random.default_rng(44)\n\nn_samples = 200\nxmax = 14.499999\nxmin = -xmax\n\n\n# Create the needles\ncenters_x = rng.uniform(xmin, xmax, n_samples)\nangles = rng.uniform(0, 2 * np.pi, n_samples)\ncenters_y = rng.uniform(-2, 2, n_samples)\n\n# Compute the right borders of the needles\nborders_right = np.zeros((n_samples, 2))\nborders_right[:, 0] = centers_x + np.cos(angles) / 2\nborders_right[:, 1] = centers_y + np.sin(angles) / 2\n\n# Compute the left borders of the needles\nborders_left = np.zeros((n_samples, 2))\nborders_left[:, 0] = centers_x + np.cos(angles + np.pi) / 2\nborders_left[:, 1] = centers_y + np.sin(angles + np.pi) / 2\n\ncenters_x_round = np.round(centers_x)\noverlap = (borders_right[:, 0] - centers_x_round) * (\n    borders_left[:, 0] - centers_x_round\n) &lt; 0\noverlap = np.where(overlap, 1, 0)\nn_overlap = int(np.sum(overlap))\n\n\n# Check if the needles cross a line\nborders_red = np.empty((3 * n_overlap, 2), dtype=object)\nborders_red.fill(None)\nborders_red[::3, :] = borders_right[overlap == 1]\nborders_red[1::3, :] = borders_left[overlap == 1]\n\nborders_blue = np.empty((3 * (n_samples - n_overlap), 2), dtype=object)\nborders_blue.fill(None)\nborders_blue[::3, :] = borders_right[overlap == 0]\nborders_blue[1::3, :] = borders_left[overlap == 0]\n\noverlaps = np.empty((3 * n_samples), dtype=object)\noverlaps.fill(None)\noverlaps[::3] = overlap\noverlaps[1::3] = overlap\noverlaps[2::3] = overlap\n\nidx_red = np.cumsum(overlaps)\nidx_blue = np.cumsum(1 - overlaps)\n\n\n# Create subplots with 2 rows and 1 column with ratio x /  y  of 10\nfig = make_subplots(rows=2, cols=1, vertical_spacing=0.1, row_heights=[2, 1])\n\n# Use a loop to plot vertical lines equation \"y=c\" for integer values c in [-2, -1, 0, 1, 2]\nfor i in range(int(np.round(xmin)), int(np.round(xmax)) + 1):\n    fig.add_shape(\n        type=\"line\",\n        y0=-3,\n        x0=i,\n        y1=3,\n        x1=i,\n        line=dict(\n            color=\"black\",\n            width=2,\n        ),\n        row=1,\n        col=1,\n    )\n\ncolor = np.where(overlaps, 1.0, 0.0)\n\nn_samples_array = np.arange(1, n_samples + 1)\npi_estimate = 2 / (np.cumsum(overlap) / n_samples_array)\nt = n_samples\n\nfig.update_layout(\n    template=\"simple_white\",\n    xaxis=dict(range=[xmin, xmax], constrain=\"domain\", showgrid=False),\n    yaxis_scaleanchor=\"x\",\n    xaxis_visible=False,\n    yaxis_visible=False,\n)\n\nfor i in range(3, t):\n    fig.add_trace(\n        go.Scatter(\n            x=borders_red[: idx_red[3 * i] + 1, 0],\n            y=borders_red[: idx_red[3 * i] + 1, 1],\n            mode=\"lines\",\n            line=dict(width=2),\n            marker=dict(color=\"red\"),\n            name=\"Avec intersection\",\n            visible=False,\n        ),\n        row=1,\n        col=1,\n    )\n    fig.add_trace(\n        go.Scatter(\n            x=borders_blue[: idx_blue[3 * i] + 1, 0],\n            y=borders_blue[: idx_blue[3 * i] + 1, 1],\n            mode=\"lines\",\n            line=dict(width=2),\n            marker=dict(color=\"darkblue\"),\n            name=\"Sans intersection\",\n            visible=False,\n        ),\n        row=1,\n        col=1,\n    )\n\n    fig.add_trace(\n        go.Scatter(\n            x=n_samples_array[:i],\n            y=pi_estimate[:i],\n            mode=\"lines\",\n            line=dict(width=1),\n            marker=dict(color=\"red\"),\n            showlegend=False,\n            visible=False,\n        ),\n        row=2,\n        col=1,\n    )\n\nfig.add_annotation(\n    dict(\n        x=1.25,\n        y=0.14,\n        xref=\"paper\",\n        yref=\"paper\",\n        text=\"Estimation de pi\",\n        showarrow=False,\n        font=dict(color=\"red\"),\n    )\n)\n\nfig.add_annotation(\n    dict(x=-0.04, y=0.19, xref=\"paper\", yref=\"paper\", text=\"pi\", showarrow=False)\n)\n\nfig.update_xaxes(title_text=\"Nombre d'aiguilles tirées\", row=2, col=1)\n\nfig.update_layout(\n    template=\"none\",\n    xaxis2=dict(showgrid=True, zeroline=True, zerolinewidth=1, range=[0, n_samples]),\n    yaxis2=dict(showgrid=True, zeroline=True, zerolinewidth=1, range=[0, 6]),\n)\n# plot a dash line at y=pi\nfig.add_shape(\n    type=\"line\",\n    y0=np.pi,\n    x0=0,\n    y1=np.pi,\n    x1=n_samples,\n    line=dict(\n        color=\"black\",\n        width=1,\n        dash=\"dashdot\",\n    ),\n    row=2,\n    col=1,\n)\n\n\nfig.data[10 * 3].visible = True\nfig.data[10 * 3 + 1].visible = True\nfig.data[10 * 3 + 2].visible = True\n\n\nsteps = []\nfor i in range(len(fig.data) // 3):\n    step = dict(\n        label=str(i + 4),\n        method=\"update\",\n        args=[\n            {\"visible\": [False] * len(fig.data)},\n            {\n                \"title\": \"Estimation avec \"\n                + str(i + 4)\n                + f\" aiguilles: pi = {pi_estimate[i]:.4f}\"\n            },\n        ],\n    )\n    step[\"args\"][0][\"visible\"][3 * i] = True\n    step[\"args\"][0][\"visible\"][3 * i + 1] = True\n    step[\"args\"][0][\"visible\"][3 * i + 2] = True\n\n    steps.append(step)\n\nslider = dict(\n    active=0,\n    currentvalue={\"prefix\": \"Nombre d'aiguilles: \"},\n    pad={\"t\": 50},\n    y=-0.32,\n    steps=steps,\n)\n\nfig.update_layout(legend=dict(x=0.5, y=1, xanchor='center', yanchor='bottom'))\nfig.update_layout(sliders=[slider])\nfig.show()\n\n\n\n\n                                                \n\n\n\nOn va fournir ici le calcul de la probabilité P. Pour cela on aura besoin de quelques éléments décrits dans le dessin ci-dessous.\n\nx : distance entre le centre de l’aiguille et la ligne de la trame du parquet la plus proche\n\\theta : angle entre l’aiguille et la ligne de la trame du parquet la plus proche\n1 : longueur de l’aiguille (et donc la demi longueur est \\frac{1}{2})\n\\frac{1}{2}\\sin(\\theta) : distance entre l’extrémité de l’aiguille et la ligne de la trame du parquet la plus proche\n\n\n\n\n\n\n\n\nSans croisement\n\n\n\n\n\nAvec croisement\n\n\n\n\n\nFigure 1: Configuration sans croisement (à gauche) et avec croisement (à droite) de l’aiguille avec une ligne de la trame du parquet.\n\n\n\nAvec les éléments ci-dessus, on voit qu’il y a chevauchement si et seulement si: \\frac{1}{2}\\sin(\\theta) \\geq x.\nMaintenant par des arguments de symétrie on voit qu’on peut se restreindre à \\theta \\in [0, \\frac{\\pi}{2}], et à x \\in [0, \\frac{1}{2}]. Les lois de générations des variables aléatoires X et \\Theta sont les suivantes:\n\nX \\sim \\mathcal{U}([0, \\frac{1}{2}]), de densité f_X(x) = 2 {1\\hspace{-3.8pt} 1}_{[0, \\frac{1}{2}]}(x)\n\\Theta \\sim \\mathcal{U}([0, \\frac{\\pi}{2}]) de densité f_\\Theta(\\theta) = \\frac{2}{\\pi} {1\\hspace{-3.8pt} 1}_{[0, \\frac{\\pi}{2}]}(\\theta)\n\nDe plus on suppose que X et \\Theta sont indépendantes.\nMaintenant pour calculer la probabilité P on procède comme suit: \n\\begin{align*}\nP\n& = \\mathbb{P}\\left(\\frac{1}{2}\\sin(\\Theta) \\geq X\\right) \\\\\n& = \\int_{\\mathbb{R}^2} {1\\hspace{-3.8pt} 1}_{\\{\\frac{1}{2}\\sin(\\theta) \\geq x\\}} f_{\\Theta}(\\theta) f_X(x) d\\theta dx  \\quad (\\text{par indépendance})\\\\\n& = \\int_{\\mathbb{R}} \\int_{\\mathbb{R}}\n{1\\hspace{-3.8pt} 1}_{\\{\\frac{1}{2}\\sin(\\theta) \\geq x\\}} \\frac{2}{\\pi} {1\\hspace{-3.8pt} 1}_{[0, \\frac{\\pi}{2}]}(\\theta) \\cdot 2 {1\\hspace{-3.8pt} 1}_{[0, \\frac{1}{2}]}(x) d\\theta dx \\\\\n& = \\int_{0}^{\\frac{\\pi}{2}} \\int_{0}^{\\frac{1}{2}\\sin(\\theta)} \\frac{4}{\\pi} dx  d\\theta \\\\\n& = \\frac{4}{\\pi} \\int_{0}^{\\frac{\\pi}{2}} {\\frac{1}{2}\\sin(\\theta)}  d\\theta\\\\  \n& = \\frac{2}{\\pi} \\Big[ -\\cos(\\theta)\\Big]_{0}^{\\frac{\\pi}{2}} \\\\\n& = \\frac{2}{\\pi} \\enspace.\n\\end{align*}\n\n\n\n\n\n\n\nExercice: rendre le jeu équilibré?\n\n\n\nEn reprenant le même type de raisonnement que ci-dessus, trouver la distance entre les lattes du parquet qui rend le jeu équilibrer entre les deux joueurs introduit par Buffon (l’un pariant sur le fait que l’aiguille croise une ligne de la trame du parquet, l’autre pariant sur le fait que l’aiguille ne croise pas une ligne de la trame du parquet).",
    "crumbs": [
      "Cours",
      "Perspectives historiques"
    ]
  },
  {
    "objectID": "Courses/perspective_historique.html#méthode-de-monte-carlo",
    "href": "Courses/perspective_historique.html#méthode-de-monte-carlo",
    "title": "Perspectives historiques",
    "section": "Méthode de Monte-Carlo",
    "text": "Méthode de Monte-Carlo\nLa méthode de Monte-Carlo, est une méthode de calcul numérique qui consiste à utiliser des nombres aléatoires pour résoudre des problèmes déterministes. Elle est utilisée dans de nombreux domaines, comme la physique, la chimie, la biologie, la finance, ou encore l’apprentissage automatique. Cette méthode basée sur la loi des grands nombres a été mis au point à Los Alamos, dans le cadre du projet Manhattan (dont l’objectif était le développement du nucléaire civil et militaire) par un groupe de scientifiques dont les plus connus sont: John von Neumann2, Nicholas Metropolis3 ou encore Stanisław Ulam4\n2 John von Neumann: (1903-1957) mathématicien et physicien américano-hongrois, un des pères de l’informatique. 3 Nicholas Metropolis: (1915-1999), physicien gréco-américain, est des initiateurs de la méthode de Monte Carlo et du recuit simulé 4 Stanisław Ulam: (1909-1984) Dans le cadre du projet Manhattan, il s’agissait de calculer des intégrales de manière numérique pour modéliser l’évolution de particules, en utilisant des nombres aléatoires.\nEckhardt (1987) donne un bref aperçu historique, et mentionne les premières description de la méthode du rejet et de la méthode de l’inversion dans des lettres entre Von Neumann et Ulam datant de 1947. Ulam aurait une l’idée d’utiliser de telles méthodes pour résoudre le jeu du solitaire lors d’un séjour à l’hôpital en 1946, et éviter ainsi de faire des calculs combinatoires fastidieux. Rapidement, la possibilité d’appliquer cette approche pour des calculs en physique mathématiques (diffusion des neutrons notamment) lui serait apparue prometteuse. Le développement de l’informatique naissante allait permettre une mise en oeuvre pratique de ces idées, et c’est ainsi que la méthode de Monte-Carlo est née. Le nom Monte-Carlo est lui venu du besoin de confidentialité du projet, et provient du nom de la ville de Monte-Carlo, connue pour ses jeux de hasard, où l’oncle de Stanisław Ulam aimait se rendre pour assouvir sa soif de jeu. Ce serait N. Metropolis qui aurait proposé ce nom (cf. Metropolis 1987):\n\nEckhardt, R. 1987. « Stan Ulam, John Von Neumann, and the Monte Carlo Method ». Los Alamos Science, nᵒ 15: 131‑37.\n\nMetropolis, Nicholas. 1987. « The beginning of the Monte Carlo method ». Los Alamos Science, nᵒ 15: 125‑30.\nIt was at that time that I suggested an obvious name for the statistical method—a suggestion not unrelated to the fact that Stan had an uncle who would borrow money from relatives because he “just had to go to Monte Carlo”.",
    "crumbs": [
      "Cours",
      "Perspectives historiques"
    ]
  },
  {
    "objectID": "Courses/perspective_historique.html#autres-méthodes-stochastiques-populaires",
    "href": "Courses/perspective_historique.html#autres-méthodes-stochastiques-populaires",
    "title": "Perspectives historiques",
    "section": "Autres méthodes stochastiques populaires",
    "text": "Autres méthodes stochastiques populaires\n\nMéthode d’Hasting-Metropolis\nL’algorithme de Hasting-Metropolis est une méthode MCMC (🇬🇧: Monte Carlo Markov Chains) dont le but est d’obtenir un échantillonnage aléatoire d’une distribution de probabilité quand l’échantillonnage direct en est difficile (en particulier en grande dimension)\nUn avantage est qu’il ne requiert la connaissance de loi de densité qu’à constante multiplicative près.\n\n\n🇬🇧: Recuit simulé\nLe recuit simulé est une méthode (empirique) d’optimisation, inspirée d’un processus, le recuit, utilisé en métallurgie. On alterne dans cette dernière des cycles de refroidissement lent et de réchauffage (recuit) qui ont pour effet de minimiser l’énergie du matériau. Cette méthode est transposée en optimisation pour trouver les extrema d’une fonction.",
    "crumbs": [
      "Cours",
      "Perspectives historiques"
    ]
  },
  {
    "objectID": "Courses/slides.html",
    "href": "Courses/slides.html",
    "title": "Slides: menu principal",
    "section": "",
    "text": "Vous trouverez ci-dessous la listes des slides associés:\nCours introduction, plein écran\n\nCours: notations premiers pas, plein écran\n\nCours: théorème asymptotiques, plein écran\n\nCours: simulation, méthodes classiques\n\n\n\n\n Retour au sommet",
    "crumbs": [
      "Slides",
      "Slides: menu principal"
    ]
  },
  {
    "objectID": "Slides/slides_intro.html#section",
    "href": "Slides/slides_intro.html#section",
    "title": "Introduction",
    "section": "",
    "text": "PS: n’oubliez pas de mettre [HAX603X] dans le titre de vos mails!",
    "crumbs": [
      "Slides",
      "Introduction"
    ]
  },
  {
    "objectID": "Slides/slides_intro.html#enseignants",
    "href": "Slides/slides_intro.html#enseignants",
    "title": "Introduction",
    "section": "Enseignants",
    "text": "Enseignants\n\n\nJoseph Salmon : CM et TP\n\nSituation actuelle : Professeur à l’Université de Montpellier\nPrécédemment : Paris Diderot-Paris 7, Duke Univ., Télécom ParisTech, Univ. Washington\nSpécialités : statistiques, optimisation, traitement des images, sciences participatives\nBureau : 415, Bat. 9\n\n\n\n\n\n\n\n\nBenjamin Charlier : CM, TD et TP\n\nSituation actuelle : Maître de conférences à l’Université de Montpellier\nPrécédemment : Université Paul Sabatier, ENS Paris-Saclay\nSpécialités : traitement des images, statistiques, différentiation automatique\nBureau : 423, Bat. 9",
    "crumbs": [
      "Slides",
      "Introduction"
    ]
  },
  {
    "objectID": "Slides/slides_intro.html#ressources-en-ligne",
    "href": "Slides/slides_intro.html#ressources-en-ligne",
    "title": "Introduction",
    "section": "Ressources en ligne",
    "text": "Ressources en ligne\n\nInformations principales : site du cours http://josephsalmon.github.io/HAX603X\n\n\n\nSyllabus\nCours (détaillé: site web)\nSlides (résumé)\nFeuilles de TD\nFeuilles de TP\nRendu TP : Moodle de l’université (https://moodle.umontpellier.fr/course/view.php?id=5558)",
    "crumbs": [
      "Slides",
      "Introduction"
    ]
  },
  {
    "objectID": "Slides/slides_intro.html#validation",
    "href": "Slides/slides_intro.html#validation",
    "title": "Introduction",
    "section": "Validation",
    "text": "Validation\n\nTP notés : Rendu = fichier Python .py unique\n\nTP noté 1 : rendre en fin de session (en S11)\nTP noté 2 : rendre en fin de session (en S17)\n\nCC : devoir sur table d’une heure (S18)\n\n\n\nCoefficients:\n\nNote Session 1 : (40% CC + 30% TP 1 + 30% TP 2)\nNote Session 2 : (30% CC + 35% TP 1 + 35% TP 2)\n\n\n\n\n\n\n\n\nImportant\n\n\nLe rendu est individuel pour le TP noté !!!",
    "crumbs": [
      "Slides",
      "Introduction"
    ]
  },
  {
    "objectID": "Slides/slides_intro.html#notation-pour-les-tps",
    "href": "Slides/slides_intro.html#notation-pour-les-tps",
    "title": "Introduction",
    "section": "Notation pour les TPs",
    "text": "Notation pour les TPs\nRendu : sur Moodle, en déposant un fichier nom_prenom.py dans le dossier adéquat.\nDétails de la notation des TPs :\n\nQualité des réponses aux questions\nQualité de rédaction et d’orthographe\nQualité des graphiques (légendes, couleurs)\nQualité du code (noms de variables, clairs, commentaires utiles, code synthétique, etc.)\nCode reproductible et absence de bug\n\n\n\n\n\n\n\n\nPénalités\n\n\n\nEnvoi par mail : zéro\nRetard : zéro (uploader avant la fin, fermeture automatique de moodle)",
    "crumbs": [
      "Slides",
      "Introduction"
    ]
  },
  {
    "objectID": "Slides/slides_intro.html#prérequis---à-revoir-seul",
    "href": "Slides/slides_intro.html#prérequis---à-revoir-seul",
    "title": "Introduction",
    "section": "Prérequis - à revoir seul",
    "text": "Prérequis - à revoir seul\n\n \n\nBases de probabilités (en particulier “HAX506X- Théorie des Probabilités”): probabilité, densité, espérance, fonction de répartition, mesure, intégration, analyse numérique élémentaire, etc. (Foata et Fuchs 1996; Barbe et Ledoux 2006; Ouvrard 2007, 2008)\n\n\n\nProgrammation élémentaire (en Python): if … then… else …, for, while, fonctions, etc. HLMA310 - Logiciels scientifiques, (Courant et al. 2013), Cours de Python: Univ. Paris Diderot\n\n\n\nPour aller plus loin: conditionnement, martingales (Williams 1991)",
    "crumbs": [
      "Slides",
      "Introduction"
    ]
  },
  {
    "objectID": "Slides/slides_intro.html#description-du-cours",
    "href": "Slides/slides_intro.html#description-du-cours",
    "title": "Introduction",
    "section": "Description du cours",
    "text": "Description du cours\n\n\nGénérer l’aléa\n\ngénérateurs pseudo-aléatoires, simulations de variables aléatoires (inverse, rejet, etc.)\nillustrations numériques et visualisation en Python (loi des grands nombres, TCL)\n\nMéthode de Monte-Carlo\n\nméthode de Monte-Carlo pour le calcul approché d’une intégrale\nréduction de la variance : variables antithétiques, variables de contrôle, etc.\n\nCompléments\n\nvecteurs gaussiens et lien avec les lois usuelles de la statistique inférentielle (student, chi2)\nconstruction d’intervalles de confiance.\nmarche aléatoire simple, etc.",
    "crumbs": [
      "Slides",
      "Introduction"
    ]
  },
  {
    "objectID": "Slides/slides_intro.html#buffon-et-les-prémisses-de-la-méthode-de-monte-carlo",
    "href": "Slides/slides_intro.html#buffon-et-les-prémisses-de-la-méthode-de-monte-carlo",
    "title": "Introduction",
    "section": "Buffon et les prémisses de la méthode de Monte-Carlo",
    "text": "Buffon et les prémisses de la méthode de Monte-Carlo\n\n\n\n\n1733: l’aiguille de Buffon, méthode d’estimation de la valeur de \\(\\pi\\).\n\n\n\n\nProblème initial: une aiguille de taille 1 tombe sur un parquet composé de lattes de largeur \\(1\\): quelle est alors la probabilité \\(P\\) que l’aiguille croise une ligne de la trame du parquet ?\n\n\n\n\n\n\n                                                \n\n\n\n\n\n\nGeorges-Louis Leclerc, Comte de Buffon(1707-1788) : naturaliste, mathématicien et industriel français du siècle des Lumières",
    "crumbs": [
      "Slides",
      "Introduction"
    ]
  },
  {
    "objectID": "Slides/slides_intro.html#laiguille-de-buffon-suite",
    "href": "Slides/slides_intro.html#laiguille-de-buffon-suite",
    "title": "Introduction",
    "section": "L’aiguille de Buffon (suite)",
    "text": "L’aiguille de Buffon (suite)\n\nProblème initial: une aiguille de taille 1 tombe sur un parquet composé de lattes de largeur \\(1\\): quelle est alors la probabilité \\(P\\) que l’aiguille croise une ligne de la trame du parquet ?\n\n\n\nRéponse: \\[\nP = \\frac{2}{\\pi} \\approx 0.6366 \\enspace.\n\\] Une preuve de ce résultat est donnée ici.",
    "crumbs": [
      "Slides",
      "Introduction"
    ]
  },
  {
    "objectID": "Slides/slides_intro.html#principe-de-monte-carlo-et-estimation",
    "href": "Slides/slides_intro.html#principe-de-monte-carlo-et-estimation",
    "title": "Introduction",
    "section": "Principe de Monte Carlo et estimation",
    "text": "Principe de Monte Carlo et estimation\nIdée sous-jacente de Buffon :\nsi l’on répète cette expérience un grand nombre de fois, on peut approché la quantité \\(P\\) numériquement, par exemple en proposant un estimateur \\(\\hat{P}_n\\) qui compte la proportion de chevauchement après avoir fait \\(n\\) répétition des lancers.\n Estimation de \\(\\pi\\):\n\\[\n\\pi \\approx \\frac{2}{\\hat{P}_n}\n\\]",
    "crumbs": [
      "Slides",
      "Introduction"
    ]
  },
  {
    "objectID": "Slides/slides_intro.html#principe-de-monte-carlo-pour-lestimation-suite",
    "href": "Slides/slides_intro.html#principe-de-monte-carlo-pour-lestimation-suite",
    "title": "Introduction",
    "section": "Principe de Monte Carlo pour l’estimation (suite)",
    "text": "Principe de Monte Carlo pour l’estimation (suite)",
    "crumbs": [
      "Slides",
      "Introduction"
    ]
  },
  {
    "objectID": "Slides/slides_intro.html#méthode-de-monte-carlo",
    "href": "Slides/slides_intro.html#méthode-de-monte-carlo",
    "title": "Introduction",
    "section": "Méthode de Monte-Carlo",
    "text": "Méthode de Monte-Carlo\nMéthode de calcul numérique qui consiste à utiliser des nombres aléatoires pour résoudre des problèmes déterministes.\n\nDomaines d’applications:\n\nla physique\nla chimie\nla biologie\nla finance\nl’apprentissage automatique",
    "crumbs": [
      "Slides",
      "Introduction"
    ]
  },
  {
    "objectID": "Slides/slides_intro.html#contexte-de-la-naissance-de-la-méthode-de-monte-carlo",
    "href": "Slides/slides_intro.html#contexte-de-la-naissance-de-la-méthode-de-monte-carlo",
    "title": "Introduction",
    "section": "Contexte de la naissance de la méthode de Monte Carlo",
    "text": "Contexte de la naissance de la méthode de Monte Carlo\n\n\n\n\nLieu: Los Alamos\nÉpoque: seconde guerre mondial\nContexte: Projet Manathan, produire une bombe atomique\nBesoins: modéliser les réactions nucléaires en chaîne (combinatoires)\n\n\n\n\n\n\n\n\nJohn von Neumann (1903-1957), mathématicien et physicien américano-hongrois, un des pères de l’informatique.\n\n\n\n\n\n\n\nNicholas Metropolis (1915-1999), physicien gréco-américain, un des initiateurs de la méthode de Monte Carlo et du recuit simulé\n\n\n\n\n\n\n\nStanisław Ulam (1909-1984), mathématicien polono-américainm, un des initiateurs de la méthode de Monte Carlo et de la propulsion nucléaire pulsée\n\n\n\n\n\n\n\n\n\n\nExplosion de Trinity (16 Juillet 1945)",
    "crumbs": [
      "Slides",
      "Introduction"
    ]
  },
  {
    "objectID": "Slides/slides_intro.html#lorigine-du-nom-monte-carlo",
    "href": "Slides/slides_intro.html#lorigine-du-nom-monte-carlo",
    "title": "Introduction",
    "section": "L’origine du nom “Monte-Carlo”",
    "text": "L’origine du nom “Monte-Carlo”\nInitialement: besoin de confidentialité du projet Manhattan\n\nMonte-Carlo: connue pour ses jeux de hasard, où l’oncle de Stanisław Ulam aimait se rendre pour assouvir sa soif de jeu.\n Ce serait N. Metropolis qui aurait proposé ce nom, cf. (Metropolis 1987):\nIt was at that time that I suggested an obvious name for the statistical method—a suggestion not unrelated to the fact that Stan had an uncle who would borrow money from relatives because he “just had to go to Monte Carlo”.",
    "crumbs": [
      "Slides",
      "Introduction"
    ]
  },
  {
    "objectID": "Slides/slides_intro.html#essor-de-la-méthode-de-monte-carlo",
    "href": "Slides/slides_intro.html#essor-de-la-méthode-de-monte-carlo",
    "title": "Introduction",
    "section": "Essor de la méthode de Monte Carlo",
    "text": "Essor de la méthode de Monte Carlo\n\n\n\n\nPopularisation croissante:\n\nEssor de l’informatique (depuis les années 80)\nEssor des méthodes de calcul parallèle (GPUs, clusters, etc.)\n\n\n\n\n\nDomaine principaux impactés:\n\nfinance : évaluation des prix de produits dérivés\napprentissage automatique: utilisation de l’aléatoire pour généré des scénarios\nExemples: Alphago (2016), AlphaGeometry (2024)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRecherche arborescente Monte-Carlo (🇬🇧: Monte Carlo tree search): analyse des scénarios les plus prometteurs, en élargissant l’arbre de recherche sur la base d’un échantillonnage aléatoire de l’espace entier (ingrédient important d’AlphaGo)",
    "crumbs": [
      "Slides",
      "Introduction"
    ]
  },
  {
    "objectID": "Slides/slides_intro.html#bibliographie",
    "href": "Slides/slides_intro.html#bibliographie",
    "title": "Introduction",
    "section": "Bibliographie",
    "text": "Bibliographie\n\n\nBarbe, Philippe, et Michel Ledoux. 2006. « Cours de Probabilite Pour la Licence (corrigée) ».\n\n\nCourant, J., M. de Falco, S. Gonnord, J.-C. Filliâtre, S. Conchon, G. Dowek, et B. Wack. 2013. Informatique pour tous en classes préparatoires aux grandes écoles: Manuel d’algorithmique et programmation structurée avec Python. Eyrolles.\n\n\nFoata, D., et A. Fuchs. 1996. Calcul des probabilités: cours et exercices corrigés. Masson.\n\n\nMetropolis, Nicholas. 1987. « The beginning of the Monte Carlo method ». Los Alamos Science, nᵒ 15: 125‑30.\n\n\nOuvrard, J.-Y. 2007. Probabilités : Tome 2, Licence - CAPES. 2ᵉ éd. Enseignement des mathématiques. Cassini.\n\n\n———. 2008. Probabilités : Tome 1, Licence - CAPES. 2ᵉ éd. Enseignement des mathématiques. Cassini.\n\n\nWilliams, D. 1991. Probability with martingales. Cambridge Mathematical Textbooks. Cambridge: Cambridge University Press.\n\n\n\n\nIntroduction",
    "crumbs": [
      "Slides",
      "Introduction"
    ]
  },
  {
    "objectID": "Slides/slides_notations_premiers_pas.html#notation-et-rappels-de-probabilités",
    "href": "Slides/slides_notations_premiers_pas.html#notation-et-rappels-de-probabilités",
    "title": "Notations et premiers pas",
    "section": "Notation et rappels de probabilités",
    "text": "Notation et rappels de probabilités\n\n\nEspace probabilisé: \\((\\Omega, {\\mathcal{F}}, \\mathbb{P})\\)\n\ncomposé d’un ensemble: \\(\\Omega\\)\nd’une tribu: \\(\\mathcal{F}\\)\nd’une mesure de probabilité: \\(\\mathbb{P}\\)\n\n\n\n\n\nDéfinition 1 (Variable aléatoire, v.a.) Soit \\((E, \\mathcal{E})\\) un espace mesurable. Une variable aléatoire est une application mesurable \\[\n    \\begin{array}{ccccc}\n        X & : & \\Omega & \\to     & E            \\\\\n            &   & \\omega & \\mapsto & X(\\omega)\\,.\n    \\end{array}\n\\] Ainsi \\(\\{\\omega \\in \\Omega : X(\\omega) \\in B\\} = X^{-1}(B) = \\{X \\in B\\} \\in \\mathcal{F}, \\forall B \\in \\mathcal{E}\\)",
    "crumbs": [
      "Slides",
      "Notations et premiers pas"
    ]
  },
  {
    "objectID": "Slides/slides_notations_premiers_pas.html#loi-dune-variable-aléatoire-unidimensionnelle",
    "href": "Slides/slides_notations_premiers_pas.html#loi-dune-variable-aléatoire-unidimensionnelle",
    "title": "Notations et premiers pas",
    "section": "Loi d’une variable aléatoire (unidimensionnelle)",
    "text": "Loi d’une variable aléatoire (unidimensionnelle)\n\nDéfinition 2 (Loi d’une variable aléatoire) \nSoit \\(X : (\\Omega, \\mathcal{F}, \\mathbb{P}) \\to (E, \\mathcal{E})\\) une v.a. On appelle loi de \\(X\\) la mesure de probabilité sur \\((E, \\mathcal{E})\\) définie par \\[\n        \\begin{array}{ccccc}\n            \\mathbb{P}_X & : & \\mathcal{E} & \\to     & [0,1]          \\\\\n                 &   & B           & \\mapsto & \\mathbb{P}(X \\in B) \\enspace.\n        \\end{array}\n\\]\n\n\n\n\n\n\n\n\nNote\n\n\nLes propriétés de \\(\\mathbb{P}\\) assurent que \\(\\mathbb{P}_X\\) est bien une mesure de probabilité sur l’espace mesurable \\((E, \\mathcal{E})\\)",
    "crumbs": [
      "Slides",
      "Notations et premiers pas"
    ]
  },
  {
    "objectID": "Slides/slides_notations_premiers_pas.html#lois-discrètes",
    "href": "Slides/slides_notations_premiers_pas.html#lois-discrètes",
    "title": "Notations et premiers pas",
    "section": "Lois discrètes",
    "text": "Lois discrètes\nLes variables aléatoires discrètes sont celles à valeurs dans un ensemble \\(E\\) discret, le plus souvent \\(\\mathbb{N}\\), muni de la tribu pleine \\(\\mathcal{F} = \\mathcal{P}(E)\\).\n\nExemple 1 (Loi de Bernoulli) Soit un paramètre \\(p \\in [0,1]\\), et \\(E=\\{0,1\\}\\), alors la loi de Bernouilli est donnée par \\(\\mathbb{P}(X=1)=1-\\mathbb{P}(X=0) = p\\).\nNotation: \\(\\quad\\) \\(X \\sim \\mathcal{B}(p)\\)\n\nExemple physique:  loi d’un tirage de pile ou face, de biais \\(p\\)\n\nExemple 2 (Loi binomiale) Soient \\(p \\in [0,1]\\) (biais) et \\(n \\in \\mathbb{N}^*\\) (nombre de tirages) alors la loi Binomiale est donnée par \\(\\mathbb{P}(X=k) = \\binom{n}{k} p^k (1-p)^{n-k}\\), pour \\(k \\in E=\\{0,\\dots,n\\}\\).\nNotation: \\(\\quad\\) \\(X \\sim \\mathcal{B}(n,p)\\)\n\nExemple physique:      loi du nombre de succès obtenus lors de \\(n\\) répétitions indépendantes d’une expérience aléatoire de Bernoulli de paramètre \\(p\\)",
    "crumbs": [
      "Slides",
      "Notations et premiers pas"
    ]
  },
  {
    "objectID": "Slides/slides_notations_premiers_pas.html#lois-discrètes-ii",
    "href": "Slides/slides_notations_premiers_pas.html#lois-discrètes-ii",
    "title": "Notations et premiers pas",
    "section": "Lois discrètes (II)",
    "text": "Lois discrètes (II)\n\nExemple 3 (Loi géométrique) Soient \\(p \\in [0,1]\\) (biais), alors la loi géométrique est donnée par \\(\\mathbb{P}(X=k) = p (1-p)^{k-1}\\), pour \\(k \\in E=\\mathbb{N}^*\\).\nNotation: \\(\\quad\\) \\(X \\sim \\mathcal{G}(p)\\)\n\n\nExemple physique:     \n\nloi du nombre tirage nécessaire avant d’obtenir un succès obtenus en répétant indépendamment des expériences aléatoires de Bernoulli de paramètre \\(p\\)\n\n\n\n\nExemple 4 (Loi de Poisson) Pour \\(\\lambda&gt;0\\), la loi de Poisson de paramètre \\(\\lambda\\) est définie par \\(\\mathbb{P}(X=k) = e^{-\\lambda} \\lambda^k / k!\\), pour tout \\(k \\in E=\\mathbb{N}\\).\nNotation: \\(\\quad\\) \\(X \\sim \\mathcal{P}(\\lambda)\\)\n\nExemple physique: comportement du nombre d’événements se produisant avec une fréquence connue, et indépendamment du temps écoulé depuis l’événement précédent (e.g., nombre de clients dans une file d’attente, nombre de mutations dans un gène, etc.)",
    "crumbs": [
      "Slides",
      "Notations et premiers pas"
    ]
  },
  {
    "objectID": "Slides/slides_notations_premiers_pas.html#lois-continues",
    "href": "Slides/slides_notations_premiers_pas.html#lois-continues",
    "title": "Notations et premiers pas",
    "section": "Lois continues",
    "text": "Lois continues\nLoi d’une v.a. admettant une fonction de densité, c’est-à-dire qu’il existe une fonction mesurable \\(f : \\mathbb{R} \\to [0, \\infty[\\) d’intégrale \\(1\\), telle que pour tout \\(A \\in \\mathcal{B}(\\mathbb{R})\\) \\[\n    \\mathbb{P}(X \\in A) = \\int_A f(x) dx \\enspace.\n\\]\n\n\n\n\n\n\nNote\n\n\nLes propriétés de l’intégrale de Lebesgue assurent que cette formule définit bien une loi de probabilité.\n\n\n\n\nEspérance: \\(\\mathbb{E}(X) = \\displaystyle\\int_{\\mathbb{R}} x f(x) dx\\)\nVariance: \\(\\mathbb{V}(X) = \\mathbb{E}((X-\\mathbb{E}(X))^2) = \\displaystyle\\int_{\\mathbb{R}} (x-\\mathbb{E}(X))^2 f(x) dx\\)",
    "crumbs": [
      "Slides",
      "Notations et premiers pas"
    ]
  },
  {
    "objectID": "Slides/slides_notations_premiers_pas.html#lois-continues-usuelles",
    "href": "Slides/slides_notations_premiers_pas.html#lois-continues-usuelles",
    "title": "Notations et premiers pas",
    "section": "Lois continues usuelles",
    "text": "Lois continues usuelles\n\nExemple 5 (Loi uniforme) La loi uniforme sur un ensemble \\(B \\in \\mathcal{B}(\\mathbb{R})\\), s’obtient avec la densité définie par \\[\nf(x) = {1\\hspace{-3.8pt} 1}_B(x) / \\lambda (B) \\enspace,\n\\] où \\(\\lambda (B)\\) représente la mesure de Lebesgue de l’ensemble \\(B\\).\n\nCas particulier: pour la loi uniforme sur \\([0,1]\\), on obtient la fonction suivante: \\[\nf(x) = {1\\hspace{-3.8pt} 1}_{[0,1]}(x)\\enspace.\n\\] Notation: \\(\\quad\\) \\(X \\sim \\mathcal{U}([0,1])\\)\n\n\n\n\n\n\nNote\n\n\nUne telle loi est caractérisée ainsi : tous les intervalles de même longueur inclus dans le support de la loi ont la même probabilité.",
    "crumbs": [
      "Slides",
      "Notations et premiers pas"
    ]
  },
  {
    "objectID": "Slides/slides_notations_premiers_pas.html#lois-continues-usuelles-ii",
    "href": "Slides/slides_notations_premiers_pas.html#lois-continues-usuelles-ii",
    "title": "Notations et premiers pas",
    "section": "Lois continues usuelles (II)",
    "text": "Lois continues usuelles (II)\n\n\nExemple 6 (Loi exponentielle) La loi exponentielle de paramètre \\(\\gamma &gt; 0\\) est obtenue avec la densité donnée par \\[\nf(x) = \\gamma e^{-\\gamma x} {1\\hspace{-3.8pt} 1}_{\\mathbb{R}_+}(x)\\enspace.\n\\] Notation: \\(\\quad X \\sim \\mathcal{Exp}(\\gamma)\\)\n\n\n\n\nProposition 1 (Absence de mémoire) La loi exponentielle modélise la durée de vie d’un phénomène sans mémoire (ou sans vieillissement), c’est-à-dire que pour tout \\(s,t&gt;0\\), on a \\[\n\\mathbb{P}(X&gt;t+s | X&gt;t)=\\mathbb{P}(X&gt;s) \\enspace.\n\\]",
    "crumbs": [
      "Slides",
      "Notations et premiers pas"
    ]
  },
  {
    "objectID": "Slides/slides_notations_premiers_pas.html#lois-continues-usuelles-iii",
    "href": "Slides/slides_notations_premiers_pas.html#lois-continues-usuelles-iii",
    "title": "Notations et premiers pas",
    "section": "Lois continues usuelles (III)",
    "text": "Lois continues usuelles (III)\n\nExemple 7 (Loi normale/gaussienne univariée) Pour des paramètres \\(\\mu \\in \\mathbb{R}\\) (espérance) et \\(\\sigma^2 &gt; 0\\) (variance), la loi normale associée correspond à la fonction de densité : \\[\nf(x) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}}e^{-\\frac{1}{2 \\sigma^2}(x-\\mu)^2} \\enspace.\n\\] Notation: \\(X \\sim \\mathcal{N}(\\mu,\\sigma^2)\\),\n\nOn nomme loi normale centrée réduite le cas canonique: \\(\\mu = 0, \\sigma^2 = 1\\).\nSi \\(X\\sim \\mathcal{N}(\\mu,\\sigma^2)\\), alors l’espérance et la variance de \\(X\\) valent \\(\\mathbb{E}(X) = \\mu\\) et \\(\\mathbb{V}(X) = \\sigma^2\\).\n\n\n\n\n\n\n\nNote\n\n\nLes lois normales sont omniprésente grâce au théorème central limite.",
    "crumbs": [
      "Slides",
      "Notations et premiers pas"
    ]
  },
  {
    "objectID": "Slides/slides_notations_premiers_pas.html#enjeu-de-la-fonction-de-répartition",
    "href": "Slides/slides_notations_premiers_pas.html#enjeu-de-la-fonction-de-répartition",
    "title": "Notations et premiers pas",
    "section": "Enjeu de la fonction de répartition",
    "text": "Enjeu de la fonction de répartition\n\nEnjeux: caractériser la loi d’une v.a. en ne considérant que l’espace d’arrivée \\((E, \\mathcal{E})\\)\n\n\nOutils:\n\n\nla fonction de répartition (v.a. réelles),\nla fonction caractéristique (v.a. dans \\(\\mathbb{R}^d\\)), en gros la transformée de Fourier de la loi!\nla fonction génératrice des moments (v.a. discrètes)\netc.",
    "crumbs": [
      "Slides",
      "Notations et premiers pas"
    ]
  },
  {
    "objectID": "Slides/slides_notations_premiers_pas.html#fonction-de-répartition",
    "href": "Slides/slides_notations_premiers_pas.html#fonction-de-répartition",
    "title": "Notations et premiers pas",
    "section": "Fonction de répartition",
    "text": "Fonction de répartition\n\nDéfinition 3 (Fonction de répartition 🇬🇧: cumulative distribution function) \nSoit \\(X\\) une variable aléatoire sur \\((\\mathbb{R}, \\mathcal{B}(\\mathbb{R}))\\). La fonction de répartition de \\(X\\) est la fonction \\(F_X\\) définie sur \\(\\mathbb{R}\\) par \\[\n\\begin{align*}\n     F_X(x) & = \\mathbb{P}(X \\leq x)\\\\\n     & \\class{fragment}{{} = \\mathbb{P}(X \\in ]-\\infty, x])}\n\\end{align*}\n\\]",
    "crumbs": [
      "Slides",
      "Notations et premiers pas"
    ]
  },
  {
    "objectID": "Slides/slides_notations_premiers_pas.html#propriété-élémentaire-de-la-fonction-de-répartition",
    "href": "Slides/slides_notations_premiers_pas.html#propriété-élémentaire-de-la-fonction-de-répartition",
    "title": "Notations et premiers pas",
    "section": "Propriété élémentaire de la fonction de répartition",
    "text": "Propriété élémentaire de la fonction de répartition\n\nProposition 2 (Propriétés élémentaires) Soit \\(X\\) une v.a. de fonction de répartition \\(F_X\\).\n\n\n\\(F_X\\) est une fonction croissante, de limite \\(0\\) en \\(-\\infty\\) et de limite \\(1\\) en \\(+\\infty\\).\n\\(F_X\\) est continue à droite en tout point.\n\\(\\forall x \\in \\mathbb{R}\\), on a \\(\\mathbb{P}(X=x) = F_X(x) - \\lim_{\\epsilon \\to 0+}F_X(x- \\epsilon)\\).\nSi \\(X\\) a pour densité \\(f\\), alors \\(F_X\\) est dérivable \\(\\lambda\\)-presque partout de dérivée \\(f\\).\n\n\n\n\nDémonstration: voir par exemple (Barbe et Ledoux 2006)\n\n\n\n\n\n\n\n\nNote\n\n\n\nProp. 1. et 2. : \\(F_X\\) est càdlàg (continue à droite, limite à gauche).\nProp 3. (cas discret): les valeurs prises par \\(X\\) correspondent aux discontinuités de \\(F_X\\), les probabilités, à la hauteur du saut.\nProp. 4. (cas continu): le lien entre la fonction de répartition et densité.",
    "crumbs": [
      "Slides",
      "Notations et premiers pas"
    ]
  },
  {
    "objectID": "Slides/slides_notations_premiers_pas.html#fonction-de-répartition-et-caractérisation-de-la-loi",
    "href": "Slides/slides_notations_premiers_pas.html#fonction-de-répartition-et-caractérisation-de-la-loi",
    "title": "Notations et premiers pas",
    "section": "Fonction de répartition et caractérisation de la loi",
    "text": "Fonction de répartition et caractérisation de la loi\n\nThéorème 1 (Caractérisation de la loi d’une variable aléatoire réelle) \nLa fonction de répartition d’une variable aléatoire caractérise sa loi : deux variables aléatoires ont même loi si et seulement si elles ont même fonction de répartition.\n\n\n\nDémonstration: voir Wikipedia \nRappel: la tribu des boréliens est engendrée par la famille d’ensembles \\(\\{]-\\infty,x], x \\in \\mathbb{R}\\}\\) \nInterprétation: connaître \\(\\mathbb{P}_X\\) sur cette famille d’ensembles \\(\\implies\\) la connaître partout",
    "crumbs": [
      "Slides",
      "Notations et premiers pas"
    ]
  },
  {
    "objectID": "Slides/slides_notations_premiers_pas.html#fonction-de-répartition-cas-discret",
    "href": "Slides/slides_notations_premiers_pas.html#fonction-de-répartition-cas-discret",
    "title": "Notations et premiers pas",
    "section": "Fonction de répartition: cas discret",
    "text": "Fonction de répartition: cas discret\n Dans le cas d’une loi discrète, la fonction de répartition est une fonction en escalier, constante par morceaux, et croissante.   \n\nExemple 8 (Cas discret) Soit \\((x_i)_{i \\in I}\\) une suite ordonnée de réels, avec \\(I \\subset \\mathbb{N}\\). Si \\(X\\) est une variable aléatoire discrète prenant les valeurs \\((x_i)_{i \\in I}\\) et de loi \\((p_i = \\mathbb{P}(X=x_i))_{i \\in I}\\), alors \\[\n\\forall x \\in \\mathbb{R}, \\quad F_X(x) = \\sum_{i \\in I} p_i {1\\hspace{-3.8pt} 1}_{[x_i, \\infty[}(x) \\enspace.\n\\]",
    "crumbs": [
      "Slides",
      "Notations et premiers pas"
    ]
  },
  {
    "objectID": "Slides/slides_notations_premiers_pas.html#fonction-de-répartition-cas-continu",
    "href": "Slides/slides_notations_premiers_pas.html#fonction-de-répartition-cas-continu",
    "title": "Notations et premiers pas",
    "section": "Fonction de répartition: cas continu",
    "text": "Fonction de répartition: cas continu\n\n\n\n\nExemple 9 (Cas continu) Si \\(X\\) est une variable aléatoire de densité \\(f\\), alors \\[\n\\forall x \\in \\mathbb{R}, \\quad F_X(x) = \\int_{-\\infty}^x f(t) \\, \\mathrm dt \\enspace.\n\\]\n\nVocabulaire: densité (🇬🇧: probability density function)",
    "crumbs": [
      "Slides",
      "Notations et premiers pas"
    ]
  },
  {
    "objectID": "Slides/slides_notations_premiers_pas.html#loi-normale",
    "href": "Slides/slides_notations_premiers_pas.html#loi-normale",
    "title": "Notations et premiers pas",
    "section": "Loi normale",
    "text": "Loi normale\nCas de la loi normale centrée réduite, \\(X \\sim \\mathcal{N}(0,1)\\): \\(F_X=\\Phi\\), avec \\(\\Phi\\) définie par \\[\nF_X(x) \\triangleq \\Phi(x) = \\dfrac{1}{\\sqrt{2\\pi}} \\int_{-\\infty}^x e^{-\\frac{t^2}{2}}\\, \\mathrm d t\\enspace,\n\\]\n\n\n\n\n\n\nNote\n\n\nL’intégrale ne peut être obtenue à partir d’une formule fermée1. Autrefois, les valeurs de \\(\\Phi(x)\\) étaient reportées dans des tables2.\n\n\n\n\n\nTransformation affine: si \\(X \\sim \\mathcal{N}(\\mu, \\sigma^2)\\) — i.e., \\(X=\\mu + \\sigma Y\\), avec \\(Y\\sim \\mathcal{N}(0,1)\\) — alors \\[\nF_X(x)=\\Phi\\left(\\frac{x-\\mu}{\\sigma}\\right)\n\\]\n\n\nWikipedia: Théorème de LiouvilleWikipedia: loi normale",
    "crumbs": [
      "Slides",
      "Notations et premiers pas"
    ]
  },
  {
    "objectID": "Slides/slides_notations_premiers_pas.html#fonction-quantile-inverse-généralisée-à-gauche",
    "href": "Slides/slides_notations_premiers_pas.html#fonction-quantile-inverse-généralisée-à-gauche",
    "title": "Notations et premiers pas",
    "section": "Fonction quantile, inverse généralisée à gauche",
    "text": "Fonction quantile, inverse généralisée à gauche\n\nDéfinition 4 (Fonction quantile/ inverse généralisée 🇬🇧: quantile distribution function) \nSoit \\(X\\) une variable aléatoire sur \\((\\mathbb{R}, \\mathcal{B}(\\mathbb{R}))\\) et \\(F_X\\) sa fonction de répartion. La fonction quantile associée \\(F_X^\\leftarrow: ]0,1[ \\rightarrow \\mathbb{R}\\) est définie par \\[\n  F_X^\\leftarrow(p) = \\inf\\{ x \\in \\mathbb{R} : F_X(x)\\geq p\\} \\enspace.\n\\]\n\n\n\\(F_X\\) est bijective \\(\\implies\\) \\(F^{-1}=F_X^\\leftarrow\\)\n\nVocabulaire:\n\nla fonction quantile s’appelle aussi inverse de Levy ou inverse généralisée (à gauche)\nmédiane : \\(F_X^\\leftarrow(1/2)\\)\npremier/troisième quartile: \\(F_X^\\leftarrow(1/4), F_X^\\leftarrow(3/4)\\)\ndéciles : \\(F_X^\\leftarrow(k/10)\\) pour \\(k=1,\\dots, 9\\)",
    "crumbs": [
      "Slides",
      "Notations et premiers pas"
    ]
  },
  {
    "objectID": "Slides/slides_notations_premiers_pas.html#quantiles-cas-continu",
    "href": "Slides/slides_notations_premiers_pas.html#quantiles-cas-continu",
    "title": "Notations et premiers pas",
    "section": "Quantiles: cas continu",
    "text": "Quantiles: cas continu\n\n\n\nPlotly = require('plotly.js-dist');\ndists = require( 'https://cdn.jsdelivr.net/gh/stdlib-js/stats-base-dists@umd/browser.js' );\n// see source here: https://github.com/stdlib-js/stats-base-dists/tree/umd\n// continuous case\njstatPDFs = () =&gt; {\n  const distributions = Object.keys(dists);\n  // Get in continuousDistributions the distributions whose pdf, cdf and quantile are defined\n    const continuousDistributions = distributions.filter(name =&gt; dists[name].pdf && dists[name].cdf && dists[name].quantile);\n  return continuousDistributions\n};\noutput = jstatPDFs();\n\nexcludedPDFs = [];\npdfNames_unsorted = output.filter(name =&gt; !excludedPDFs.includes(name));\npdfNames=pdfNames_unsorted.toSorted();\nviewof inputs = Inputs.form([\n      Inputs.range([-10, 10], {value: 0.1, step: 0.001, label: tex`\\mu`, width: 500}),\n      Inputs.range([0.01, 5], {value: 1.01, step: 0.001, label: tex`\\sigma`, width: 500}),\n      Inputs.range([0.001, 0.999], {value: 0.75, step: 0.001, label:tex`\\alpha`, width: 500}),\n    ]);\n\n\nviewof distrib_name = Inputs.select(pdfNames, {value: \"normal\", label: \"Loi\", width: 500});\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n{\n  const x = d3.range(-5, 5, 0.01);\n  const pdf = x.map(x =&gt; dists[distrib_name].pdf(x, mu, sigma));\n  const cdf = x.map(x =&gt; dists[distrib_name].cdf(x, mu, sigma));\n  const inv = x.map(x =&gt; dists[distrib_name].quantile(x, mu, sigma));\n  const quantile = dists[distrib_name].quantile(alpha, mu, sigma);\n  const filteredX = x.filter(coord =&gt; coord &lt; quantile);\n  const filteredPdf = pdf.filter((_, i) =&gt; x[i] &lt;= quantile);\n  const filteredCdf = cdf.filter(coord =&gt; coord &lt;= quantile);\n\n\n{\nvar trace1 = {\n      type: \"scatter\",\n      name: 'Quantile',\n      x: cdf,\n      y: x,\n      line: {color: 'black'},\n      xaxis: 'x1',\n      yaxis: 'y3',\n\n\n};\n\nvar trace12 = {\n        x : [alpha, alpha],\n        y : [x[0], quantile],\n        mode: 'lines',\n        line: {\n            dash: 'dash',\n            width: 1,\n            color: ' #428BCA',\n            marks: {\n                size: 0,\n                color: ' #428BCA',\n            }\n\n          },\n      xaxis: 'x1',\n      yaxis: 'y3',\n\n\n};\n\n\n\nvar trace13 = {\n    x: [0, alpha],\n    y: [quantile, quantile],\n    mode: 'scatter',\n    line: {\n        dash: 'dash',\n        width: 1,\n        color: ' #428BCA',\n        marks: {\n            size: 0,\n        }\n    },\n    xaxis: 'x1',\n    yaxis: 'y3'\n}\n\n\nvar trace2 = {\nx: cdf,\ny: cdf,\ntype: 'scatter',\nname: 'identity',\nline: {color: 'black'},\nxaxis: 'x1',\nyaxis: 'y2'\n\n};\n\n\nvar trace21 = {\n        x : [alpha, 1],\n        y : [alpha, alpha],\n        mode: 'lines',\n        line: {\n            dash: 'dash',\n            width: 1,\n            color: ' #428BCA',\n            marks: {\n                size: 0,\n                color: ' #428BCA',\n            }\n\n          },\n      xaxis: 'x1',\n      yaxis: 'y2'\n}\n\nvar trace22 = {\n        x : [alpha, alpha],\n        y : [alpha, 1],\n        mode: 'lines',\n        line: {\n            dash: 'dash',\n            width: 1,\n            color: ' #428BCA',\n            marks: {\n                size: 0,\n                color: ' #428BCA',\n            }\n\n          },\n      xaxis: 'x1',\n      yaxis: 'y2'\n}\n\n\n\nvar trace23 = {\n    x: [alpha],\n    y: [alpha],\n    mode: 'scatter',\n    line: {\n        dash: 'dash',\n        width: 1,\n        color: ' #428BCA',\n        marks: {\n            size: 0,\n        }\n    },\n    xaxis: 'x1',\n    yaxis: 'y2'\n}\n\n\nvar trace31 = {\n      type: \"scatter\",\n      mode: \"lines\",\n      name: 'PDF2',\n      x: x,\n      y: cdf,\n      line: {color: 'black'},\n      xaxis: 'x2',\n      yaxis: 'y2',\n};\n\nvar trace32 = {\n        x: filteredX,\n        y: filteredX.map(x =&gt; alpha),\n        mode: 'lines',\n        line: {\n            dash: 'dash',\n            width: 1,\n            color: ' #428BCA',\n            marks: {\n                size: 0,\n                color: ' #428BCA',\n            }\n          },\n      xaxis: 'x2',\n      yaxis: 'y2'\n}\n\nvar trace33 = {\n        x : [quantile, quantile],\n        y : [0, alpha],\n        mode: 'lines',\n        line: {\n            dash: 'dash',\n            width: 1,\n            color: ' #428BCA',\n            marks: {\n                size: 0,\n                color: ' #428BCA',\n            }\n          },\n\n      xaxis: 'x2',\n      yaxis: 'y2'\n}\n\nvar trace34 = {\n    x: [quantile],\n    y: [alpha],\n    mode: 'scatter',\n    line: {\n        dash: 'dash',\n        width: 1,\n        color: ' #428BCA',\n        marks: {\n            size: 0,\n        }\n    },\n    xaxis: 'x2',\n    yaxis: 'y2'\n}\n\n\nvar trace41 = {\n\n  type: \"scatter\",\n  name: 'Quantile2',\n  fill: 'tozeroy',\n  x: filteredX,\n  y: filteredPdf,\n  opacity: 0.9,\n  line: {color: ' #428BCA'},\n  xaxis: 'x2',\n  yaxis: 'y1'\n\n};\n\n\nvar trace42 = {\n\n  type: \"scatter\",\n  mode: \"lines\",\n  name: 'PDF2',\n  x: x,\n  y: pdf,\n  line: {color: 'black'},\n  xaxis: 'x2',\n  yaxis: 'y1'\n\n};\n\nvar data = [\n  trace1,\n  trace12, trace13,\n  trace2, trace21, trace22, trace23,\n  trace31, trace32, trace33, trace34,\n  trace41, trace42];\n\n\nvar layout = {\n\n  title: 'Distribution et quantile',\n  xaxis: {\n    domain: [0, 0.32],\n    anchor: 'y1'\n  },\n  yaxis: {\n    domain: [0, 0.24],\n    anchor: 'x1'\n\n  },\n  xaxis2: {\n    domain: [0.35, 1],\n    anchor: 'y'\n  },\n\n  yaxis2: {\n    domain: [0.26, 0.49],\n    anchor: 'x1'\n  },\n\n  yaxis3: {\n    domain: [0.5, 1],\n    anchor: 'x1'\n  },\n\n// legend offset\n  showlegend: false,\n  height: 680,\n  annotations: [\n\n    {\n      x: 1/4,\n      y: quantile - (x[0]-x.slice(-1))/20,\n      xref: 'x1',\n      yref: 'y3',\n      text: 'q= ' + quantile.toFixed(2),\n      font: {\n        size: 12,\n        color: '#428BCA',\n\n      },\n      showarrow: false,\n      arrowhead: 0,\n      ax: 25,\n      ay: -10,\n\n    },\n    {\n      x: 1/4,\n      y: alpha,\n      xref: 'x1',\n      yref: 'y2',\n      text: 'alpha= ' + alpha.toFixed(2),\n      font: {\n        size: 12,\n        color: '#428BCA',\n\n      },\n      showarrow: false,\n      arrowhead: 0,\n      ax: 25,\n      ay: -10,\n\n    },\n    {\n        text: \"Fonction quantile\",\n      font: {\n      size: 15,\n      color: 'black',\n    },\n    showarrow: false,\n    align: 'center',\n    x: -0.01,\n    y: 1.05,\n    xref: 'paper',\n    yref: 'paper',\n    },\n    {\n      text: \"Fonction de répartition\",\n      font: {\n      size: 15,\n      color: 'black',\n            },\n    showarrow: false,\n    align: 'center',\n    x: 0.65,\n    y: 0.52,\n    xref: 'paper',\n    yref: 'paper',\n    },\n    {\n      text: \"Fonction de densité\",\n      font: {\n      size: 15,\n      color: 'black',\n            },\n    showarrow: false,\n    align: 'center',\n    x: 0.65,\n    y: 0.225,\n    xref: 'paper',\n    yref: 'paper',\n    },\n  ]\n\n};\n// XXX: TODO: put the xticks labels on the middle plot for x and on the right plot for y\n    var config = {responsive: true}\n    const div = DOM.element('div');\n    Plotly.newPlot(div, data, layout, config);\n    return div;\n  }\n\n}\n\n\n\n\n\n\n\nmu = inputs[0];\nsigma = inputs[1];\nalpha = inputs[2]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote: voir aussi Notations et rappels",
    "crumbs": [
      "Slides",
      "Notations et premiers pas"
    ]
  },
  {
    "objectID": "Slides/slides_notations_premiers_pas.html#quantiles-cas-discret",
    "href": "Slides/slides_notations_premiers_pas.html#quantiles-cas-discret",
    "title": "Notations et premiers pas",
    "section": "Quantiles: cas discret",
    "text": "Quantiles: cas discret\n\n\n\ndiscretePDFs = () =&gt; {\n  const distributions = Object.keys(dists);\n  // Get in continuousDistributions the distributions whose pdf, cdf and quantile are defined\n    const continuousDistributions = distributions.filter(name =&gt; dists[name].pmf && dists[name].cdf && dists[name].quantile);\n  return continuousDistributions\n};\noutput_discr = discretePDFs();\n\npmfNames_unsorted = output_discr.filter(name =&gt; !excludedPDFs.includes(name));\npmfNames=pmfNames_unsorted.toSorted();\nviewof inputs_disc = Inputs.form([\n      Inputs.range([-10, 10], {value: 0.1, step: 0.001, label: tex`\\mu `, width: 500}),\n      Inputs.range([0.01, 5], {value: 1.01, step: 0.001, label: tex`\\sigma `, width: 500}),\n      Inputs.range([0.001, 0.999], {value: 0.75, step: 0.001, label: tex`\\alpha`, width: 500}),\n    ]);\n\nviewof distrib_name_discr = Inputs.select(pmfNames, {value: \"normal\", label: \"Loi\", width: 500});\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n{\n  const z = d3.range(-5, 5, 0.01);\n  const pmf = z.map(z =&gt; dists[distrib_name_discr].pmf(z, mu_disc, sigma_disc));\n  const cdf = z.map(z =&gt; dists[distrib_name_discr].cdf(z, mu_disc, sigma_disc));\n  const inv = z.map(z =&gt; dists[distrib_name_discr].quantile(z, mu_disc, sigma_disc));\n  const quantile = dists[distrib_name_discr].quantile(alpha_disc, mu_disc, sigma_disc);\n  const filteredX = z.filter(coord =&gt; coord &lt;= quantile);\n  const filteredPmf = pmf.filter((_, i) =&gt; z[i] &lt;= quantile);\n  const filteredCdf = cdf.filter(coord =&gt; coord &lt;= quantile);\n\n\n{\nvar trace1 = {\n      type: \"scatter\",\n      name: 'Quantile',\n      x: cdf,\n      y: z,\n      line: {color: 'black'},\n      xaxis: 'x1',\n      yaxis: 'y3',\n\n\n};\n\nvar trace12 = {\n        x : [alpha_disc, alpha_disc],\n        y : [z[0], quantile],\n        mode: 'lines',\n        line: {\n            dash: 'dash',\n            width: 1,\n            color: ' #428BCA',\n            marks: {\n                size: 0,\n                color: ' #428BCA',\n            }\n\n          },\n      xaxis: 'x1',\n      yaxis: 'y3',\n\n\n};\n\n\n\nvar trace13 = {\n    x: [0, alpha_disc],\n    y: [quantile, quantile],\n    mode: 'scatter',\n    line: {\n        dash: 'dash',\n        width: 1,\n        color: ' #428BCA',\n        marks: {\n            size: 0,\n        }\n    },\n    xaxis: 'x1',\n    yaxis: 'y3'\n}\n\n\nvar trace2 = {\nx: cdf,\ny: cdf,\ntype: 'scatter',\nname: 'identity',\nline: {color: 'black'},\nxaxis: 'x1',\nyaxis: 'y2'\n\n};\n\n\nvar trace21 = {\n        x : [alpha_disc, 1],\n        y : [alpha_disc, alpha_disc],\n        mode: 'lines',\n        line: {\n            dash: 'dash',\n            width: 1,\n            color: ' #428BCA',\n            marks: {\n                size: 0,\n                color: ' #428BCA',\n            }\n\n          },\n      xaxis: 'x1',\n      yaxis: 'y2'\n}\n\nvar trace22 = {\n        x : [alpha_disc, alpha_disc],\n        y : [alpha_disc, 1],\n        mode: 'lines',\n        line: {\n            dash: 'dash',\n            width: 1,\n            color: ' #428BCA',\n            marks: {\n                size: 0,\n                color: ' #428BCA',\n            }\n\n          },\n      xaxis: 'x1',\n      yaxis: 'y2'\n}\n\nvar trace23 = {\n    x: [alpha_disc],\n    y: [alpha_disc],\n    mode: 'scatter',\n    line: {\n        dash: 'dash',\n        width: 1,\n        color: ' #428BCA',\n        marks: {\n            size: 0,\n        }\n    },\n    xaxis: 'x1',\n    yaxis: 'y2'\n}\n\n\nvar trace31 = {\n      type: \"scatter\",\n      mode: \"lines\",\n      name: 'PDF2',\n      x: z,\n      y: cdf,\n      line: {color: 'black'},\n      xaxis: 'x2',\n      yaxis: 'y2',\n};\n\nvar trace32 = {\n        x: filteredX,\n        y: filteredX.map(z =&gt; alpha_disc),\n        mode: 'lines',\n        line: {\n            dash: 'dash',\n            width: 1,\n            color: ' #428BCA',\n            marks: {\n                size: 0,\n                color: ' #428BCA',\n            }\n          },\n      xaxis: 'x2',\n      yaxis: 'y2'\n}\n\nvar trace33 = {\n        x : [quantile, quantile],\n        y : [0, alpha_disc],\n        mode: 'lines',\n        line: {\n            dash: 'dash',\n            width: 1,\n            color: ' #428BCA',\n            marks: {\n                size: 0,\n                color: ' #428BCA',\n            }\n          },\n\n      xaxis: 'x2',\n      yaxis: 'y2'\n}\n\nvar trace34 = {\n    x: [quantile],\n    y: [alpha_disc],\n    mode: 'scatter',\n    line: {\n        dash: 'dash',\n        width: 1,\n        color: ' #428BCA',\n        marks: {\n            size: 0,\n        }\n    },\n    xaxis: 'x2',\n    yaxis: 'y2'\n}\n\n\nvar trace41 = {\n\n  type: \"scatter\",\n  name: 'Quantile2',\n  fill: 'tozeroy',\n  x : filteredX,\n  y : filteredPmf,\n  opacity: 0.9,\n  line: {color: ' #428BCA'},\n  xaxis: 'x2',\n  yaxis: 'y1'\n\n};\n\n\nvar trace42 = {\n\n  type: \"scatter\",\n  mode: \"lines\",\n  name: 'PDF2',\n  x: z,\n  y: pmf,\n  line: {color: 'black'},\n  xaxis: 'x2',\n  yaxis: 'y1'\n\n};\n\nvar data = [\n  trace1,\n  trace12, trace13,\n  trace2, trace21, trace22, trace23,\n  trace31, trace32, trace33, trace34,\n  trace41, trace42];\n\n\nvar layout = {\n\n  title: 'Distribution et quantile',\n  xaxis: {\n    domain: [0, 0.32],\n    anchor: 'y1'\n  },\n  yaxis: {\n    domain: [0, 0.24],\n    anchor: 'x1'\n\n  },\n  xaxis2: {\n    domain: [0.35, 1],\n    anchor: 'y'\n  },\n\n  yaxis2: {\n    domain: [0.26, 0.49],\n    anchor: 'x1'\n  },\n\n\n\n  yaxis3: {\n    domain: [0.5, 1],\n    anchor: 'x1'\n\n  },\n\n  showlegend: false,\n  height: 680,\n\n\n  annotations: [\n\n    {\n      x: 1/4,\n      y: quantile - (z[0]-z.slice(-1))/20,\n      xref: 'x1',\n      yref: 'y3',\n      text: 'q=' + quantile.toFixed(2),\n      font: {\n        size: 12,\n        color: '#428BCA',\n\n      },\n      showarrow: false,\n      arrowhead: 0,\n      ax: 25,\n      ay: -10,\n\n    },\n    {\n      x: 1/4,\n      y: alpha_disc,\n      xref: 'x1',\n      yref: 'y2',\n      text: 'alpha=' + alpha_disc.toFixed(2),\n      font: {\n        size: 12,\n        color: '#428BCA',\n\n      },\n      showarrow: false,\n      arrowhead: 0,\n      ax: 25,\n      ay: -10,\n\n    },\n    {\n        text: \"Fonction quantile\",\n      font: {\n      size: 15,\n      color: 'black',\n    },\n    showarrow: false,\n    align: 'center',\n    x: -0.01,\n    y: 1.05,\n    xref: 'paper',\n    yref: 'paper',\n    },\n    {\n      text: \"Fonction de répartition\",\n      font: {\n      size: 15,\n      color: 'black',\n            },\n    showarrow: false,\n    align: 'center',\n    x: 0.65,\n    y: 0.52,\n    xref: 'paper',\n    yref: 'paper',\n    },\n    {\n      text: \"Fonction de densité\",\n      font: {\n      size: 15,\n      color: 'black',\n            },\n    showarrow: false,\n    align: 'center',\n    x: 0.65,\n    y: 0.225,\n    xref: 'paper',\n    yref: 'paper',\n    },\n  ]\n\n};\n// XXX: TODO: put the xticks labels on the middle plot for x and on the right plot for y\n\n    var config = {responsive: true}\n    const div = DOM.element('div');\n    Plotly.newPlot(div, data, layout, config);\n    return div;\n  }\n\n}\n\n\n\n\n\n\n\nmu_disc = inputs_disc[0];\nsigma_disc = inputs_disc[1];\nalpha_disc = inputs_disc[2];\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote: voir aussi Notations et rappels",
    "crumbs": [
      "Slides",
      "Notations et premiers pas"
    ]
  },
  {
    "objectID": "Slides/slides_notations_premiers_pas.html#bibliographie",
    "href": "Slides/slides_notations_premiers_pas.html#bibliographie",
    "title": "Notations et premiers pas",
    "section": "Bibliographie",
    "text": "Bibliographie\n\n\nBarbe, Philippe, et Michel Ledoux. 2006. « Cours de Probabilite Pour la Licence (corrigée) ».\n\n\n\n\nNotations et premiers pas",
    "crumbs": [
      "Slides",
      "Notations et premiers pas"
    ]
  },
  {
    "objectID": "Slides/slides_th_asymptotique.html#loi-forte-des-grands-nombres",
    "href": "Slides/slides_th_asymptotique.html#loi-forte-des-grands-nombres",
    "title": "Théorèmes asymptotiques",
    "section": "Loi forte des grands nombres",
    "text": "Loi forte des grands nombres\n\nRésultat fondamental: concerne le comportement asymptotique de la moyenne empirique: \\[\n\\bar X_n = \\dfrac{X_1 + \\cdots + X_n}{n} \\enspace.\n\\] quand on observe \\(n\\) variables aléatoires i.i.d \\(X_1,\\dots,X_n\\), ayant une espérance finie.\n\n\nThéorème 1 (Loi forte des grands nombres) \nSoit \\((X_n)_{n \\geq 1}\\) une suite de variables aléatoires indépendantes et identiquement distribuées (i.i.d.) dans \\(L^1(\\Omega, \\mathcal{F}, \\mathbb{P})\\). Notons \\(\\mu = \\mathbb{E}[X_1]\\). Alors \\(\\bar X_n\\) converge vers \\(\\mu\\) presque sûrement : \\[\n\\mathbb{P}\\bigg( \\dfrac{X_1 + \\cdots + X_n}{n} \\underset{n \\to \\infty}{\\longrightarrow} \\mu \\bigg) = 1\\,.\n\\]",
    "crumbs": [
      "Slides",
      "Théorèmes asymptotiques"
    ]
  },
  {
    "objectID": "Slides/slides_th_asymptotique.html#interprétation",
    "href": "Slides/slides_th_asymptotique.html#interprétation",
    "title": "Théorèmes asymptotiques",
    "section": "Interprétation",
    "text": "Interprétation\nIntuitivement, la probabilité d’un événement \\(A\\) correspond à la fréquence d’apparition de \\(A\\) quand on répète une expérience qui fait intervenir cet événement.\n\nExemple 1 (Cas Bernouilli: pile ou face) La probabilité d’apparition du côté pile (noté \\(p\\)) peut-être estimée en lançant la pièce un grand nombre de fois et en comptant le nombre de pile obtenu.\nLa loi des grands nombres justifie cette intuition : si \\(X_1, \\ldots, X_n\\) sont i.i.d. de loi de Bernoulli de paramètre \\(p\\), alors \\[\n    \\dfrac{X_1 + \\cdots + X_n}{n} \\xrightarrow[n \\to \\infty]{p.s.} p =\\mathbb{E}(X_1)\n\\]\n\nMembre de gauche : la fréquence empirique de piles\nMembre de droite : la fréquence théorique de piles",
    "crumbs": [
      "Slides",
      "Théorèmes asymptotiques"
    ]
  },
  {
    "objectID": "Slides/slides_th_asymptotique.html#visualisation-de-lexemple-du-pile-ou-face",
    "href": "Slides/slides_th_asymptotique.html#visualisation-de-lexemple-du-pile-ou-face",
    "title": "Théorèmes asymptotiques",
    "section": "Visualisation de l’exemple du pile ou face",
    "text": "Visualisation de l’exemple du pile ou face\n#| echo: false\n#| standalone: true\n#| viewerHeight: 550\nimport numpy as np\nfrom shiny import ui, render, App, reactive\nfrom shinywidgets import output_widget, register_widget\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objs as go\n\n\nn_init = 42\n\n\napp_ui = ui.page_fluid(\n    ui.tags.head(\n        ui.tags.style(\"\"\"\n            .bslib-sidebar-layout &gt; .sidebar &gt; .sidebar-content {\n            display: flex;\n            flex-direction: column;\n            padding: 0.5rem;\n            }\n            .irs--shiny .irs-line {\n            top: 27px;\n            height: 4px;\n            }\n            .irs.irs--shiny .irs-bar {\n            top: 27px;\n            height: 4px;\n            }\n            .irs--shiny .irs-handle {\n            top: 23px;\n            width: 22px;\n            height: 10px;\n        \"\"\")\n    ),\n    ui.panel_title(\"Loi des grands nombres\"),\n    ui.layout_sidebar(\n        ui.panel_sidebar(\n            ui.input_action_button(\"seed\", \"Nouveau tirage\", class_=\"btn-primary\"),\n            ui.input_slider( \"p\", \"Espérance: p\", 0.01, 0.99, value=0.35, step=0.01, ticks=False,\n            ),\n            ui.input_slider( \"n_samples\", \"Échantillons: n\", 2, 1000, value=15, step=1, ticks=False),\n        width=3),\n    ui.panel_main(output_widget(\"my_widget\"), width = 9)\n    )\n)\n\n\n\n\ndef server(input, output, session):\n    seed = reactive.Value(42)\n\n    @reactive.Effect\n    @reactive.event(input.seed)\n    def _():\n        seed.set(np.random.randint(0, 1000))\n\n    subplots = make_subplots(\n        rows=2,\n        cols=1,\n        vertical_spacing=0.45,\n        horizontal_spacing=0.04,\n        row_heights=[5, 1],\n        subplot_titles=(\n            f\"Moyenne empirique: loi de Bernoulli\",\n            \"Tirages aléatoires &lt;span style='color:rgb(66, 139, 202)'&gt;bleu: 0&lt;/span&gt;, &lt;span style='color:rgb(255, 0, 0)'&gt;rouge: 1&lt;/span&gt; (seed=\"\n            + f\"{n_init:03}\"\n            + \")\",\n        ),\n    )\n    fig = go.FigureWidget(subplots)\n    fig.add_trace(\n        go.Scatter(\n            mode=\"lines\",\n            line=dict(color=\"black\", width=3),\n            x=[],\n            y=[],\n            name=r\"Moyenne &lt;br&gt; empirique\",\n        ),\n        row=1,\n        col=1,\n    )\n    fig.add_trace(\n        go.Scatter(\n            mode=\"lines\",\n            line=dict(dash=\"dash\", color=\"black\", width=1),\n            marker={},\n            x=[],\n            y=[],\n            name=r\"p\",\n        ),\n        row=1,\n        col=1,\n    )\n    fig.add_trace(\n        go.Heatmap(\n            x=[],\n            z=[],\n            colorscale=[[0, \"rgb(66, 139, 202)\"], [1, \"rgb(255,0,0)\"]],\n            showscale=False,\n        ),\n        row=2,\n        col=1,\n    )\n\n    fig.update_yaxes(range=[0, 1.1], row=1, col=1)\n    fig.update_xaxes(matches=\"x1\", row=2, col=1)\n    fig.update_yaxes(visible=False, row=2, col=1)\n    fig.update_xaxes(visible=False, row=2, col=1)\n\n    fig.update_layout(\n        template=\"simple_white\",\n        showlegend=True,\n        xaxis_title=\"Échantillons: n\",\n    )\n    fig.update_layout(autosize=True)\n\n    fig.update_layout(\n        legend=dict(\n            yanchor=\"top\",\n            y=1.18,\n            x=0.85,\n            bgcolor=\"rgba(0,0,0,0)\",\n        )\n    )\n    fig.update_layout(\n        margin=dict(l=0, r=0, b=10, t=70),\n    )\n\n    register_widget(\"my_widget\", fig)\n\n    @reactive.Effect\n    def _():\n        p = input.p()\n        n_samples = input.n_samples()\n\n        rng = np.random.default_rng(seed())\n        iterations = np.arange(1, n_samples + 1)\n        samples = rng.binomial(1, p, size=n_samples)\n        means_samples = np.cumsum(samples) / np.arange(1, n_samples + 1)\n\n        # Update data in fig:\n        fig.data[0].x = iterations\n        fig.data[0].y = means_samples\n\n        fig.data[1].x = iterations\n        fig.data[1].y = np.full((n_samples), p)\n\n        fig.data[2].x = iterations\n        fig.data[2].z = [samples]\n\n        fig.update_xaxes(range=[1, n_samples + 1])\n\n        # Update the subplot titles:\n        fig.layout.annotations[1].update(\n            text=f\"Tirages aléatoires (seed=\"\n            + f\"{seed():03}\"\n            + \") &lt;br&gt; &lt;span style='color:rgb(66, 139, 202)'&gt;bleu: 0&lt;/span&gt;, &lt;span style='color:rgb(255, 0, 0)'&gt;rouge: 1&lt;/span&gt; \"\n        )\n\n\napp = App(app_ui, server)\n\nPour aller plus loin:\nQuand \\(p\\) varie (\\(n\\) fixé), les signaux générés sont très très proches, ce qui ne devrait pas être le cas sans structuration particulière de la génération. L’aléa est imparfait (structure sous-jacente)!",
    "crumbs": [
      "Slides",
      "Théorèmes asymptotiques"
    ]
  },
  {
    "objectID": "Slides/slides_th_asymptotique.html#au-delà-de-la-loi-des-grands-nombres",
    "href": "Slides/slides_th_asymptotique.html#au-delà-de-la-loi-des-grands-nombres",
    "title": "Théorèmes asymptotiques",
    "section": "Au delà de la loi des grands nombres",
    "text": "Au delà de la loi des grands nombres\n\n1er ordre d’approximation de la convergence de \\(\\bar{X}_n\\): loi des grands nombres\n2ème ordre d’approximation: théorème central limite\n\nEnjeu: quantifier les variations de \\(\\bar X_n - \\mu\\)\nRéponse: théorème central limite (TCL), avec la convergence en loi d’une transformation affine de la moyenne empirique",
    "crumbs": [
      "Slides",
      "Théorèmes asymptotiques"
    ]
  },
  {
    "objectID": "Slides/slides_th_asymptotique.html#théorème-central-limite-1",
    "href": "Slides/slides_th_asymptotique.html#théorème-central-limite-1",
    "title": "Théorèmes asymptotiques",
    "section": "Théorème central limite",
    "text": "Théorème central limite\n\nThéorème 2 (Théorème central limite) Soit \\(X_1, \\ldots, X_n\\) une suite de variables aléatoires i.i.d de variance \\(\\sigma^2 = {\\rm var}(X_1) \\in ]0, \\infty[\\). On note \\(\\mu = \\mathbb{E}[X_1]\\) leur espérance. Alors \\[\n\\sqrt n \\left(\\tfrac{\\bar X_n - \\mu}{\\sigma} \\right) \\xrightarrow[n \\to +\\infty]{\\mathcal{L}} N\\enspace,\n\\] où \\(N\\) suit une loi normale centrée réduite : \\(N \\sim\\mathcal{N}(0,1)\\).\n\nInterprétation: la moyenne empirique de v.a. i.i.d de variance \\(\\sigma^2\\) se comporte asymptotiquement comme une loi normale \\(\\mathcal{N}(\\mu, \\tfrac{\\sigma^2}{n})\\): \\(\\quad \\bar X_n \\approx \\mathcal{N}(\\mu, \\frac{\\sigma^2}{n})\\).\n\n\n\n\n\n\nNote\n\n\nHypothèses du théorème plutôt faibles: variance finie uniquement",
    "crumbs": [
      "Slides",
      "Théorèmes asymptotiques"
    ]
  },
  {
    "objectID": "Slides/slides_th_asymptotique.html#formulation-de-la-convergence",
    "href": "Slides/slides_th_asymptotique.html#formulation-de-la-convergence",
    "title": "Théorèmes asymptotiques",
    "section": "Formulation de la convergence",
    "text": "Formulation de la convergence\nConvergence en loi \\(\\iff\\) convergence des fonctions de répartition (aux pts de continuité)\nNotations:\n\n\\(\\varphi\\) : la densité d’une loi normale centrée réduite \\(\\varphi(x) = \\tfrac{e^{-\\frac{x^2}{2}}}{\\sqrt{2\\pi}}\\)\n\\(\\Phi\\) : la fonction de répartition d’une loi normale centrée réduite \\(\\Phi(x) = \\displaystyle \\int_{-\\infty}^{x}\\varphi(u) du\\)\n\n\nRé-écriture du TCL: pour tout \\(a &lt; b\\) on a alors\n\\[\n\\begin{align}\n    \\mathbb{P} \\left(\\bar X_n \\in [ \\mu + \\tfrac{a \\sigma}{\\sqrt{n}}, \\mu + \\tfrac{ b \\sigma}{\\sqrt{n}}] \\right)\n    & =\n    \\mathbb{P} \\left(\\tfrac{\\bar X_n - \\mu}{\\sigma} \\in [ \\tfrac{a}{\\sqrt{n}},\\tfrac{b}{\\sqrt{n}}]\\right) \\nonumber\\\\\n    & \\class{fragment}{{} = \\mathbb{P} \\bigg( a \\leq \\sqrt n \\left(\\tfrac{\\bar X_n - \\mu}{\\sigma} \\right) \\leq b\\bigg) \\nonumber}\n    \\\\\n    & \\class{fragment}{{} \\underset{n \\to \\infty}{\\longrightarrow}  \\int_a^b \\varphi(x) \\,  dx = \\Phi(b) - \\Phi(a) \\nonumber}\\\\\n\\end{align}\n\\]",
    "crumbs": [
      "Slides",
      "Théorèmes asymptotiques"
    ]
  },
  {
    "objectID": "Slides/slides_th_asymptotique.html#lien-intervalle-de-confiance-et-tcl",
    "href": "Slides/slides_th_asymptotique.html#lien-intervalle-de-confiance-et-tcl",
    "title": "Théorèmes asymptotiques",
    "section": "Lien intervalle de confiance et TCL",
    "text": "Lien intervalle de confiance et TCL\n\n\nQuestion: comment choisir \\(a\\) et \\(b\\) pour obtenir un intervalle de confiance à 95% pour \\(\\mu\\)?\nNotation: \\(\\alpha_n=\\mathbb{P} \\left(\\bar X_n \\notin [ \\mu + \\tfrac{a \\sigma}{\\sqrt{n}}, \\mu + \\tfrac{ b \\sigma}{\\sqrt{n}}] \\right)\\), on cherche \\(a, b\\) tels que \\(\\alpha_n \\approx 0.05\\).\nSimplification: choix d’un intervalle symétrique autour de \\(\\mu \\implies q=a=-b\\) \\[\n\\begin{align}\n& 1-\\alpha_n \\approx \\int_{-q}^q \\varphi(x) \\,  dx=\\Phi(q)-\\Phi(-q)=2 \\Phi(q)-1 \\\\\n\\implies & \\boxed{q\\approx\\Phi^{-1}(1-\\tfrac{\\alpha_n}{2})}\n\\end{align}\n\\]\nInterpretation: \\(q\\) est (approx.) le quantile de niveau \\(1-\\tfrac{\\alpha_n}{2}\\) de la loi normale centrée réduite\nNumériquement: on peut facilement évaluer \\(q\\) et vérifier que \\(q\\approx 1.96\\) avec scipy\n\n\n\n\nfrom scipy.stats import norm  # import du module \"norm\" de scipy.stats\nq = norm.ppf((1-0.05/2))      # Calcul du quantile (en: Percent point function) de niveau 1-0.05/2\nprint(f\"{q:.2f}\")             # Affichage à 2 décimales\n\n1.96",
    "crumbs": [
      "Slides",
      "Théorèmes asymptotiques"
    ]
  },
  {
    "objectID": "Slides/slides_th_asymptotique.html#visualization-du-tcl",
    "href": "Slides/slides_th_asymptotique.html#visualization-du-tcl",
    "title": "Théorèmes asymptotiques",
    "section": "Visualization du TCL",
    "text": "Visualization du TCL\n#| standalone: true\n#| viewerHeight: 600\nimport numpy as np\nfrom scipy.stats import norm\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nfrom shiny import ui, render, App, reactive\nfrom shinywidgets import output_widget, render_widget\n\n\napp_ui = ui.page_fluid(\n    ui.tags.head(\n        ui.tags.style(\"\"\"\n            .bslib-sidebar-layout &gt; .sidebar &gt; .sidebar-content {\n            display: flex;\n            flex-direction: column;\n            padding: 0.1rem;\n            }\n            .irs--shiny .irs-line {\n            top: 27px;\n            height: 4px;\n            }\n            .irs.irs.irs--shiny .irs-bar {\n            top: 27px;\n            height: 4px;\n            }\n            .irs--shiny .irs-handle {\n            top: 23px;\n            width: 22px;\n            height: 10px;\n        \"\"\")\n    ),\n    ui.panel_title(\"TCL\"),\n    ui.layout_sidebar(\n        ui.panel_sidebar(\n            ui.input_action_button(\"seed\", \"Nouveau tirage\",class_=\"btn-primary\"),\n            ui.input_slider(\"p\", \"Espérance: p\", 0.01, 0.99, value=0.5, step=0.01, ticks=False),\n            ui.input_slider(\"n_samples\", \"Échantillons: n\", 1, 200, value=100, step=1, ticks=False),\n            ui.input_slider(\"n_repetitions\", \"Répétitions: t\", 1, 300, value=200, step=1, ticks=False),\n        width=3\n        ),\n    ui.panel_main(output_widget(\"my_widget\"), width = 9)\n    )\n)\n\n\n\ndef server(input, output, session):\n    seed = reactive.Value(42)\n\n\n    @reactive.Effect\n    @reactive.event(input.seed)\n    def _():\n        seed.set(np.random.randint(0, 1000))\n\n\n    @output\n    @render_widget\n    def my_widget():\n\n        rng = np.random.default_rng(seed())\n        p = input.p()\n        n_repetitions = input.n_repetitions()\n        n_samples = input.n_samples()\n        iterations = np.arange(1, n_samples + 1)\n        samples = rng.binomial(1, p, size=(n_repetitions, n_samples))\n        means_samples = np.cumsum(samples, axis=1) / np.arange(1, n_samples + 1)\n        x_hist = np.linspace(0, 1, num=300)\n\n        # Create figure\n        fig = make_subplots(\n                    rows=1,\n                    cols=3,\n                    # vertical_spacing=0.5,\n                    horizontal_spacing=0.02,\n                    column_widths=[20, 2, 3],\n                    subplot_titles=(\"t = \" + str(n_repetitions) + \" répétitions\",\"\",\"\")\n                )\n\n\n        for i in range(n_repetitions):\n            fig.add_trace(\n                    go.Scatter(\n                        mode='lines',\n                        line=dict(color=\"rgba(0,0,0,0.05)\", width=1),\n                        x=iterations,\n                        y=means_samples[i,:],\n                        ),\n                        row=1, col=1,\n            )\n        fig.add_trace(\n                go.Scatter(\n                    mode='lines',\n                    line=dict(dash=\"dash\", color=\"blue\", width=1),\n                    marker={},\n                    x=iterations,\n                    y=np.full((n_samples), p),\n                    name='p'),\n                    row=1, col=1,\n        )\n        fig.update_layout(\n            template=\"simple_white\",\n            showlegend=False,\n        )\n        fig.add_trace(\n                go.Scatter(\n                    mode='markers',\n                    marker=dict(color=\"rgba(0,0,0,0.05)\", size=4),\n                    x=np.zeros(n_samples),\n                    y=means_samples[:,-1],\n                ),\n                row=1, col=2,\n\n        )\n        y_hist, bins = np.histogram(means_samples[:,-1], bins=int(np.sqrt(n_repetitions)), density=True)\n        fig.add_trace(\n            go.Bar(x=y_hist, y=bins[:-1] + np.diff(bins)/2,\n                    opacity=0.75,\n                    marker_color = 'black',\n                    orientation='h',\n                    width=np.diff(bins),\n                    name=\"Tirages de moyennes empiriques\",\n                    ),\n                row=1, col=3,\n        )\n        fig.add_trace(\n            go.Scatter(x=norm.pdf(x_hist, p, np.sqrt(p*(1-p) / n_samples)),\n                       y=x_hist,\n                       mode='lines',\n                       line=dict(color=\"red\"),\n                       legendgroup='1',\n                       name=\"TCL\"\n                       ),\n                row=1, col=3,\n        )\n\n        fig.update_xaxes(range=[1, n_samples + 1])\n        fig.update_yaxes(range=[-.05, 1.05], row=1, col=1)\n        fig.update_yaxes(matches=\"y1\",visible = False,  row=1, col=2)\n        fig.update_xaxes(range=[-0.2, 0.2], visible = False, row=1, col=2)\n\n        fig.update_yaxes(matches=\"y1\", row=1, col=3, visible=False)\n        fig.update_xaxes(range=[0, 1.1 / np.sqrt(2*np.pi* p*(1-p) / n_samples)], row=1, col=3)\n        fig.update_xaxes(visible=False, row=1, col=3)\n\n\n\n        for trace in fig['data']:\n            print(trace)\n            if (trace['name'] != 'TCL') and (trace['name'] != 'p'):\n                trace['showlegend'] = False\n        fig.update_layout(\n        margin=dict(l=0, r=0, b=10, t=100),\n        )\n        fig.update_layout(\n            title=dict(text=\"Distribution de la moyenne empirique&lt;br&gt; (cas loi de Bernoulli)\", yanchor=\"top\", y=0.95),\n            title_x=0.5,\n            showlegend=True,\n\n        )\n        fig.update_layout(\n            legend=dict(\n                yanchor=\"top\",\n                y=1.24,\n                xanchor=\"left\",\n                x=0.85,\n                bgcolor=\"rgba(0,0,0,0)\",\n            )\n        )\n        fig['layout']['xaxis']['title']='Échantillons: n'\n\n        fig.update_layout(autosize=True)\n\n        return fig\n\n\napp = App(app_ui, server)",
    "crumbs": [
      "Slides",
      "Théorèmes asymptotiques"
    ]
  },
  {
    "objectID": "Slides/slides_th_asymptotique.html#pour-aller-plus-loin-vision-des-convolutions",
    "href": "Slides/slides_th_asymptotique.html#pour-aller-plus-loin-vision-des-convolutions",
    "title": "Théorèmes asymptotiques",
    "section": "Pour aller plus loin: vision des convolutions",
    "text": "Pour aller plus loin: vision des convolutions\n\n\nNotation: Soient \\(f\\) et \\(g\\) définies sur \\(\\mathbb{R}\\) (intégrables au sens de Lebesgue).\n\nDéfinition 1 (Convolution) La convolution de \\(f\\) par \\(g\\) est la fonction \\(f*g\\) suivante: \\[\n\\begin{align}\nf*g:\n\\mathbb{R} &\\mapsto \\mathbb{R} \\nonumber\\\\\nx &\\to \\int_{-\\infty}^{+\\infty} f(x-y)g(y) dy \\enspace.\\nonumber\n\\end{align}\n\\]\n\n\n\n\n\n\n\n\nNote\n\n\nOn peut aussi obtenir \\(f*g(x)\\) en calculant \\(\\int_{\\mathbb{R}^2} f(u)g(v) {1\\hspace{-3.8pt} 1}_{u+v=x} du dv\\).",
    "crumbs": [
      "Slides",
      "Théorèmes asymptotiques"
    ]
  },
  {
    "objectID": "Slides/slides_th_asymptotique.html#somme-et-convolutions",
    "href": "Slides/slides_th_asymptotique.html#somme-et-convolutions",
    "title": "Théorèmes asymptotiques",
    "section": "Somme et convolutions",
    "text": "Somme et convolutions\n\nThéorème 3 (Loi de la somme et convolutions) Soient \\(X\\) et \\(Y\\) des v.a. indépendantes de densités respectives \\(f\\) et \\(g\\), alors la densité de \\(X+Y\\) est donnée par la convolution \\(f*g\\).\n\nRappel: pour un scalaire \\(\\alpha\\neq 0\\), la densité de \\(\\alpha X\\) est donnée par la fonction \\(x \\mapsto \\frac{1}{|\\alpha|} \\cdot f(\\frac{x}{\\alpha})\\).\n\nCorollaire 1 (Loi de la moyenne) Soient \\(X_1,\\dots,X_n\\) des v.a. i.i.d. de densité \\(f\\), la densité de \\(\\bar{X}_n\\) est donnée par la fonction \\(x \\mapsto n \\cdot [f*\\dots*f](n \\cdot x)\\).",
    "crumbs": [
      "Slides",
      "Théorèmes asymptotiques"
    ]
  },
  {
    "objectID": "Slides/slides_th_asymptotique.html#convolution-et-tcl",
    "href": "Slides/slides_th_asymptotique.html#convolution-et-tcl",
    "title": "Théorèmes asymptotiques",
    "section": "Convolution et TCL",
    "text": "Convolution et TCL\nPour \\(X_1, \\dots, X_n\\), i.i.d., de densité \\(f\\), on affiche la loi de \\(\\bar{X}_n\\) (à une constante près)\n#| standalone: true\n#| viewerHeight: 550\nimport numpy as np\nfrom shiny import ui, render, App, reactive\nfrom shinywidgets import output_widget, register_widget\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objs as go\nfrom scipy import signal\n\n\napp_ui = ui.page_fluid(\n    ui.tags.head(\n        ui.tags.style(\"\"\"\n            .bslib-sidebar-layout &gt; .sidebar &gt; .sidebar-content {\n            display: flex;\n            flex-direction: column;\n            padding: 0.5rem;\n            }\n            .irs--shiny .irs-line {\n            top: 27px;\n            height: 2px;\n            }\n            .irs.irs--shiny .irs-bar {\n            top: 27px;\n            height: 2px;\n            }\n            .irs--shiny .irs-handle {\n            top: 23px;\n            width: 22px;\n            height: 10px;\n        \"\"\")\n    ),\n    ui.panel_title(\"TCL et convolutions\"),\n    ui.layout_sidebar(\n        ui.panel_sidebar(\n            ui.input_select(\n                \"loi\",\n                \"Loi sous-jacente\",\n                {'uniforme': 'Uniforme', 'laplace' : 'Laplace'},\n            ),\n            ui.input_slider(\n                \"n_iter\",\n                \"Échantillons: n\",\n                1,\n                10,\n                value=1,\n                step=1,\n                ticks=False,\n            ), width = 3\n    ),\n    ui.panel_main(\n        output_widget(\"my_widget\"), width = 9\n    )\n)\n)\n\n\nnnzeros = 10001\nx_min = -20\nx_max = 20\n\nx = np.linspace(x_min, x_max, nnzeros)\ny = np.zeros(nnzeros)\nmask = np.where(np.abs(x) &lt;= 0.5, 1, 0)\ny[mask == 1] = 1\ndelta = (x_max - x_min) / nnzeros\nvar = np.sum(y * x**2 *(delta)) - (np.sum(y * x * delta))**2\n\ndef convolve(signal_in, n_convolutions, delta):\n    output = np.zeros(len(signal_in))\n    if n_convolutions == 0:\n        return output\n    elif n_convolutions == 1:\n        return signal_in\n    else:\n        output = signal_in.copy()\n        for i in range(n_convolutions - 1):\n            output = signal.fftconvolve(\n                output * delta, signal_in, mode=\"same\"\n            )\n        return output\n\n\ndef server(input, output, session):\n\n    fig = go.FigureWidget()\n    fig.add_trace(\n        go.Scatter(\n            mode=\"lines\",\n            line=dict(color=\"black\", width=3),\n            x=[],\n            y=[],\n            name=\"loi de de la moyenne empirique&lt;br&gt;(variance adéquate)\",\n        )\n    )\n\n    fig.add_trace(\n        go.Scatter(\n            x=x,\n            y=np.exp(-(x**2) / (2 * var)) / np.sqrt(2 * var * np.pi),\n            mode=\"lines\",\n            line=dict(dash=\"dash\", color=\"red\", width=2),\n            name=f\"Loi normale&lt;br&gt;(variance adéquate)\",\n        )\n    )\n    fig.update_xaxes(range=[-3, 3], position=0.)\n    fig.update_yaxes(range=[0, 2 * np.max(y)], position=0.5, showticklabels=False)\n\n\n    fig.update_layout(\n        template=\"simple_white\",\n        showlegend=True,\n        autosize=True,\n        title=dict(text=\"Densité : &lt;br&gt; moyenne de n variables aléatoires\", yanchor=\"top\", y=0.95),\n        title_x=0.5,\n    )\n    fig.update_layout(legend=dict(\n        yanchor=\"top\",\n        y=0.95,\n        xanchor=\"left\",\n        x=0.8,\n        font=dict(size= 18)\n    ))\n    fig.update_layout(\n    height=250,\n    margin=dict(l=0, r=0, b=10, t=100),\n    )\n\n    register_widget(\"my_widget\", fig)\n\n    @reactive.Effect\n    def _():\n        if str(input.loi()) == 'uniforme':\n            y = np.zeros(nnzeros)\n            mask = np.where(np.abs(x) &lt;= 0.5, 1, 0)\n            y[mask == 1] = 1\n\n        else:\n            y=np.exp(-np.abs(x)) / 2\n            y = y / (np.sum(y) * delta)\n\n        var = np.sum(y * x**2 * delta) - (np.sum(y * x * delta))**2\n\n        y_display = convolve(y, input.n_iter(), delta)\n        # Update data in fig:\n        fig.data[0].x = x / np.sqrt(input.n_iter())\n        fig.data[0].y = y_display * np.sqrt(input.n_iter())\n        fig.data[1].y = np.exp(-(x**2) / (2 * var)) / np.sqrt(2 * var * np.pi)\n        fig.update_yaxes(range=[0, 2 * np.max(y)], position=0.5, showticklabels=False)\n\n\napp = App(app_ui, server)\n\n\n\n\n\n\n\nPour aller plus loin\n\n\nPour plus d’info sur les convolutions, voir la vidéo de 3Blue1Brown : Convolutions | Why X+Y in probability is a beautiful mess, 🇬🇧\n\n\n\n\n\nThéorèmes asymptotiques",
    "crumbs": [
      "Slides",
      "Théorèmes asymptotiques"
    ]
  },
  {
    "objectID": "TP/TP1.html",
    "href": "TP/TP1.html",
    "title": "TP1: Prise en main de Python",
    "section": "",
    "text": "Objectifs de ce TP\n\n\n\n\nUtiliser les opérateurs classiques en Python (+,-,*,/,**,@), etc., savoir créer une fonction, générer un graphique clair et lisible\nQu’est-ce que la précision de calcul ? Comment utiliser de une visualisation pour mieux comprendre un théorème ou une fonction ?\nComprendre au mieux comment utiliser les fonctions aléatoires (principalement les générateurs) en numpy.",
    "crumbs": [
      "TP",
      "TP1: Prise en main de Python"
    ]
  },
  {
    "objectID": "TP/TP1.html#prise-en-main-de-python",
    "href": "TP/TP1.html#prise-en-main-de-python",
    "title": "TP1: Prise en main de Python",
    "section": "Prise en main de Python",
    "text": "Prise en main de Python\nPython est un langage ouvert qui permet de manipuler des données, faire des analyses statistiques, tracer des graphes, et bien d’autres choses encore. Il est distribué gratuitement et vous pouvez le télécharger et l’installer sur une machine personnelle. Dans ce premier TP, on présente les bases de Python.\nPour plus de détails on pourra consulter les ouvrages:\n\nIntroduction à Python Cours de Python 🇫🇷\nHLMA310 - Logiciels scientifiques 🇫🇷\nManuel d’algorithmique en Python (Courant et al. 2013) 🇫🇷\n\n\nCourant, J., M. de Falco, S. Gonnord, J.-C. Filliâtre, S. Conchon, G. Dowek, et B. Wack. 2013. Informatique pour tous en classes préparatoires aux grandes écoles: Manuel d’algorithmique et programmation structurée avec Python. Eyrolles.\n\nInstallation de Python\nTout est déjà installé sur les ordinateurs de l’université. Cette section n’est utile que si vous souhaitez utiliser votre propre machine.\nLe conseil principal, est d’installer VSCode et d’utiliser l’extension Python associée. Pour Python, privilégier Conda (ou Mamba) pour installer les packages, voir par exemple: installer-anaconda.\n\n\nL’environnement de travail VSCode / VSCodium\nOn travaillera sous VSCodium (une variante de VSCode) sur les machines de l’Université, un éditeur de texte qui permet de travailler avec Python, mais aussi avec LaTeX, Markdown, R, etc.\n\nLancer l’application VSCodium, par exemple en cliquant sur l’icône “Application Menu” en haut à gauche de votre écran, puis en tapant “VSCodium” dans la barre de recherche. La démarche à suivre est visible dans la vidéo ci-dessous:\n\n\n\n\nSi besoin (à ne faire qu’une fois), il vous faut installer l’extension “https://open-vsx.org/extension/ms-python/python”. Pour cela, il y a plusieurs stratégies. La plus simple consiste à cliquer sur le menu d’installation et chercher l’application Python “Python, extension for Visual Studio Code”, proposée par Microsoft, Intellisense, (attention il y a beaucoup, choisir la bonne, avec plusieurs millions d’étoiles et de téléchargements):\n\n\nRemarque: Une alternative est d’aller dans le menu “View/Command Palette” (accessible avec ctrl + shift + p), et taper “Extensions : install extensions” et installer l’extension “Python, extension for Visual Studio Code” (proposée par Microsoft). Au besoin, il faudra recharger (reload) VSCodium. Si vous avez déjà installé l’extension Python sur votre machine personnelle, vous pouvez passer cette étape.\nLa même opération devra être faite pour installer l’extension “https://open-vsx.org/extension/ms-toolsai/jupyter” (proposée par Microsoft,) qui nous permettra de manipuler des fenêtres interactives:\n\n\n\n\nPremiers pas\n\nCréer un nouveau fichier dans VSCodium intitulé HAX603X_tp1.py, et sauvegarder le dans un dossier HAX603X.\nDans ce fichier, copier-coller le code de la boîte suivante. On pourra alors lancer des cellules de code en tapant sur shift + enter dans une cellule délimitée par les symboles \\# \\%\\%. On peut aussi lancer la cellule en cliquant sur le bouton “run cell” dans VSCodium (ou clique droit puis une option de type “run cell” ou “run all cell”).\n\n\n# %%\n# Début de cellule\nprint(1 + 3)  # commentaire en ligne\n# %%\n# Une autre cellule\nprint(2**3)  # commentaire en ligne\n\n4\n8\n\n\n\n\n\nCliquer dans VSCodium sur la version de “Python” en bas de votre écran et choisir sur les machines de l’école l’environnement ‘datascience’ (version: 3.10.6 au 20/01/2024). Si vous travaillez sur votre machine personnelle, choisissez un environnement de base, ou bien créer un environnement conda qui vous conviendra, par exemple avec Miniconda1.\nVérifier que maintenant vous pouvez lancer une cellule, par exemple en tapant crtl + enter, ou bien en cliquant sur le bouton “run cell”.\n\n1 Installer un environnement de développement Python avec Conda\n\nL’environnement de travail\nVous voyez apparaître plusieurs fenêtres :\n\nla console (à droite), avec les environnement et l’historique (en haut à droite)\nla fenêtre de texte (à gauche)\n\nLa console permet d’exécuter des instructions ou commandes. C’est ici que vous donnez vos instructions et que s’affichent les résultats demandés. La fenêtre d’environnement et d’historique recense l’historique des commandes et les variables qui ont été définies. Enfin, la fenêtre de texte permet d’écrire du texte, des commentaires, bref les fichiers que vous conserverez.\nUne manière simple de garder traces de vos calculs/instructions est de les écrire dans un fichier texte (ici HAX603X_tp1.py), et de les délimiter par des symboles \\# \\%\\% (voir ci-dessus), et de les lancer en tapant shift + enter dans une cellule délimitée par les symboles \\# \\%\\%.\nUne première utilisation basique de Python concerne les calculs. Vous pouvez entrer toutes les opérations classiques : addition +, soustraction -, multiplication *, division /, puissance **, etc. Les fonctions usuelles sont également déjà programmées en Python, mais nécessite le chargement du package numpy : exponentielle, logarithme, fonctions trigonométriques, racine carrée, etc.\nPour cela il suffit de taper import numpy as np dans une cellule de code, puis d’utiliser les fonctions de numpy comme suit par exemple:\nimport numpy as np\n\nprint(np.exp(1))\nprint(np.log(2))\nprint(np.sin(np.pi))\n\n\n\n2.718281828459045\n0.6931471805599453\n1.2246467991473532e-16\n\n\n\n\nQuestion : fonctions mathématiques\nEntrez quelques opérations de base pour vous familiariser avec les instructions sur Python. Faire de même avec les fonctions np.exp, np.log, np.sin, np.cos, np.tan, np.sqrt, np.abs,np.round. Entrer les instructions 1/0 et np.sqrt(-2). Que constatez-vous ?\nOn remarquera qu’on peut utiliser le symbole np.inf pour représenter l’infini. Par ailleurs, si un résultat n’est pas possible (par exemple en tapant np.sqrt(-2) ou np.inf - np.inf), alors on obtient nan qui signifie Not a Number.\nIl faut se souvenir que les calculs numériques ne sont pas toujours exacts du fait de la discrétisation des nombres sur machine. Taper par exemple np.sin(0), np.sin(2*np.pi) et np.sin(np.pi*10**16). Voir aussi les différences entre:\n\nprint(0.6, 0.3 + 0.2 + 0.1)\nprint(0.6, 0.1 + 0.2 + 0.3)\n\n0.6 0.6\n0.6 0.6000000000000001\n\n\n\n\n\n\n\n\nPour aller plus loin\n\n\n\nOn pourra consulter https://0.30000000000000004.com/ pour plus de détails sur les ce type de phénomènes.\n\n\n\n\n\nAide en Python\nOn peut utiliser l’aide de base de python avec les commandes help(la-fonction) ou ?la-fonction. L’aide en ligne est aussi conseillée, surtout pour la création de graphiques avec matplotlib pour avoir plus de détails et des galleries de visualisation.\n\n\nRépertoire de travail\nLe répertoire de travail (🇬🇧: working directory) est le répertoire par défaut, c’est-à-dire le répertoire qui s’ouvre quand vous cliquez sur le bouton pour enregistrer un fichier. La commande pour connaître le répertoire de travail actuel est getcwd du package os:\n\nimport os\nprint(os.getcwd())\n\n/home/jsalmon/Documents/Mes_cours/Montpellier/HAX603X/HAX603X/TP\n\n\nPour changer le répertoire de travail, on pourra utiliser la commande os.chdir avec un nom de répertoire (valide) entre guillemets, par exemple sous Linux la commande suivante permet de remonter d’un cran dans l’arborescence des répertoires:\n\nos.chdir('../')\n\nSi l’on ferme la fenêtre interactive (à droite), alors exécuter une cellule lancera une nouvelle fenêtre interactive dans le répertoire de travail qui correspond au fichier courant que l’on édite (ici le fichier HAX603X_tp1.py).\n\n\nCréation et affectation de variables\nPour créer des objets, il suffit d’utiliser la commande =.\n\nQuestion : variables\nCréer une variable x qui contient la valeur 12. Effectuer des calculs du type x+3, x**4, 4*x pour vérifier que tout se passe comme prévu.\nEn pratique on donnera des noms d’objets pertinents, par exemple\n\ndistance = 105  # en km\ntemps = 2  # en heures\nvitesse = distance/temps  # en km/h\n\nOn remarquera que lorsque l’on crée des objets, ils sont stockées dans l’environnement de travail (chercher l’onglet variables de la fenêtre interactive).",
    "crumbs": [
      "TP",
      "TP1: Prise en main de Python"
    ]
  },
  {
    "objectID": "TP/TP1.html#numpy-et-calcul-scientifique-en-python",
    "href": "TP/TP1.html#numpy-et-calcul-scientifique-en-python",
    "title": "TP1: Prise en main de Python",
    "section": "numpy et calcul scientifique en Python",
    "text": "numpy et calcul scientifique en Python\nnumpy est l’outil de base en Python pour faire du calcul vectoriel et matriciel.\n\nVecteurs en numpy\nPour créer un vecteur, la commande de base est la fonction np.array:\n\nv = np.array([1, 2, 3])\nprint(v)\n\n[1 2 3]\n\n\nEnsuite, on peut concaténer des vecteurs, les multiplier par une constante, leur ajouter une constante, les élever à une certaine puissance, etc. Si on manipule deux vecteurs, on prendra garde à leur taille.\n\nQuestion : Vecteurs\nCréez un vecteur v1 composé des réels 7, 8, 3, un vecteur v2 composé des réels -0.5, 120, -12, et un vecteur v3 composé des réels 0, 1, 0, 1. Testez les commandes suivantes : v1-7, v2**4, 10*v3, v1+v2, v1*v2, v1/v2, v1+v3.\nIl existe un grand nombre de fonctions mathématiques qui s’appliquent directement sur un vecteur : np.sum(), np.prod(), len(), np.min(), np.max(), np.nanmax(), np.argmax(), np.mean(), np.median(), np.var(), np.std().\nEnfin on peut aussi utiliser des fonctions de tri, partiel ou non: np.sort(), np.argsort(), np.partition(), np.argpartition() on pourra consulter l’aide en ligne pour plus de détails: https://numpy.org/doc/stable/reference/routines.sort.html, et les teser sur le vecteur v2 par exemple.\n\n\nQuestion : Opérations sur les vecteurs\nCréez un vecteur de taille 5 et appliquez-lui les fonctions précédentes. Si vous ne comprenez pas la sortie (utiliser l’aide avec ? ou la documentation en ligne).\n\n\n\n\n\n\nPour aller plus loin: vectorisation\n\n\n\nVous pourrez consulter les commandes décrites visuellement ici pour créer des vecteurs et/ou des matrices classiques.\n\n\n\n\n\nSuites régulières\nUne autre manière de créer des vecteurs consiste à créer des suites régulières :\n\nLa commande np.arange(n1, n2) crée un vecteur de réels partant de n1 et croissant d’une unité pour arriver à n2 (exclu). On peut changer le pas en ajoutant un argument optionnel np.arange(n1, n2, step=pas). Ainsi,\n\nnp.arange(0, 10, step=2)\n\narray([0, 2, 4, 6, 8])\n\n\nLa commande np.tile() permet de répéter un vecteur un nombre de fois fixé.\n\n\nQuestion: arange et tile\nExécutez les commandes suivantes et essayer d’analyser les sorties :\n\nnp.arange(9, 13)\nnp.arange(3, -8, step=-1)\nnp.arange(9, 13, step=2)\nnp.arange(9, 13, step=3)\nnp.tile(np.arange(9, 13, step=3), (4, 1))\n\narray([[ 9, 12],\n       [ 9, 12],\n       [ 9, 12],\n       [ 9, 12]])\n\n\nEnfin, pour extraire la valeur d’indice i d’un vecteur x, on tapera x[i] (avec la convention que Python commence à énumérer à 0). Plus généralement, pour extraire les valeurs associées aux indices 3, 4 et 7, on tapera x[[3,4,7]]. Le vérifier sur un vecteur de taille 10. On peut aussi extraire des sous parties de vecteurs, par exemple x[3:7] pour extraire les valeurs d’indice 3, 4, 5 et 6, ou bien x[3:] pour extraire les valeurs d’indice 3, 4, 5, etc. jusqu’à la fin.\n\n\n\nMatrices en numpy\nLa fonction np.shape permet de connaître la taille d’un vecteur ou d’une matrice. On regardera son comportement sur les vecteurs notamment.\n\nQuestion : opérations élémentaires\nManipulez les opérations classiques sur des matrices (arrays) de numpy (si vous êtes déjà habitué à numpy vous pouvez continuer)\nOpérations termes à termes:\n\n# Somme de deux vecteurs\nA = np.array([1.0, 2, 3])\nB = np.array([-1, -2, -3.0])\n\n# Attribuer à la variable C la somme de A et B\nsum_A_B = ...  # XXX TODO\n\nnp.testing.assert_allclose(np.zeros((3,)), sum_A_B)\nprint(\"it worked\")\n\n# Le produit terme à terme avec *\nprod_A_B = ...  # XXX TODO\n\nnp.testing.assert_allclose(np.array([-1.0, -4, -9]), prod_A_B)\nprint(\"it worked\")\n\n# Remarque: la même chose fonctionne terme à terme avec \\, ** (puissance)\nnp.testing.assert_allclose(np.array([1.0, 4, 9]), A ** 2)\nprint(\"it worked: even for powers\")\n\nLe produit scalaire (ou matriciel) est l’opérateur @. Vérifiez que pour la matrice J ci-dessous J^3 = Id de deux façons. Pour cela on pourra aussi utiliser la puissance matricielle avec np.linalg.matrix_power:\n\nJ = np.array([[0, 0, 1.0], [1.0, 0, 0], [0, 1.0, 0]])\n\nI3 = np.eye(3)\n\nnp.testing.assert_allclose(I3, ...)  # XXX TODO\nprint(\"it worked: method 1\")\nnp.testing.assert_allclose(I3, ...)  # XXX TODO\nprint(\"it worked: method 2\")\n\n\n\nQuestion : résolution de systèmes linéaires\nPour résoudre le système de la forme Ax=b en mathématiques, la formule explicite est x=A^{-1}b (dans le cas où A est inversible).\n\n\n\n\n\n\nImportant\n\n\n\nEn pratique vous n’utiliserez (presque) jamais l’inversion de matrice ! En effet, on n’inverse JAMAIS JAMAIS (!) une matrice sans une très bonne raison. La plupart du temps il existe des méthodes plus rapides pour résoudre un système numériquement !\n\n\n\nprint(f\"L'inverse de la matrice: \\n {J} \\n est \\n {np.linalg.inv(J)}\")\n\nn = 20  # XXX TODO: tester avec n=100\nJbig = np.roll(np.eye(n), -1, axis=1)  # matrice de permutation de taille n\nprint(Jbig)\n\nb = np.arange(n)\nprint(b)\n\n# on peut transposer une matrice facilement de 2 manières:\nprint(Jbig)\nprint(Jbig.T)\nprint(np.transpose(Jbig))\n\nComparons niveau temps d’execution l’inversion explicite vs. l’utilisation d’un solveur de système linéaire tel que np.linalg.solve:\n\nimport time\n# Résolution de système par une méthode naive: inversion de matrice\nt0 = time.perf_counter()  # XXX TODO\ny1 = ... @ b\ntiming_naive = time.perf_counter() - t0\nprint(\n    f\"Temps pour résoudre un système avec la formule mathématique: {timing_naive - t0:.4f} s.\"\n)\n\n# Résolution de système par une méthode adaptée: fonctions dédiée de `numpy``\nt0 = time.perf_counter()\ny2 = ...\ntiminig_optimized = time.perf_counter()\nprint(\n    f\"Temps pour résoudre un système avec la formule mathématique: {timing_optimized:.4f} s.\\nC'est donc {timing_naive / timing_optimized} fois plus rapide d'utiliser la seconde formulation\"\n)\n\nnp.testing.assert_allclose(y1, y2)\nprint(\"Les deux méthodes trouvent le même résultat\")\n\n\n\n\n\n\n\nAstuce\n\n\n\nPour des comparaisons d’efficacité temporelle plus poussées on pourra utiliser le package timeit2 ou voir la discussion ici: https://superfastpython.com/time-time-vs-time-perf_counter/.\n\n\n2 lien vers la documentation de timeit\n\nQuestion découpage (🇬🇧: slicing)\nLe découpage permet d’extraire des éléments selon un critère (position, condition, etc.). La notation : signifie “tout le monde”, et l’indexation commence en 0. Pour partir de la fin, il est possible de mettre le signe - devant le nombre: ainsi -1 renvoie donc au dernier élément. Enfin, on peut extraire des sous suites d’indices pair ou impair, par exemple x[::2] pour extraire les valeurs d’indice pair, ou bien x[1::2] pour extraire les valeurs d’indice impair de x. Enfin on peut aussi utiliser le signe - pour partir de la fin, par exemple x[-1] pour extraire la dernière valeur, ou bien x[-2] pour extraire l’avant-dernière valeur.\n\nprint(f\"The first column is {J[:, 0]}\")\n\n# Afficher la deuxième ligne de J\nprint(f\"The second row is {...}\")  # XXX TODO\n\nMettre à zéro une ligne sur 2 de la matrice identité de taille 5\\times 5\n\nC = np.eye(5, 5)\nC[...,...] = 0  # mettre à zéro une ligne sur deux. # XXX TODO",
    "crumbs": [
      "TP",
      "TP1: Prise en main de Python"
    ]
  },
  {
    "objectID": "TP/TP1.html#visualisation-dune-figure",
    "href": "TP/TP1.html#visualisation-dune-figure",
    "title": "TP1: Prise en main de Python",
    "section": "Visualisation d’une figure",
    "text": "Visualisation d’une figure\nPour lancer une figure on peut utiliser la package matplotlib. Un exemple utilisant le package numpy pour créer une figure simple est donné ci-dessous, dans la Figure 1.\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = np.cos(2 * np.pi * r)\n\nfig, ax = plt.subplots()\nax.plot(r,theta)\nax.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\nFigure 1: Une figure simple.",
    "crumbs": [
      "TP",
      "TP1: Prise en main de Python"
    ]
  },
  {
    "objectID": "TP/TP1.html#aspects-aléatoires",
    "href": "TP/TP1.html#aspects-aléatoires",
    "title": "TP1: Prise en main de Python",
    "section": "Aspects aléatoires",
    "text": "Aspects aléatoires\nLe module random de numpy permet d’utiliser l’aléatoire et des lois usuelles en Python. On crée d’abord un générateur qui nous permettra ensuite d’appeler les lois voulues comme suite:\n\nimport numpy as np  # package de calcul scientifique\nimport matplotlib.pyplot as plt  # package graphique\n\ngenerateur = np.random.default_rng()\ngenerateur.normal()\n\n-0.07033936605617876\n\n\n\nQuestion : Matrices aléatoires\nCréer une matrice de taille 4\\times 5 dont les entrées sont i.i.d de loi de Laplace d’espérance 0 et de variance 2. Lancer plusieurs fois le code et observez les changements. On pourra s’aider de l’aide en ligne si besoin.\n\ngenerateur = np.random.default_rng()\nM = ...  # XXX TODO\nprint(M)\n\n\n\nQuestion : Reproduire des résultats\nPour reproduire des résultats ou débugger un code, il est utile de “figer” l’aléatoire. On utilise pour cela une graine (🇫🇷 seed) dans la création du générateur. Fixez la graine à 0 dans default_rng() et lancez une génération aléatoire. Commenter.\n\nrng = np.random.default_rng(0)\nrng.normal()\nrng2 = np.random.default_rng(...)\nrng.normal()\n\n\n\nQuestion : afficher un histogramme\nAvec plt.subplot, créer 3 histogrammes de 100 tirages aléatoires de distributions suivantes:\n\nloi gaussienne (centrée-réduite)\nloi de Cauchy\nloi de Laplace.\n\nOn utilisera les mêmes paramètres de centrage et d’échelle pour les trois lois.\n\nn_samples = 10000\nX = np.empty([n_samples, 3])\nX[:, 0] = ...\nX[:, 1] = ...\nX[:, 2] = ...\n\nlois = [\"Loi de Gauss\", \"Loi de Laplace\", \"Loi de Cauchy\"]\n\nfig_hist, ax = plt.subplots(3, 1, figsize=(3, 3))\n\nfor i, name in enumerate(lois):\n    ax[i].hist(..., bins=100, density=True)\n    ax[i].set_title(name)\n\nplt.tight_layout()\nplt.show()\n\nDe manière complémentaire le module scipy.stats permet d’utiliser des lois usuelles, et de faire des tests statistiques. On pourra consulter la documentation en ligne pour plus de détails: https://docs.scipy.org/doc/scipy/reference/stats.html.\n\n\n\n\n\n\nPour aller plus loin.\n\n\n\nLa plupart des lois usuelles sont disponibles, cf. la documentation; vous pourrez en manipuler avec des widgets ici.",
    "crumbs": [
      "TP",
      "TP1: Prise en main de Python"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "HAX603X: Modélisation stochastique",
    "section": "",
    "text": "Aspects numériques de la modélisation aléatoire et statistiques (cours de Licence 3). Les sections en haut de page regroupe le contenus pédagogique : les slides seront présentés en cours, et pour aller un peu plus loin le “poly” est disponible en format “html” sur la page Cours.\n\n\n\n\n\n\nAvertissement\n\n\n\nSite en construction…(un peu de patience donc)\n\n\n\n\n\nJoseph Salmon: joseph.salmon@umontpellier.fr,\nBenjamin Charlier: benjamin.charlier@umontpellier.fr\n\nCe cours est issu du travail antérieur de la part de:\n\nNicolas Meyer\nBenoîte de Saporta et de l’aide de François-David Collin.\n\n\n\n\n\nBases de probabilités (en particulier “HAX506X- Théorie des Probabilités”): probabilité, densité, espérance, fonction de répartition, mesure, intégration, analyse numérique élémentaire, etc. (Foata et Fuchs 1996; Barbe et Ledoux 2006; Ouvrard 2007, 2008)\nProgrammation élémentaire (en Python): if … then… else …, for, while, fonctions, etc. HLMA310 - Logiciels scientifiques, (Courant et al. 2013), Cours de Python: Univ. Paris Diderot\n\n\nFoata, D., et A. Fuchs. 1996. Calcul des probabilités: cours et exercices corrigés. Masson.\n\nBarbe, Philippe, et Michel Ledoux. 2006. « Cours de Probabilite Pour la Licence (corrigée) ».\n\nOuvrard, J.-Y. 2007. Probabilités : Tome 2, Licence - CAPES. 2ᵉ éd. Enseignement des mathématiques. Cassini.\n\n———. 2008. Probabilités : Tome 1, Licence - CAPES. 2ᵉ éd. Enseignement des mathématiques. Cassini.\n\n\n\n\nGénérer l’aléa\n\ngénérateurs pseudo-aléatoires\nillustrations numériques et visualisation en Python (loi des grands nombres, théorème central limite)\nsimulations de variables aléatoires (méthode de l’inverse, méthode du rejet, cas spécifiques, etc.)\n\nMéthode de Monte-Carlo \n\nméthode de Monte-Carlo pour le calcul approché d’une intégrale\nréduction de la variance : variables antithétiques, variables de contrôle, échantillonnage préférentiel.\n\nCompléments\n\nvecteurs gaussiens et lien avec les lois usuelles de la statistique inférentielle (student, chi2)\nconstruction d’intervalles de confiance.\nmarche aléatoire simple, etc.\n\n\n\n\n\n\nTP notés : Rendu = fichier Python .py unique\n\nTP noté 1 : rendre en fin de session\nTP noté 2 : rendre en fin de session\n\nCC : devoir sur table d’une heure\nCoefficients:\n\nNote Session 1 : (40% CC + 30% TP 1 + 30% TP 2)\nNote Session 2 : (30% CC + 35% TP 1 + 35% TP 2)\n\n\n\n\n\nMoodle: HAX603X Modélisation Stochastique\n\n\n\n\n\n\nIntroduction à Python Cours de Python 🇫🇷\nHLMA310 - Logiciels scientifiques 🇫🇷\nManuel d’algorithmique en Python (Courant et al. 2013) 🇫🇷\nData Science: Python Data Science Handbook, With Application to Understanding Data by J. Van DerPlas, 2016; 🇬🇧  videos: Reproducible Data Analysis in Jupyter\nMath for journalists by Naël Shiab 🇬🇧\n\n\nCourant, J., M. de Falco, S. Gonnord, J.-C. Filliâtre, S. Conchon, G. Dowek, et B. Wack. 2013. Informatique pour tous en classes préparatoires aux grandes écoles: Manuel d’algorithmique et programmation structurée avec Python. Eyrolles.\n\n\n\n\nSoftware Dev. for datascience by J. Salmon and B. Charlier, 🇬🇧\nMonte Carlo Methods and Applications by Keenan Crane 🇬🇧\nChaîne de Markov: Markov Chains by Ethan N. Epperly 🇬🇧\nAdvanced Data Analysis from an Elementary Point of View by Cosma Shalizi; 🇬🇧\nMaximum likelihood by numerical optimization 🇬🇧\nConditionnement, martingales et autres preuves de la loi des grands nombres: (Williams 1991) 🇬🇧\n\n\n\n\nWilliams, D. 1991. Probability with martingales. Cambridge Mathematical Textbooks. Cambridge: Cambridge University Press.",
    "crumbs": [
      "HAX603X: Modélisation stochastique"
    ]
  },
  {
    "objectID": "index.html#professeurs",
    "href": "index.html#professeurs",
    "title": "HAX603X: Modélisation stochastique",
    "section": "",
    "text": "Joseph Salmon: joseph.salmon@umontpellier.fr,\nBenjamin Charlier: benjamin.charlier@umontpellier.fr\n\nCe cours est issu du travail antérieur de la part de:\n\nNicolas Meyer\nBenoîte de Saporta et de l’aide de François-David Collin.",
    "crumbs": [
      "HAX603X: Modélisation stochastique"
    ]
  },
  {
    "objectID": "index.html#prérequis",
    "href": "index.html#prérequis",
    "title": "HAX603X: Modélisation stochastique",
    "section": "",
    "text": "Bases de probabilités (en particulier “HAX506X- Théorie des Probabilités”): probabilité, densité, espérance, fonction de répartition, mesure, intégration, analyse numérique élémentaire, etc. (Foata et Fuchs 1996; Barbe et Ledoux 2006; Ouvrard 2007, 2008)\nProgrammation élémentaire (en Python): if … then… else …, for, while, fonctions, etc. HLMA310 - Logiciels scientifiques, (Courant et al. 2013), Cours de Python: Univ. Paris Diderot\n\n\nFoata, D., et A. Fuchs. 1996. Calcul des probabilités: cours et exercices corrigés. Masson.\n\nBarbe, Philippe, et Michel Ledoux. 2006. « Cours de Probabilite Pour la Licence (corrigée) ».\n\nOuvrard, J.-Y. 2007. Probabilités : Tome 2, Licence - CAPES. 2ᵉ éd. Enseignement des mathématiques. Cassini.\n\n———. 2008. Probabilités : Tome 1, Licence - CAPES. 2ᵉ éd. Enseignement des mathématiques. Cassini.",
    "crumbs": [
      "HAX603X: Modélisation stochastique"
    ]
  },
  {
    "objectID": "index.html#description-du-cours",
    "href": "index.html#description-du-cours",
    "title": "HAX603X: Modélisation stochastique",
    "section": "",
    "text": "Générer l’aléa\n\ngénérateurs pseudo-aléatoires\nillustrations numériques et visualisation en Python (loi des grands nombres, théorème central limite)\nsimulations de variables aléatoires (méthode de l’inverse, méthode du rejet, cas spécifiques, etc.)\n\nMéthode de Monte-Carlo \n\nméthode de Monte-Carlo pour le calcul approché d’une intégrale\nréduction de la variance : variables antithétiques, variables de contrôle, échantillonnage préférentiel.\n\nCompléments\n\nvecteurs gaussiens et lien avec les lois usuelles de la statistique inférentielle (student, chi2)\nconstruction d’intervalles de confiance.\nmarche aléatoire simple, etc.",
    "crumbs": [
      "HAX603X: Modélisation stochastique"
    ]
  },
  {
    "objectID": "index.html#modalité-de-contrôle-des-connaissances",
    "href": "index.html#modalité-de-contrôle-des-connaissances",
    "title": "HAX603X: Modélisation stochastique",
    "section": "",
    "text": "TP notés : Rendu = fichier Python .py unique\n\nTP noté 1 : rendre en fin de session\nTP noté 2 : rendre en fin de session\n\nCC : devoir sur table d’une heure\nCoefficients:\n\nNote Session 1 : (40% CC + 30% TP 1 + 30% TP 2)\nNote Session 2 : (30% CC + 35% TP 1 + 35% TP 2)",
    "crumbs": [
      "HAX603X: Modélisation stochastique"
    ]
  },
  {
    "objectID": "index.html#moodle",
    "href": "index.html#moodle",
    "title": "HAX603X: Modélisation stochastique",
    "section": "",
    "text": "Moodle: HAX603X Modélisation Stochastique",
    "crumbs": [
      "HAX603X: Modélisation stochastique"
    ]
  },
  {
    "objectID": "index.html#ressources-supplémentaires",
    "href": "index.html#ressources-supplémentaires",
    "title": "HAX603X: Modélisation stochastique",
    "section": "",
    "text": "Introduction à Python Cours de Python 🇫🇷\nHLMA310 - Logiciels scientifiques 🇫🇷\nManuel d’algorithmique en Python (Courant et al. 2013) 🇫🇷\nData Science: Python Data Science Handbook, With Application to Understanding Data by J. Van DerPlas, 2016; 🇬🇧  videos: Reproducible Data Analysis in Jupyter\nMath for journalists by Naël Shiab 🇬🇧\n\n\nCourant, J., M. de Falco, S. Gonnord, J.-C. Filliâtre, S. Conchon, G. Dowek, et B. Wack. 2013. Informatique pour tous en classes préparatoires aux grandes écoles: Manuel d’algorithmique et programmation structurée avec Python. Eyrolles.\n\n\n\n\nSoftware Dev. for datascience by J. Salmon and B. Charlier, 🇬🇧\nMonte Carlo Methods and Applications by Keenan Crane 🇬🇧\nChaîne de Markov: Markov Chains by Ethan N. Epperly 🇬🇧\nAdvanced Data Analysis from an Elementary Point of View by Cosma Shalizi; 🇬🇧\nMaximum likelihood by numerical optimization 🇬🇧\nConditionnement, martingales et autres preuves de la loi des grands nombres: (Williams 1991) 🇬🇧\n\n\n\n\nWilliams, D. 1991. Probability with martingales. Cambridge Mathematical Textbooks. Cambridge: Cambridge University Press.",
    "crumbs": [
      "HAX603X: Modélisation stochastique"
    ]
  }
]