[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "HAX603X: Modélisation stochastique",
    "section": "",
    "text": "Aspects numériques de la modélisation aléatoire et statistiques (cours de Licence 3). Les sections en haut de page regroupe le contenus pédagogique : les slides seront présentés en cours, et pour aller un peu plus loin le “poly” est disponible en format “html” sur la page Cours.\n\n\n\n\n\n\nAvertissement\n\n\n\nSite en construction…(un peu de patience donc)\n\n\n\n\n\nJoseph Salmon: joseph.salmon@umontpellier.fr,\nBenjamin Charlier: benjamin.charlier@umontpellier.fr\n\nCe cours est issu du travail antérieur de la part de:\n\nNicolas Meyer\nBenoîte de Saporta et de l’aide de François-David Collin.\n\n\n\n\n\nBases de probabilités (en particulier “HAX506X- Théorie des Probabilités”): probabilité, densité, espérance, fonction de répartition, mesure, intégration, analyse numérique élémentaire, etc. (Foata et Fuchs 1996; Barbe et Ledoux 2006; Ouvrard 2007, 2008)\nProgrammation élémentaire (en Python): if … then… else …, for, while, fonctions, etc. HLMA310 - Logiciels scientifiques, (Courant et al. 2013), Cours de Python: Univ. Paris Diderot\n\n\nFoata, D., et A. Fuchs. 1996. Calcul des probabilités: cours et exercices corrigés. Masson.\n\nBarbe, Philippe, et Michel Ledoux. 2006. « Cours de Probabilite Pour la Licence (corrigée) ».\n\nOuvrard, J.-Y. 2007. Probabilités : Tome 2, Licence - CAPES. 2ᵉ éd. Enseignement des mathématiques. Cassini.\n\n———. 2008. Probabilités : Tome 1, Licence - CAPES. 2ᵉ éd. Enseignement des mathématiques. Cassini.\n\n\n\n\nGénérer l’aléa\n\ngénérateurs pseudo-aléatoires\nillustrations numériques et visualisation en Python (loi des grands nombres, théorème central limite)\nsimulations de variables aléatoires (méthode de l’inverse, méthode du rejet, cas spécifiques, etc.)\n\nMéthode de Monte-Carlo \n\nméthode de Monte-Carlo pour le calcul approché d’une intégrale\nréduction de la variance : variables antithétiques, variables de contrôle, échantillonnage préférentiel.\n\nCompléments\n\nvecteurs gaussiens et lien avec les lois usuelles de la statistique inférentielle (student, chi2)\nconstruction d’intervalles de confiance.\nmarche aléatoire simple, etc.\n\n\n\n\n\n\nTP notés : Rendu = fichier Python .py unique\n\nTP noté 1 : rendre en fin de session\nTP noté 2 : rendre en fin de session\n\nCC : devoir sur table d’une heure\nCoefficients:\n\nNote Session 1 : (40% CC + 30% TP 1 + 30% TP 2)\nNote Session 2 : (30% CC + 35% TP 1 + 35% TP 2)\n\n\n\n\n\nMoodle: HAX603X Modélisation Stochastique\n\n\n\n\n\n\nIntroduction à Python Cours de Python 🇫🇷\nHLMA310 - Logiciels scientifiques 🇫🇷\nManuel d’algorithmique en Python (Courant et al. 2013) 🇫🇷\nData Science: Python Data Science Handbook, With Application to Understanding Data by J. Van DerPlas, 2016; 🇬🇧  videos: Reproducible Data Analysis in Jupyter\nMath for journalists by Naël Shiab 🇬🇧\n\n\nCourant, J., M. de Falco, S. Gonnord, J.-C. Filliâtre, S. Conchon, G. Dowek, et B. Wack. 2013. Informatique pour tous en classes préparatoires aux grandes écoles: Manuel d’algorithmique et programmation structurée avec Python. Eyrolles.\n\n\n\n\nSoftware Dev. for datascience by J. Salmon and B. Charlier, 🇬🇧\nChaîne de Markov: Markov Chains by Ethan N. Epperly 🇬🇧\nAdvanced Data Analysis from an Elementary Point of View by Cosma Shalizi; 🇬🇧\nMaximum likelihood by numerical optimization 🇬🇧\nConditionnement, martingales et autres preuves de la loi des grands nombres: (Williams 1991) 🇬🇧\n\n\n\n\nWilliams, D. 1991. Probability with martingales. Cambridge Mathematical Textbooks. Cambridge: Cambridge University Press.",
    "crumbs": [
      "HAX603X: Modélisation stochastique"
    ]
  },
  {
    "objectID": "index.html#professeurs",
    "href": "index.html#professeurs",
    "title": "HAX603X: Modélisation stochastique",
    "section": "",
    "text": "Joseph Salmon: joseph.salmon@umontpellier.fr,\nBenjamin Charlier: benjamin.charlier@umontpellier.fr\n\nCe cours est issu du travail antérieur de la part de:\n\nNicolas Meyer\nBenoîte de Saporta et de l’aide de François-David Collin.",
    "crumbs": [
      "HAX603X: Modélisation stochastique"
    ]
  },
  {
    "objectID": "index.html#prérequis",
    "href": "index.html#prérequis",
    "title": "HAX603X: Modélisation stochastique",
    "section": "",
    "text": "Bases de probabilités (en particulier “HAX506X- Théorie des Probabilités”): probabilité, densité, espérance, fonction de répartition, mesure, intégration, analyse numérique élémentaire, etc. (Foata et Fuchs 1996; Barbe et Ledoux 2006; Ouvrard 2007, 2008)\nProgrammation élémentaire (en Python): if … then… else …, for, while, fonctions, etc. HLMA310 - Logiciels scientifiques, (Courant et al. 2013), Cours de Python: Univ. Paris Diderot\n\n\nFoata, D., et A. Fuchs. 1996. Calcul des probabilités: cours et exercices corrigés. Masson.\n\nBarbe, Philippe, et Michel Ledoux. 2006. « Cours de Probabilite Pour la Licence (corrigée) ».\n\nOuvrard, J.-Y. 2007. Probabilités : Tome 2, Licence - CAPES. 2ᵉ éd. Enseignement des mathématiques. Cassini.\n\n———. 2008. Probabilités : Tome 1, Licence - CAPES. 2ᵉ éd. Enseignement des mathématiques. Cassini.",
    "crumbs": [
      "HAX603X: Modélisation stochastique"
    ]
  },
  {
    "objectID": "index.html#description-du-cours",
    "href": "index.html#description-du-cours",
    "title": "HAX603X: Modélisation stochastique",
    "section": "",
    "text": "Générer l’aléa\n\ngénérateurs pseudo-aléatoires\nillustrations numériques et visualisation en Python (loi des grands nombres, théorème central limite)\nsimulations de variables aléatoires (méthode de l’inverse, méthode du rejet, cas spécifiques, etc.)\n\nMéthode de Monte-Carlo \n\nméthode de Monte-Carlo pour le calcul approché d’une intégrale\nréduction de la variance : variables antithétiques, variables de contrôle, échantillonnage préférentiel.\n\nCompléments\n\nvecteurs gaussiens et lien avec les lois usuelles de la statistique inférentielle (student, chi2)\nconstruction d’intervalles de confiance.\nmarche aléatoire simple, etc.",
    "crumbs": [
      "HAX603X: Modélisation stochastique"
    ]
  },
  {
    "objectID": "index.html#modalité-de-contrôle-des-connaissances",
    "href": "index.html#modalité-de-contrôle-des-connaissances",
    "title": "HAX603X: Modélisation stochastique",
    "section": "",
    "text": "TP notés : Rendu = fichier Python .py unique\n\nTP noté 1 : rendre en fin de session\nTP noté 2 : rendre en fin de session\n\nCC : devoir sur table d’une heure\nCoefficients:\n\nNote Session 1 : (40% CC + 30% TP 1 + 30% TP 2)\nNote Session 2 : (30% CC + 35% TP 1 + 35% TP 2)",
    "crumbs": [
      "HAX603X: Modélisation stochastique"
    ]
  },
  {
    "objectID": "index.html#moodle",
    "href": "index.html#moodle",
    "title": "HAX603X: Modélisation stochastique",
    "section": "",
    "text": "Moodle: HAX603X Modélisation Stochastique",
    "crumbs": [
      "HAX603X: Modélisation stochastique"
    ]
  },
  {
    "objectID": "index.html#ressources-supplémentaires",
    "href": "index.html#ressources-supplémentaires",
    "title": "HAX603X: Modélisation stochastique",
    "section": "",
    "text": "Introduction à Python Cours de Python 🇫🇷\nHLMA310 - Logiciels scientifiques 🇫🇷\nManuel d’algorithmique en Python (Courant et al. 2013) 🇫🇷\nData Science: Python Data Science Handbook, With Application to Understanding Data by J. Van DerPlas, 2016; 🇬🇧  videos: Reproducible Data Analysis in Jupyter\nMath for journalists by Naël Shiab 🇬🇧\n\n\nCourant, J., M. de Falco, S. Gonnord, J.-C. Filliâtre, S. Conchon, G. Dowek, et B. Wack. 2013. Informatique pour tous en classes préparatoires aux grandes écoles: Manuel d’algorithmique et programmation structurée avec Python. Eyrolles.\n\n\n\n\nSoftware Dev. for datascience by J. Salmon and B. Charlier, 🇬🇧\nChaîne de Markov: Markov Chains by Ethan N. Epperly 🇬🇧\nAdvanced Data Analysis from an Elementary Point of View by Cosma Shalizi; 🇬🇧\nMaximum likelihood by numerical optimization 🇬🇧\nConditionnement, martingales et autres preuves de la loi des grands nombres: (Williams 1991) 🇬🇧\n\n\n\n\nWilliams, D. 1991. Probability with martingales. Cambridge Mathematical Textbooks. Cambridge: Cambridge University Press.",
    "crumbs": [
      "HAX603X: Modélisation stochastique"
    ]
  },
  {
    "objectID": "TD/TD1.html#test2",
    "href": "TD/TD1.html#test2",
    "title": "TD1:…",
    "section": "test2",
    "text": "test2",
    "crumbs": [
      "TD",
      "TD1:..."
    ]
  },
  {
    "objectID": "Slides/slides_notations_premiers_pas.html#notation-et-rappels-de-probabilités",
    "href": "Slides/slides_notations_premiers_pas.html#notation-et-rappels-de-probabilités",
    "title": "HAX603X: Modélisation stochastique",
    "section": "Notation et rappels de probabilités",
    "text": "Notation et rappels de probabilités\n\n\nEspace probabilisé: \\((\\Omega, {\\mathcal{F}}, \\mathbb{P})\\)\n\ncomposé d’un ensemble: \\(\\Omega\\)\nd’une tribu: \\(\\mathcal{F}\\)\nd’une mesure de probabilité: \\(\\mathbb{P}\\)\n\n\n\n\n\nDéfinition 1 (Variable aléatoire, v.a.) Soit \\((E, \\mathcal{E})\\) un espace mesurable. Une variable aléatoire est une application mesurable \\[\n    \\begin{array}{ccccc}\n        X & : & \\Omega & \\to     & E            \\\\\n            &   & \\omega & \\mapsto & X(\\omega)\\,.\n    \\end{array}\n\\] Ainsi \\(\\{\\omega \\in \\Omega : X(\\omega) \\in B\\} = X^{-1}(B) = \\{X \\in B\\} \\in \\mathcal{F}, \\forall B \\in \\mathcal{E}\\)"
  },
  {
    "objectID": "Slides/slides_notations_premiers_pas.html#loi-dune-variable-aléatoire-unidimensionnelle",
    "href": "Slides/slides_notations_premiers_pas.html#loi-dune-variable-aléatoire-unidimensionnelle",
    "title": "HAX603X: Modélisation stochastique",
    "section": "Loi d’une variable aléatoire (unidimensionnelle)",
    "text": "Loi d’une variable aléatoire (unidimensionnelle)\n\nDéfinition 2 (Loi d’une variable aléatoire) \nSoit \\(X : (\\Omega, \\mathcal{F}, \\mathbb{P}) \\to (E, \\mathcal{E})\\) une v.a. On appelle loi de \\(X\\) la mesure de probabilité sur \\((E, \\mathcal{E})\\) définie par \\[\n        \\begin{array}{ccccc}\n            \\mathbb{P}_X & : & \\mathcal{E} & \\to     & [0,1]          \\\\\n                 &   & B           & \\mapsto & \\mathbb{P}(X \\in B) \\enspace.\n        \\end{array}\n\\]\n\n\n\n\n\n\n\n\nNote\n\n\nLes propriétés de \\(\\mathbb{P}\\) assurent que \\(\\mathbb{P}_X\\) est bien une mesure de probabilité sur l’espace mesurable \\((E, \\mathcal{E})\\)"
  },
  {
    "objectID": "Slides/slides_notations_premiers_pas.html#lois-discrètes",
    "href": "Slides/slides_notations_premiers_pas.html#lois-discrètes",
    "title": "HAX603X: Modélisation stochastique",
    "section": "Lois discrètes",
    "text": "Lois discrètes\nLes variables aléatoires discrètes sont celles à valeurs dans un ensemble \\(E\\) discret, le plus souvent \\(\\mathbb{N}\\), muni de la tribu pleine \\(\\mathcal{F} = \\mathcal{P}(E)\\).\n\nExemple 1 (Loi de Bernoulli) Soit un paramètre \\(p \\in [0,1]\\), et \\(E=\\{0,1\\}\\), alors la loi de Bernouilli est donnée par \\(\\mathbb{P}(X=1)=1-\\mathbb{P}(X=0) = p\\).\nNotation: \\(\\quad\\) \\(X \\sim \\mathcal{B}(p)\\)\n\nExemple physique:  loi d’un tirage de pile ou face, de biais \\(p\\)\n\nExemple 2 (Loi binomiale) Soient \\(p \\in [0,1]\\) (biais) et \\(n \\in \\mathcal{N}^*\\) (nombre de tirages) alors la loi Binomiale est donnée par \\(\\mathbb{P}(X=k) = \\binom{n}{k} p^k (1-p)^{n-k}\\), pour \\(k \\in E=\\{0,\\dots,n\\}\\).\nNotation: \\(\\quad\\) \\(X \\sim \\mathcal{B}(n,p)\\)\n\nExemple physique:      loi du nombre de succès obtenus lors de \\(n\\) répétition indépendante d’une expérience aléatoire de Bernoulli de paramètre \\(p\\)"
  },
  {
    "objectID": "Slides/slides_notations_premiers_pas.html#lois-discrètes-ii",
    "href": "Slides/slides_notations_premiers_pas.html#lois-discrètes-ii",
    "title": "HAX603X: Modélisation stochastique",
    "section": "Lois discrètes (II)",
    "text": "Lois discrètes (II)\n\nExemple 3 (Loi géométrique) Soient \\(p \\in [0,1]\\) (biais), alors la loi géométrique est donnée par \\(\\mathbb{P}(X=k) = p (1-p)^{k-1}\\), pour \\(k \\in E=\\mathbb{N}^*\\).\nNotation: \\(\\quad\\) \\(X \\sim \\mathcal{G}(p)\\)\n\n\nExemple physique:     \n\nloi du nombre tirage nécessaire avant d’obtenir un succès obtenus en répétant indépendamment des expériences aléatoires de Bernoulli de paramètre \\(p\\)\n\n\n\n\nExemple 4 (Loi de Poisson) Pour \\(\\lambda&gt;0\\), la loi de Poisson de paramètre \\(\\lambda\\) est définie par \\(\\mathbb{P}(X=k) = e^{-\\lambda} \\lambda^k / k!\\), pour tout \\(k \\in E=\\mathbb{N}\\).\nNotation: \\(\\quad\\) \\(X \\sim \\mathcal{P}(\\lambda)\\)\n\nExemple physique: comportement du nombre d’événements se produisant avec une fréquence connue, et indépendamment du temps écoulé depuis l’événement précédent (e.g., nombre de clients dans une file d’attente, nombre de mutations dans un gène, etc.)"
  },
  {
    "objectID": "Slides/slides_notations_premiers_pas.html#lois-continues",
    "href": "Slides/slides_notations_premiers_pas.html#lois-continues",
    "title": "HAX603X: Modélisation stochastique",
    "section": "Lois continues",
    "text": "Lois continues\nLoi d’une v.a. admettant une fonction de densité, c’est-à-dire qu’il existe une fonction mesurable \\(f : \\mathbb{R} \\to [0, \\infty[\\) d’intégrale \\(1\\), telle que pour tout \\(A \\in \\mathcal{B}(\\mathbb{R})\\) \\[\n    \\mathbb{P}(X \\in A) = \\int_A f(x) dx \\enspace.\n\\]\n\n\n\n\n\n\nNote\n\n\nLes propriétés de l’intégrale de Lebesgue assure que cette formule définit bien une loi de probabilité.\n\n\n\n\nEspérance: \\(\\mathbb{E}(X) = \\displaystyle\\int_{\\mathbb{R}} x f(x) dx\\)\nVariance: \\(\\mathbb{V}(X) = \\mathbb{E}((X-\\mathbb{E}(X))^2) = \\displaystyle\\int_{\\mathbb{R}} (x-\\mathbb{E}(X))^2 f(x) dx\\)"
  },
  {
    "objectID": "Slides/slides_notations_premiers_pas.html#lois-continues-usuelles",
    "href": "Slides/slides_notations_premiers_pas.html#lois-continues-usuelles",
    "title": "HAX603X: Modélisation stochastique",
    "section": "Lois continues usuelles",
    "text": "Lois continues usuelles\n\nExemple 5 (Loi uniforme) La loi uniforme sur un ensemble \\(B \\in \\mathcal{B}(\\mathbb{R})\\), s’obtient avec la densité définie par \\[\nf(x) = {1\\hspace{-3.8pt} 1}_B(x) / \\lambda (B) \\enspace,\n\\] où \\(\\lambda (B)\\) représente la mesure de Lebesgue de l’ensemble \\(B\\).\n\nCas particulier: pour la loi uniforme sur \\([0,1]\\), on obtient la fonction suivante: \\[\nf(x) = {1\\hspace{-3.8pt} 1}_{[0,1]}(x)\\enspace.\n\\] Notation: \\(\\quad\\) \\(X \\sim \\mathcal{U}([0,1])\\)\n\n\n\n\n\n\nNote\n\n\nUne telle loi est caractérisée ainsi : tous les intervalles de même longueur inclus dans le support de la loi ont la même probabilité."
  },
  {
    "objectID": "Slides/slides_notations_premiers_pas.html#lois-continues-usuelles-ii",
    "href": "Slides/slides_notations_premiers_pas.html#lois-continues-usuelles-ii",
    "title": "HAX603X: Modélisation stochastique",
    "section": "Lois continues usuelles (II)",
    "text": "Lois continues usuelles (II)\n\n\nExemple 6 (Loi exponentielle) La loi exponentielle de paramètre \\(\\gamma &gt; 0\\) est obtenue avec la densité donnée par \\[\nf(x) = \\gamma e^{-\\gamma x} {1\\hspace{-3.8pt} 1}_{\\mathbb{R}_+}(x)\\enspace.\n\\] Notation: \\(X \\sim \\mathcal{Exp}(\\gamma)\\).\n\n\n\n\nProposition 1 (Absence de mémoire) La loi exponentielle modélise la durée de vie d’un phénomène sans mémoire (ou sans vieillissement), c’est-à-dire que pour tout \\(s,t&gt;0\\), on a \\[\n\\mathbb{P}(X&gt;t+s | X&gt;t)=\\mathbb{P}(X&gt;s) \\enspace.\n\\]"
  },
  {
    "objectID": "Slides/slides_notations_premiers_pas.html#lois-continues-usuelles-iii",
    "href": "Slides/slides_notations_premiers_pas.html#lois-continues-usuelles-iii",
    "title": "HAX603X: Modélisation stochastique",
    "section": "Lois continues usuelles (III)",
    "text": "Lois continues usuelles (III)\n\nExemple 7 (Loi normale/gaussienne univariée) Pour des paramètres \\(\\mu \\in \\mathbb{R}\\) (espérance) et \\(\\sigma^2 &gt; 0\\) (variance), la loi normale associée correspond à la fonction de densité : \\[\nf(x) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}}e^{-\\frac{1}{2 \\sigma^2}(x-\\mu)^2} \\enspace.\n\\] Notation: \\(X \\sim \\mathcal{N}(\\mu,\\sigma^2)\\),\n\nOn nomme loi normale centrée réduite le cas canonique: \\(\\mu = 0, \\sigma^2 = 1\\).\nSi \\(X\\sim \\mathcal{N}(\\mu,\\sigma^2)\\), alors l’espérance et la variance de \\(X\\) valent \\(\\mathbb{E}(X) = \\mu\\) et \\(\\mathbb{V}(X) = \\sigma^2\\).\n\n\n\n\n\n\n\nNote\n\n\nLes lois normales sont omniprésente grâce au théorème central limite."
  },
  {
    "objectID": "Slides/slides_notations_premiers_pas.html#enjeu-de-la-fonction-de-répartition",
    "href": "Slides/slides_notations_premiers_pas.html#enjeu-de-la-fonction-de-répartition",
    "title": "HAX603X: Modélisation stochastique",
    "section": "Enjeu de la fonction de répartition",
    "text": "Enjeu de la fonction de répartition\n\nEnjeux: caractériser la loi d’une v.a. en ne considérant que l’espace d’arrivée \\((E, \\mathcal{E})\\) .\n\n\nOutils:\n\n\nla fonction de répartition (v.a. réelles),\nla fonction caractéristique (v.a. dans \\(\\mathbb{R}^d\\)), en gros la transformée de Fourier de la loi!\nla fonction génératrice des moments (v.a. discrètes)\netc."
  },
  {
    "objectID": "Slides/slides_notations_premiers_pas.html#fonction-de-répartition",
    "href": "Slides/slides_notations_premiers_pas.html#fonction-de-répartition",
    "title": "HAX603X: Modélisation stochastique",
    "section": "Fonction de répartition",
    "text": "Fonction de répartition\n\nDéfinition 3 (Fonction de répartition 🇬🇧: cumulative distribution function) \nSoit \\(X\\) une variable aléatoire sur \\((\\mathbb{R}, \\mathcal{B}(\\mathbb{R}))\\). La fonction de répartition de \\(X\\) est la fonction \\(F_X\\) définie sur \\(\\mathbb{R}\\) par \\[\n\\begin{align*}\n     F_X(x) & = \\mathbb{P}(X \\leq x)\\\\\n     & \\class{fragment}{{} = \\mathbb{P}(X \\in ]-\\infty, x]) \\enspace.}\n\\end{align*}\n\\]"
  },
  {
    "objectID": "Slides/slides_notations_premiers_pas.html#propriété-élémentaire-de-la-fonction-de-répartition",
    "href": "Slides/slides_notations_premiers_pas.html#propriété-élémentaire-de-la-fonction-de-répartition",
    "title": "HAX603X: Modélisation stochastique",
    "section": "Propriété élémentaire de la fonction de répartition",
    "text": "Propriété élémentaire de la fonction de répartition\n\nProposition 2 (Propriétés élémentaires) Soit \\(X\\) une v.a. de fonction de répartition \\(F_X\\).\n\n\n\\(F_X\\) est une fonction croissante, de limite \\(0\\) en \\(-\\infty\\) et de limite \\(1\\) en \\(+\\infty\\).\n\\(F_X\\) est continue à droite en tout point.\nPour tout \\(x \\in \\mathbb{R}\\), on a \\(\\mathbb{P}(X=x) = F_X(x) - F_X(x-)\\), où \\(F_X(x-) = \\lim_{\\epsilon \\to 0+} F_X(x- \\epsilon)\\).\nSi \\(X\\) a pour densité \\(f\\), alors \\(F_X\\) est dérivable \\(\\lambda\\)-presque partout de dérivée \\(f\\).\n\n\n\n\nDémonstration: voir par exemple (Barbe et Ledoux 2006)\n\n\n\n\n\n\n\n\nNote\n\n\n\nProp. 1. et 2.: \\(F_X\\) est càdlàg (continue à droite, limite à gauche).\nProp 3. (cas discret): les valeurs prises par \\(X\\) correspondent aux points de discontinuité de \\(F_X\\), les probabilités, à la hauteur du saut.\nProp. 4. (cas continu): le lien entre la fonction de répartition et densité."
  },
  {
    "objectID": "Slides/slides_notations_premiers_pas.html#fonction-de-répartition-et-caractérisation-de-la-loi",
    "href": "Slides/slides_notations_premiers_pas.html#fonction-de-répartition-et-caractérisation-de-la-loi",
    "title": "HAX603X: Modélisation stochastique",
    "section": "Fonction de répartition et caractérisation de la loi",
    "text": "Fonction de répartition et caractérisation de la loi\n\nThéorème 1 (Caractérisation de la loi d’une variable aléatoire réelle) \nLa fonction de répartition d’une variable aléatoire caractérise sa loi : deux variables aléatoires ont même loi si et seulement si elles ont même fonction de répartition.\n\nDémonstration: voir Wikipedia\n\nRappel: la tribu des boréliens est engendrée par la famille d’ensembles \\(\\{]-\\infty,x], x \\in \\mathbb{R}\\}\\)\n\nInterprétation: si on connaît la mesure \\(\\mathbb{P}_X\\) sur cette famille d’ensembles, on la connaît partout"
  },
  {
    "objectID": "Slides/slides_notations_premiers_pas.html#fonction-de-répartition-cas-discret",
    "href": "Slides/slides_notations_premiers_pas.html#fonction-de-répartition-cas-discret",
    "title": "HAX603X: Modélisation stochastique",
    "section": "Fonction de répartition: cas discret",
    "text": "Fonction de répartition: cas discret\nDans le cas d’une loi discrète, la fonction de répartition est une fonction en escalier, constante par morceaux, et croissante.\n\nExemple 8 (Cas discret) Soit \\((x_i)_{i \\in I}\\) une suite ordonnée de réels, avec \\(I \\subset \\mathbb{N}\\). Si \\(X\\) est une variable aléatoire discrète prenant les valeurs \\((x_i)_{i \\in I}\\) et de loi \\((p_i = \\mathbb{P}(X=x_i))_{i \\in I}\\), alors \\[\n\\forall x \\in \\mathbb{R}, \\quad F_X(x) = \\sum_{i \\in I} p_i {1\\hspace{-3.8pt} 1}_{[x_i, \\infty[}(x) \\enspace.\n\\]"
  },
  {
    "objectID": "Slides/slides_notations_premiers_pas.html#fonction-de-répartition-cas-continu",
    "href": "Slides/slides_notations_premiers_pas.html#fonction-de-répartition-cas-continu",
    "title": "HAX603X: Modélisation stochastique",
    "section": "Fonction de répartition: cas continu",
    "text": "Fonction de répartition: cas continu\n\nExemple 9 (Cas continu) Si \\(X\\) est une variable aléatoire de densité \\(f\\), alors \\[\n\\forall x \\in \\mathbb{R}, \\quad F_X(x) = \\int_{-\\infty}^x f(t) \\, \\mathrm dt \\enspace.\n\\]"
  },
  {
    "objectID": "Slides/slides_notations_premiers_pas.html#loi-normale",
    "href": "Slides/slides_notations_premiers_pas.html#loi-normale",
    "title": "HAX603X: Modélisation stochastique",
    "section": "Loi normale",
    "text": "Loi normale\nCas de la loi normale centrée réduite, \\(X \\sim \\mathcal{N}(0,1)\\): \\(F_X=\\Phi\\), avec \\(\\Phi\\) définie par \\[\nF_X(x) \\triangleq \\Phi(x) = \\dfrac{1}{\\sqrt{2\\pi}} \\int_{-\\infty}^x e^{-\\frac{t^2}{2}}\\, \\mathrm d t\\enspace,\n\\]\n\n\n\n\n\n\nNote\n\n\nL’intégrale ne peut être obtenue à partir d’une formule fermée1 Autrefois, les valeurs de \\(\\Phi(x)\\) étaient reportées dans des tables2.\n\n\n\n\n\nTransformation affine: si \\(X \\sim \\mathcal{N}(\\mu, \\sigma^2)\\) — i.e., \\(X=\\mu + \\sigma Y\\), avec \\(Y\\sim \\mathcal{N}(0,1)\\) — alors \\[\nF_X(x)=\\Phi\\left(\\frac{x-\\mu}{\\sigma}\\right)\n\\]\n\n\nWikipedia: Théorème de LiouvilleWikipedia: loi normale"
  },
  {
    "objectID": "Slides/slides_notations_premiers_pas.html#fonction-quantile-inverse-généralisée-à-gauche",
    "href": "Slides/slides_notations_premiers_pas.html#fonction-quantile-inverse-généralisée-à-gauche",
    "title": "HAX603X: Modélisation stochastique",
    "section": "Fonction quantile, inverse généralisée à gauche",
    "text": "Fonction quantile, inverse généralisée à gauche\n\nDéfinition 4 (Fonction quantile/ inverse généralisée 🇬🇧: quantile distribution function) \nSoit \\(X\\) une variable aléatoire sur \\((\\mathbb{R}, \\mathcal{B}(\\mathbb{R}))\\) et \\(F_X\\) sa fonction de répartion. La fonction quantile associée \\(F_X^\\leftarrow: [0,1] \\rightarrow \\mathbb{R}\\) est définie par \\[\n  F_X^\\leftarrow(p) = \\inf\\{ x \\in \\mathbb{R} \\colon F_X(x)\\geq p\\} \\enspace.\n\\]\n\n\n\\(F_X\\) est bijective \\(\\implies\\) \\(F^{-1}=F_X^\\leftarrow\\).\n\nVocabulaire:\n\nla fonction quantile s’appelle aussi inverse au sens de Levy ou inverse généralisée (à gauche).\nmédiane : \\(F_X^\\leftarrow(1/2)\\)\npremier quartile : \\(F_X^\\leftarrow(1/4)\\)\ntroisième quartile : \\(F_X^\\leftarrow(3/4)\\)\ndéciles : \\(F_X^\\leftarrow(k/10)\\) pour \\(k=1,\\dots, 9\\)"
  },
  {
    "objectID": "Slides/slides_notations_premiers_pas.html#quantiles-cas-continu",
    "href": "Slides/slides_notations_premiers_pas.html#quantiles-cas-continu",
    "title": "HAX603X: Modélisation stochastique",
    "section": "Quantiles: cas continu",
    "text": "Quantiles: cas continu\n\n\n\nPlotly = require('plotly.js-dist');\ndists = require( 'https://cdn.jsdelivr.net/gh/stdlib-js/stats-base-dists@umd/browser.js' );\n// see source here: https://github.com/stdlib-js/stats-base-dists/tree/umd\n// continuous case\njstatPDFs = () =&gt; {\n  const distributions = Object.keys(dists);\n  // Get in continuousDistributions the distributions whose pdf, cdf and quantile are defined\n    const continuousDistributions = distributions.filter(name =&gt; dists[name].pdf && dists[name].cdf && dists[name].quantile);\n  return continuousDistributions\n};\noutput = jstatPDFs();\n\nexcludedPDFs = [];\npdfNames_unsorted = output.filter(name =&gt; !excludedPDFs.includes(name));\npdfNames=pdfNames_unsorted.toSorted();\nviewof inputs = Inputs.form([\n      Inputs.range([-10, 10], {value: 0.1, step: 0.001, label: tex`\\mu`, width: 500}),\n      Inputs.range([0.01, 5], {value: 1.01, step: 0.001, label: tex`\\sigma`, width: 500}),\n      Inputs.range([0.001, 0.999], {value: 0.75, step: 0.001, label:tex`\\alpha`, width: 500}),\n    ]);\n\n\nviewof distrib_name = Inputs.select(pdfNames, {value: \"normal\", label: \"Loi\", width: 500});\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n{\n  const x = d3.range(-5, 5, 0.01);\n  const pdf = x.map(x =&gt; dists[distrib_name].pdf(x, mu, sigma));\n  const cdf = x.map(x =&gt; dists[distrib_name].cdf(x, mu, sigma));\n  const inv = x.map(x =&gt; dists[distrib_name].quantile(x, mu, sigma));\n  const quantile = dists[distrib_name].quantile(alpha, mu, sigma);\n  const filteredX = x.filter(coord =&gt; coord &lt; quantile);\n  const filteredPdf = pdf.filter((_, i) =&gt; x[i] &lt;= quantile);\n  const filteredCdf = cdf.filter(coord =&gt; coord &lt;= quantile);\n\n\n{\nvar trace1 = {\n      type: \"scatter\",\n      name: 'Quantile',\n      x: cdf,\n      y: x,\n      line: {color: 'black'},\n      xaxis: 'x1',\n      yaxis: 'y3',\n\n\n};\n\nvar trace12 = {\n        x : [alpha, alpha],\n        y : [x[0], quantile],\n        mode: 'lines',\n        line: {\n            dash: 'dash',\n            width: 1,\n            color: ' #428BCA',\n            marks: {\n                size: 0,\n                color: ' #428BCA',\n            }\n\n          },\n      xaxis: 'x1',\n      yaxis: 'y3',\n\n\n};\n\n\n\nvar trace13 = {\n    x: [0, alpha],\n    y: [quantile, quantile],\n    mode: 'scatter',\n    line: {\n        dash: 'dash',\n        width: 1,\n        color: ' #428BCA',\n        marks: {\n            size: 0,\n        }\n    },\n    xaxis: 'x1',\n    yaxis: 'y3'\n}\n\n\nvar trace2 = {\nx: cdf,\ny: cdf,\ntype: 'scatter',\nname: 'identity',\nline: {color: 'black'},\nxaxis: 'x1',\nyaxis: 'y2'\n\n};\n\n\nvar trace21 = {\n        x : [alpha, 1],\n        y : [alpha, alpha],\n        mode: 'lines',\n        line: {\n            dash: 'dash',\n            width: 1,\n            color: ' #428BCA',\n            marks: {\n                size: 0,\n                color: ' #428BCA',\n            }\n\n          },\n      xaxis: 'x1',\n      yaxis: 'y2'\n}\n\nvar trace22 = {\n        x : [alpha, alpha],\n        y : [alpha, 1],\n        mode: 'lines',\n        line: {\n            dash: 'dash',\n            width: 1,\n            color: ' #428BCA',\n            marks: {\n                size: 0,\n                color: ' #428BCA',\n            }\n\n          },\n      xaxis: 'x1',\n      yaxis: 'y2'\n}\n\n\n\nvar trace23 = {\n    x: [alpha],\n    y: [alpha],\n    mode: 'scatter',\n    line: {\n        dash: 'dash',\n        width: 1,\n        color: ' #428BCA',\n        marks: {\n            size: 0,\n        }\n    },\n    xaxis: 'x1',\n    yaxis: 'y2'\n}\n\n\nvar trace31 = {\n      type: \"scatter\",\n      mode: \"lines\",\n      name: 'PDF2',\n      x: x,\n      y: cdf,\n      line: {color: 'black'},\n      xaxis: 'x2',\n      yaxis: 'y2',\n};\n\nvar trace32 = {\n        x: filteredX,\n        y: filteredX.map(x =&gt; alpha),\n        mode: 'lines',\n        line: {\n            dash: 'dash',\n            width: 1,\n            color: ' #428BCA',\n            marks: {\n                size: 0,\n                color: ' #428BCA',\n            }\n          },\n      xaxis: 'x2',\n      yaxis: 'y2'\n}\n\nvar trace33 = {\n        x : [quantile, quantile],\n        y : [0, alpha],\n        mode: 'lines',\n        line: {\n            dash: 'dash',\n            width: 1,\n            color: ' #428BCA',\n            marks: {\n                size: 0,\n                color: ' #428BCA',\n            }\n          },\n\n      xaxis: 'x2',\n      yaxis: 'y2'\n}\n\nvar trace34 = {\n    x: [quantile],\n    y: [alpha],\n    mode: 'scatter',\n    line: {\n        dash: 'dash',\n        width: 1,\n        color: ' #428BCA',\n        marks: {\n            size: 0,\n        }\n    },\n    xaxis: 'x2',\n    yaxis: 'y2'\n}\n\n\nvar trace41 = {\n\n  type: \"scatter\",\n  name: 'Quantile2',\n  fill: 'tozeroy',\n  x: filteredX,\n  y: filteredPdf,\n  opacity: 0.9,\n  line: {color: ' #428BCA'},\n  xaxis: 'x2',\n  yaxis: 'y1'\n\n};\n\n\nvar trace42 = {\n\n  type: \"scatter\",\n  mode: \"lines\",\n  name: 'PDF2',\n  x: x,\n  y: pdf,\n  line: {color: 'black'},\n  xaxis: 'x2',\n  yaxis: 'y1'\n\n};\n\nvar data = [\n  trace1,\n  trace12, trace13,\n  trace2, trace21, trace22, trace23,\n  trace31, trace32, trace33, trace34,\n  trace41, trace42];\n\n\nvar layout = {\n\n  title: 'Distribution et quantile',\n  xaxis: {\n    domain: [0, 0.32],\n    anchor: 'y1'\n  },\n  yaxis: {\n    domain: [0, 0.24],\n    anchor: 'x1'\n\n  },\n  xaxis2: {\n    domain: [0.35, 1],\n    anchor: 'y'\n  },\n\n  yaxis2: {\n    domain: [0.26, 0.49],\n    anchor: 'x1'\n  },\n\n  yaxis3: {\n    domain: [0.5, 1],\n    anchor: 'x1'\n  },\n\n// legend offset\n  showlegend: false,\n  height: 680,\n  annotations: [\n\n    {\n      x: 1/4,\n      y: quantile - (x[0]-x.slice(-1))/20,\n      xref: 'x1',\n      yref: 'y3',\n      text: 'q= ' + quantile.toFixed(2),\n      font: {\n        size: 12,\n        color: '#428BCA',\n\n      },\n      showarrow: false,\n      arrowhead: 0,\n      ax: 25,\n      ay: -10,\n\n    },\n    {\n      x: 1/4,\n      y: alpha,\n      xref: 'x1',\n      yref: 'y2',\n      text: 'alpha= ' + alpha.toFixed(2),\n      font: {\n        size: 12,\n        color: '#428BCA',\n\n      },\n      showarrow: false,\n      arrowhead: 0,\n      ax: 25,\n      ay: -10,\n\n    },\n    {\n        text: \"Fonction quantile\",\n      font: {\n      size: 15,\n      color: 'black',\n    },\n    showarrow: false,\n    align: 'center',\n    x: -0.01,\n    y: 1.05,\n    xref: 'paper',\n    yref: 'paper',\n    },\n    {\n      text: \"Fonction de répartition\",\n      font: {\n      size: 15,\n      color: 'black',\n            },\n    showarrow: false,\n    align: 'center',\n    x: 0.65,\n    y: 0.52,\n    xref: 'paper',\n    yref: 'paper',\n    },\n    {\n      text: \"Fonction de densité\",\n      font: {\n      size: 15,\n      color: 'black',\n            },\n    showarrow: false,\n    align: 'center',\n    x: 0.65,\n    y: 0.225,\n    xref: 'paper',\n    yref: 'paper',\n    },\n  ]\n\n};\n// XXX: TODO: put the xticks labels on the middle plot for x and on the right plot for y\n    var config = {responsive: true}\n    const div = DOM.element('div');\n    Plotly.newPlot(div, data, layout, config);\n    return div;\n  }\n\n}\n\n\n\n\n\n\n\nmu = inputs[0];\nsigma = inputs[1];\nalpha = inputs[2]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote: voir aussi Notations et rappels"
  },
  {
    "objectID": "Slides/slides_notations_premiers_pas.html#quantiles-cas-discret",
    "href": "Slides/slides_notations_premiers_pas.html#quantiles-cas-discret",
    "title": "HAX603X: Modélisation stochastique",
    "section": "Quantiles: cas discret",
    "text": "Quantiles: cas discret\n\n\n\ndiscretePDFs = () =&gt; {\n  const distributions = Object.keys(dists);\n  // Get in continuousDistributions the distributions whose pdf, cdf and quantile are defined\n    const continuousDistributions = distributions.filter(name =&gt; dists[name].pmf && dists[name].cdf && dists[name].quantile);\n  return continuousDistributions\n};\noutput_discr = discretePDFs();\n\npmfNames_unsorted = output_discr.filter(name =&gt; !excludedPDFs.includes(name));\npmfNames=pmfNames_unsorted.toSorted();\nviewof inputs_disc = Inputs.form([\n      Inputs.range([-10, 10], {value: 0.1, step: 0.001, label: tex`\\mu `, width: 500}),\n      Inputs.range([0.01, 5], {value: 1.01, step: 0.001, label: tex`\\sigma `, width: 500}),\n      Inputs.range([0.001, 0.999], {value: 0.75, step: 0.001, label: tex`\\alpha`, width: 500}),\n    ]);\n\nviewof distrib_name_discr = Inputs.select(pmfNames, {value: \"normal\", label: \"Loi\", width: 500});\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n{\n  const z = d3.range(-5, 5, 0.01);\n  const pmf = z.map(z =&gt; dists[distrib_name_discr].pmf(z, mu_disc, sigma_disc));\n  const cdf = z.map(z =&gt; dists[distrib_name_discr].cdf(z, mu_disc, sigma_disc));\n  const inv = z.map(z =&gt; dists[distrib_name_discr].quantile(z, mu_disc, sigma_disc));\n  const quantile = dists[distrib_name_discr].quantile(alpha_disc, mu_disc, sigma_disc);\n  const filteredX = z.filter(coord =&gt; coord &lt;= quantile);\n  const filteredPmf = pmf.filter((_, i) =&gt; z[i] &lt;= quantile);\n  const filteredCdf = cdf.filter(coord =&gt; coord &lt;= quantile);\n\n\n{\nvar trace1 = {\n      type: \"scatter\",\n      name: 'Quantile',\n      x: cdf,\n      y: z,\n      line: {color: 'black'},\n      xaxis: 'x1',\n      yaxis: 'y3',\n\n\n};\n\nvar trace12 = {\n        x : [alpha_disc, alpha_disc],\n        y : [z[0], quantile],\n        mode: 'lines',\n        line: {\n            dash: 'dash',\n            width: 1,\n            color: ' #428BCA',\n            marks: {\n                size: 0,\n                color: ' #428BCA',\n            }\n\n          },\n      xaxis: 'x1',\n      yaxis: 'y3',\n\n\n};\n\n\n\nvar trace13 = {\n    x: [0, alpha_disc],\n    y: [quantile, quantile],\n    mode: 'scatter',\n    line: {\n        dash: 'dash',\n        width: 1,\n        color: ' #428BCA',\n        marks: {\n            size: 0,\n        }\n    },\n    xaxis: 'x1',\n    yaxis: 'y3'\n}\n\n\nvar trace2 = {\nx: cdf,\ny: cdf,\ntype: 'scatter',\nname: 'identity',\nline: {color: 'black'},\nxaxis: 'x1',\nyaxis: 'y2'\n\n};\n\n\nvar trace21 = {\n        x : [alpha_disc, 1],\n        y : [alpha_disc, alpha_disc],\n        mode: 'lines',\n        line: {\n            dash: 'dash',\n            width: 1,\n            color: ' #428BCA',\n            marks: {\n                size: 0,\n                color: ' #428BCA',\n            }\n\n          },\n      xaxis: 'x1',\n      yaxis: 'y2'\n}\n\nvar trace22 = {\n        x : [alpha_disc, alpha_disc],\n        y : [alpha_disc, 1],\n        mode: 'lines',\n        line: {\n            dash: 'dash',\n            width: 1,\n            color: ' #428BCA',\n            marks: {\n                size: 0,\n                color: ' #428BCA',\n            }\n\n          },\n      xaxis: 'x1',\n      yaxis: 'y2'\n}\n\nvar trace23 = {\n    x: [alpha_disc],\n    y: [alpha_disc],\n    mode: 'scatter',\n    line: {\n        dash: 'dash',\n        width: 1,\n        color: ' #428BCA',\n        marks: {\n            size: 0,\n        }\n    },\n    xaxis: 'x1',\n    yaxis: 'y2'\n}\n\n\nvar trace31 = {\n      type: \"scatter\",\n      mode: \"lines\",\n      name: 'PDF2',\n      x: z,\n      y: cdf,\n      line: {color: 'black'},\n      xaxis: 'x2',\n      yaxis: 'y2',\n};\n\nvar trace32 = {\n        x: filteredX,\n        y: filteredX.map(z =&gt; alpha_disc),\n        mode: 'lines',\n        line: {\n            dash: 'dash',\n            width: 1,\n            color: ' #428BCA',\n            marks: {\n                size: 0,\n                color: ' #428BCA',\n            }\n          },\n      xaxis: 'x2',\n      yaxis: 'y2'\n}\n\nvar trace33 = {\n        x : [quantile, quantile],\n        y : [0, alpha_disc],\n        mode: 'lines',\n        line: {\n            dash: 'dash',\n            width: 1,\n            color: ' #428BCA',\n            marks: {\n                size: 0,\n                color: ' #428BCA',\n            }\n          },\n\n      xaxis: 'x2',\n      yaxis: 'y2'\n}\n\nvar trace34 = {\n    x: [quantile],\n    y: [alpha_disc],\n    mode: 'scatter',\n    line: {\n        dash: 'dash',\n        width: 1,\n        color: ' #428BCA',\n        marks: {\n            size: 0,\n        }\n    },\n    xaxis: 'x2',\n    yaxis: 'y2'\n}\n\n\nvar trace41 = {\n\n  type: \"scatter\",\n  name: 'Quantile2',\n  fill: 'tozeroy',\n  x : filteredX,\n  y : filteredPmf,\n  opacity: 0.9,\n  line: {color: ' #428BCA'},\n  xaxis: 'x2',\n  yaxis: 'y1'\n\n};\n\n\nvar trace42 = {\n\n  type: \"scatter\",\n  mode: \"lines\",\n  name: 'PDF2',\n  x: z,\n  y: pmf,\n  line: {color: 'black'},\n  xaxis: 'x2',\n  yaxis: 'y1'\n\n};\n\nvar data = [\n  trace1,\n  trace12, trace13,\n  trace2, trace21, trace22, trace23,\n  trace31, trace32, trace33, trace34,\n  trace41, trace42];\n\n\nvar layout = {\n\n  title: 'Distribution et quantile',\n  xaxis: {\n    domain: [0, 0.32],\n    anchor: 'y1'\n  },\n  yaxis: {\n    domain: [0, 0.24],\n    anchor: 'x1'\n\n  },\n  xaxis2: {\n    domain: [0.35, 1],\n    anchor: 'y'\n  },\n\n  yaxis2: {\n    domain: [0.26, 0.49],\n    anchor: 'x1'\n  },\n\n\n\n  yaxis3: {\n    domain: [0.5, 1],\n    anchor: 'x1'\n\n  },\n\n  showlegend: false,\n  height: 680,\n\n\n  annotations: [\n\n    {\n      x: 1/4,\n      y: quantile - (z[0]-z.slice(-1))/20,\n      xref: 'x1',\n      yref: 'y3',\n      text: 'q=' + quantile.toFixed(2),\n      font: {\n        size: 12,\n        color: '#428BCA',\n\n      },\n      showarrow: false,\n      arrowhead: 0,\n      ax: 25,\n      ay: -10,\n\n    },\n    {\n      x: 1/4,\n      y: alpha_disc,\n      xref: 'x1',\n      yref: 'y2',\n      text: 'alpha=' + alpha_disc.toFixed(2),\n      font: {\n        size: 12,\n        color: '#428BCA',\n\n      },\n      showarrow: false,\n      arrowhead: 0,\n      ax: 25,\n      ay: -10,\n\n    },\n    {\n        text: \"Fonction quantile\",\n      font: {\n      size: 15,\n      color: 'black',\n    },\n    showarrow: false,\n    align: 'center',\n    x: -0.01,\n    y: 1.05,\n    xref: 'paper',\n    yref: 'paper',\n    },\n    {\n      text: \"Fonction de répartition\",\n      font: {\n      size: 15,\n      color: 'black',\n            },\n    showarrow: false,\n    align: 'center',\n    x: 0.65,\n    y: 0.52,\n    xref: 'paper',\n    yref: 'paper',\n    },\n    {\n      text: \"Fonction de densité\",\n      font: {\n      size: 15,\n      color: 'black',\n            },\n    showarrow: false,\n    align: 'center',\n    x: 0.65,\n    y: 0.225,\n    xref: 'paper',\n    yref: 'paper',\n    },\n  ]\n\n};\n// XXX: TODO: put the xticks labels on the middle plot for x and on the right plot for y\n\n    var config = {responsive: true}\n    const div = DOM.element('div');\n    Plotly.newPlot(div, data, layout, config);\n    return div;\n  }\n\n}\n\n\n\n\n\n\n\nmu_disc = inputs_disc[0];\nsigma_disc = inputs_disc[1];\nalpha_disc = inputs_disc[2];\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote: voir aussi Notations et rappels"
  },
  {
    "objectID": "Slides/slides_notations_premiers_pas.html#bibliographie",
    "href": "Slides/slides_notations_premiers_pas.html#bibliographie",
    "title": "HAX603X: Modélisation stochastique",
    "section": "Bibliographie",
    "text": "Bibliographie\n\n\nBarbe, Philippe, et Michel Ledoux. 2006. « Cours de Probabilite Pour la Licence (corrigée) ».\n\n\n\n\nHAX603X: Modélisation stochastique"
  },
  {
    "objectID": "Courses/th_asymptotique.html",
    "href": "Courses/th_asymptotique.html",
    "title": "Théorèmes asymptotiques",
    "section": "",
    "text": "Le premier résultat fondamental en probabilités concerne le comportement asymptotique de la moyenne empirique: \n\\bar X_n = \\dfrac{X_1 + \\cdots + X_n}{n} \\enspace.\n quand on observe n variables aléatoires i.i.d X_1,\\dots,X_n, ayant une espérance finie.\n\nThéorème 1 (Loi forte des grands nombres) \nSoit (X_n)_{n \\geq 1} une suite de variables aléatoires indépendantes et identiquement distribuées (i.i.d.) dans L^1(\\Omega, \\mathcal{F}, \\mathbb{P}). Notons \\mu = \\mathbb{E}[X_1]. Alors \\bar X_n converge vers \\mu presque sûrement : \n\\mathbb{P}\\bigg( \\dfrac{X_1 + \\cdots + X_n}{n} \\underset{n \\to \\infty}{\\longrightarrow} \\mu \\bigg) = 1\\,.\n\n\nInterprétation: Intuitivement, la probabilité d’un événement A correspond à la fréquence d’apparition de A quand on répète une expérience qui fait intervenir cet événement. Par exemple, si on dispose une pièce truquée, on estimera la probabilité d’apparition du côté pile en lançant la pièce un grand nombre de fois et en comptant le nombre de pile obtenu. La loi des grands nombres justifie a posteriori cette intuition : si X_1, \\ldots, X_n sont i.i.d. de loi de Bernoulli de paramètre p, alors \n    \\dfrac{X_1 + \\cdots + X_n}{n} \\xrightarrow[n \\to \\infty]{p.s.} p \\enspace.\n Le membre de gauche correspond au nombre empirique de pile obtenu, celui de droite à la valeur théorique.\nRemarque: Bien qu’assez intuitif, ce théorème est difficile à démontrer, cf.[@Ouvrard08;@Barbe_Ledoux06] ou encore [@Williams91] pour une version de preuve avec des martingales.\n#| standalone: true\n#| viewerHeight: 550\nimport numpy as np\nfrom shiny import ui, render, App, reactive\nfrom shinywidgets import output_widget, register_widget\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objs as go\n\n\nn_init = 42\n\n\napp_ui = ui.page_fluid(\n    ui.tags.head(\n        ui.tags.style(\"\"\"\n            .bslib-sidebar-layout &gt; .sidebar &gt; .sidebar-content {\n            display: flex;\n            flex-direction: column;\n            padding: 0.5rem;\n            }\n            .irs--shiny .irs-line {\n            top: 27px;\n            height: 4px;\n            }\n            .irs.irs--shiny .irs-bar {\n            top: 27px;\n            height: 4px;\n            }\n            .irs--shiny .irs-handle {\n            top: 23px;\n            width: 22px;\n            height: 10px;\n        \"\"\")\n    ),\n    ui.panel_title(\"Loi des grands nombres\"),\n    ui.layout_sidebar(\n        ui.panel_sidebar(\n            ui.input_action_button(\"seed\", \"Nouveau tirage\", class_=\"btn-primary\"),\n            ui.input_slider( \"p\", \"Espérance: p\", 0.01, 0.99, value=0.35, step=0.01, ticks=False,\n            ),\n            ui.input_slider( \"n_samples\", \"Échantillons: n\", 2, 1000, value=15, step=1, ticks=False),\n        width=3),\n    ui.panel_main(output_widget(\"my_widget\"), width = 9)\n    )\n)\n\n\n\n\ndef server(input, output, session):\n    seed = reactive.Value(42)\n\n    @reactive.Effect\n    @reactive.event(input.seed)\n    def _():\n        seed.set(np.random.randint(0, 1000))\n\n    subplots = make_subplots(\n        rows=2,\n        cols=1,\n        vertical_spacing=0.45,\n        horizontal_spacing=0.04,\n        row_heights=[5, 1],\n        subplot_titles=(\n            f\"Moyenne empirique: loi de Bernoulli\",\n            \"Tirages aléatoires &lt;span style='color:rgb(66, 139, 202)'&gt;bleu: 0&lt;/span&gt;, &lt;span style='color:rgb(255, 0, 0)'&gt;rouge: 1&lt;/span&gt; (seed=\"\n            + f\"{n_init:03}\"\n            + \")\",\n        ),\n    )\n    fig = go.FigureWidget(subplots)\n    fig.add_trace(\n        go.Scatter(\n            mode=\"lines\",\n            line=dict(color=\"black\", width=3),\n            x=[],\n            y=[],\n            name=r\"Moyenne &lt;br&gt; empirique\",\n        ),\n        row=1,\n        col=1,\n    )\n    fig.add_trace(\n        go.Scatter(\n            mode=\"lines\",\n            line=dict(dash=\"dash\", color=\"black\", width=1),\n            marker={},\n            x=[],\n            y=[],\n            name=r\"p\",\n        ),\n        row=1,\n        col=1,\n    )\n    fig.add_trace(\n        go.Heatmap(\n            x=[],\n            z=[],\n            colorscale=[[0, \"rgb(66, 139, 202)\"], [1, \"rgb(255,0,0)\"]],\n            showscale=False,\n        ),\n        row=2,\n        col=1,\n    )\n\n    fig.update_yaxes(range=[0, 1.1], row=1, col=1)\n    fig.update_xaxes(matches=\"x1\", row=2, col=1)\n    fig.update_yaxes(visible=False, row=2, col=1)\n    fig.update_xaxes(visible=False, row=2, col=1)\n\n    fig.update_layout(\n        template=\"simple_white\",\n        showlegend=True,\n        xaxis_title=\"Échantillons: n\",\n    )\n    fig.update_layout(autosize=True)\n\n    fig.update_layout(\n        legend=dict(\n            yanchor=\"top\",\n            y=0.99,\n            xanchor=\"left\",\n            x=0.65,\n            bgcolor=\"rgba(0,0,0,0)\",\n        )\n    )\n    fig.update_layout(\n        margin=dict(l=0, r=0, b=10, t=70),\n    )\n\n    register_widget(\"my_widget\", fig)\n\n    @reactive.Effect\n    def _():\n        p = input.p()\n        n_samples = input.n_samples()\n\n        rng = np.random.default_rng(seed())\n        iterations = np.arange(1, n_samples + 1)\n        samples = rng.binomial(1, p, size=n_samples)\n        means_samples = np.cumsum(samples) / np.arange(1, n_samples + 1)\n\n        # Update data in fig:\n        fig.data[0].x = iterations\n        fig.data[0].y = means_samples\n\n        fig.data[1].x = iterations\n        fig.data[1].y = np.full((n_samples), p)\n\n        fig.data[2].x = iterations\n        fig.data[2].z = [samples]\n\n        fig.update_xaxes(range=[1, n_samples + 1])\n\n        # Update the subplot titles:\n        fig.layout.annotations[1].update(\n            text=f\"Tirages aléatoires (seed=\"\n            + f\"{seed():03}\"\n            + \") &lt;br&gt; &lt;span style='color:rgb(66, 139, 202)'&gt;bleu: 0&lt;/span&gt;, &lt;span style='color:rgb(255, 0, 0)'&gt;rouge: 1&lt;/span&gt; \"\n        )\n\n\napp = App(app_ui, server)\n\nPour aller plus loin:\nQuant p varie, à n fixé…les signaux générés sont très très proches, ce qui ne devrait pas être le cas sans structuration particulière de la génération. L’aléa est imparfait!",
    "crumbs": [
      "Cours",
      "Théorèmes asymptotiques"
    ]
  },
  {
    "objectID": "Courses/th_asymptotique.html#loi-des-grands-nombres",
    "href": "Courses/th_asymptotique.html#loi-des-grands-nombres",
    "title": "Théorèmes asymptotiques",
    "section": "",
    "text": "Le premier résultat fondamental en probabilités concerne le comportement asymptotique de la moyenne empirique: \n\\bar X_n = \\dfrac{X_1 + \\cdots + X_n}{n} \\enspace.\n quand on observe n variables aléatoires i.i.d X_1,\\dots,X_n, ayant une espérance finie.\n\nThéorème 1 (Loi forte des grands nombres) \nSoit (X_n)_{n \\geq 1} une suite de variables aléatoires indépendantes et identiquement distribuées (i.i.d.) dans L^1(\\Omega, \\mathcal{F}, \\mathbb{P}). Notons \\mu = \\mathbb{E}[X_1]. Alors \\bar X_n converge vers \\mu presque sûrement : \n\\mathbb{P}\\bigg( \\dfrac{X_1 + \\cdots + X_n}{n} \\underset{n \\to \\infty}{\\longrightarrow} \\mu \\bigg) = 1\\,.\n\n\nInterprétation: Intuitivement, la probabilité d’un événement A correspond à la fréquence d’apparition de A quand on répète une expérience qui fait intervenir cet événement. Par exemple, si on dispose une pièce truquée, on estimera la probabilité d’apparition du côté pile en lançant la pièce un grand nombre de fois et en comptant le nombre de pile obtenu. La loi des grands nombres justifie a posteriori cette intuition : si X_1, \\ldots, X_n sont i.i.d. de loi de Bernoulli de paramètre p, alors \n    \\dfrac{X_1 + \\cdots + X_n}{n} \\xrightarrow[n \\to \\infty]{p.s.} p \\enspace.\n Le membre de gauche correspond au nombre empirique de pile obtenu, celui de droite à la valeur théorique.\nRemarque: Bien qu’assez intuitif, ce théorème est difficile à démontrer, cf.[@Ouvrard08;@Barbe_Ledoux06] ou encore [@Williams91] pour une version de preuve avec des martingales.\n#| standalone: true\n#| viewerHeight: 550\nimport numpy as np\nfrom shiny import ui, render, App, reactive\nfrom shinywidgets import output_widget, register_widget\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objs as go\n\n\nn_init = 42\n\n\napp_ui = ui.page_fluid(\n    ui.tags.head(\n        ui.tags.style(\"\"\"\n            .bslib-sidebar-layout &gt; .sidebar &gt; .sidebar-content {\n            display: flex;\n            flex-direction: column;\n            padding: 0.5rem;\n            }\n            .irs--shiny .irs-line {\n            top: 27px;\n            height: 4px;\n            }\n            .irs.irs--shiny .irs-bar {\n            top: 27px;\n            height: 4px;\n            }\n            .irs--shiny .irs-handle {\n            top: 23px;\n            width: 22px;\n            height: 10px;\n        \"\"\")\n    ),\n    ui.panel_title(\"Loi des grands nombres\"),\n    ui.layout_sidebar(\n        ui.panel_sidebar(\n            ui.input_action_button(\"seed\", \"Nouveau tirage\", class_=\"btn-primary\"),\n            ui.input_slider( \"p\", \"Espérance: p\", 0.01, 0.99, value=0.35, step=0.01, ticks=False,\n            ),\n            ui.input_slider( \"n_samples\", \"Échantillons: n\", 2, 1000, value=15, step=1, ticks=False),\n        width=3),\n    ui.panel_main(output_widget(\"my_widget\"), width = 9)\n    )\n)\n\n\n\n\ndef server(input, output, session):\n    seed = reactive.Value(42)\n\n    @reactive.Effect\n    @reactive.event(input.seed)\n    def _():\n        seed.set(np.random.randint(0, 1000))\n\n    subplots = make_subplots(\n        rows=2,\n        cols=1,\n        vertical_spacing=0.45,\n        horizontal_spacing=0.04,\n        row_heights=[5, 1],\n        subplot_titles=(\n            f\"Moyenne empirique: loi de Bernoulli\",\n            \"Tirages aléatoires &lt;span style='color:rgb(66, 139, 202)'&gt;bleu: 0&lt;/span&gt;, &lt;span style='color:rgb(255, 0, 0)'&gt;rouge: 1&lt;/span&gt; (seed=\"\n            + f\"{n_init:03}\"\n            + \")\",\n        ),\n    )\n    fig = go.FigureWidget(subplots)\n    fig.add_trace(\n        go.Scatter(\n            mode=\"lines\",\n            line=dict(color=\"black\", width=3),\n            x=[],\n            y=[],\n            name=r\"Moyenne &lt;br&gt; empirique\",\n        ),\n        row=1,\n        col=1,\n    )\n    fig.add_trace(\n        go.Scatter(\n            mode=\"lines\",\n            line=dict(dash=\"dash\", color=\"black\", width=1),\n            marker={},\n            x=[],\n            y=[],\n            name=r\"p\",\n        ),\n        row=1,\n        col=1,\n    )\n    fig.add_trace(\n        go.Heatmap(\n            x=[],\n            z=[],\n            colorscale=[[0, \"rgb(66, 139, 202)\"], [1, \"rgb(255,0,0)\"]],\n            showscale=False,\n        ),\n        row=2,\n        col=1,\n    )\n\n    fig.update_yaxes(range=[0, 1.1], row=1, col=1)\n    fig.update_xaxes(matches=\"x1\", row=2, col=1)\n    fig.update_yaxes(visible=False, row=2, col=1)\n    fig.update_xaxes(visible=False, row=2, col=1)\n\n    fig.update_layout(\n        template=\"simple_white\",\n        showlegend=True,\n        xaxis_title=\"Échantillons: n\",\n    )\n    fig.update_layout(autosize=True)\n\n    fig.update_layout(\n        legend=dict(\n            yanchor=\"top\",\n            y=0.99,\n            xanchor=\"left\",\n            x=0.65,\n            bgcolor=\"rgba(0,0,0,0)\",\n        )\n    )\n    fig.update_layout(\n        margin=dict(l=0, r=0, b=10, t=70),\n    )\n\n    register_widget(\"my_widget\", fig)\n\n    @reactive.Effect\n    def _():\n        p = input.p()\n        n_samples = input.n_samples()\n\n        rng = np.random.default_rng(seed())\n        iterations = np.arange(1, n_samples + 1)\n        samples = rng.binomial(1, p, size=n_samples)\n        means_samples = np.cumsum(samples) / np.arange(1, n_samples + 1)\n\n        # Update data in fig:\n        fig.data[0].x = iterations\n        fig.data[0].y = means_samples\n\n        fig.data[1].x = iterations\n        fig.data[1].y = np.full((n_samples), p)\n\n        fig.data[2].x = iterations\n        fig.data[2].z = [samples]\n\n        fig.update_xaxes(range=[1, n_samples + 1])\n\n        # Update the subplot titles:\n        fig.layout.annotations[1].update(\n            text=f\"Tirages aléatoires (seed=\"\n            + f\"{seed():03}\"\n            + \") &lt;br&gt; &lt;span style='color:rgb(66, 139, 202)'&gt;bleu: 0&lt;/span&gt;, &lt;span style='color:rgb(255, 0, 0)'&gt;rouge: 1&lt;/span&gt; \"\n        )\n\n\napp = App(app_ui, server)\n\nPour aller plus loin:\nQuant p varie, à n fixé…les signaux générés sont très très proches, ce qui ne devrait pas être le cas sans structuration particulière de la génération. L’aléa est imparfait!",
    "crumbs": [
      "Cours",
      "Théorèmes asymptotiques"
    ]
  },
  {
    "objectID": "Courses/th_asymptotique.html#théorème-central-limite-tcl",
    "href": "Courses/th_asymptotique.html#théorème-central-limite-tcl",
    "title": "Théorèmes asymptotiques",
    "section": "Théorème central limite (TCL)",
    "text": "Théorème central limite (TCL)\nUne fois la loi des grands nombres établie, on peut se demander quel est l’ordre suivant dans le développement asymptotique de \\bar X_n - \\mu, ou de manière équivalente de S_n - n \\mu, où S_n = X_1 + \\cdots + X_n. Le théorème suivant répond à cette question, en donnant une convergence en loi d’une transformation affine de la moyenne empirique:\n\nThéorème 2 (Théorème central limite) Soit X_1, \\ldots, X_n une suite de variables aléatoires i.i.d de variance \\sigma^2 = {\\rm var}(X_1) \\in ]0, \\infty[. On note \\mu = \\mathbb{E}[X_1] leur espérance. Alors \n\\sqrt n \\left(\\tfrac{\\bar X_n - \\mu}{\\sigma} \\right) \\xrightarrow[n \\to +\\infty]{\\mathcal{L}} N\\enspace,\n où N suit une loi normale centrée réduite : N \\sim\\mathcal{N}(0,1).\n\nPreuve: cf.[@Ouvrard08;@Barbe_Ledoux06].\nOn peut interpréter ce théorème grossièrement de la façon suivante: la moyenne empirique de variables aléatoires i.i.d de variance \\sigma^2 se comporte asymptotiquement comme une loi normale \\mathcal{N}(\\mu, \\tfrac{\\sigma^2}{n}), ce que l’on écrit avec un abus de notation:\n\n\\bar X_n \\approx \\mathcal{N}(\\mu, \\frac{\\sigma^2}{n}) \\enspace.\n\nEn termes de somme cumulée empirique, la convergence se réécrit\n\n    \\tfrac{S_n - n \\mu}{\\sqrt n \\sigma} \\xrightarrow[n \\to +\\infty]{\\mathcal{L}} N \\enspace.\n\nLes hypothèses de ce théorème sont plutôt faibles (il suffit de supposer une variance finie). Pourtant, le résultat est universel : la loi de départ peut être aussi farfelue que l’on veut, elle se rapprochera toujours asymptotiquement d’une loi normale.\nOn rappelle que la convergence en loi est équivalente à la convergence des fonctions de répartition en tout point de continuité de la limite. Ainsi, le théorème central limite se réécrit de la manière suivante : pour tout a &lt; b, notons \\alpha_n=\\mathbb{P} \\left(\\bar X_n \\notin [ \\mu + \\tfrac{a \\sigma}{\\sqrt{n}}, \\mu + \\tfrac{ b \\sigma}{\\sqrt{n}}] \\right). Ainsi\n\n\\mathbb{P} \\left(\\tfrac{\\bar X_n - \\mu}{\\sigma}\\in [ \\tfrac{a}{\\sqrt{n}},\\tfrac{b}{\\sqrt{n}}] \\right)\\\\\n\\begin{align}\n    1-\\alpha_n& = \\mathbb{P} \\left(\\bar X_n \\in [ \\mu + \\tfrac{a \\sigma}{\\sqrt{n}}, \\mu + \\tfrac{ b \\sigma}{\\sqrt{n}}] \\right)\\nonumber\\\\\n    & =\n    \\mathbb{P} \\left(\\tfrac{\\bar X_n - \\mu}{\\sigma} \\in [ \\tfrac{a}{\\sqrt{n}},\\tfrac{b}{\\sqrt{n}}]\\right) \\nonumber\\\\\n    & =\n    \\mathbb{P} \\bigg( a \\leq \\sqrt n \\left(\\tfrac{\\bar X_n - \\mu}{\\sigma} \\right) \\leq b\\bigg) \\nonumber\\\\\n    & \\underset{n \\to \\infty}{\\longrightarrow}  \\int_a^b \\varphi(x) \\,  dx\\,. \\nonumber\\\\\n\\end{align}\n où l’on note \\varphi (resp. \\Phi) la densité (resp. la fonction de répartition) d’une loi normale centrée réduite, définie pour tout x\\in\\mathbb{R} par \\varphi(x)=\\tfrac{e^{-\\frac{x^2}{2}}}{\\sqrt{2\\pi}} (resp. \\Phi(x)= \\int_{-\\infty}^{x}\\varphi(u) du).\nDans le cas classique d’un intervalle de confiance à 95%, c’est-à-dire quand \\alpha_n=0.05, et en prenant un intervalle de confiance symétrique (alors a=-t et b=q) on obtient 1-\\alpha_n= \\int_{-q}^q \\varphi(x) \\, dx=\\Phi(q)-\\Phi(-q)=2 \\Phi(q)-1 \\implies \\boxed{q=\\Phi^{-1}(1-\\tfrac{\\alpha_n}{2})} et q est donc le quantile de niveau 1-\\tfrac{\\alpha_n}{2} de la loi normale centrée réduite. Numériquement on peut facilement évaluer q et vérifier que q\\approx 1.96 avec scipy:\n\nfrom scipy.stats import norm\nq = norm.ppf(1-0.05/2)\nprint(f\"Gaussienne centrée réduite,\\nQuantile de niveau (1-α/2):\\nq = {q:.2f}\")\n\nGaussienne centrée réduite,\nQuantile de niveau (1-α/2):\nq = 1.96\n\n\n\nExemple 1 (Loi de Bernoulli) On considère des variables aléatoires X_1, \\ldots, X_n i.i.d. suivant une loi de Bernoulli de paramètre p \\in ]0,1[, dont l’espérance et la variance sont respectivemenbt p et p(1-p). Le théorème central limite donne alors \n    \\sqrt n \\left(\\frac{\\bar X_n - p}{p (1-p)} \\right) \\xrightarrow[n \\to +\\infty]{\\mathcal{L}} N\\,,\n avec N \\sim \\mathcal{N}(0,1). Cette convergence est illustrée dans le widget ci-dessous. Le contexte est le suivant. On répète t fois le processus, qui consiste à afficher (\\bar{X}_k)_{k \\in [n]}, où les n variables aléatoires sont i.i.d. et suivent une loi de Bernoulli de paramètre p.\n\n#| standalone: true\n#| viewerHeight: 600\nimport numpy as np\nfrom scipy.stats import norm\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nfrom shiny import ui, render, App, reactive\nfrom shinywidgets import output_widget, render_widget\n\n\napp_ui = ui.page_fluid(\n    ui.tags.head(\n        ui.tags.style(\"\"\"\n            .bslib-sidebar-layout &gt; .sidebar &gt; .sidebar-content {\n            display: flex;\n            flex-direction: column;\n            padding: 0.1rem;\n            }\n            .irs--shiny .irs-line {\n            top: 27px;\n            height: 4px;\n            }\n            .irs.irs.irs--shiny .irs-bar {\n            top: 27px;\n            height: 4px;\n            }\n            .irs--shiny .irs-handle {\n            top: 23px;\n            width: 22px;\n            height: 10px;\n        \"\"\")\n    ),\n    ui.panel_title(\"TCL\"),\n    ui.layout_sidebar(\n        ui.panel_sidebar(\n            ui.input_action_button(\"seed\", \"Nouveau tirage\",class_=\"btn-primary\"),\n            ui.input_slider(\"p\", \"Espérance: p\", 0.01, 0.99, value=0.5, step=0.01, ticks=False),\n            ui.input_slider(\"n_samples\", \"Échantillons: n\", 1, 200, value=100, step=1, ticks=False),\n            ui.input_slider(\"n_repetitions\", \"Répétitions: t\", 1, 300, value=200, step=1, ticks=False),\n        width=3\n        ),\n    ui.panel_main(output_widget(\"my_widget\"), width = 9)\n    )\n)\n\n\n\ndef server(input, output, session):\n    seed = reactive.Value(42)\n\n\n    @reactive.Effect\n    @reactive.event(input.seed)\n    def _():\n        seed.set(np.random.randint(0, 1000))\n\n\n    @output\n    @render_widget\n    def my_widget():\n\n        rng = np.random.default_rng(seed())\n        p = input.p()\n        n_repetitions = input.n_repetitions()\n        n_samples = input.n_samples()\n        iterations = np.arange(1, n_samples + 1)\n        samples = rng.binomial(1, p, size=(n_repetitions, n_samples))\n        means_samples = np.cumsum(samples, axis=1) / np.arange(1, n_samples + 1)\n        x_hist = np.linspace(0, 1, num=300)\n\n        # Create figure\n        fig = make_subplots(\n                    rows=1,\n                    cols=3,\n                    # vertical_spacing=0.5,\n                    horizontal_spacing=0.02,\n                    column_widths=[20, 2, 3],\n                    subplot_titles=(\"t = \" + str(n_repetitions) + \" répétitions\",\"\",\"\")\n                )\n\n\n        for i in range(n_repetitions):\n            fig.add_trace(\n                    go.Scatter(\n                        mode='lines',\n                        line=dict(color=\"rgba(0,0,0,0.05)\", width=1),\n                        x=iterations,\n                        y=means_samples[i,:],\n                        ),\n                        row=1, col=1,\n            )\n        fig.add_trace(\n                go.Scatter(\n                    mode='lines',\n                    line=dict(dash=\"dash\", color=\"blue\", width=1),\n                    marker={},\n                    x=iterations,\n                    y=np.full((n_samples), p),\n                    name='p'),\n                    row=1, col=1,\n        )\n        fig.update_layout(\n            template=\"simple_white\",\n            showlegend=False,\n        )\n        fig.add_trace(\n                go.Scatter(\n                    mode='markers',\n                    marker=dict(color=\"rgba(0,0,0,0.05)\", size=4),\n                    x=np.zeros(n_samples),\n                    y=means_samples[:,-1],\n                ),\n                row=1, col=2,\n\n        )\n        y_hist, bins = np.histogram(means_samples[:,-1], bins=int(np.sqrt(n_repetitions)), density=True)\n        fig.add_trace(\n            go.Bar(x=y_hist, y=bins[:-1] + np.diff(bins)/2,\n                    opacity=0.75,\n                    marker_color = 'black',\n                    orientation='h',\n                    width=np.diff(bins),\n                    name=\"Tirages de moyennes empiriques\",\n                    ),\n                row=1, col=3,\n        )\n        fig.add_trace(\n            go.Scatter(x=norm.pdf(x_hist, p, np.sqrt(p*(1-p) / n_samples)),\n                       y=x_hist,\n                       mode='lines',\n                       line=dict(color=\"red\"),\n                       legendgroup='1',\n                       name=\"TCL\"\n                       ),\n                row=1, col=3,\n        )\n\n        fig.update_xaxes(range=[1, n_samples + 1])\n        fig.update_yaxes(range=[-.05, 1.05], row=1, col=1)\n        fig.update_yaxes(matches=\"y1\",visible = False,  row=1, col=2)\n        fig.update_xaxes(range=[-0.2, 0.2], visible = False, row=1, col=2)\n\n        fig.update_yaxes(matches=\"y1\", row=1, col=3, visible=False)\n        fig.update_xaxes(range=[0, 1.1 / np.sqrt(2*np.pi* p*(1-p) / n_samples)], row=1, col=3)\n        fig.update_xaxes(visible=False, row=1, col=3)\n\n\n\n        for trace in fig['data']:\n            print(trace)\n            if (trace['name'] != 'TCL') and (trace['name'] != 'p'):\n                trace['showlegend'] = False\n        fig.update_layout(\n        margin=dict(l=0, r=0, b=10, t=100),\n        )\n        fig.update_layout(\n            title=dict(text=\"Distribution de la moyenne empirique&lt;br&gt; (cas loi de Bernoulli)\", yanchor=\"top\", y=0.95),\n            title_x=0.5,\n            showlegend=True,\n\n        )\n        fig.update_layout(\n            legend=dict(\n                yanchor=\"top\",\n                y=0.99,\n                xanchor=\"left\",\n                x=0.89,\n                bgcolor=\"rgba(0,0,0,0)\",\n            )\n        )\n        fig['layout']['xaxis']['title']='Échantillons: n'\n\n        fig.update_layout(autosize=True)\n\n        return fig\n\n\napp = App(app_ui, server)\n\nUne autre illustration possible de la convergence donnée par le TCL est celle qui correspond au point de vue donnée par l’analyse. Pour cela supposons que l’on ait une suite de variables aléatoires réelles X_1, \\dots, X_n, i.i.d. dont la fonction de densité commune est notée par f.\nOn rappel quelques éléments de probabilités concernant les densités. Pour cela on rappelle la définition de la convolution deux fonctions. Pour cela prenons deux fonctions f et g définies sur \\mathbb{R} et qui sont intégrables au sens de Lebesgue. La convolution de f par g est alors la fonction f*g suivante:\n\n\\begin{align}\n\\mathbb{R} &\\mapsto \\mathbb{R} \\nonumber\\\\\nx &\\to \\int_{-\\infty}^{+\\infty} f(x-y)g(y) dy \\enspace.\\nonumber\n\\end{align}\n\n\n\n\n\n\n\nNote\n\n\n\nOn peut aussi obtenir f*g(x) en calculant \\int_{\\mathbb{R}^2} f(u)g(v) {1\\hspace{-3.8pt} 1}_{u+v=x} du dv.\n\n\n\nThéorème 3 (Loi de la somme et convolutions) Soient X et Y des v.a. de densités f et g respectivement, la loi de X+Y est donnée par la convolution f*g.\n\nDessous, pour X_1, \\dots, X_n, i.i.d., de densité f, on affiche la densité de la loi de \\bar{X}_n.\n#| standalone: true\n#| viewerHeight: 550\nimport numpy as np\nfrom shiny import ui, render, App, reactive\nfrom shinywidgets import output_widget, register_widget\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objs as go\nfrom scipy import signal\n\n\napp_ui = ui.page_fluid(\n    ui.tags.head(\n        ui.tags.style(\"\"\"\n            .bslib-sidebar-layout &gt; .sidebar &gt; .sidebar-content {\n            display: flex;\n            flex-direction: column;\n            padding: 0.5rem;\n            }\n            .irs--shiny .irs-line {\n            top: 27px;\n            height: 2px;\n            }\n            .irs.irs--shiny .irs-bar {\n            top: 27px;\n            height: 2px;\n            }\n            .irs--shiny .irs-handle {\n            top: 23px;\n            width: 22px;\n            height: 10px;\n        \"\"\")\n    ),\n    ui.panel_title(\"TCL et convolutions\"),\n    ui.layout_sidebar(\n        ui.panel_sidebar(\n            ui.input_select(\n                \"loi\",\n                \"Loi sous-jacente\",\n                {'uniforme': 'Uniforme', 'laplace' : 'Laplace'},\n            ),\n            ui.input_slider(\n                \"n_iter\",\n                \"Échantillons: n\",\n                1,\n                10,\n                value=1,\n                step=1,\n                ticks=False,\n            ), width = 3\n    ),\n    ui.panel_main(\n        output_widget(\"my_widget\"), width = 9\n    )\n)\n)\n\n\nnnzeros = 10001\nx_min = -20\nx_max = 20\n\nx = np.linspace(x_min, x_max, nnzeros)\ny = np.zeros(nnzeros)\nmask = np.where(np.abs(x) &lt;= 0.5, 1, 0)\ny[mask == 1] = 1\ndelta = (x_max - x_min) / nnzeros\nvar = np.sum(y * x**2 *(delta)) - (np.sum(y * x * delta))**2\n\ndef convolve(signal_in, n_convolutions, delta):\n    output = np.zeros(len(signal_in))\n    if n_convolutions == 0:\n        return output\n    elif n_convolutions == 1:\n        return signal_in\n    else:\n        output = signal_in.copy()\n        for i in range(n_convolutions - 1):\n            output = signal.fftconvolve(\n                output * delta, signal_in, mode=\"same\"\n            )\n        return output\n\n\ndef server(input, output, session):\n\n    fig = go.FigureWidget()\n    fig.add_trace(\n        go.Scatter(\n            mode=\"lines\",\n            line=dict(color=\"black\", width=3),\n            x=[],\n            y=[],\n            name=r\"f*...*f\",\n        )\n    )\n\n    fig.add_trace(\n        go.Scatter(\n            x=x,\n            y=np.exp(-(x**2) / (2 * var)) / np.sqrt(2 * var * np.pi),\n            mode=\"lines\",\n            line=dict(dash=\"dash\", color=\"red\", width=2),\n            name=f\"Loi normale&lt;br&gt;(variance adéquate)\",\n        )\n    )\n    fig.update_xaxes(range=[-3, 3], position=0.)\n    fig.update_yaxes(range=[0, 2 * np.max(y)], position=0.5, showticklabels=False)\n\n\n    fig.update_layout(\n        template=\"simple_white\",\n        showlegend=True,\n        autosize=True,\n        title=dict(text=\"Densité : &lt;br&gt; moyenne de n variables aléatoires\", yanchor=\"top\", y=0.95),\n        title_x=0.5,\n    )\n    fig.update_layout(legend=dict(\n        yanchor=\"top\",\n        y=0.90,\n        xanchor=\"left\",\n        x=-0.1,\n        font=dict(size= 10)\n    ))\n    fig.update_layout(\n    height=250,\n    margin=dict(l=0, r=0, b=10, t=100),\n    )\n\n    register_widget(\"my_widget\", fig)\n\n    @reactive.Effect\n    def _():\n        if str(input.loi()) == 'uniforme':\n            y = np.zeros(nnzeros)\n            mask = np.where(np.abs(x) &lt;= 0.5, 1, 0)\n            y[mask == 1] = 1\n\n        else:\n            # else str(input.loi()) == 'laplace':\n            y=np.exp(-np.abs(x))/2\n            # y = np.zeros(nnzeros)\n            # mask = np.where(np.abs(x) &gt;= 0.5, 1, 0)\n            # y[mask == 1] = 0\n            # y = np.cos(np.pi * x) + 1\n            y = y / (np.sum(y) * delta)\n\n        var = np.sum(y * x**2 * delta) - (np.sum(y * x * delta))**2\n\n        y_display = convolve(y, input.n_iter(), delta)\n        # Update data in fig:\n        fig.data[0].x = x / np.sqrt(input.n_iter())\n        fig.data[0].y = y_display * np.sqrt(input.n_iter())\n        fig.data[1].y = np.exp(-(x**2) / (2 * var)) / np.sqrt(2 * var * np.pi)\n        fig.update_yaxes(range=[0, 2 * np.max(y)], position=0.5, showticklabels=False)\n\n\napp = App(app_ui, server)\n\nPour aller plus loin sur les convolutions, voir la vidéo de 3Blue1Brown à ce sujet: Convolutions | Why X+Y in probability is a beautiful mess",
    "crumbs": [
      "Cours",
      "Théorèmes asymptotiques"
    ]
  },
  {
    "objectID": "Courses/simulation.html",
    "href": "Courses/simulation.html",
    "title": "Simulation",
    "section": "",
    "text": "Dans ce chapitre on se demande comment simuler en pratique des variables aléatoires i.i.d. L’idée est de commencer par le cas de variables aléatoires de loi uniforme et d’en déduire les autres lois.",
    "crumbs": [
      "Cours",
      "Simulation"
    ]
  },
  {
    "objectID": "Courses/simulation.html#variables-aléatoires-uniformes",
    "href": "Courses/simulation.html#variables-aléatoires-uniformes",
    "title": "Simulation",
    "section": "Variables aléatoires uniformes",
    "text": "Variables aléatoires uniformes\nOn rappelle qu’une variable aléatoire U suit une loi uniforme sur [0,1], noté \\mathcal{U}([0,1]) si sa fonction de répartition F_U est donnée par \nF_U(x)\n=\n\\begin{cases}\n    0, & \\text{si }x &lt; 0\\,,        \\\\\n    x, & \\text{si }x \\in [0,1]\\,,  \\\\\n    1, & \\text{si }x &gt; 1\\,.        \\\\\n\\end{cases}\n\n\n\n\n\n                                                \n\n\nFigure 1: Fonction de répartition de la loi uniforme\n\n\n\n\nL’objectif est de simuler sur machine une suite U_1, \\ldots, U_n de variables aléatoires i.i.d. de loi \\mathcal{U}([0,1]). Plusieurs problèmes apparaissent alors :\n\nUne machine est déterministe.\nLes nombres entre 0 et 1 donnés par la machine sont de la forme k/2^p, pour k \\in \\{0, \\ldots, 2^{p-1}\\}. On ne pourra donc jamais générer des nombres qui ne sont pas de cette forme.\nVérifier qu’une suite est bien i.i.d. est un problème difficile.\n\n\nDéfinition 1 (Générateur de nombres pseudo-aléatoires) \nUn générateur de nombres pseudo-aléatoires (🇬🇧: Pseudo Random Number Generator, PRNG), est un algorithme déterministe récursif qui renvoie une suite U_1, \\ldots, U_n dans [0,1] qui a un “comportement similaire” à une suite i.i.d. de loi \\mathcal{U}([0,1]). Pour être plus rigoureux, ces nombres sont en fait des nombres entiers générés uniformément sur un certain interval. Dans un second une transformation simple permet d’obtenir des nombres flottants (🇬🇧: floats) entre 0 et 1.\n\n\n\n\n\n\n\nPour aller plus loin\n\n\n\nParfois il est utile d’aller chercher dans le code source certaines information pour savoir comment les fonctions sont codées dans les packages que l’on utiliser. Par exemple, pour numpy que l’on utilise fréquement, on peut voir l’opération choisie ici: Random: int -&gt; float en numpy.\n\n\nUn tel algorithme se construit de la manière suivante :\n\nOn part d’une graine (🇬🇧: seed) U_0 qui détermine la première valeur de manière la plus arbitraire possible.\nLa procédure récursive s’écrit U_{n+1} = f(U_n), où f est une transformation déterministe, de sorte que U_{n+1} est le plus indépendant possible de U_1, \\dots·, U_n.\n\n\nLa fonction f est déterministe et prend ses valeurs dans un ensemble fini, donc l’algorithme est périodique. Le but est donc d’avoir la plus grande période possible.\nNotons qu’une fois que la graine est fixée, alors l’algorithme donne toujours les mêmes valeurs. Fixer la graine peut donc être très utile pour répéter des simulations dans des conditions identiques et ainsi repérer des erreurs.\n\n\n\n\n\n\n\nExercice: bug ou feature?\n\n\n\nReprendre les widgets du chapitre Théorèmes asymptotiques et faites varier doucement le paramètre p (de Bernoulli). Que constatez-vous? Proposer une explication potentielle.\n\n\n\nGénérateur congruentiel linéaire\nLa plupart des PRNG s’appuient sur des résultats arithmétiques. Un des plus connus est celui appelé Générateur congruentiel linéaire (🇬🇧 Linear congruential generator, LCG). Il est défini comme suit: on construit récursivement une suite d’entiers X_i via la congruence \n  X_{n+1} = a X_n + b \\quad \\text{mod } m \\enspace,\n où a,b,m sont des entiers bien choisis pour que la suite obtenue ait de bonnes propriétés. Il suffit alors de considérer X_n/m. Par exemple, la fonction rand sur scilab utilise cette congruence avec m=2^{31}, a=843\\; 314\\; 861, et b=453\\; 816\\; 693.\n\n\nGénérateurs alternatifs\nLes langages Python et R utilisent par défaut le générateur Mersenne-Twister qui s’appuie sur la multiplication vectorielle, mais d’autres générateurs sont aussi disponibles. Ce générateur a pour période m =2^{19937}-1, nombre qu’on peut raisonnablement considérer comme grand.\nPour numpy la méthode par défaut est PCG64 (cf. documentation de numpy), qui dispose de meilleures garanties statistiques (Voir le site https://www.pcg-random.org pour cela).\n\n\nUsage en numpy\nOn suppose désormais disposer d’un générateur pseudo-aléatoire sur [0,1]. En numpy depuis la version 1.17, une bonne manière d’utiliser des éléments aléatoires est d’utiliser un générateur que l’on définit soi-même:\n\nseed = 12345  # Toujours être conscient qu'une graine existe\nrng = np.random.default_rng(seed)  #\nprint(rng.random())  ##  un tirage uniforme sur [0,1]\nprint(rng.random(size=5))  ## cinq tirages uniformes sur [0,1]\nprint(rng.random(size=(3, 2)))  ## matrice 3x2, à entrées unif. sur [0,1]\n\n0.22733602246716966\n[0.31675834 0.79736546 0.67625467 0.39110955 0.33281393]\n[[0.59830875 0.18673419]\n [0.67275604 0.94180287]\n [0.24824571 0.94888115]]\n\n\nDans la suite on va voir comment générer d’autres lois à partir de la loi uniforme, mais il est clair que les logiciels modernes propose un large éventail de distribution classique (gaussienne, exponentielle, etc.). Une liste exhaustive est donnée ici pour numpy.\n\n\n\n\n\n\nPour aller plus loin\n\n\n\nUne excellent discussion sur les bonnes pratiques aléatoires en numpy, et l’usage de np.random.default_rng est donnée dans ce blog post d’Albert Thomas.\n\n\n\n\nPropriété de la loi uniforme\nOn verra souvent apparaître la variable aléatoire 1-U où U \\sim \\mathcal{U}([0,1]). Il se trouve que 1-U suit aussi une loi uniforme sur [0,1] comme le montre le calcul de sa fonction de répartition. Ainsi pour tout x \\in [0,1] on obtient \n\\begin{align*}\n\\mathbb{P}(1-U \\leq x) & = \\mathbb{P}(U \\geq 1-x),\\\\\n                       & = 1-(1-x), \\\\\n                       & = x\\,.\n\\end{align*}\n On peut démontrer facilement la même relation pour x&lt;0 et x&gt;1, d’où le résultat.",
    "crumbs": [
      "Cours",
      "Simulation"
    ]
  },
  {
    "objectID": "Courses/simulation.html#méthode-dinversion",
    "href": "Courses/simulation.html#méthode-dinversion",
    "title": "Simulation",
    "section": "Méthode d’inversion",
    "text": "Méthode d’inversion\nL’idée de la méthode d’inversion repose sur le résultat suivant : si F est une fonction de répartition bijective et U \\sim\\mathcal{U}([0,1]), alors la variable aléatoire F^{-1}(U) a pour fonction de répartition F. C’est une conséquence de la suite d’égalités : \n\\begin{align}\n  \\mathbb{P}( F^{-1}(U) \\leq x ) & = \\mathbb{P}( U \\leq F(x) ) \\\\\n                                 & = F(x)\\,,\n\\end{align}\n\\tag{1}\noù la deuxième égalité résulte de la bijectivité de F. Ainsi, si F est facilement inversible, on peut simuler une variable aléatoire X de loi F en simulant une variable aléatoire uniforme U et en posant X = F^{-1}(U).\n\nExemple 1 (Simulation d’une loi exponentielle) On rappelle que la loi exponentielle de paramètre \\lambda &gt; 0 a pour densité \nf_{\\lambda}(x) = \\lambda e^{-\\lambda x} {1\\hspace{-3.8pt} 1}_{\\mathbb{R}_+}(x)\\enspace.\n et donc pour fonction de répartition \nF_{\\lambda}(x) = 1 - e^{-\\lambda x} {1\\hspace{-3.8pt} 1}_{\\mathbb{R}_+}(x)\\enspace.\n On vérifie que F_{\\lambda} est bijective de \\mathbb{R}_+ dans ]0,1[ et que son inverse est donnée pour tout u \\in ]0,1[ par \nF_{\\lambda}^{-1}(u) = -\\frac{1}{\\lambda} \\log(1-u)\\enspace.\n\n\nMalheureusement, la fonction F n’est pas toujours inversible (penser aux lois discrètes) c’est donc pourquoi on utilise l’inverse l’inverse généralisée ou fonction quantile introduite dans la section Notations: \n\n  F^\\leftarrow(p)=  \\inf\\{ x \\in \\mathbb{R} \\colon F(x)\\geq p\\} \\enspace.\n\nInterprétation: Définir l’inverse d’une fonction de répartition F revient à résoudre l’équation F(x) = \\alpha d’inconnue x pour un \\alpha fixé. Si F n’est pas bijective, deux problèmes apparaissent :\n\nl’équation n’a aucune solution ce qui revient à dire que F n’est pas surjectif (graphiquement, F présente des sauts) ;\nl’équation a plusieurs solutions ce qui revient à dire que F n’est pas injective (graphiquement cela se matérialise par un plateau à la hauteur \\alpha). Un exemple classique est celui où F est la fonction de répartition d’une variable aléatoire discrète.\n\nLe passage à l’inéquation F(x) \\geq u permet de contourner la non-surjectivité : on ne regarde non plus les droites horizontales y=u mais la région \\{y \\geq \\alpha\\}. Le choix de l’\\inf dans la définition de F^{\\leftarrow} permet de contourner la non-injectivité : vu qu’il y a possiblement plusieurs x tels que F(x) \\geq u, on choisit le “premier”. sCes considérations sont illustrées en Figure Figure 2.\n\n\n\n\n                                                \n\n\nFigure 2\n\n\n\n\nRemarques additionnelles:\n\nLa fonction F étant croissante, la quantité F^\\leftarrow(u) correspond au premier instant où F dépasse \\alpha. Si F est bijective (ce qui équivaut dans ce cas à strictement croissante et injective), alors F^\\leftarrow = F^{-1}.\nLa fonction F^\\leftarrow n’est rien d’autre que la fonction quantile : si 0 &lt; \\alpha &lt; 1, q_{1-\\alpha} = F^\\leftarrow(1-\\alpha) est le quantile d’ordre (1-\\alpha) de F. Par exemple, F^\\leftarrow(1/2) correspond à la médiane.\nNotons que si u=0, on peut alors naturellement poser F^{\\leftarrow}(0) = -\\infty. De même, avec la convention la convention \\inf \\emptyset = +\\infty, on peut alors étendre la définition de F^\\leftarrow à u=1 (mais F^\\leftarrow(1) n’est pas toujours égal à \\infty, voir les exemples ci-dessous).\n\n\nDéfinition 2 (Loi à support fini) \nSoit X une variable aléatoire discrète prenant uniquement les valeurs x_1 &lt; \\dots &lt; x_r avec probabilité p_1, \\dots, p_r, et tel que p_1 + \\dots + p_r=1. On vérifie que pour tout u \\in ]0,1[, \n        F^\\leftarrow(u) =\n        \\begin{cases}\n            x_1 & \\text{si } 0 &lt; u \\leq p_1\\,,                  \\\\\n            x_2 & \\text{si } p_1 &lt; u \\leq p_1+p_2\\,,            \\\\\n                & \\vdots                                        \\\\\n            x_r & \\text{si }  p_1 + \\dots + p_{r-1} &lt; u &lt; 1\\,.\n        \\end{cases}\n\nSur cet exemple, on peut prolonger la définition de F^\\leftarrow à u=1 en posant F^\\leftarrow(1) = x_r. L’inverse généralisée se réécrit alors sous la forme \n        F^\\leftarrow(u) = \\sum_{k=1}^r x_k {1\\hspace{-3.8pt} 1}_{p_1 + \\dots + p_{k-1} &lt; u \\leq p_1 + \\dots + p_k}\\enspace,\n où on a posé p_0=0. Cette expression s’étend directement au cas où X prend un nombre infini dénombrable de valeurs (la somme devient alors une série).\n\nLa méthode est illustré ci-dessous pour quelques lois intéressantes:\n\nviewof dist = Inputs.select(['normal','cauchy','laplace','bimodal'], {value: \"bimodal\", label: \"Loi\"})\nviewof replay = html`&lt;button&gt;Relancer`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ninvcdfboard = require(await FileAttachment(\"../inverse-vizu/dist/invcdfboard.umd.cjs\").url())\n\n{\n  replay\n  const canvas = DOM.canvas(500, 500);\n  const galton = invcdfboard.galton(canvas,dist);\n  \n  return html`${galton.canvas}`\n}",
    "crumbs": [
      "Cours",
      "Simulation"
    ]
  },
  {
    "objectID": "Courses/simulation.html#méthode-de-rejet",
    "href": "Courses/simulation.html#méthode-de-rejet",
    "title": "Simulation",
    "section": "Méthode de rejet",
    "text": "Méthode de rejet\nL’idée de la méthode de rejet est la suivante. On souhaite simuler une variable aléatoire X de densité f, appelée loi cible, mais f est trop compliquée pour que la simulation puisse se faire directement. On dispose cependant d’une autre densité g possédant les propriétés suivantes :\n\non sait simuler Y de loi g,\nil existe m &gt; 0 tel que f(x) \\leq m \\cdot g(x),\non sait évaluer le rapport d’acceptation r(x) = \\frac{f(x)}{mg(x)}.\n\nRemarquons d’ores et déjà que la constante m est nécessairement plus grande que 1 car \n    1 = \\int_\\mathbb{R} f(x) \\, \\mathrm dx \\leq m \\int_\\mathbb{R} g(x)\\, \\mathrm dx = m\\,.\n\nL’idée est alors de considérer deux suites iid de variables aléatoires indépendantes entre elles :\n\n(Y_n)_{n \\geq 1} de loi g,\n(U_n)_{n \\geq 1} de loi uniforme sur [0,1].\n\nEn pratique, Y_n correspond à une proposition et U_n permettra de décider si on accepte la proposition ou non. Si oui, alors on conserve Y_n, sinon on simule Y_{n+1}. Le rapport d’acceptation, c’est-à-dire la proportion de Y_n acceptées, correspond à r(x).\nAutrement dit, pour simuler X de densité f, il suffit de simuler Y de densité g et U uniforme jusqu’à ce que U \\leq r(Y) (voir Figure ??? pour une illustration). La proposition suivante assure que cette méthode donne bien le résultat voulu.\n\nProposition 1 (Méthode de rejet) \nSoit T = \\inf \\{n \\geq 1 : U_n \\leq r(Y_n)\\} le premier instant où le tirage est accepté. Alors :\n\nT suit une loi géométrique de paramètre 1/m,\nla variable aléatoire X = Y_T a pour densité f et est indépendante de T.\n\n\nDémonstration:\nIl s’agit d’étudier la loi du couple (X,T). Pour x \\in \\mathbb{R} et n \\in \\mathbb{N}, on écrit \\mathbb{P}(X \\leq x, T=n)= \\mathbb{P}(U_1 &gt; r(Y_1), \\dots, U_{n-1} &gt; r(Y_{n-1}), U_n \\leq r(Y_n), Y_n \\leq x). Les n tirages étant iid, on obtient \n    \\mathbb{P}(X \\leq x, T=n) = \\mathbb{P}(U_1 &gt; r(Y_1))^{n-1} \\mathbb{P}(U_n \\leq r(Y_n), Y_n \\leq x)\\,.\n\nConcernant le premier terme, les variables aléatoires Y_1 et U_1 sont indépendantes donc leur loi jointe correspond au produit des densités : \\begin{align*}\n        \\mathbb{P}(U_1 &gt; r(Y_1)\n         & = \\mathbb{P}((U_1, Y_1) \\in \\{(u,y) \\in \\mathbb{R}^2 : u &gt; r(y)\\})                             \\\\\n         & = \\int_{\\mathbb{R}^2} {1\\hspace{-3.8pt} 1}_{\\{u &gt; r(y)\\}} ({1\\hspace{-3.8pt} 1}_{[0,1]}(u) g(y)) \\, \\mathrm du \\mathrm dy  \\\\\n         & = \\int_\\mathbb{R} \\bigg( \\int_0^1 {1\\hspace{-3.8pt} 1}_{\\{u &gt; r(y)\\}} \\, \\mathrm du\\bigg) g(y)\\, \\mathrm d y \\\\\n         & =  \\int_\\mathbb{R} (1-r(y)) \\, g(y)\\, \\mathrm d y\\,,\n    \\end{align*}\n ce qui se réécrit, comme f et g sont des densités et que r(y) = f(y)/(m \\cdot g(y)): \\begin{align*}\n        \\mathbb{P}(U_1 &gt; r(Y_1))\n        & = \\int_\\mathbb{R} g(y)\\, \\mathrm d y - \\int_\\mathbb{R} \\dfrac{f(y)}{m}\\, \\mathrm dy \\\\\n        & = 1 - \\dfrac{1}{m}\\,.\n    \\end{align*}\n Le deuxième terme se calcule de manière analogue : \n\\begin{align*}\n    \\mathbb{P}(U_n \\leq r(Y_n), Y_n \\leq x)\n        & = \\int_{\\mathbb{R}^2} {1\\hspace{-3.8pt} 1}_{\\{u \\leq r(y)\\}} {1\\hspace{-3.8pt} 1}_{\\{y \\leq x\\}} ({1\\hspace{-3.8pt} 1}_{[0,1]}(u) g(y)) \\, \\mathrm du \\mathrm dy       \\\\\n        & = \\int_\\mathbb{R} \\bigg( \\int_0^1  {1\\hspace{-3.8pt} 1}_{\\{u \\leq r(y)\\}} \\, \\mathrm du\\bigg) {1\\hspace{-3.8pt} 1}_{\\{y \\leq x\\}}  g(y)\\, \\mathrm d y\\,,\n\\end{align*}\n c’est-à-dire \n\\begin{align*}\n        \\mathbb{P}(U_n \\leq r(Y_n), Y_n \\leq x)\n        & = \\int_\\mathbb{R} r(y) {1\\hspace{-3.8pt} 1}_{\\{y \\leq x\\}}  g(y)\\, \\mathrm d y \\\\\n        & = \\int_{-\\infty}^x \\dfrac{f(y)}{m}\\, \\mathrm d y \\\\\n        & = \\dfrac{F(x)}{m}\\,,\n\\end{align*}\n où F est la fonction de répartition de la loi de densité f. On peut ainsi conclure que \n    \\mathbb{P}(X \\leq x, T=n)\n    =\n    \\bigg(1 - \\dfrac{1}{m}\\bigg)^{n-1} \\dfrac{F(x)}{m}\\,.\n Il ne reste plus qu’à étudier les lois marginales. D’une part, par continuité monotone croissante, \n    \\mathbb{P}(T=n)\n    = \\lim_{q \\to \\infty} \\mathbb{P}(X \\in ]-\\infty, q], T=n)\\,,\n ce qui donne \n    \\mathbb{P}(T=n)\n    = \\lim_{q \\to \\infty} \\bigg(1 - \\dfrac{1}{m}\\bigg)^{n-1} \\dfrac{F(q)}{m}\n    = \\bigg(1 - \\dfrac{1}{m}\\bigg)^{n-1} \\dfrac{1}{m}\\,.\n On en déduit que T suit une loi géométrique de paramètre 1/m. D’autre part, par \\sigma-additivité, \n    \\mathbb{P}(X \\leq x)\n    = \\mathbb{P}(X \\leq x, T \\in \\mathbb{N}^*)\n    = \\sum_{n=1}^\\infty \\mathbb{P}(X \\leq x, T=n)\\,,\n ce qui donne \n\\begin{align*}\n    \\mathbb{P}(X \\leq x)\n    & = \\sum_{n=1}^\\infty \\bigg(1 - \\dfrac{1}{m}\\bigg)^{n-1} \\dfrac{F(x)}{m}\\\\\n    & = \\dfrac{1}{1-(1-1/m)} \\dfrac{F(x)}{m}\\\\\n    & = F(x)\\,,\n\\end{align*}\n ce qui prouve que X a pour loi F.\nEnfin, la loi du couple (X,T) est égale au produit des lois \n    \\mathbb{P}(X \\leq x, T=n)\n    = \\bigg(1 - \\dfrac{1}{m}\\bigg)^{n-1} \\dfrac{F(x)}{m}\n    = \\mathbb{P}(T=n) \\mathbb{P}(X \\leq x)\\,,\n\n\n\nce qui prouve l’indépendance de X et T.\n\n\n□\n\n\n\ndef accept_reject(n, f, g, g_sampler, m):\n    \"\"\"\n    n: number of samples\n    f: target density\n    g: proposal density\n    m: constant such that f(x) &lt;= m*g(x)\n    \"\"\"\n    x_samples = np.zeros(n)\n    u_samples = np.zeros(n)\n    accepted = np.zeros(n)\n    n_accepted = 0\n    while n_accepted &lt; n:\n        x = g_sampler()\n        u = np.random.uniform()\n        alpha = u * m * g(x)\n        u_samples [n_accepted] = alpha\n        x_samples[n_accepted] = x\n        if  alpha &lt;= f(x):\n            accepted[n_accepted] = 1\n        n_accepted += 1\n    return x_samples, u_samples, accepted\n\n\n\n\n\n\n\nEn pratique…\n\n\n\nOn simule U_1 et Y_1. Si U_1 \\leq r(Y_1) c’est gagné, on pose X=Y_1. Sinon, on simule U_2 et Y_2 et on teste à nouveau l’inégalité U_2 \\leq r(Y_2). Et ainsi de suite. Comme T suit une loi géométrique de paramètre 1/m, son espérance vaut m : il faut en moyenne m tentatives pour obtenir une simulation de la loi de densité f. L’objectif est alors de choisir un couple (g, m) de sorte que m soit le plus proche possible de 1.\n\n\n\nExemple 2 (Rejet d’une loi polynomiale) Donnons un exemple jouet (on étudiera des exemples plus pertinents en TD). On considère la densité f(x) = 4x^3 {1\\hspace{-3.8pt} 1}_{[0,1]}(x). Comme f est majorée par 4, on peut choisir pour g la densité de la loi uniforme sur [0,1] et m=4. Alors, r(x) =f(x) / (mg(x)) = x^3, pour x \\in [0,1]. On simule donc (Y_1, U_1) et on teste si U_1 \\leq Y_1^3, etc.\nBien évidemment, on privilégiera ici une simulation via F^\\leftarrow qui permet de générer des variables aléatoires de loi f plus rapidement.\n\n\n\n\n                                                \n\n\nFigure 3: Visualisation des zones d’acceptations/rejet (g uniforme)\n\n\n\n\nNous pouvons facilement améliorer la proportion de point acceptés en proposant par exemple g définie par g(x) = x {1\\hspace{-3.8pt} 1}_{[0, 1]}(x).\n\n\n\n\n                                                \n\n\nFigure 4: Visualisation des zones d’acceptations/rejet (g triangulaire)\n\n\n\n\n\n\n\n\n\n\n\n\n\nmd`Le taux d'acceptation est passé de **${ratio1}** à\n**${ratio2}** en utilisant une loi triangulaire au lieu d'une loi uniforme.`\n\n\n\n\n\n\n\n\n\nExemple 3 (Rejet d’une loi de densité d’Andrews) Considérons la densité d’Andrews définie par f(x) = \\frac{1}{S} \\frac{\\sin(\\pi\\cdot x)}{\\pi \\cdot x} {1\\hspace{-3.8pt} 1}_{[-1,1]}(x), avec S = \\int_{-1}^{1}\\frac{\\sin(\\pi\\cdot x)}{\\pi \\cdot x}dx. Dans ce contexte, on ne connait pas la valeur exacte de S, et on va donc utiliser la méthode de rejet pour simuler des variables aléatoires de loi f sans cette information. On peut l’adapter en adaptant le test de la manière suivante: si l’on prend m=2/S et g(x) = \\frac{1}{2} {1\\hspace{-3.8pt} 1}_{[-1,1]}(x), on observe que tester u\\leq \\frac{f(x)}{m \\cdot g(x)} est équivalent à tester u \\leq r(x)=\\frac{1}{S} \\frac{\\sin(\\pi\\cdot x)}{\\pi \\cdot x} \\cdot \\frac{1}{\\frac{2}{S} g(x)} = \\frac{\\sin(\\pi\\cdot x)}{\\pi \\cdot x} \\cdot \\frac{1}{2 \\cdot g(x)}, ce qui peut se faire sans connaissance de S. De plus on peut vérifier que g(x) = \\frac{1}{2} {1\\hspace{-3.8pt} 1}_{[-1,1]}(x) définit une densité et que f(x) \\leq m \\cdot g(x) pour tout x\\in \\mathbb{R}.\n\nn = 10000\ng = lambda x: np.ones_like(x) / 2\ng_sampler = lambda: 2 * np.random.uniform() - 1\nm = 2\n\nx_samples, u_samples, accepted = accept_reject(n, np.sinc, g, g_sampler, m)\nratio = np.sum(accepted) / n\n# Note: https://stackoverflow.com/questions/70804891/how-to-vectorize-a-function-in-python-that-includes-a-limit-e-g-sinx-x\n\n\n\n\n\n\nOn peut approcher numériquement la valeur exacte de S en utilisant une méthode de calcul approchée, ce qui permet de comparer ici notre méthode de rejet avec la densité sous jacente:\n\nfrom scipy import integrate\nS = integrate.quad(np.sinc, -1, 1)[0]\nprint(f\"En utilisant la méthode de rejet, on trouve que S = {S:.3f}\")\n\nEn utilisant la méthode de rejet, on trouve que S = 1.179\n\n\nEnfin, on peut visualiser la qualité l’approximation de la densité par la méthode de rejet en comparant la densité approchée (avec un histogramme) avec la densité exacte:\n\nfig = go.Figure()\nfig.add_trace(\n    go.Histogram(\n        x=x_samples[accepted == 1], histnorm=\"probability density\", name=\"Échantillons\"\n    )\n)\n\n# Plot the density\nx = np.linspace(-1, 1, 100)\nfig.add_trace(\n    go.Scatter(\n        x=x,\n        y=np.sinc(x) / S,\n        mode=\"lines\",\n        line=dict(color=\"black\", dash=\"dash\"),\n        name=\"Densité\",\n    )\n)\n\nfig.update_layout(template=\"simple_white\", showlegend=True)\n\n\n\n                                                \n\n\nFigure 5: Méthode de rejet pour simuler une loi de densité de type Andrews, sans connaissance de la valeur exacte de la constante de normalisation.\n\n\n\n\n\nmd`Dans cet example, le taux d'acceptation est ici de **${ratio}**.`\n\n\n\n\n\n\n\n\nCas mutlidimensionnel\nCommençons par un cas de dimension deux.\nPour cela on va utiliser la méthode de rejet pour simuler une loi de densité f sur \\mathbb{R}^2. En particulier, un exemple classique est de tirer des points dans le disque unité, c’est-à-dire de simuler une loi uniforme sur le disque unité. Pour cela, on va utiliser la méthode de rejet avec g la densité de la loi uniforme sur le carré [-1,1]^2, et m=1 fonctionne.\nMais prenons un autre exemple, à savoir tirer des points uniformément dans la surface délimité par une cardioïde. Pour cela, on va utiliser la méthode de rejet avec g la densité de la loi uniforme sur le carré [-2,2]^2, et m=1 fonctionne.\n\n\n\n\n                                                \n\n\nFigure 6: Méthode de rejet pour simuler une loi uniforme sur une surface délimitée par une cardioïde.\n\n\n\n\n4.74\n\n\n\n\n\n\n                                                \n\n\nFigure 7: Méthode de rejet pour simuler une loi uniforme sur un disque unité.\n\n\n\n\n3.1260000000000003\n\n\n\n\n\n\n\n\nEXERCISE loi uniforme sur un cylindre\n\n\n\nProposer une méthode pour simuler une loi uniforme sur un cylindre de rayon 1 et de hauteur 10.",
    "crumbs": [
      "Cours",
      "Simulation"
    ]
  },
  {
    "objectID": "Courses/simulation.html#autres-méthodes",
    "href": "Courses/simulation.html#autres-méthodes",
    "title": "Simulation",
    "section": "Autres méthodes",
    "text": "Autres méthodes\n\nSommation de variables aléatoires\nPour simuler une variable aléatoire de loi binomiale \\mathcal{B}(n,p), on peut utiliser la méthode d’inversion. Cependant, cela nécessite le calcul de l’inverse généralisée de F, donc de coefficients binomiaux et de puissances de p et 1-p. À la place, on utilisera plutôt la relation bien connue suivante : si X_1, \\ldots, X_n est une suite iid de variables aléatoires de loi de Bernoulli de paramètre p, alors \n    X = X_1 + \\cdots + X_n \\sim \\mathcal{B}(n,p)\\,.\n\nPour simuler des variables aléatoires de Bernoulli, on utilise la méthode d’inversion (voir Exemple ). Ainsi, si U_1, \\ldots, U_n sont des variables aléatoires iid de loi uniforme sur [0,1], alors \n    \\sum_{i=1}^n {1\\hspace{-3.8pt} 1}_{\\{U_i \\leq p\\}} \\sim \\mathcal{B}(n,p)\\,.\n\n\n\nLoi de Poisson\nRappelons qu’une variable aléatoire X suit une loi de Poisson de paramètre \\lambda &gt; 0, notée X \\sim \\mathcal{P}(\\lambda) si \n    \\mathbb{P}(X = k) = e^{-\\lambda} \\dfrac{\\lambda^k}{k!}\\,, \\quad k \\in \\mathbb{N}^*\\,.\n Une méthode pour simuler une variable aléatoire de loi de Poisson est donnée par la proposition suivante.\n\nProposition 2 (Génération de v.a. de loi de Poisson) \nSoit (E_n)_{n \\geq 1} des variables aléatoires iid de loi exponentielle de paramètre \\lambda &gt; 0. On pose S_k = E_1 + \\cdots + E_k. Alors \n    \\mathbb{P}(S_n \\leq 1 &lt; S_{n+1}) =  e^{-\\lambda} \\dfrac{\\lambda^n}{n!}\\,, \\quad n \\in \\mathbb{N}^*\\,.\n Ainsi, la variable aléatoire T définie par \n    T := \\sup \\{n \\in \\mathbb{N}^* : E_1 + \\cdots + E_n \\leq 1\\}\n suit une loi de Poisson de paramètre \\lambda : T \\sim \\mathcal{P}(\\lambda).\n\nLa preuve repose sur le lemme suivant.\n\nLemme 1 (Loi de Erlang) \nSoit n variables aléatoires E_1, \\dots, E_n iid de loi exponentielle de paramètre \\lambda &gt;0. La somme E_1+\\dots+E_n suit une loi d’Erlang de paramètres (n,\\lambda), donnée par la fonction de répartition \n    F_{n,\\lambda}(t) = 1 - \\sum_{k=0}^{n-1} e^{-\\lambda t} \\frac{(\\lambda t)^k}{k!}\\,.\n\n\nDémonstration:\nOn montre le résultat pour n=2. La généralisation à k quelconque se fait par récurrence. Soit t &gt; 0, et f_{\\lambda}(x)={1\\hspace{-3.8pt} 1}_{\\{x \\geq 0 \\}} \\lambda e^{-\\lambda x} la densité d’une loi exponentielle de paramètre \\lambda. Les variables aléatoires E_1 et E_2 étant indépendantes et suivant des lois exponentielles de paramètre \\lambda_1 et \\lambda_2, on a \n\\begin{align*}\n    \\mathbb{P}(E_1+E_2 \\leq t)\n        & = \\int_{\\mathbb{R}^2} {1\\hspace{-3.8pt} 1}_{\\{x_1 + x_2 \\leq t\\}} f_{\\lambda}(x_1) f_{\\lambda}(x_2)\\, \\mathrm d x_1 \\mathrm d x_2 \\\\\n        & = \\int_{\\mathbb{R}^2} {1\\hspace{-3.8pt} 1}_{\\{x_1 + x_2 \\leq t\\}} \\lambda^2 e^{-\\lambda (x_1+x_2)} {1\\hspace{-3.8pt} 1}_{\\{x_1 \\geq 0\\}} {1\\hspace{-3.8pt} 1}_{\\{x_2 \\geq 0\\}}\\, \\mathrm d x_1 \\mathrm d x_2 \\\\\n        & = \\int_{\\mathbb{R}^2} {1\\hspace{-3.8pt} 1}_{\\{0 \\leq x_1 \\leq t\\}} {1\\hspace{-3.8pt} 1}_{\\{0 \\leq x_2 \\leq t-x_1\\}} \\lambda^2 e^{-\\lambda x_1} e^{-\\lambda x_2}\\, \\mathrm d x_1 \\mathrm d x_2             \\\\\n        & = \\int_0^t \\lambda e^{-\\lambda x_1} \\bigg(\\int _0^{t-x_1} \\lambda e^{-\\lambda x_2}\\, \\mathrm d x_2\\bigg)  \\mathrm d x_1\\,.\n\\end{align*}\n La première intégrale se calcule alors facilement : \n    \\int _0^{t-x_1} \\lambda e^{-\\lambda x_2}\\, \\mathrm d x_2 = 1 - e^{-\\lambda(t-x_1)}\\,.\n On obtient alors \n\\begin{align*}\n    \\mathbb{P}(E_1+E_2 \\leq t)\n   & = \\int_0^t \\lambda e^{-\\lambda x_1}dx_1 -  \\int_0^t e^{-\\lambda t} \\mathrm d x_1\\\\\n   & = 1 - e^{-\\lambda t} - \\lambda t e^{-\\lambda t}\\,.\n\\end{align*}\n Si t&lt;0, alors comme les E_i ne prennent que des valeurs positives on trouve \\mathbb{P}(E_1 + E_2 \\leq t) = 0. Ceci prouve le résultat pour n=2.\n\n\n\n\n\n□\n\n\nOn peut désormais prouver le résultat de la Proposition Proposition 2.\nDémonstration:\nPour n \\in \\mathbb{N}^*, on décompose la probabilité \\mathbb{P}(S_n \\leq 1 &lt; S_{n+1}) via \n\\begin{align*}\n    \\mathbb{P}(S_n \\leq 1 &lt; S_{n+1})\n    & = \\mathbb{P}(\\{S_n \\leq 1\\} \\setminus \\{S_{n+1} \\leq 1\\})\\\\\n    & = \\mathbb{P}(S_n \\leq 1) - \\mathbb{P}(S_{n+1} \\leq 1)\\,.\n\\end{align*}\n Le lemme précédent donne \n    \\mathbb{P}(S_n \\leq 1) = 1 - \\sum_{n=0}^{n-1} e^{-\\lambda} \\dfrac{\\lambda^k}{k!}\n et \n    \\mathbb{P}(S_{n+1} \\leq 1) = 1 - \\sum_{k=0}^{n} e^{-\\lambda} \\dfrac{\\lambda^k}{k!}\\,.\n On obtient alors le résultat souhaité : \n    \\mathbb{P}(S_n \\leq 1 &lt; S_{n+1})\n    = e^{-\\lambda} \\dfrac{\\lambda^n}{n!}\\,.\n\nOn conclut la preuve de la proposition en remarquant que \n    \\mathbb{P}(T=n) = \\mathbb{P}(S_n \\leq 1 &lt; S_{n+1})\\,.\n\n\n\n\n\n\n□\n\n\nLa simulation d’une variable aléatoire de Poisson repose donc sur la simulation de lois exponentielles qui se fait via la méthode d’inversion, comme vu dans Exemple 1. En pratique, on simule E_1 et on teste si E_1 &gt; 1. Si oui, on pose alors T=0. Si non, on simule E_2 et on teste si E_1 + E_2 &gt; 1. Si oui, on pose T=1. Sinon on continue la procédure.\n\n\nBibliographie et pour aller plus loin\n\nGenerating Random Floating-Point Numbers by Dividing Integers: a Case Study par Frédéric Goualard\nGenerating Pseudo-random Floating-Point Values par Allen Downey.",
    "crumbs": [
      "Cours",
      "Simulation"
    ]
  },
  {
    "objectID": "Courses/notations.html",
    "href": "Courses/notations.html",
    "title": "Notations et rappels",
    "section": "",
    "text": "On considère un espace probabilisé (\\Omega, {\\mathcal{F}}, \\mathbb{P}), composé d’un ensemble \\Omega, d’une tribu \\mathcal{F}, et d’une mesure de probabilité \\mathbb{P}.\nCette définition permet de transposer l’aléa qui provient de \\Omega dans l’espace E. L’hypothèse \\{X \\in B\\} \\in \\mathcal{F} assure que cet ensemble est bien un évènement et donc que l’on peut calculer sa probabilité.\nUne fois que l’aléa a été transposé de \\Omega vers E, on souhaite également transposer la probabilité \\mathbb{P} sur E. Ceci motive l’introduction de la notion de loi.\nLes propriétés de \\mathbb{P} assurent que \\mathbb{P}_X est bien une loi de probabilité sur l’espace mesurable (E, \\mathcal{E}).",
    "crumbs": [
      "Cours",
      "Notations et rappels"
    ]
  },
  {
    "objectID": "Courses/notations.html#loi-discrètes",
    "href": "Courses/notations.html#loi-discrètes",
    "title": "Notations et rappels",
    "section": "Loi discrètes",
    "text": "Loi discrètes\nLes variables aléatoires discrètes sont celles à valeurs dans un ensemble E discret, le plus souvent \\mathbb{N}, muni de la tribu pleine \\mathcal{F} = \\mathcal{P}(E).\n\nExemple 1 (Loi de Bernoulli) La loi la plus simple est la loi de Bernoulli de paramètre p \\in [0,1], définie sur \\{0,1\\} par \\mathbb{P}(X=1) = 1-\\mathbb{P}(X=0) = p qui modélise une expérience aléatoire à deux issues (succès = 1 et échec = 0).\n\n\nExemple 2 (Loi binomiale) En sommant des variables aléatoires indépendantes de loi de Bernoulli on obtient une loi binomiale : \\mathbb{P}(X=k) = \\binom{n}{k} p^k (1-p)^{n-k}, pour k \\in \\{0,\\ldots,n\\}, qui modélise le nombre de succès parmi n lancers.\n\n\nExemple 3 (Loi géométrique) En observant le nombre d’expériences nécessaires avant d’obtenir un succès, on obtient une loi géométrique : \\mathbb{P}(X=k) = p (1-p)^{k-1}, pour k \\geq 1. C’est une loi de probabilité discrète qui décrit le comportement du nombre d’événements se produisant dans un intervalle de temps fixé, si ces événements se produisent avec une fréquence moyenne ou espérance connue, et indépendamment du temps écoulé depuis l’événement précédent (e.g., nombre de clients dans une file d’attente, nombre de mutations dans un gène, etc.).\n\n\nExemple 4 (Loi de Poisson) La loi de Poisson de paramètre \\lambda &gt; 0 est définie par \\mathbb{P}(X=k) = e^{-\\lambda} \\lambda^k / k!, pour k \\in \\mathbb{N}",
    "crumbs": [
      "Cours",
      "Notations et rappels"
    ]
  },
  {
    "objectID": "Courses/notations.html#lois-continues",
    "href": "Courses/notations.html#lois-continues",
    "title": "Notations et rappels",
    "section": "Lois continues",
    "text": "Lois continues\nParmi les variables aléatoires réelles non discrètes, beaucoup peuvent se représenter avec une densité, c’est-à-dire qu’il existe une fonction mesurable f : \\mathbb{R} \\to [0, \\infty[ d’intégrale 1. La loi d’une telle variable aléatoire X est alors donnée pour tout A \\in \\mathcal{B}(\\mathbb{R}) par \n    \\mathbb{P}(X \\in A) = \\int_A f(x) \\, \\mathrm d x \\enspace.\n Les propriétés de l’intégrale de Lebesgue assure que cette formule définit bien une loi de probabilité.\n\nExemple 5 (Loi uniforme) La loi uniforme sur un ensemble B \\in \\mathcal{B}(\\mathbb{R}), s’obtient avec la densité définie par \nf(x) = {1\\hspace{-3.8pt} 1}_B(x) / \\lambda (B) \\enspace,\n où \\lambda (B) représente la mesure de Lebesgue de l’ensemble B. En particulier pour la loi uniforme sur le segment [0,1] on obtient la fonction suivante: \nf(x) = {1\\hspace{-3.8pt} 1}_{[0,1]}(x)\\enspace.\n Si une variable aléatoire U suit une telle loi on note U \\sim \\mathcal{U}([0,1]).\n\n\nExemple 6 (Loi exponentielle) La loi exponentielle de paramètre \\gamma &gt; 0 est obtenue avec la densité donnée par \nf(x) = \\gamma e^{-\\gamma x} {1\\hspace{-3.8pt} 1}_{\\mathbb{R}_+}(x)\\enspace.\n Si une variable aléatoire X suit cette loi on note X \\sim \\mathcal{Exp}(\\gamma).\n\n\nExemple 7 (Loi normale/gaussienne univariée) On obtient la loi normale de paramètre \\mu \\in \\mathbb{R} et \\sigma^2 &gt; 0 correspond à loi dont la densité est donnée par la fonction réelle: \nf(x) = \\frac{1}{\\sqrt{2 \\pi} \\sigma}e^{-\\frac{1}{2 \\sigma^2}(x-\\mu)^2} \\enspace.\n Si une variable aléatoire X suit une telle loi on note X \\sim \\mathcal{N}(\\mu,\\sigma^2), \\mu correspondant à l’espérance de la loi, et \\sigma^2 à sa variance. On nomme loi normale centrée réduite le cas correspondant à \\mu = 0 et \\sigma^2 = 1.\n\n\nExemple 8 (Loi normale multivariée) On peut étendre les lois normales au cas multi-dimensionnel. Fixons d\\in\\mathbb{N}^* un entier non nul. Pour un vecteur \\mu \\in \\mathbb{R}^d et une matrice symétrique-définie positive \\Sigma\\in \\mathbb{R^{d\\times d}}, la densité normale mutlivariée associée est donnée par la fonction: \nf(x) = \\frac{1}{{(2 \\pi)}^{\\frac{d}{2}} {\\rm det}(\\Sigma)} e^{-\\frac{1}{2}(x-\\mu)^\\top \\Sigma ^{-1}(x-\\mu)}\n Notons que \\mu est l’espérance de la loi et \\Sigma la matrice de variance-covariance.",
    "crumbs": [
      "Cours",
      "Notations et rappels"
    ]
  },
  {
    "objectID": "Courses/notations.html#fonction-de-répartition",
    "href": "Courses/notations.html#fonction-de-répartition",
    "title": "Notations et rappels",
    "section": "Fonction de répartition",
    "text": "Fonction de répartition\nLa notion de variable aléatoire n’est pas facile à manipuler puisqu’elle part d’un espace \\Omega dont on ne sait rien. On souhaite donc caractériser la loi d’une variable aléatoire en ne considérant que l’espace d’arrivée (E, \\mathcal{E}) .\nPlusieurs outils existent : la fonction de répartition (pour des variables aléatoires réelles), la fonction caractéristique (pour des variables aléatoires dans \\mathbb{R}^d), la fonction génératrice des moments (pour des variables aléatoires discrètes), etc. On se contente ici de la fonction de répartition qui nous sera utile pour simuler des variables aléatoires, ainsi que son inverse au sens de Levy.\n\nDéfinition 3 (Fonction de répartition 🇬🇧: cumulative distribution function) \nSoit X une variable aléatoire sur (\\mathbb{R}, \\mathcal{B}(\\mathbb{R})). La fonction de répartition de X est la fonction F_X définie sur \\mathbb{R} par \n\\begin{align*}\n    F_X(x) & = \\mathbb{P}(X \\leq x)\\\\\n           & = \\mathbb{P}(X \\in ]-\\infty, x]) \\enspace.\n\\end{align*}\n\n\n\nExemple 9 (Cas discret) Soit (x_i)_{i \\in I} une suite ordonnée de réels, avec I \\subset \\mathbb{N}. Si X est une variable aléatoire discrète prenant les valeurs (x_i)_{i \\in I} et de loi (p_i = \\mathbb{P}(X=x_i))_{i \\in I}, alors \n    F_X(x) = \\sum_{i \\in I} p_i {1\\hspace{-3.8pt} 1}_{[x_i, \\infty[}(x) \\enspace.\n\n\n\nExemple 10 (Cas continu) Si X est une variable aléatoire de densité f, alors \n    F_X(x) = \\int_{-\\infty}^x f(t) \\, \\mathrm dt \\enspace.\n\n\nLe graphe des fonctions de répartition des loi de Bernoulli, uniforme et normale sont représentées dans le widget ci-dessous. Notons que la fonction de répartition de la loi normale \\mathcal{N}(0,1), souvent notée \\Phi, n’admet pas d’expression explicite autre que \n\\Phi(x) = \\dfrac{1}{\\sqrt{2\\pi}} \\int_{-\\infty}^x e^{-\\frac{t^2}{2}}\\, \\mathrm d t\\enspace,\n Les valeurs numériques de \\Phi(x) étaient autrefois reportées dans des tables1. Par transformation affine, si X \\sim \\mathcal{N}(\\mu, \\sigma^2) — ce que l’on peut aussi écrire : X=\\mu + \\sigma Y, avec Y\\sim \\mathcal{N}(0,1) — alors sa fonction de répartition est donnée par F_X(x)=\\Phi((x-\\mu)/\\sigma).\n1 Wikipedia: loi normale\nProposition 1 (Propriétés de la fonction de répartition) Soit X une variable aléatoire de fonction de répartition F_X.\n\nF_X est une fonction croissante, de limite 0 en -\\infty et de limite 1 en +\\infty.\nF_X est continue à droite en tout point.\nPour tout x \\in \\mathbb{R}, on a \\mathbb{P}(X=x) = F_X(x) - F_X(x-), où F_X(x-) = \\lim_{\\epsilon \\to 0+} F_X(x- \\epsilon).\nSi X a pour densité f, alors F_X est dérivable \\lambda-presque partout de dérivée f.\n\n\nPour les démonstrations, voir par exemple [@Barbe_Ledoux06].\nLa propriété 3. est utile dans le cas discret : les valeurs prises par X correspondent aux points de discontinuité de F_X et les probabilités associées correspondent à la hauteur du saut.\nLa propriété 4. donne le lien entre la fonction de répartition d’une variable aléatoire à densité et sa densité. On peut donc retrouver la loi de X à partir de sa fonction de répartition. Le théorème suivant généralise ce résultat à toute variable aléatoire réelle (pas nécessairement discrète ou à densité).\n\nThéorème 1 (Caractérisation de la loi d’une variable aléatoire réelle) La fonction de répartition d’une variable aléatoire caractérise sa loi : deux variables aléatoires ont même loi si et seulement si elles ont même fonction de répartition.\n\nDémonstration: voir Wikipedia\nOn rappelle que la tribu des boréliens est engendrée par la famille d’ensembles \\{]-\\infty,x], x \\in \\mathbb{R}\\}. Le théorème précédent assure que si on connaît la mesure \\mathbb{P}_X sur cette famille d’ensembles alors on la connaît partout.\n\nExemple 11 (Loi exponentielle depuis une loi uniforme) On considère une variable aléatoire U de loi uniforme sur [0,1] et on pose X = -\\ln(1-U). Déterminons la loi de X en calculant sa fonction de répartition. Pour tout x \\in \\mathbb{R}, \n\\begin{align*}\nF_X(x) = & \\mathbb{P}(X \\leq x) \\\\\n       = & \\mathbb{P}(-\\ln(1-U) \\leq x) \\\\\n       = & \\mathbb{P}(U \\leq 1-e^{-x}) \\\\\n       = &\n    \\begin{cases}\n        0           & \\text{ si }x &lt; 0\\,,    \\\\\n        1 - e^{-x} & \\text{ si }x \\geq 0\\,,\n    \\end{cases}\n\\end{align*}\n\noù on a utilisé l’égalité \\mathbb{P}(U \\leq t) = t pour tout t \\in [0,1]. Ainsi la variable aléatoire X a la même fonction de répartition qu’une loi exponentielle de paramètre 1. On en conclut que X \\sim \\mathcal{Exp}(1). Notons que l’on peut aussi montrer que -\\ln(X)\\sim\\mathcal{E}(1), sachant que U et 1-U ont la même loi.",
    "crumbs": [
      "Cours",
      "Notations et rappels"
    ]
  },
  {
    "objectID": "Courses/notations.html#fonction-quantile-inverse-généralisée-à-gauche",
    "href": "Courses/notations.html#fonction-quantile-inverse-généralisée-à-gauche",
    "title": "Notations et rappels",
    "section": "Fonction quantile, inverse généralisée à gauche",
    "text": "Fonction quantile, inverse généralisée à gauche\nLa fonction de répartition étant une fonction croissante on peut donner un sens à son inverse généralisée de la manière suivante.\n\nDéfinition 4 (Fonction quantile/ inverse généralisée 🇬🇧: quantile distribution function) \nSoit X une variable aléatoire sur (\\mathbb{R}, \\mathcal{B}(\\mathbb{R})) et F_X sa fonction de répartion. La fonction quantile associée F_X^\\leftarrow: [0,1] \\rightarrow \\mathbb{R} est définie par \n  F_X^\\leftarrow(p)=  \\inf\\{ x \\in \\mathbb{R} \\colon F_X(x)\\geq p\\} \\enspace.\n\n\nOn parle parfois aussi d’inverse au sens de Levy pour cette inverse généralisée.\nDans le cas où la fonction de répartition F_X est bijective, alors l’inverse de la fonction de répartition coincide avec la fonction quantile.\nLa médiane est égale à F_X^\\leftarrow(1/2), les premiers et troisièmes quartiles sont égaux à F_X^\\leftarrow(1/4) et F_X^\\leftarrow(3/4). Enfin, les déciles sont les quantiles F_X^\\leftarrow(k/10) pour k=1,\\dots, 9.",
    "crumbs": [
      "Cours",
      "Notations et rappels"
    ]
  },
  {
    "objectID": "Courses/notations.html#visualisation-densité-fonction-de-répartition-quantiles-etc.",
    "href": "Courses/notations.html#visualisation-densité-fonction-de-répartition-quantiles-etc.",
    "title": "Notations et rappels",
    "section": "Visualisation: densité, fonction de répartition, quantiles, etc.",
    "text": "Visualisation: densité, fonction de répartition, quantiles, etc.\n\nCas des variables continues\n\nObservablePython / Shiny\n\n\n\nPlotly = require('plotly.js-dist');\ndists = require( 'https://cdn.jsdelivr.net/gh/stdlib-js/stats-base-dists@umd/browser.js' );\n// see source here: https://github.com/stdlib-js/stats-base-dists/tree/umd\n// continuous case\njstatPDFs = () =&gt; {\n  const distributions = Object.keys(dists);\n  // Get in continuousDistributions the distributions whose pdf, cdf and quantile are defined\n    const continuousDistributions = distributions.filter(name =&gt; dists[name].pdf && dists[name].cdf && dists[name].quantile);\n  return continuousDistributions\n};\noutput = jstatPDFs();\n\nexcludedPDFs = [];\npdfNames_unsorted = output.filter(name =&gt; !excludedPDFs.includes(name));\npdfNames=pdfNames_unsorted.toSorted();\nviewof inputs = Inputs.form([\n      Inputs.range([-10, 10], {value: 0.1, step: 0.001, label: tex`\\mu`}),\n      Inputs.range([0.01, 5], {value: 1.01, step: 0.001, label: tex`\\sigma`}),\n      Inputs.range([0.001, 0.999], {value: 0.75, step: 0.001, label:tex`\\alpha`}),\n    ]);\n\nviewof distrib_name = Inputs.select(pdfNames, {value: \"normal\", label: \"Distribution\"});\n\n\n{\n  const x = d3.range(-5, 5, 0.01);\n  const pdf = x.map(x =&gt; dists[distrib_name].pdf(x, mu, sigma));\n  const cdf = x.map(x =&gt; dists[distrib_name].cdf(x, mu, sigma));\n  const inv = x.map(x =&gt; dists[distrib_name].quantile(x, mu, sigma));\n  const quantile = dists[distrib_name].quantile(alpha, mu, sigma);\n  const filteredX = x.filter(coord =&gt; coord &lt;= quantile);\n  const filteredPdf = pdf.filter((_, i) =&gt; x[i] &lt;= quantile);\n  const filteredCdf = cdf.filter(coord =&gt; coord &lt;= quantile);\n\n\n{\nvar trace1 = {\n      type: \"scatter\",\n      name: 'Quantile',\n      x : cdf,\n      y : x,\n      line: {color: 'black'},\n      xaxis: 'x1',\n      yaxis: 'y3',\n\n\n};\n\nvar trace12 = {\n        x : [alpha, alpha],\n        y : [x[0], quantile],\n        mode: 'lines',\n        line: {\n            dash: 'dash',\n            width: 1,\n            color: ' #428BCA',\n            marks: {\n                size: 0,\n                color: ' #428BCA',\n            }\n\n          },\n      xaxis: 'x1',\n      yaxis: 'y3',\n\n\n};\n\n\n\nvar trace13 = {\n    x: [0, alpha],\n    y: [quantile, quantile],\n    mode: 'scatter',\n    line: {\n        dash: 'dash',\n        width: 1,\n        color: ' #428BCA',\n        marks: {\n            size: 0,\n        }\n    },\n    xaxis: 'x1',\n    yaxis: 'y3'\n}\n\n\nvar trace2 = {\nx: cdf,\ny: cdf,\ntype: 'scatter',\nname: 'identity',\nline: {color: 'black'},\nxaxis: 'x1',\nyaxis: 'y2'\n\n};\n\n\nvar trace21 = {\n        x : [alpha, 1],\n        y : [alpha, alpha],\n        mode: 'lines',\n        line: {\n            dash: 'dash',\n            width: 1,\n            color: ' #428BCA',\n            marks: {\n                size: 0,\n                color: ' #428BCA',\n            }\n\n          },\n      xaxis: 'x1',\n      yaxis: 'y2'\n}\n\nvar trace22 = {\n        x : [alpha, alpha],\n        y : [alpha, 1],\n        mode: 'lines',\n        line: {\n            dash: 'dash',\n            width: 1,\n            color: ' #428BCA',\n            marks: {\n                size: 0,\n                color: ' #428BCA',\n            }\n\n          },\n      xaxis: 'x1',\n      yaxis: 'y2'\n}\n\n\n\nvar trace23 = {\n    x: [alpha],\n    y: [alpha],\n    mode: 'scatter',\n    line: {\n        dash: 'dash',\n        width: 1,\n        color: ' #428BCA',\n        marks: {\n            size: 0,\n        }\n    },\n    xaxis: 'x1',\n    yaxis: 'y2'\n}\n\n\nvar trace31 = {\n      type: \"scatter\",\n      mode: \"lines\",\n      name: 'PDF2',\n      x: x,\n      y: cdf,\n      line: {color: 'black'},\n      xaxis: 'x2',\n      yaxis: 'y2',\n};\n\nvar trace32 = {\n        x : filteredX,\n        y : filteredX.map(x =&gt; alpha),\n        mode: 'lines',\n        line: {\n            dash: 'dash',\n            width: 1,\n            color: ' #428BCA',\n            marks: {\n                size: 0,\n                color: ' #428BCA',\n            }\n          },\n      xaxis: 'x2',\n      yaxis: 'y2'\n}\n\nvar trace33 = {\n        x : [quantile, quantile],\n        y : [0, alpha],\n        mode: 'lines',\n        line: {\n            dash: 'dash',\n            width: 1,\n            color: ' #428BCA',\n            marks: {\n                size: 0,\n                color: ' #428BCA',\n            }\n          },\n\n      xaxis: 'x2',\n      yaxis: 'y2'\n}\n\nvar trace34 = {\n    x: [quantile],\n    y: [alpha],\n    mode: 'scatter',\n    line: {\n        dash: 'dash',\n        width: 1,\n        color: ' #428BCA',\n        marks: {\n            size: 0,\n        }\n    },\n    xaxis: 'x2',\n    yaxis: 'y2'\n}\n\n\nvar trace41 = {\n\n  type: \"scatter\",\n  name: 'Quantile2',\n  fill: 'tozeroy',\n  x : filteredX,\n  y : filteredPdf,\n  opacity: 0.9,\n  line: {color: ' #428BCA'},\n  xaxis: 'x2',\n  yaxis: 'y1'\n\n};\n\n\nvar trace42 = {\n\n  type: \"scatter\",\n  mode: \"lines\",\n  name: 'PDF2',\n  x: x,\n  y: pdf,\n  line: {color: 'black'},\n  xaxis: 'x2',\n  yaxis: 'y1'\n\n};\n\nvar data = [\n  trace1,\n  trace12, trace13,\n  trace2, trace21, trace22, trace23,\n  trace31, trace32, trace33, trace34,\n  trace41, trace42];\n\n\nvar layout = {\n\n  title: 'Distribution et quantile',\n  xaxis: {\n    domain: [0, 0.32],\n    anchor: 'y1'\n  },\n  yaxis: {\n    domain: [0, 0.24],\n    anchor: 'x1'\n\n  },\n  xaxis2: {\n    domain: [0.35, 1],\n    anchor: 'y'\n  },\n\n  yaxis2: {\n    domain: [0.26, 0.49],\n    anchor: 'x1'\n  },\n\n  yaxis3: {\n    domain: [0.5, 1],\n    anchor: 'x1'\n  },\n\n// legend offset\n  showlegend: false,\n  height: 680,\n  annotations: [\n\n    {\n      x: 1/4,\n      y: quantile - (x[0]-x.slice(-1))/20,\n      xref: 'x1',\n      yref: 'y3',\n      text: 'q= ' + quantile.toFixed(2),\n      font: {\n        size: 12,\n        color: '#428BCA',\n\n      },\n      showarrow: false,\n      arrowhead: 0,\n      ax: 25,\n      ay: -10,\n\n    },\n    {\n      x: 1/4,\n      y: alpha,\n      xref: 'x1',\n      yref: 'y2',\n      text: 'alpha= ' + alpha.toFixed(2),\n      font: {\n        size: 12,\n        color: '#428BCA',\n\n      },\n      showarrow: false,\n      arrowhead: 0,\n      ax: 25,\n      ay: -10,\n\n    },\n    {\n        text: \"Fonction quantile\",\n      font: {\n      size: 15,\n      color: 'black',\n    },\n    showarrow: false,\n    align: 'center',\n    x: -0.01,\n    y: 1.05,\n    xref: 'paper',\n    yref: 'paper',\n    },\n    {\n      text: \"Fonction de répartition\",\n      font: {\n      size: 15,\n      color: 'black',\n            },\n    showarrow: false,\n    align: 'center',\n    x: 0.65,\n    y: 0.52,\n    xref: 'paper',\n    yref: 'paper',\n    },\n    {\n      text: \"Fonction de densité\",\n      font: {\n      size: 15,\n      color: 'black',\n            },\n    showarrow: false,\n    align: 'center',\n    x: 0.65,\n    y: 0.225,\n    xref: 'paper',\n    yref: 'paper',\n    },\n  ]\n\n};\n// XXX: TODO: put the xticks labels on the middle plot for x and on the right plot for y\n    var config = {responsive: true}\n    const div = DOM.element('div');\n    Plotly.newPlot(div, data, layout, config);\n    return div;\n  }\n\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmu = inputs[0];\nsigma = inputs[1];\nalpha = inputs[2]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#| standalone: true\n#| viewerHeight: 830\nimport numpy as np\nfrom scipy import stats\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nfrom shiny import ui, render, App\nfrom shinywidgets import output_widget, render_widget\n\n\ndef keep_no_param_distribution():\n    distributions = stats._continuous_distns._distn_names\n    distributions_0 = []\n    for _, name in enumerate(distributions):\n        dist = getattr(stats, name)\n        if not dist.shapes or len(dist.shapes) == 0:\n            distributions_0.append(name)\n    distributions_0_val = [\n        getattr(stats.distributions, string) for string in distributions_0\n    ]\n    distributions_0_dict = dict(zip(distributions_0, distributions_0_val))\n    return distributions_0_dict\n\n\ndistributions_0_dict = keep_no_param_distribution()\n\nmu = 0\nsigma = 1\n\napp_ui = ui.page_fluid(\n    ui.div(\n        ui.input_slider(\"alpha\", \"Quantile\", 0.01, 0.99, value=0.5, step=0.01),\n        ui.input_slider(\"xrange\", \"x-range\", -10, 10, value=(-5, 5), step=0.2),\n        ui.input_select(\n            \"distrib\",\n            \"Distribution\",\n            list(distributions_0_dict.keys()),\n            selected='norm'\n        ),\n        class_=\"d-flex gap-3\",\n    ),\n    output_widget(\"my_widget\"),\n)\n\n\ndef server(input, output, session):\n    @output\n    @render_widget\n    def my_widget():\n        fig = make_subplots(\n            rows=3,\n            cols=2,\n            vertical_spacing=0.1,\n            horizontal_spacing=0.15,\n            subplot_titles=(\n                \"Fonction quantile\",\n                \"\",\n                \"\",\n                \"Fonction de répartition\",\n                \"\",\n                \"Densité et quantile\",\n            ),\n            column_widths=[0.2, 0.5],\n            row_heights=[0.35, 0.17, 0.17],\n        )\n\n        alpha = input.alpha()\n        distribution = distributions_0_dict[input.distrib()]\n        x = np.linspace(input.xrange()[0], input.xrange()[1], num=400)\n        cdf_data = distribution.cdf(x, loc=mu, scale=sigma)\n        pdf_data = distribution.pdf(x, loc=mu, scale=sigma)\n        q_alpha = distribution.ppf(alpha, loc=mu, scale=sigma)\n\n        fig.update_layout(autosize=True, height=700)\n\n        # Quantile plot\n        fig.add_trace(\n            go.Scatter(\n                x=cdf_data, y=x, mode=\"lines\", marker={\"color\": \"black\"}\n            ),\n            row=1,\n            col=1,\n        )\n        # Diagonal\n        fig.add_trace(\n            go.Scatter(\n                x=cdf_data, y=cdf_data, mode=\"lines\", marker={\"color\": \"black\"}\n            ),\n            row=2,\n            col=1,\n        )\n        # Cdf part\n        fig.add_trace(\n            go.Scatter(\n                x=x, y=cdf_data, mode=\"lines\", marker={\"color\": \"black\"}\n            ),\n            row=2,\n            col=2,\n        )\n        # pdf part\n        fig.add_scatter(\n            x=x[x &lt; q_alpha],\n            y=pdf_data[x &lt; q_alpha],\n            fill=\"tozeroy\",\n            mode=\"none\",\n            fillcolor=\"rgb(66, 139, 202)\",\n            row=3,\n            col=2,\n        )\n\n        fig.add_trace(\n            go.Scatter(\n                x=x, y=pdf_data, mode=\"lines\", marker={\"color\": \"black\"}\n            ),\n            row=3,\n            col=2,\n        )\n\n        # Dots\n        fig.add_trace(\n            go.Scatter(\n                x=[alpha],\n                y=[q_alpha],\n                mode=\"markers\",\n                marker={\"color\": \"rgb(66, 139, 202)\"},\n                marker_symbol=\"x\",\n                marker_size=8,\n            ),\n            row=1,\n            col=1,\n        )\n\n        fig.add_trace(\n            go.Scatter(\n                x=[alpha],\n                y=[alpha],\n                mode=\"markers\",\n                marker={\"color\": \"rgb(66, 139, 202)\"},\n                marker_symbol=\"x\",\n                marker_size=8,\n            ),\n            row=2,\n            col=1,\n        )\n\n        fig.add_trace(\n            go.Scatter(\n                x=[q_alpha],\n                y=[alpha],\n                mode=\"markers\",\n                marker={\"color\": \"rgb(66, 139, 202)\"},\n                marker_symbol=\"x\",\n                marker_size=8,\n            ),\n            row=2,\n            col=2,\n        )\n\n        # Lines\n        fig.add_trace(\n            go.Scatter(\n                x=[alpha, alpha],\n                y=[x[0], q_alpha],\n                mode=\"lines\",\n                line=dict(dash=\"dash\", color=\"rgb(66, 139, 202)\")\n            ),\n            row=1,\n            col=1\n        )\n\n        fig.add_trace(\n            go.Scatter(\n                x=[alpha, alpha],\n                y=[alpha, 1.],\n                mode=\"lines\",\n                line=dict(dash=\"dash\", color=\"rgb(66, 139, 202)\")\n            ),\n            row=2,\n            col=1\n        )\n        fig.add_trace(\n            go.Scatter(\n                x=[alpha, 1],\n                y=[alpha, alpha],\n                mode=\"lines\",\n                line=dict(dash=\"dash\", color=\"rgb(66, 139, 202)\")\n            ),\n            row=2,\n            col=1\n        )\n        fig.add_trace(\n            go.Scatter(\n                x=[x[0], q_alpha],\n                y=[alpha, alpha],\n                mode=\"lines\",\n                line=dict(dash=\"dash\", color=\"rgb(66, 139, 202)\")\n            ),\n            row=2,\n            col=2\n        )\n        fig.add_trace(\n            go.Scatter(\n                x=[x[0], q_alpha],\n                y=[x[0], 0],\n                mode=\"lines\",\n                line=dict(dash=\"dash\", color=\"rgb(66, 139, 202)\")\n            ),\n            row=2,\n            col=2\n        )\n        # Axes ranges\n        fig.update_xaxes(range=[0, 1.], row=1, col=1)\n        fig.update_yaxes(matches=\"x6\", row=1, col=1)\n\n        fig.update_yaxes(range=[0, 1.], row=2, col=1)\n        fig.update_xaxes(matches=\"x1\", row=2, col=1)\n\n        fig.update_yaxes(rangemode=\"tozero\", row=3, col=2)\n        fig.update_xaxes(range=[x[0], x[-1]], row=3, col=2)\n\n        fig.update_xaxes(matches=\"x6\", row=2, col=2)\n        fig.update_yaxes(matches=\"y3\", row=2, col=2)\n\n        # Add dropdown\n        fig.update_layout(\n            showlegend=False,\n            template=\"simple_white\",\n        )\n        return fig\n\n\napp = App(app_ui, server)\n\n\n\n\n\n\nCas des variables discrètes\n\nObservablePython / Shiny\n\n\n\ndiscretePDFs = () =&gt; {\n  const distributions = Object.keys(dists);\n  // Get in continuousDistributions the distributions whose pdf, cdf and quantile are defined\n    const continuousDistributions = distributions.filter(name =&gt; dists[name].pmf && dists[name].cdf && dists[name].quantile);\n  return continuousDistributions\n};\noutput_discr = discretePDFs();\n\npmfNames_unsorted = output_discr.filter(name =&gt; !excludedPDFs.includes(name));\npmfNames=pmfNames_unsorted.toSorted();\nviewof inputs_disc = Inputs.form([\n      Inputs.range([-10, 10], {value: 0.1, step: 0.001, label: tex`\\mu `}),\n      Inputs.range([0.01, 5], {value: 1.01, step: 0.001, label: tex`\\sigma `}),\n      Inputs.range([0.001, 0.999], {value: 0.75, step: 0.001, label:tex`\\alpha`}),\n    ]);\n\nviewof distrib_name_discr = Inputs.select(pmfNames, {value: \"normal\", label: \"Distribution\"});\n\n\n{\n  const z = d3.range(-5, 5, 0.01);\n  const pmf = z.map(z =&gt; dists[distrib_name_discr].pmf(z, mu_disc, sigma_disc));\n  const cdf = z.map(z =&gt; dists[distrib_name_discr].cdf(z, mu_disc, sigma_disc));\n  const inv = z.map(z =&gt; dists[distrib_name_discr].quantile(z, mu_disc, sigma_disc));\n  const quantile = dists[distrib_name_discr].quantile(alpha_disc, mu_disc, sigma_disc);\n  const filteredX = z.filter(coord =&gt; coord &lt;= quantile);\n  const filteredPmf = pmf.filter((_, i) =&gt; z[i] &lt;= quantile);\n  const filteredCdf = cdf.filter(coord =&gt; coord &lt;= quantile);\n\n\n{\nvar trace1 = {\n      type: \"scatter\",\n      name: 'Quantile',\n      x : cdf,\n      y : z,\n      line: {color: 'black'},\n      xaxis: 'x1',\n      yaxis: 'y3',\n\n\n};\n\nvar trace12 = {\n        x : [alpha_disc, alpha_disc],\n        y : [z[0], quantile],\n        mode: 'lines',\n        line: {\n            dash: 'dash',\n            width: 1,\n            color: ' #428BCA',\n            marks: {\n                size: 0,\n                color: ' #428BCA',\n            }\n\n          },\n      xaxis: 'x1',\n      yaxis: 'y3',\n\n\n};\n\n\n\nvar trace13 = {\n    x: [0, alpha_disc],\n    y: [quantile, quantile],\n    mode: 'scatter',\n    line: {\n        dash: 'dash',\n        width: 1,\n        color: ' #428BCA',\n        marks: {\n            size: 0,\n        }\n    },\n    xaxis: 'x1',\n    yaxis: 'y3'\n}\n\n\nvar trace2 = {\nx: cdf,\ny: cdf,\ntype: 'scatter',\nname: 'identity',\nline: {color: 'black'},\nxaxis: 'x1',\nyaxis: 'y2'\n\n};\n\n\nvar trace21 = {\n        x : [alpha_disc, 1],\n        y : [alpha_disc, alpha_disc],\n        mode: 'lines',\n        line: {\n            dash: 'dash',\n            width: 1,\n            color: ' #428BCA',\n            marks: {\n                size: 0,\n                color: ' #428BCA',\n            }\n\n          },\n      xaxis: 'x1',\n      yaxis: 'y2'\n}\n\nvar trace22 = {\n        x : [alpha_disc, alpha_disc],\n        y : [alpha_disc, 1],\n        mode: 'lines',\n        line: {\n            dash: 'dash',\n            width: 1,\n            color: ' #428BCA',\n            marks: {\n                size: 0,\n                color: ' #428BCA',\n            }\n\n          },\n      xaxis: 'x1',\n      yaxis: 'y2'\n}\n\nvar trace23 = {\n    x: [alpha_disc],\n    y: [alpha_disc],\n    mode: 'scatter',\n    line: {\n        dash: 'dash',\n        width: 1,\n        color: ' #428BCA',\n        marks: {\n            size: 0,\n        }\n    },\n    xaxis: 'x1',\n    yaxis: 'y2'\n}\n\n\nvar trace31 = {\n      type: \"scatter\",\n      mode: \"lines\",\n      name: 'PDF2',\n      x: z,\n      y: cdf,\n      line: {color: 'black'},\n      xaxis: 'x2',\n      yaxis: 'y2',\n};\n\nvar trace32 = {\n        x : filteredX,\n        y : filteredX.map(z =&gt; alpha_disc),\n        mode: 'lines',\n        line: {\n            dash: 'dash',\n            width: 1,\n            color: ' #428BCA',\n            marks: {\n                size: 0,\n                color: ' #428BCA',\n            }\n          },\n      xaxis: 'x2',\n      yaxis: 'y2'\n}\n\nvar trace33 = {\n        x : [quantile, quantile],\n        y : [0, alpha_disc],\n        mode: 'lines',\n        line: {\n            dash: 'dash',\n            width: 1,\n            color: ' #428BCA',\n            marks: {\n                size: 0,\n                color: ' #428BCA',\n            }\n          },\n\n      xaxis: 'x2',\n      yaxis: 'y2'\n}\n\nvar trace34 = {\n    x: [quantile],\n    y: [alpha_disc],\n    mode: 'scatter',\n    line: {\n        dash: 'dash',\n        width: 1,\n        color: ' #428BCA',\n        marks: {\n            size: 0,\n        }\n    },\n    xaxis: 'x2',\n    yaxis: 'y2'\n}\n\n\nvar trace41 = {\n\n  type: \"scatter\",\n  name: 'Quantile2',\n  fill: 'tozeroy',\n  x : filteredX,\n  y : filteredPmf,\n  opacity: 0.9,\n  line: {color: ' #428BCA'},\n  xaxis: 'x2',\n  yaxis: 'y1'\n\n};\n\n\nvar trace42 = {\n\n  type: \"scatter\",\n  mode: \"lines\",\n  name: 'PDF2',\n  x: z,\n  y: pmf,\n  line: {color: 'black'},\n  xaxis: 'x2',\n  yaxis: 'y1'\n\n};\n\nvar data = [\n  trace1,\n  trace12, trace13,\n  trace2, trace21, trace22, trace23,\n  trace31, trace32, trace33, trace34,\n  trace41, trace42];\n\n\nvar layout = {\n\n  title: 'Distribution et quantile',\n  xaxis: {\n    domain: [0, 0.32],\n    anchor: 'y1'\n  },\n  yaxis: {\n    domain: [0, 0.24],\n    anchor: 'x1'\n\n  },\n  xaxis2: {\n    domain: [0.35, 1],\n    anchor: 'y'\n  },\n\n  yaxis2: {\n    domain: [0.26, 0.49],\n    anchor: 'x1'\n  },\n\n\n\n  yaxis3: {\n    domain: [0.5, 1],\n    anchor: 'x1'\n\n  },\n\n  showlegend: false,\n  height: 680,\n\n\n  annotations: [\n\n    {\n      x: 1/4,\n      y: quantile - (z[0]-z.slice(-1))/20,\n      xref: 'x1',\n      yref: 'y3',\n      text: 'q=' + quantile.toFixed(2),\n      font: {\n        size: 12,\n        color: '#428BCA',\n\n      },\n      showarrow: false,\n      arrowhead: 0,\n      ax: 25,\n      ay: -10,\n\n    },\n    {\n      x: 1/4,\n      y: alpha_disc,\n      xref: 'x1',\n      yref: 'y2',\n      text: 'alpha=' + alpha_disc.toFixed(2),\n      font: {\n        size: 12,\n        color: '#428BCA',\n\n      },\n      showarrow: false,\n      arrowhead: 0,\n      ax: 25,\n      ay: -10,\n\n    },\n    {\n        text: \"Fonction quantile\",\n      font: {\n      size: 15,\n      color: 'black',\n    },\n    showarrow: false,\n    align: 'center',\n    x: -0.01,\n    y: 1.05,\n    xref: 'paper',\n    yref: 'paper',\n    },\n    {\n      text: \"Fonction de répartition\",\n      font: {\n      size: 15,\n      color: 'black',\n            },\n    showarrow: false,\n    align: 'center',\n    x: 0.65,\n    y: 0.52,\n    xref: 'paper',\n    yref: 'paper',\n    },\n    {\n      text: \"Fonction de densité\",\n      font: {\n      size: 15,\n      color: 'black',\n            },\n    showarrow: false,\n    align: 'center',\n    x: 0.65,\n    y: 0.225,\n    xref: 'paper',\n    yref: 'paper',\n    },\n  ]\n\n};\n// XXX: TODO: put the xticks labels on the middle plot for x and on the right plot for y\n\n    var config = {responsive: true}\n    const div = DOM.element('div');\n    Plotly.newPlot(div, data, layout, config);\n    return div;\n  }\n\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmu_disc = inputs_disc[0];\nsigma_disc = inputs_disc[1];\nalpha_disc = inputs_disc[2];\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#| standalone: true\n#| viewerHeight: 830\nimport numpy as np\nfrom scipy import stats\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nfrom shiny import ui, render, App\nfrom shinywidgets import output_widget, render_widget\n\n\ndef keep_no_param_distribution_disc():\n    distributions = stats._discrete_distns._distn_names\n    distributions_0 = [name for name in distributions if not getattr(stats, name).shapes or len(getattr(stats, name).shapes) in [1, 2]]\n    distributions_0_val = [getattr(stats.distributions, string) for string in distributions_0]\n    distributions_0_dict = dict(zip(distributions_0, distributions_0_val))\n    return distributions_0_dict\n\ndef cdf_tool(x, dtype='int64'):\n    y = np.zeros(2*(len(x)), dtype=dtype)\n    y[::2]=x\n    y[1::2]=x\n    return y[1::], y[:-1], y\n\ndef pmf_tool(x, dtype='int64'):\n    y = np.zeros(2*(len(x)), dtype=dtype)\n    y[::2]=x\n    return y[1::], y[:-1], y\n\ndef insert_nones(my_list):\n    for i, val in enumerate(my_list):\n        if i % 3 == 2:\n            my_list.insert(i, None)\n    return my_list\n\ndistributions_0_dict = keep_no_param_distribution_disc()\n\napp_ui = ui.page_fluid(\n    ui.div(\n        ui.input_slider(\"alpha\", \"Quantile\", 0.01, 0.99, value=0.5, step=0.01),\n        ui.input_slider(\"xrange\", \"x-range\", -10, 10, value=(-5.5, 5.5), step=0.2),\n        ui.input_select(\n            \"distrib\",\n            \"Distribution\",\n            list(distributions_0_dict.keys()),\n            selected='poisson'\n        ),\n        class_=\"d-flex gap-3\",\n    ),\n    output_widget(\"my_widget\"),\n)\n\n\ndef server(input, output, session):\n    @output\n    @render_widget\n    def my_widget():\n        fig = make_subplots(\n            rows=3,\n            cols=2,\n            vertical_spacing=0.1,\n            horizontal_spacing=0.15,\n            subplot_titles=(\n                \"Fonction quantile\",\n                \"\",\n                \"\",\n                \"Fonction de répartition\",\n                \"\",\n                \"Fonction de masse et quantile\",\n            ),\n            column_widths=[0.2, 0.5],\n            row_heights=[0.35, 0.17, 0.17],\n        )\n\n\n        alpha = input.alpha()\n        # alpha=0.5\n\n        mu = 0.5  # Param needed for some distribution\n        if input.distrib()=='zipf':\n            mu = 2\n        distribution = distributions_0_dict[input.distrib()]\n        # distribution=distributions_0_dict['poisson']\n        x = np.arange(np.floor(input.xrange()[0]), np.ceil(input.xrange()[1]))\n        # x = np.arange(np.floor(-5.5), np.ceil(5.5))\n\n        cdf_data = distribution.cdf(x, mu)\n        pmf_data = distribution.pmf(x, mu)\n        q_alpha = distribution.ppf(alpha, mu)\n        support = pmf_data.nonzero()[0]\n        fig.update_layout(autosize=True, height=700)\n\n        # Quantile plot\n        new_x, new_y, new_z = cdf_tool(support)\n        _, _, new_pmf = pmf_tool(support)\n\n        fig.add_trace(\n            go.Scatter(\n                x=insert_nones(list(np.append(cdf_data[new_y[::-1]], distribution.cdf(x[0], mu)))),\n                y=insert_nones(list(np.append(x[new_x[::-1]], x[new_x[0]]))),\n                mode=\"lines\",\n                line=dict(color=\"black\")\n            ),\n            row=1,\n            col=1,\n        )\n        fig.add_trace(\n             go.Scatter(\n                x=cdf_data[support], y=x[support],\n                mode=\"markers\", marker={\"color\": \"black\"}\n            ),\n            row=1,\n            col=1,\n        )\n        # Diagonal\n        fig.add_trace(\n            go.Scatter(\n                x=cdf_data, y=cdf_data, mode=\"lines\", marker={\"color\": \"black\"}\n            ),\n            row=2,\n            col=1,\n        )\n        # Cdf part\n        fig.add_trace(\n            go.Scatter(\n                x=x[support], y=cdf_data[support],\n                mode=\"markers\", marker={\"color\": \"black\"}\n            ),\n            row=2,\n            col=2,\n        )\n        fig.add_trace(\n            go.Scatter(\n                x=insert_nones(list(np.append(np.insert(x[new_x], 0, [x[0], x[new_x[0]]]),x[-1]))),\n                y=insert_nones(list(np.append(np.insert(cdf_data[new_y], 0, [0,0]), cdf_data[-1]))),\n                mode=\"lines\",\n                line=dict(color=\"black\")\n            ),\n            row=2,\n            col=2\n        )\n\n        fig.add_trace(\n            go.Scatter(\n                x=x, y=pmf_data, mode=\"markers\", marker={\"color\": \"black\"}\n            ),\n            row=3,\n            col=2,\n        )\n        fig.add_trace(\n            go.Scatter(\n                x=x, y=pmf_data, mode=\"markers\", marker={\"color\": \"black\"}\n            ),\n            row=3,\n            col=2,\n        )\n        x_bar = insert_nones(list(x[new_z]))\n        y_bar = insert_nones(list(pmf_data[new_pmf]))\n        fig.add_trace(\n            go.Scatter(\n                x=x_bar,\n                y=y_bar,\n                mode=\"lines\",\n                line=dict(color=\"black\")\n            ),\n            row=3,\n            col=2\n        )\n        _,_, devil_x = cdf_tool(x[x&lt;=q_alpha])\n        _,_, devil_y = cdf_tool(pmf_data[x&lt;q_alpha], dtype='float64')\n\n        x_bar_blue = insert_nones(list(devil_x))\n        y_bar_blue = np.array(insert_nones(list(devil_y)))\n        y_bar_blue[::-3]=0.\n        y_bar_blue = list(y_bar_blue)\n        fig.add_trace(\n            go.Scatter(\n                x=x_bar_blue,\n                y=y_bar_blue,\n                mode=\"lines\",\n                line=dict(color=\"rgb(66, 139, 202)\")\n            ),\n            row=3,\n            col=2\n        )\n        # pdf part\n        fig.add_scatter(\n            x=x[x &lt;= q_alpha],\n            y=pmf_data[x &lt;= q_alpha],\n            mode=\"markers\",\n            marker={\"color\":\"rgb(66, 139, 202)\"},\n            row=3,\n            col=2,\n        )\n\n        # Dots\n        fig.add_trace(\n            go.Scatter(\n                x=[alpha],\n                y=[q_alpha],\n                mode=\"markers\",\n                marker={\"color\": \"rgb(66, 139, 202)\"},\n                marker_symbol=\"x\",\n                marker_size=8,\n            ),\n            row=1,\n            col=1,\n        )\n\n        fig.add_trace(\n            go.Scatter(\n                x=[alpha],\n                y=[alpha],\n                mode=\"markers\",\n                marker={\"color\": \"rgb(66, 139, 202)\"},\n                marker_symbol=\"x\",\n                marker_size=8,\n            ),\n            row=2,\n            col=1,\n        )\n\n        fig.add_trace(\n            go.Scatter(\n                x=[q_alpha],\n                y=[alpha],\n                mode=\"markers\",\n                marker={\"color\": \"rgb(66, 139, 202)\"},\n                marker_symbol=\"x\",\n                marker_size=8,\n            ),\n            row=2,\n            col=2,\n        )\n\n        # Lines\n        fig.add_trace(\n            go.Scatter(\n                x=[alpha, alpha],\n                y=[x[0], q_alpha],\n                mode=\"lines\",\n                line=dict(dash=\"dash\", color=\"rgb(66, 139, 202)\")\n            ),\n            row=1,\n            col=1\n        )\n\n        fig.add_trace(\n            go.Scatter(\n                x=[alpha, alpha],\n                y=[alpha, 1.],\n                mode=\"lines\",\n                line=dict(dash=\"dash\", color=\"rgb(66, 139, 202)\")\n            ),\n            row=2,\n            col=1\n        )\n        fig.add_trace(\n            go.Scatter(\n                x=[alpha, 1],\n                y=[alpha, alpha],\n                mode=\"lines\",\n                line=dict(dash=\"dash\", color=\"rgb(66, 139, 202)\")\n            ),\n            row=2,\n            col=1\n        )\n        fig.add_trace(\n            go.Scatter(\n                x=[x[0], q_alpha],\n                y=[alpha, alpha],\n                mode=\"lines\",\n                line=dict(dash=\"dash\", color=\"rgb(66, 139, 202)\")\n            ),\n            row=2,\n            col=2\n        )\n        fig.add_trace(\n            go.Scatter(\n                x=[x[0], q_alpha],\n                y=[x[0], 0],\n                mode=\"lines\",\n                line=dict(dash=\"dash\", color=\"rgb(66, 139, 202)\")\n            ),\n            row=2,\n            col=2\n        )\n        # Axes ranges\n        fig.update_xaxes(range=[0, 1.05], row=1, col=1)\n        fig.update_yaxes(matches=\"x6\", row=1, col=1)\n\n        fig.update_yaxes(range=[0, 1.05], row=2, col=1)\n        fig.update_xaxes(matches=\"x1\", row=2, col=1)\n\n        fig.update_yaxes(rangemode=\"tozero\", row=3, col=2)\n        fig.update_xaxes(range=[x[0], x[-1]], row=3, col=2)\n\n        fig.update_xaxes(matches=\"x6\", row=2, col=2)\n        fig.update_yaxes(matches=\"y3\", row=2, col=2)\n\n        # Add dropdown\n        fig.update_layout(\n            showlegend=False,\n            template=\"simple_white\",\n        )\n        return fig\n\n\napp = App(app_ui, server)",
    "crumbs": [
      "Cours",
      "Notations et rappels"
    ]
  },
  {
    "objectID": "Courses/loi_normale1D.html",
    "href": "Courses/loi_normale1D.html",
    "title": "Loi normale (1D)",
    "section": "",
    "text": "On considère ici \\mathbb{R}^d muni du produit scalaire euclidien \\langle \\cdot, \\cdot \\rangle et de la norme euclidienne \\|\\cdot\\| associée.",
    "crumbs": [
      "Cours",
      "Loi normale (1D)"
    ]
  },
  {
    "objectID": "Courses/loi_normale1D.html#la-loi-normale",
    "href": "Courses/loi_normale1D.html#la-loi-normale",
    "title": "Loi normale (1D)",
    "section": "La loi normale",
    "text": "La loi normale\n\nDéfinitions et propriétés\nOn rappelle que la loi normale de paramètres \\mu \\in \\mathbb{R} et \\nu &gt; 0 a une densité donnée pour tout x \\in \\mathbb{R} par\n\n    \\varphi_{\\mu, \\nu}(x)=\\frac{1}{\\sqrt{2 \\pi \\nu}}\\exp\\Big(-\\frac{(x-\\mu)^2}{2\\nu}\\Big)\\enspace.\n\nOn note X \\sim \\mathcal{N}(\\mu, \\nu), si X est une variable aléatoire ayant pour densité \\varphi_{\\mu, \\nu}. Notons que si X \\sim \\mathcal{N}(\\mu,\\nu), alors X a pour espérance \\mu et pour variance \\nu. Le cas particulier \\mu=0 et \\nu=1 correspond à une variable aléatoire dite centrée réduite.\nLa loi normale vérifie la propriété de stabilité par transformation affine : si X \\sim \\mathcal{N}(\\mu, \\nu) et si (a,b) \\in \\mathbb{R}^* \\times \\mathbb{R}, alors la variable aléatoire a X + b suit une loi normale \\mathcal{N}(a\\mu + b, a^2 \\nu). On peut donc facilement passer d’une loi normale centrée réduite à une loi normale quelconque via une transformation affine :\n\nsi X \\sim \\mathcal{N}(0,1), alors \\sqrt{\\nu} X + \\mu \\sim \\mathcal{N}(\\mu, \\nu),\nsi X \\sim \\mathcal{N}(\\mu, \\nu), alors (X-\\mu)/\\sqrt{\\nu} \\sim \\mathcal{N}(0,1).\n\nAinsi, savoir simuler une loi normale centrée réduite, permet de simuler n’importe quelle loi normale.\nRappelons enfin que la fonction caractéristique d’une variable aléatoire X \\sim \\mathcal{N}(\\mu, \\nu) est donnée pour tout t \\in \\mathbb{R} par \n\\begin{align*}\n\\phi_{\\mu, \\nu}(t) & \\triangleq \\mathbb{E}(e^{i t X})  \\\\\n\\phi_X(t) & = \\exp\\Big( i \\mu t - \\dfrac{\\nu t^2}{2}\\Big)\\enspace.\n\\end{align*}\n\n\n\nSimulation d’une loi normale\n\n\n\n\n\n\nUne mauvaise piste pour simuler une loi normale\n\n\n\nOn peut simuler une loi normale à partir de variables aléatoires uniformes U_1, \\ldots, U_n iid en appliquant le théorème central limite à \n    \\dfrac{U_1 + \\cdots + U_n - n/2}{\\sqrt{n/12}}\\,.\n Cependant, cette méthode ne donne qu’une approximation d’une loi normale. Par ailleurs, la vitesse de convergence étant relativement lente (de l’ordre de \\sqrt n), il faudra simuler beaucoup de variables aléatoires uniformes pour avoir une approximation correcte, ce qui demande un temps de calcul assez élevé.\n\n\n\nChangement de variables\nLe théorème suivant permet de passer de la loi d’un couple (X,Y) à celle de (U,V) = \\phi(X,Y), où \\phi est un C^1-difféomorphisme, c’est-à-dire une application bijective dont la réciproque est également de classe C^1.\nPour cela rappelons que la jacobienne de \\phi^{-1} correspond à la matrice (application linéaire) des dérivées partielles. Ainsi, si \\phi(x,y) = (u,v) \\iff (x,y) = \\phi^{-1}(u,v), alors \n{\\rm{Jac}}~\\phi^{-1} (u,v)\n=\n\\begin{pmatrix}\n  \\frac{\\partial x}{\\partial u} & \\frac{\\partial x}{\\partial v}    \\\\\n  \\frac{\\partial y}{\\partial u} & \\frac{\\partial y}{\\partial v}\n\\end{pmatrix} \\enspace.\n\n\nThéorème 1 (Caractérisation de la loi d’une variable aléatoire réelle) XXX source\nSoit (X,Y) un vecteur aléatoire de densité f_{(X,Y)} définie sur l’ouvert A \\subset \\mathbb{R}^2 et \\phi : A \\to B \\subset \\mathbb{R}^2 un C^1-difféomorphisme. Le vecteur aléatoire (U,V)=\\phi(X,Y) admet alors pour densité f_{(U,V)} définie sur B pour tout (u,v) \\in \\mathbb{R}^2 par \n  f_{(U,V)} (u,v)\n  = f_{(X,Y)} (\\phi^{-1}(u,v)) |\\det ({\\rm{Jac}}~\\phi^{-1} (u,v))| {1\\hspace{-3.8pt} 1}_B(u,v)\\enspace.\n\n\nOn a énoncé le résultat en dimension 2 par simplicité. Il s’étend bien évidemment à une dimension d quelconque. En particulier, pour d=1, on retrouve le changement de variable classique dans le cas de l’intégration d’une fonction à valeurs réelles.\nDémonstration.\nOn rappelle que la loi de (U,V) est caractérisée par les quantités \\mathbb{E}[h(U,V)] pour tout h : \\mathbb{R}^2 \\to \\mathbb{R} mesurable bornée. On considère donc une telle fonction h et on applique la formule de transfert : \n  \\mathbb{E}[h(U,V)]\n  =\\mathbb{E}[h(\\phi(X,Y))]\n  = \\int_{\\mathbb{R}^2} h(\\phi(x,y)) f_{(X,Y)}(x,y) \\, d x d y\n  = \\int_{A} h(\\phi(x,y)) f_{(X,Y)}(x,y) \\, d x d y\\enspace.\n On applique alors la formule du changement de variables vu en théorie de l’intégration avec (u,v) = \\phi(x,y) \\iff \\phi^{-1}(u,v) = (x,y) : \n  \\int_{A} h(\\phi(x,y)) f_{(X,Y)}(x,y) \\, d x d y\n  = \\int_{B} h(u,v)) f_{(X,Y)}(\\phi^{-1}(u,v)) |\\det ({\\rm{Jac}}~\\phi^{-1} (u,v))| \\, d u d v\\,.\n On conclut alors que \n\\mathbb{E}[h(U,V)]\n= \\int_{\\mathbb{R}^2} h(u,v) f_{(X,Y)}(\\phi^{-1}(u,v)) |\\det ({\\rm{Jac}}~\\phi^{-1} (u,v))| {1\\hspace{-3.8pt} 1}_B(u,v)\\, d u d v\\,,\n ce qui donne le résultat voulu.\n\n\n\n\n\n□",
    "crumbs": [
      "Cours",
      "Loi normale (1D)"
    ]
  },
  {
    "objectID": "Courses/matterjs-inverse-vizu.html",
    "href": "Courses/matterjs-inverse-vizu.html",
    "title": "inverse-vizu",
    "section": "",
    "text": "viewof dist = Inputs.select(['normal','cauchy','laplace','bimodal'], {value: \"bimodal\", label: \"Distribution type\"})\nviewof replay = html`&lt;button&gt;replay`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ninvcdfboard = require(await FileAttachment(\"../inverse-vizu/dist/invcdfboard.umd.cjs\").url())\n\n{\n  replay\n  const canvas = DOM.canvas(500, 500);\n  const galton = invcdfboard.galton(canvas,dist);\n  \n  return html`${galton.canvas}`\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Retour au sommet"
  },
  {
    "objectID": "Courses/perspective_historique.html",
    "href": "Courses/perspective_historique.html",
    "title": "Perspectives historiques",
    "section": "",
    "text": "Nous allons présenter ici quelques éléments historiques sur les méthodes de Monte-Carlo, dont les prémisses remontent au XVIIIème siècle.",
    "crumbs": [
      "Cours",
      "Perspectives historiques"
    ]
  },
  {
    "objectID": "Courses/perspective_historique.html#laiguille-de-buffon",
    "href": "Courses/perspective_historique.html#laiguille-de-buffon",
    "title": "Perspectives historiques",
    "section": "L’aiguille de Buffon",
    "text": "L’aiguille de Buffon\nGeorges-Louis Leclerc, Comte de Buffon1 proposa en 1733 une méthode qui s’avéra être utile pour estimer la valeur de \\pi. On désigne de nos jours cette expérience sous le nom de l’aiguille de Buffon. C’est l’une des premières méthodes de Monte-Carlo référencée dans la littérature (la source du texte est disponible ici sur le site de la BNF).\n1 Georges-Louis Leclerc, Comte de Buffon: (1707-1788) naturaliste, mathématicien et industriel français du siècle des Lumières La question initiale (simplifiée ici) posée par Buffon était la suivante: une aiguille de taille 1 tombe sur un parquet composé de lattes de largeur 1: quelle est alors la probabilité P que l’aiguille croise une ligne de la trame du parquet ?\nLe contexte original était dans celui d’un jeu à deux joueurs: un joueur parie sur le fait que l’aiguille croise une ligne de la trame du parquet, l’autre sur le fait que l’aiguille ne croise pas une ligne de la trame du parquet. L’enjeu est alors de calculer la probabilité de succès de chacun des joueurs, et de voir si le jeu est équilibré ou non.\nVoilà brièvement la question que s’est posée Buffon en 1733. La réponse est donnée par la formule suivante, qui montre que le jeu qu’il propose n’est pas équilibré:\n\nP = \\frac{2}{\\pi} \\approx 0.6366 \\enspace.\n Une preuve de ce résultat sera donnée ci-dessous.\nL’idée sous-jacente de Buffon est que si l’on répète cette expérience un grand nombre de fois, on peut approché la quantité P numériquement, par exemple en proposant un estimateur \\hat{P}_n qui compte la proportion de chevauchement après avoir fait n répétition des lancers. Pour estimer \\pi, il ne restera donc plus qu’à évaluer \\frac{2}{\\hat{P}_n}.\nOn peut faire cette expérience dans le monde réelle (c’est un peu long pour n grand!), mais on peut aussi utiliser une méthode numérique pour cela. Il s’agit alors de tirer aléatoire la position du centre de l’aiguille, puis de tirer aussi de manière aléatoire son angle de chute. On teste à la fin si l’aiguille croise une ligne de la trame du parquet ou non, et on recommence l’expérience un grand nombre de fois.\nCette méthode est donnée ci-dessous, avec un exemple interactif généré en Python.\n\nCode\nimport numpy as np\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\nrng = np.random.default_rng(44)\n\nn_samples = 200\nxmax = 14.499999\nxmin = -xmax\n\n\n# Create the needles\ncenters_x = rng.uniform(xmin, xmax, n_samples)\nangles = rng.uniform(0, 2 * np.pi, n_samples)\ncenters_y = rng.uniform(-2, 2, n_samples)\n\n# Compute the right borders of the needles\nborders_right = np.zeros((n_samples, 2))\nborders_right[:, 0] = centers_x + np.cos(angles) / 2\nborders_right[:, 1] = centers_y + np.sin(angles) / 2\n\n# Compute the left borders of the needles\nborders_left = np.zeros((n_samples, 2))\nborders_left[:, 0] = centers_x + np.cos(angles + np.pi) / 2\nborders_left[:, 1] = centers_y + np.sin(angles + np.pi) / 2\n\ncenters_x_round = np.round(centers_x)\noverlap = (borders_right[:, 0] - centers_x_round) * (\n    borders_left[:, 0] - centers_x_round\n) &lt; 0\noverlap = np.where(overlap, 1, 0)\nn_overlap = int(np.sum(overlap))\n\n\n# Check if the needles cross a line\nborders_red = np.empty((3 * n_overlap, 2), dtype=object)\nborders_red.fill(None)\nborders_red[::3, :] = borders_right[overlap == 1]\nborders_red[1::3, :] = borders_left[overlap == 1]\n\nborders_blue = np.empty((3 * (n_samples - n_overlap), 2), dtype=object)\nborders_blue.fill(None)\nborders_blue[::3, :] = borders_right[overlap == 0]\nborders_blue[1::3, :] = borders_left[overlap == 0]\n\noverlaps = np.empty((3 * n_samples), dtype=object)\noverlaps.fill(None)\noverlaps[::3] = overlap\noverlaps[1::3] = overlap\noverlaps[2::3] = overlap\n\nidx_red = np.cumsum(overlaps)\nidx_blue = np.cumsum(1 - overlaps)\n\n\n# Create subplots with 2 rows and 1 column with ratio x /  y  of 10\nfig = make_subplots(rows=2, cols=1, vertical_spacing=0.1, row_heights=[2, 1])\n\n# Use a loop to plot vertical lines equation \"y=c\" for integer values c in [-2, -1, 0, 1, 2]\nfor i in range(int(np.round(xmin)), int(np.round(xmax)) + 1):\n    fig.add_shape(\n        type=\"line\",\n        y0=-3,\n        x0=i,\n        y1=3,\n        x1=i,\n        line=dict(\n            color=\"black\",\n            width=2,\n        ),\n        row=1,\n        col=1,\n    )\n\ncolor = np.where(overlaps, 1.0, 0.0)\n\nn_samples_array = np.arange(1, n_samples + 1)\npi_estimate = 2 / (np.cumsum(overlap) / n_samples_array)\nt = n_samples\n\nfig.update_layout(\n    template=\"simple_white\",\n    xaxis=dict(range=[xmin, xmax], constrain=\"domain\", showgrid=False),\n    yaxis_scaleanchor=\"x\",\n    xaxis_visible=False,\n    yaxis_visible=False,\n)\n\nfor i in range(3, t):\n    fig.add_trace(\n        go.Scatter(\n            x=borders_red[: idx_red[3 * i] + 1, 0],\n            y=borders_red[: idx_red[3 * i] + 1, 1],\n            mode=\"lines\",\n            line=dict(width=2),\n            marker=dict(color=\"red\"),\n            name=\"Avec intersection\",\n            visible=False,\n        ),\n        row=1,\n        col=1,\n    )\n    fig.add_trace(\n        go.Scatter(\n            x=borders_blue[: idx_blue[3 * i] + 1, 0],\n            y=borders_blue[: idx_blue[3 * i] + 1, 1],\n            mode=\"lines\",\n            line=dict(width=2),\n            marker=dict(color=\"darkblue\"),\n            name=\"Sans intersection\",\n            visible=False,\n        ),\n        row=1,\n        col=1,\n    )\n\n    fig.add_trace(\n        go.Scatter(\n            x=n_samples_array[:i],\n            y=pi_estimate[:i],\n            mode=\"lines\",\n            line=dict(width=1),\n            marker=dict(color=\"red\"),\n            showlegend=False,\n            visible=False,\n        ),\n        row=2,\n        col=1,\n    )\n\nfig.add_annotation(\n    dict(\n        x=1.25,\n        y=0.14,\n        xref=\"paper\",\n        yref=\"paper\",\n        text=\"Estimation de pi\",\n        showarrow=False,\n        font=dict(color=\"red\"),\n    )\n)\n\nfig.add_annotation(\n    dict(x=-0.04, y=0.19, xref=\"paper\", yref=\"paper\", text=\"pi\", showarrow=False)\n)\n\nfig.update_xaxes(title_text=\"Nombre d'aiguilles tirées\", row=2, col=1)\n\nfig.update_layout(\n    template=\"none\",\n    xaxis2=dict(showgrid=True, zeroline=True, zerolinewidth=1, range=[0, n_samples]),\n    yaxis2=dict(showgrid=True, zeroline=True, zerolinewidth=1, range=[0, 6]),\n)\n# plot a dash line at y=pi\nfig.add_shape(\n    type=\"line\",\n    y0=np.pi,\n    x0=0,\n    y1=np.pi,\n    x1=n_samples,\n    line=dict(\n        color=\"black\",\n        width=1,\n        dash=\"dashdot\",\n    ),\n    row=2,\n    col=1,\n)\n\n\nfig.data[10 * 3].visible = True\nfig.data[10 * 3 + 1].visible = True\nfig.data[10 * 3 + 2].visible = True\n\n\nsteps = []\nfor i in range(len(fig.data) // 3):\n    step = dict(\n        label=str(i + 4),\n        method=\"update\",\n        args=[\n            {\"visible\": [False] * len(fig.data)},\n            {\n                \"title\": \"Estimation avec \"\n                + str(i + 4)\n                + f\" aiguilles: pi = {pi_estimate[i]:.4f}\"\n            },\n        ],\n    )\n    step[\"args\"][0][\"visible\"][3 * i] = True\n    step[\"args\"][0][\"visible\"][3 * i + 1] = True\n    step[\"args\"][0][\"visible\"][3 * i + 2] = True\n\n    steps.append(step)\n\nslider = dict(\n    active=0,\n    currentvalue={\"prefix\": \"Nombre d'aiguilles: \"},\n    pad={\"t\": 50},\n    y=-0.32,\n    steps=steps,\n)\n\nfig.update_layout(legend=dict(x=0.5, y=1, xanchor='center', yanchor='bottom'))\nfig.update_layout(sliders=[slider])\nfig.show()\n\n\n\n\n                                                \n\n\n\nOn va fournir ici le calcul de la probabilité P. Pour cela on aura besoin de quelques éléments décrits dans le dessin ci-dessous.\n\nx : distance entre le centre de l’aiguille et la ligne de la trame du parquet la plus proche\n\\theta : angle entre l’aiguille et la ligne de la trame du parquet la plus proche\n1 : longueur de l’aiguille (et donc la demi longueur est \\frac{1}{2})\n\\frac{1}{2}\\sin(\\theta) : distance entre l’extrémité de l’aiguille et la ligne de la trame du parquet la plus proche\n\n\n\n\n\n\n\n\nSans croisement\n\n\n\n\n\nAvec croisement\n\n\n\n\n\nFigure 1: Configuration sans croisement (à gauche) et avec croisement (à droite) de l’aiguille avec une ligne de la trame du parquet.\n\n\n\nAvec les éléments ci-dessus, on voit qu’il y a chevauchement si et seulement si: \\frac{1}{2}\\sin(\\theta) \\geq x.\nMaintenant par des arguments de symétrie on voit qu’on peut se restreindre à \\theta \\in [0, \\frac{\\pi}{2}], et à x \\in [0, \\frac{1}{2}]. Les lois de générations des variables aléatoires X et \\Theta sont les suivantes:\n\nX \\sim \\mathcal{U}([0, \\frac{1}{2}]), de densité f_X(x) = 2 {1\\hspace{-3.8pt} 1}_{[0, \\frac{1}{2}]}(x)\n\\Theta \\sim \\mathcal{U}([0, \\frac{\\pi}{2}]) de densité f_\\Theta(\\theta) = \\frac{2}{\\pi} {1\\hspace{-3.8pt} 1}_{[0, \\frac{\\pi}{2}]}(\\theta)\n\nDe plus on suppose que X et \\Theta sont indépendantes.\nMaintenant pour calculer la probabilité P on procède comme suit: \n\\begin{align*}\nP\n& = \\mathbb{P}\\left(\\frac{1}{2}\\sin(\\Theta) \\geq X\\right) \\\\\n& = \\int_{\\mathbb{R}^2} {1\\hspace{-3.8pt} 1}_{\\{\\frac{1}{2}\\sin(\\theta) \\geq x\\}} f_{\\Theta}(\\theta) f_X(x) d\\theta dx  \\quad (\\text{par indépendance})\\\\\n& = \\int_{\\mathbb{R}} \\int_{\\mathbb{R}}\n{1\\hspace{-3.8pt} 1}_{\\{\\frac{1}{2}\\sin(\\theta) \\geq x\\}} \\frac{2}{\\pi} {1\\hspace{-3.8pt} 1}_{[0, \\frac{\\pi}{2}]}(\\theta) \\cdot 2 {1\\hspace{-3.8pt} 1}_{[0, \\frac{1}{2}]}(x) d\\theta dx \\\\\n& = \\int_{0}^{\\frac{\\pi}{2}} \\int_{0}^{\\frac{1}{2}\\sin(\\theta)} \\frac{4}{\\pi} dx  d\\theta \\\\\n& = \\frac{4}{\\pi} \\int_{0}^{\\frac{\\pi}{2}} {\\frac{1}{2}\\sin(\\theta)}  d\\theta\\\\  \n& = \\frac{2}{\\pi} \\Big[ -\\cos(\\theta)\\Big]_{0}^{\\frac{\\pi}{2}} \\\\\n& = \\frac{2}{\\pi} \\enspace.\n\\end{align*}\n\n\n\n\n\n\n\nExercice: rendre le jeu équilibré?\n\n\n\nEn reprenant le même type de raisonnement que ci-dessus, trouver la distance entre les lattes du parquet qui rend le jeu équilibrer entre les deux joueurs introduit par Buffon (l’un pariant sur le fait que l’aiguille croise une ligne de la trame du parquet, l’autre pariant sur le fait que l’aiguille ne croise pas une ligne de la trame du parquet).",
    "crumbs": [
      "Cours",
      "Perspectives historiques"
    ]
  },
  {
    "objectID": "Courses/perspective_historique.html#méthode-de-monte-carlo",
    "href": "Courses/perspective_historique.html#méthode-de-monte-carlo",
    "title": "Perspectives historiques",
    "section": "Méthode de Monte-Carlo",
    "text": "Méthode de Monte-Carlo\nLa méthode de Monte-Carlo, est une méthode de calcul numérique qui consiste à utiliser des nombres aléatoires pour résoudre des problèmes déterministes. Elle est utilisée dans de nombreux domaines, comme la physique, la chimie, la biologie, la finance, ou encore l’apprentissage automatique. Cette méthode basée sur la loi des grands nombres a été mis au point à Los Alamos, dans le cadre du projet Manhattan (dont l’objectif était le développement du nucléaire civil et militaire) par un groupe de scientifiques dont les plus connus sont: John von Neumann2, Nicholas Metropolis3 ou encore Stanisław Ulam4\n2 John von Neumann: (1903-1957) mathématicien et physicien américano-hongrois, un des pères de l’informatique. 3 Nicholas Metropolis: (1915-1999), physicien gréco-américain, est des initiateurs de la méthode de Monte Carlo et du recuit simulé 4 Stanisław Ulam: (1909-1984) Dans le cadre du projet Manhattan, il s’agissait de calculer des intégrales de manière numérique pour modéliser l’évolution de particules, en utilisant des nombres aléatoires.\nEckhardt (1987) donne un bref aperçu historique, et mentionne les premières description de la méthode du rejet et de la méthode de l’inversion dans des lettres entre Von Neumann et Ulam datant de 1947. Ulam aurait une l’idée d’utiliser de telles méthodes pour résoudre le jeu du solitaire lors d’un séjour à l’hôpital en 1946, et éviter ainsi de faire des calculs combinatoires fastidieux. Rapidement, la possibilité d’appliquer cette approche pour des calculs en physique mathématiques (diffusion des neutrons notamment) lui serait apparue prometteuse. Le développement de l’informatique naissante allait permettre une mise en oeuvre pratique de ces idées, et c’est ainsi que la méthode de Monte-Carlo est née. Le nom Monte-Carlo est lui venu du besoin de confidentialité du projet, et provient du nom de la ville de Monte-Carlo, connue pour ses jeux de hasard, où l’oncle de Stanisław Ulam aimait se rendre pour assouvir sa soif de jeu. Ce serait N. Metropolis qui aurait proposé ce nom (cf. Metropolis 1987):\n\nEckhardt, R. 1987. « Stan Ulam, John Von Neumann, and the Monte Carlo Method ». Los Alamos Science, nᵒ 15: 131‑37.\n\nMetropolis, Nicholas. 1987. « The beginning of the Monte Carlo method ». Los Alamos Science, nᵒ 15: 125‑30.\nIt was at that time that I suggested an obvious name for the statistical method—a suggestion not unrelated to the fact that Stan had an uncle who would borrow money from relatives because he “just had to go to Monte Carlo”.",
    "crumbs": [
      "Cours",
      "Perspectives historiques"
    ]
  },
  {
    "objectID": "Courses/perspective_historique.html#autres-méthodes-stochastiques-populaires",
    "href": "Courses/perspective_historique.html#autres-méthodes-stochastiques-populaires",
    "title": "Perspectives historiques",
    "section": "Autres méthodes stochastiques populaires",
    "text": "Autres méthodes stochastiques populaires\n\nMéthode d’Hasting-Metropolis\nL’algorithme de Hasting-Metropolis est une méthode MCMC (🇬🇧: Monte Carlo Markov Chains) dont le but est d’obtenir un échantillonnage aléatoire d’une distribution de probabilité quand l’échantillonnage direct en est difficile (en particulier en grande dimension)\nUn avantage est qu’il ne requiert la connaissance de loi de densité qu’à constante multiplicative près.\n\n\n🇬🇧: Recuit simulé\nLe recuit simulé est une méthode (empirique) d’optimisation, inspirée d’un processus, le recuit, utilisé en métallurgie. On alterne dans cette dernière des cycles de refroidissement lent et de réchauffage (recuit) qui ont pour effet de minimiser l’énergie du matériau. Cette méthode est transposée en optimisation pour trouver les extrema d’une fonction.",
    "crumbs": [
      "Cours",
      "Perspectives historiques"
    ]
  },
  {
    "objectID": "Courses/slides.html",
    "href": "Courses/slides.html",
    "title": "Slides",
    "section": "",
    "text": "Vous trouverez ci-dessous la listes des slides associés:\nCours introduction, plein écran\n\nCours: notations premiers pas, plein écran\n\nCours: théorème asymptotiques, plein écran\n\n\n\n\n Retour au sommet",
    "crumbs": [
      "Slides",
      "Slides"
    ]
  },
  {
    "objectID": "Slides/slides_intro.html#section",
    "href": "Slides/slides_intro.html#section",
    "title": "HAX603X: Modélisation stochastique",
    "section": "",
    "text": "PS: n’oubliez pas de mettre [HAX603X] dans le titre de vos mails!"
  },
  {
    "objectID": "Slides/slides_intro.html#enseignants",
    "href": "Slides/slides_intro.html#enseignants",
    "title": "HAX603X: Modélisation stochastique",
    "section": "Enseignants",
    "text": "Enseignants\n\n\nJoseph Salmon : CM et TP\n\nSituation actuelle : Professeur à l’Université de Montpellier\nPrécédemment : Paris Diderot-Paris 7, Duke Univ., Télécom ParisTech, Univ. Washington\nSpécialités : statistiques, optimisation, traitement des images, sciences participatives\nBureau : 415, Bat. 9\n\n\n\n\n\n\n\n\nBenjmain Charlier : CM, TD et TP\n\nSituation actuelle : Maître de conférences à l’Université de Montpellier\nPrécédemment : Université Paul Sabatier, ENS Paris-Saclay\nSpécialités : traitement des images, statistiques, différentiation automatique\nBureau : 423, Bat. 9"
  },
  {
    "objectID": "Slides/slides_intro.html#ressources-en-ligne",
    "href": "Slides/slides_intro.html#ressources-en-ligne",
    "title": "HAX603X: Modélisation stochastique",
    "section": "Ressources en ligne",
    "text": "Ressources en ligne\n\nInformations principales : site du cours http://josephsalmon.github.io/HAX603X\n\n\n\nSyllabus\nCours (détaillé: site web)\nSlides (résumé)\nFeuilles de TD\nFeuilles de TP\nRendu TP : Moodle de l’université (https://moodle.umontpellier.fr/course/view.php?id=5558)"
  },
  {
    "objectID": "Slides/slides_intro.html#validation",
    "href": "Slides/slides_intro.html#validation",
    "title": "HAX603X: Modélisation stochastique",
    "section": "Validation",
    "text": "Validation\n\nTP notés : Rendu = fichier Python .py unique\n\nTP noté 1 : rendre en fin de session (en S11)\nTP noté 2 : rendre en fin de session (en S17)\n\nCC : devoir sur table d’une heure (S18)\n\n\n\nCoefficients:\n\nNote Session 1 : (40% CC + 30% TP 1 + 30% TP 2)\nNote Session 2 : (30% CC + 35% TP 1 + 35% TP 2)\n\n\n\n\n\n\n\n\nImportant\n\n\nLe rendu est individuel pour le TP noté !!!"
  },
  {
    "objectID": "Slides/slides_intro.html#notation-pour-les-tps",
    "href": "Slides/slides_intro.html#notation-pour-les-tps",
    "title": "HAX603X: Modélisation stochastique",
    "section": "Notation pour les TPs",
    "text": "Notation pour les TPs\nRendu : sur Moodle, en déposant un fichier nom_prenom.py dans le dossier adéquat.\nDétails de la notation des TPs :\n\nQualité des réponses aux questions\nQualité de rédaction et d’orthographe\nQualité des graphiques (légendes, couleurs)\nQualité du code (noms de variables, clairs, commentaires utiles, code synthétique, etc.)\nCode reproductible et absence de bug\n\n\n\n\n\n\n\n\nPénalités\n\n\n\nEnvoi par mail : zéro\nRetard : zéro (uploader avant la fin, fermeture automatique de moodle)"
  },
  {
    "objectID": "Slides/slides_intro.html#prérequis---à-revoir-seul",
    "href": "Slides/slides_intro.html#prérequis---à-revoir-seul",
    "title": "HAX603X: Modélisation stochastique",
    "section": "Prérequis - à revoir seul",
    "text": "Prérequis - à revoir seul\n\n \n\nBases de probabilités (en particulier “HAX506X- Théorie des Probabilités”): probabilité, densité, espérance, fonction de répartition, mesure, intégration, analyse numérique élémentaire, etc. (Foata et Fuchs 1996; Barbe et Ledoux 2006; Ouvrard 2007, 2008)\n\n\n\nProgrammation élémentaire (en Python): if … then… else …, for, while, fonctions, etc. HLMA310 - Logiciels scientifiques, (Courant et al. 2013), Cours de Python: Univ. Paris Diderot\n\n\n\nPour aller plus loin: conditionnement, martingales (Williams 1991)"
  },
  {
    "objectID": "Slides/slides_intro.html#description-du-cours",
    "href": "Slides/slides_intro.html#description-du-cours",
    "title": "HAX603X: Modélisation stochastique",
    "section": "Description du cours",
    "text": "Description du cours\n\n\nGénérer l’aléa\n\ngénérateurs pseudo-aléatoires, simulations de variables aléatoires (inverse, rejet, etc.)\nillustrations numériques et visualisation en Python (loi des grands nombres, TCL)\n\nMéthode de Monte-Carlos\n\nméthode de Monte-Carlo pour le calcul approché d’une intégrale\nréduction de la variance : variables antithétiques, variables de contrôle, etc.\n\nCompléments\n\nvecteurs gaussiens et lien avec les lois usuelles de la statistique inférentielle (student, chi2)\nconstruction d’intervalles de confiance.\nmarche aléatoire simple, etc."
  },
  {
    "objectID": "Slides/slides_intro.html#buffon-et-les-prémisses-de-la-méthode-de-monte-carlo",
    "href": "Slides/slides_intro.html#buffon-et-les-prémisses-de-la-méthode-de-monte-carlo",
    "title": "HAX603X: Modélisation stochastique",
    "section": "Buffon et les prémisses de la méthode de Monte-Carlo",
    "text": "Buffon et les prémisses de la méthode de Monte-Carlo\n\n\n\n\n1733: l’aiguille de Buffon, méthode d’estimation de la valeur de \\pi.\n\n\n\n\nProblème initial: une aiguille de taille 1 tombe sur un parquet composé de lattes de largeur 1: quelle est alors la probabilité P que l’aiguille croise une ligne de la trame du parquet ?\n\n\n\n\n\n\n                                                \n\n\n\n\n\n\nGeorges-Louis Leclerc, Comte de Buffon(1707-1788) : naturaliste, mathématicien et industriel français du siècle des Lumières"
  },
  {
    "objectID": "Slides/slides_intro.html#laiguille-de-buffon-suite",
    "href": "Slides/slides_intro.html#laiguille-de-buffon-suite",
    "title": "HAX603X: Modélisation stochastique",
    "section": "L’aiguille de Buffon (suite)",
    "text": "L’aiguille de Buffon (suite)\n\nProblème initial: une aiguille de taille 1 tombe sur un parquet composé de lattes de largeur 1: quelle est alors la probabilité P que l’aiguille croise une ligne de la trame du parquet ?\n\n\n\nRéponse: \nP = \\frac{2}{\\pi} \\approx 0.6366 \\enspace.\n Une preuve de ce résultat est donnée ici."
  },
  {
    "objectID": "Slides/slides_intro.html#principe-de-monte-carlo-et-estimation",
    "href": "Slides/slides_intro.html#principe-de-monte-carlo-et-estimation",
    "title": "HAX603X: Modélisation stochastique",
    "section": "Principe de Monte Carlo et estimation",
    "text": "Principe de Monte Carlo et estimation\nIdée sous-jacente de Buffon :\nsi l’on répète cette expérience un grand nombre de fois, on peut approché la quantité P numériquement, par exemple en proposant un estimateur \\hat{P}_n qui compte la proportion de chevauchement après avoir fait n répétition des lancers.\n Estimation de \\pi:\n\n\\pi \\approx \\frac{2}{\\hat{P}_n}"
  },
  {
    "objectID": "Slides/slides_intro.html#principe-de-monte-carlo-pour-lestimation-suite",
    "href": "Slides/slides_intro.html#principe-de-monte-carlo-pour-lestimation-suite",
    "title": "HAX603X: Modélisation stochastique",
    "section": "Principe de Monte Carlo pour l’estimation (suite)",
    "text": "Principe de Monte Carlo pour l’estimation (suite)"
  },
  {
    "objectID": "Slides/slides_intro.html#méthode-de-monte-carlo",
    "href": "Slides/slides_intro.html#méthode-de-monte-carlo",
    "title": "HAX603X: Modélisation stochastique",
    "section": "Méthode de Monte-Carlo",
    "text": "Méthode de Monte-Carlo\nMéthode de calcul numérique qui consiste à utiliser des nombres aléatoires pour résoudre des problèmes déterministes.\n\nDomaines d’applications:\n\nla physique\nla chimie\nla biologie\nla finance\nl’apprentissage automatique"
  },
  {
    "objectID": "Slides/slides_intro.html#contexte-de-la-naissance-de-la-méthode-de-monte-carlo",
    "href": "Slides/slides_intro.html#contexte-de-la-naissance-de-la-méthode-de-monte-carlo",
    "title": "HAX603X: Modélisation stochastique",
    "section": "Contexte de la naissance de la méthode de Monte Carlo",
    "text": "Contexte de la naissance de la méthode de Monte Carlo\n\n\n\n\nLieu: Los Alamos\nÉpoque: seconde guerre mondial\nContexte: Projet Manathan, produire une bombe atomique\nBesoins: modéliser les réactions nucléaires en chaîne (combinatoires)\n\n\n\n\n\n\n\n\nJohn von Neumann (1903-1957), mathématicien et physicien américano-hongrois, un des pères de l’informatique.\n\n\n\n\n\n\n\nNicholas Metropolis (1915-1999), physicien gréco-américain, un des initiateurs de la méthode de Monte Carlo et du recuit simulé\n\n\n\n\n\n\n\nStanisław Ulam (1909-1984), mathématicien polono-américainm, un des initiateurs de la méthode de Monte Carlo et de la propulsion nucléaire pulsée\n\n\n\n\n\n\n\n\n\n\nExplosion de Trinity (16 Juillet 1945)"
  },
  {
    "objectID": "Slides/slides_intro.html#lorigine-du-nom-monte-carlo",
    "href": "Slides/slides_intro.html#lorigine-du-nom-monte-carlo",
    "title": "HAX603X: Modélisation stochastique",
    "section": "L’origine du nom “Monte-Carlo”",
    "text": "L’origine du nom “Monte-Carlo”\nInitialement: besoin de confidentialité du projet Manhattan\n\nMonte-Carlo: connue pour ses jeux de hasard, où l’oncle de Stanisław Ulam aimait se rendre pour assouvir sa soif de jeu.\n Ce serait N. Metropolis qui aurait proposé ce nom, cf. (Metropolis 1987):\nIt was at that time that I suggested an obvious name for the statistical method—a suggestion not unrelated to the fact that Stan had an uncle who would borrow money from relatives because he “just had to go to Monte Carlo”."
  },
  {
    "objectID": "Slides/slides_intro.html#essor-de-la-méthode-de-monte-carlo",
    "href": "Slides/slides_intro.html#essor-de-la-méthode-de-monte-carlo",
    "title": "HAX603X: Modélisation stochastique",
    "section": "Essor de la méthode de Monte Carlo",
    "text": "Essor de la méthode de Monte Carlo\n\n\n\n\nPopularisation croissante:\n\nEssor de l’informatique (depuis les années 80)\nEssor des méthodes de calcul parallèle (GPUs, clusters, etc.)\n\n\n\n\n\nDomaine principaux impactés:\n\nfinance : évaluation des prix de produits dérivés\napprentissage automatique: utilisation de l’aléatoire pour généré des scénarios\nExemples: Alphago (2016), AlphaGeometry (2024)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRecherche arborescente Monte-Carlo (🇬🇧: Monte Carlo tree search): analyse des scénarios les plus prometteurs, en élargissant l’arbre de recherche sur la base d’un échantillonnage aléatoire de l’espace entier (ingrédient important d’AlphaGo)"
  },
  {
    "objectID": "Slides/slides_intro.html#bibliographie",
    "href": "Slides/slides_intro.html#bibliographie",
    "title": "HAX603X: Modélisation stochastique",
    "section": "Bibliographie",
    "text": "Bibliographie\n\n\nBarbe, Philippe, et Michel Ledoux. 2006. « Cours de Probabilite Pour la Licence (corrigée) ».\n\n\nCourant, J., M. de Falco, S. Gonnord, J.-C. Filliâtre, S. Conchon, G. Dowek, et B. Wack. 2013. Informatique pour tous en classes préparatoires aux grandes écoles: Manuel d’algorithmique et programmation structurée avec Python. Eyrolles.\n\n\nFoata, D., et A. Fuchs. 1996. Calcul des probabilités: cours et exercices corrigés. Masson.\n\n\nMetropolis, Nicholas. 1987. « The beginning of the Monte Carlo method ». Los Alamos Science, nᵒ 15: 125‑30.\n\n\nOuvrard, J.-Y. 2007. Probabilités : Tome 2, Licence - CAPES. 2ᵉ éd. Enseignement des mathématiques. Cassini.\n\n\n———. 2008. Probabilités : Tome 1, Licence - CAPES. 2ᵉ éd. Enseignement des mathématiques. Cassini.\n\n\nWilliams, D. 1991. Probability with martingales. Cambridge Mathematical Textbooks. Cambridge: Cambridge University Press.\n\n\n\n\nHAX603X: Modélisation stochastique"
  },
  {
    "objectID": "Slides/slides_th_asymptotique.html#loi-forte-des-grands-nombres",
    "href": "Slides/slides_th_asymptotique.html#loi-forte-des-grands-nombres",
    "title": "HAX603X: Modélisation stochastique",
    "section": "Loi forte des grands nombres",
    "text": "Loi forte des grands nombres\n\nRésultat fondamental: concerne le comportement asymptotique de la moyenne empirique: \\[\n\\bar X_n = \\dfrac{X_1 + \\cdots + X_n}{n} \\enspace.\n\\] quand on observe \\(n\\) variables aléatoires i.i.d \\(X_1,\\dots,X_n\\), ayant une espérance finie.\n\n\nThéorème 1 (Loi forte des grands nombres) \nSoit \\((X_n)_{n \\geq 1}\\) une suite de variables aléatoires indépendantes et identiquement distribuées (i.i.d.) dans \\(L^1(\\Omega, \\mathcal{F}, \\mathbb{P})\\). Notons \\(\\mu = \\mathbb{E}[X_1]\\). Alors \\(\\bar X_n\\) converge vers \\(\\mu\\) presque sûrement : \\[\n\\mathbb{P}\\bigg( \\dfrac{X_1 + \\cdots + X_n}{n} \\underset{n \\to \\infty}{\\longrightarrow} \\mu \\bigg) = 1\\,.\n\\]"
  },
  {
    "objectID": "Slides/slides_th_asymptotique.html#interprétation",
    "href": "Slides/slides_th_asymptotique.html#interprétation",
    "title": "HAX603X: Modélisation stochastique",
    "section": "Interprétation",
    "text": "Interprétation\nIntuitivement, la probabilité d’un événement \\(A\\) correspond à la fréquence d’apparition de \\(A\\) quand on répète une expérience qui fait intervenir cet événement.\n\nExemple 1 (Cas Bernouilli: pile ou face) La probabilité d’apparition du côté pile (noté \\(p\\)) peut-être estimée en lançant la pièce un grand nombre de fois et en comptant le nombre de pile obtenu.\nLa loi des grands nombres justifie cette intuition : si \\(X_1, \\ldots, X_n\\) sont i.i.d. de loi de Bernoulli de paramètre \\(p\\), alors \\[\n    \\dfrac{X_1 + \\cdots + X_n}{n} \\xrightarrow[n \\to \\infty]{p.s.} p =\\mathbb{E}(X_1) \\enspace.\n\\]\n\nMembre de gauche : la fréquence empirique de piles\nMembre de droite : la fréquence théorique de piles"
  },
  {
    "objectID": "Slides/slides_th_asymptotique.html#visualisation-de-lexemple-du-pile-ou-face",
    "href": "Slides/slides_th_asymptotique.html#visualisation-de-lexemple-du-pile-ou-face",
    "title": "HAX603X: Modélisation stochastique",
    "section": "Visualisation de l’exemple du pile ou face",
    "text": "Visualisation de l’exemple du pile ou face\n#| echo: false\n#| standalone: true\n#| viewerHeight: 550\nimport numpy as np\nfrom shiny import ui, render, App, reactive\nfrom shinywidgets import output_widget, register_widget\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objs as go\n\n\nn_init = 42\n\n\napp_ui = ui.page_fluid(\n    ui.tags.head(\n        ui.tags.style(\"\"\"\n            .bslib-sidebar-layout &gt; .sidebar &gt; .sidebar-content {\n            display: flex;\n            flex-direction: column;\n            padding: 0.5rem;\n            }\n            .irs--shiny .irs-line {\n            top: 27px;\n            height: 4px;\n            }\n            .irs.irs--shiny .irs-bar {\n            top: 27px;\n            height: 4px;\n            }\n            .irs--shiny .irs-handle {\n            top: 23px;\n            width: 22px;\n            height: 10px;\n        \"\"\")\n    ),\n    ui.panel_title(\"Loi des grands nombres\"),\n    ui.layout_sidebar(\n        ui.panel_sidebar(\n            ui.input_action_button(\"seed\", \"Nouveau tirage\", class_=\"btn-primary\"),\n            ui.input_slider( \"p\", \"Espérance: p\", 0.01, 0.99, value=0.35, step=0.01, ticks=False,\n            ),\n            ui.input_slider( \"n_samples\", \"Échantillons: n\", 2, 1000, value=15, step=1, ticks=False),\n        width=3),\n    ui.panel_main(output_widget(\"my_widget\"), width = 9)\n    )\n)\n\n\n\n\ndef server(input, output, session):\n    seed = reactive.Value(42)\n\n    @reactive.Effect\n    @reactive.event(input.seed)\n    def _():\n        seed.set(np.random.randint(0, 1000))\n\n    subplots = make_subplots(\n        rows=2,\n        cols=1,\n        vertical_spacing=0.45,\n        horizontal_spacing=0.04,\n        row_heights=[5, 1],\n        subplot_titles=(\n            f\"Moyenne empirique: loi de Bernoulli\",\n            \"Tirages aléatoires &lt;span style='color:rgb(66, 139, 202)'&gt;bleu: 0&lt;/span&gt;, &lt;span style='color:rgb(255, 0, 0)'&gt;rouge: 1&lt;/span&gt; (seed=\"\n            + f\"{n_init:03}\"\n            + \")\",\n        ),\n    )\n    fig = go.FigureWidget(subplots)\n    fig.add_trace(\n        go.Scatter(\n            mode=\"lines\",\n            line=dict(color=\"black\", width=3),\n            x=[],\n            y=[],\n            name=r\"Moyenne &lt;br&gt; empirique\",\n        ),\n        row=1,\n        col=1,\n    )\n    fig.add_trace(\n        go.Scatter(\n            mode=\"lines\",\n            line=dict(dash=\"dash\", color=\"black\", width=1),\n            marker={},\n            x=[],\n            y=[],\n            name=r\"p\",\n        ),\n        row=1,\n        col=1,\n    )\n    fig.add_trace(\n        go.Heatmap(\n            x=[],\n            z=[],\n            colorscale=[[0, \"rgb(66, 139, 202)\"], [1, \"rgb(255,0,0)\"]],\n            showscale=False,\n        ),\n        row=2,\n        col=1,\n    )\n\n    fig.update_yaxes(range=[0, 1.1], row=1, col=1)\n    fig.update_xaxes(matches=\"x1\", row=2, col=1)\n    fig.update_yaxes(visible=False, row=2, col=1)\n    fig.update_xaxes(visible=False, row=2, col=1)\n\n    fig.update_layout(\n        template=\"simple_white\",\n        showlegend=True,\n        xaxis_title=\"Échantillons: n\",\n    )\n    fig.update_layout(autosize=True)\n\n    fig.update_layout(\n        legend=dict(\n            yanchor=\"top\",\n            y=0.99,\n            xanchor=\"left\",\n            x=0.65,\n            bgcolor=\"rgba(0,0,0,0)\",\n        )\n    )\n    fig.update_layout(\n        margin=dict(l=0, r=0, b=10, t=70),\n    )\n\n    register_widget(\"my_widget\", fig)\n\n    @reactive.Effect\n    def _():\n        p = input.p()\n        n_samples = input.n_samples()\n\n        rng = np.random.default_rng(seed())\n        iterations = np.arange(1, n_samples + 1)\n        samples = rng.binomial(1, p, size=n_samples)\n        means_samples = np.cumsum(samples) / np.arange(1, n_samples + 1)\n\n        # Update data in fig:\n        fig.data[0].x = iterations\n        fig.data[0].y = means_samples\n\n        fig.data[1].x = iterations\n        fig.data[1].y = np.full((n_samples), p)\n\n        fig.data[2].x = iterations\n        fig.data[2].z = [samples]\n\n        fig.update_xaxes(range=[1, n_samples + 1])\n\n        # Update the subplot titles:\n        fig.layout.annotations[1].update(\n            text=f\"Tirages aléatoires (seed=\"\n            + f\"{seed():03}\"\n            + \") &lt;br&gt; &lt;span style='color:rgb(66, 139, 202)'&gt;bleu: 0&lt;/span&gt;, &lt;span style='color:rgb(255, 0, 0)'&gt;rouge: 1&lt;/span&gt; \"\n        )\n\n\napp = App(app_ui, server)\n\nPour aller plus loin:\nQuant \\(p\\) varie, à \\(n\\) fixé…les signaux générés sont très très proches, ce qui ne devrait pas être le cas sans structuration particulière de la génération. L’aléa est imparfait!"
  },
  {
    "objectID": "Slides/slides_th_asymptotique.html#au-delà-de-la-loi-des-grands-nombres",
    "href": "Slides/slides_th_asymptotique.html#au-delà-de-la-loi-des-grands-nombres",
    "title": "HAX603X: Modélisation stochastique",
    "section": "Au delà de la loi des grands nombres",
    "text": "Au delà de la loi des grands nombres\n\n1er ordre d’approximation de la convergence de \\(\\bar{X}_n\\): loi des grands nombres\n2ème ordre d’approximation: théorème central limite\n\nEnjeu: quantifier les variations de \\(\\bar X_n - \\mu\\)\n[Réponse].underline: théorème central limite (TCL), avec la convergence en loi d’une transformation affine de la moyenne empirique"
  },
  {
    "objectID": "Slides/slides_th_asymptotique.html#théorème-central-limite-1",
    "href": "Slides/slides_th_asymptotique.html#théorème-central-limite-1",
    "title": "HAX603X: Modélisation stochastique",
    "section": "Théorème central limite",
    "text": "Théorème central limite\n\nThéorème 2 (Théorème central limite) Soit \\(X_1, \\ldots, X_n\\) une suite de variables aléatoires i.i.d de variance \\(\\sigma^2 = {\\rm var}(X_1) \\in ]0, \\infty[\\). On note \\(\\mu = \\mathbb{E}[X_1]\\) leur espérance. Alors \\[\n\\sqrt n \\left(\\tfrac{\\bar X_n - \\mu}{\\sigma} \\right) \\xrightarrow[n \\to +\\infty]{\\mathcal{L}} N\\enspace,\n\\] où \\(N\\) suit une loi normale centrée réduite : \\(N \\sim\\mathcal{N}(0,1)\\).\n\nInterprétation: la moyenne empirique de v.a. i.i.d de variance \\(\\sigma^2\\) se comporte asymptotiquement comme une loi normale \\(\\mathcal{N}(\\mu, \\tfrac{\\sigma^2}{n})\\): \\(\\quad \\bar X_n \\approx \\mathcal{N}(\\mu, \\frac{\\sigma^2}{n})\\).\n\n\n\n\n\n\nNote\n\n\nHypothèses du théorème plutôt faibles: variance finie uniquement"
  },
  {
    "objectID": "Slides/slides_th_asymptotique.html#formulation-de-la-convergence",
    "href": "Slides/slides_th_asymptotique.html#formulation-de-la-convergence",
    "title": "HAX603X: Modélisation stochastique",
    "section": "Formulation de la convergence",
    "text": "Formulation de la convergence\nConvergence en loi \\(\\iff\\) convergence des fonctions de répartition (aux pts de continuité)\nNotations:\n\n\\(\\varphi\\) : la densité d’une loi normale centrée réduite \\(\\varphi(x) = \\tfrac{e^{-\\frac{x^2}{2}}}{\\sqrt{2\\pi}}\\)\n\\(\\Phi\\) : la fonction de répartition d’une loi normale centrée réduite \\(\\Phi(x) = \\displaystyle \\int_{-\\infty}^{x}\\varphi(u) du\\)\n\n\nRé-écriture du TCL: pour tout \\(a &lt; b\\) on a alors\n\\[\n\\begin{align}\n    \\mathbb{P} \\left(\\bar X_n \\in [ \\mu + \\tfrac{a \\sigma}{\\sqrt{n}}, \\mu + \\tfrac{ b \\sigma}{\\sqrt{n}}] \\right)\n    & =\n    \\mathbb{P} \\left(\\tfrac{\\bar X_n - \\mu}{\\sigma} \\in [ \\tfrac{a}{\\sqrt{n}},\\tfrac{b}{\\sqrt{n}}]\\right) \\nonumber\\\\\n    & \\class{fragment}{{} = \\mathbb{P} \\bigg( a \\leq \\sqrt n \\left(\\tfrac{\\bar X_n - \\mu}{\\sigma} \\right) \\leq b\\bigg) \\nonumber}\n    \\\\\n    & \\class{fragment}{{} \\underset{n \\to \\infty}{\\longrightarrow}  \\int_a^b \\varphi(x) \\,  dx = \\Phi(b) - \\Phi(a) \\nonumber}\\\\\n\\end{align}\n\\]"
  },
  {
    "objectID": "Slides/slides_th_asymptotique.html#lien-intervalle-de-confiance-et-tcl",
    "href": "Slides/slides_th_asymptotique.html#lien-intervalle-de-confiance-et-tcl",
    "title": "HAX603X: Modélisation stochastique",
    "section": "Lien intervalle de confiance et TCL",
    "text": "Lien intervalle de confiance et TCL\n\n\n\nQuestion: comment choisir \\(a\\) et \\(b\\) pour obtenir un intervalle de confiance à 95% pour \\(\\mu\\).\nNotation: \\(\\alpha_n=\\mathbb{P} \\left(\\bar X_n \\notin [ \\mu + \\tfrac{a \\sigma}{\\sqrt{n}}, \\mu + \\tfrac{ b \\sigma}{\\sqrt{n}}] \\right)\\), on cherche donc \\(a\\) et \\(b\\) tels que \\(\\alpha_n \\approx 0.05\\).\nSimplification: choix d’un intervalle symétrique autour de \\(\\mu \\implies q=a=-b\\) \\[\n\\begin{align}\n& 1-\\alpha_n \\approx \\int_{-q}^q \\varphi(x) \\,  dx=\\Phi(q)-\\Phi(-q)=2 \\Phi(q)-1 \\\\\n\\implies & \\boxed{q\\approx\\Phi^{-1}(1-\\tfrac{\\alpha_n}{2})}\n\\end{align}\n\\]\nInterpretation: \\(q\\) est (approx.) le quantile de niveau \\(1-\\tfrac{\\alpha_n}{2}\\) de la loi normale centrée réduite\nNumériquement: on peut facilement évaluer \\(q\\) et vérifier que \\(q\\approx 1.96\\) avec scipy\n\n\n\n\n\nfrom scipy.stats import norm\nq = norm.ppf((1-0.05/2))     # Calcul du quantile de niveau 1-0.05/2\nprint(f\"{q:.2f}\")            # Affichage à 2 décimales\n\n1.96"
  },
  {
    "objectID": "Slides/slides_th_asymptotique.html#visualization-du-tcl",
    "href": "Slides/slides_th_asymptotique.html#visualization-du-tcl",
    "title": "HAX603X: Modélisation stochastique",
    "section": "Visualization du TCL",
    "text": "Visualization du TCL\n#| standalone: true\n#| viewerHeight: 600\nimport numpy as np\nfrom scipy.stats import norm\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nfrom shiny import ui, render, App, reactive\nfrom shinywidgets import output_widget, render_widget\n\n\napp_ui = ui.page_fluid(\n    ui.tags.head(\n        ui.tags.style(\"\"\"\n            .bslib-sidebar-layout &gt; .sidebar &gt; .sidebar-content {\n            display: flex;\n            flex-direction: column;\n            padding: 0.1rem;\n            }\n            .irs--shiny .irs-line {\n            top: 27px;\n            height: 4px;\n            }\n            .irs.irs.irs--shiny .irs-bar {\n            top: 27px;\n            height: 4px;\n            }\n            .irs--shiny .irs-handle {\n            top: 23px;\n            width: 22px;\n            height: 10px;\n        \"\"\")\n    ),\n    ui.panel_title(\"TCL\"),\n    ui.layout_sidebar(\n        ui.panel_sidebar(\n            ui.input_action_button(\"seed\", \"Nouveau tirage\",class_=\"btn-primary\"),\n            ui.input_slider(\"p\", \"Espérance: p\", 0.01, 0.99, value=0.5, step=0.01, ticks=False),\n            ui.input_slider(\"n_samples\", \"Échantillons: n\", 1, 200, value=100, step=1, ticks=False),\n            ui.input_slider(\"n_repetitions\", \"Répétitions: t\", 1, 300, value=200, step=1, ticks=False),\n        width=3\n        ),\n    ui.panel_main(output_widget(\"my_widget\"), width = 9)\n    )\n)\n\n\n\ndef server(input, output, session):\n    seed = reactive.Value(42)\n\n\n    @reactive.Effect\n    @reactive.event(input.seed)\n    def _():\n        seed.set(np.random.randint(0, 1000))\n\n\n    @output\n    @render_widget\n    def my_widget():\n\n        rng = np.random.default_rng(seed())\n        p = input.p()\n        n_repetitions = input.n_repetitions()\n        n_samples = input.n_samples()\n        iterations = np.arange(1, n_samples + 1)\n        samples = rng.binomial(1, p, size=(n_repetitions, n_samples))\n        means_samples = np.cumsum(samples, axis=1) / np.arange(1, n_samples + 1)\n        x_hist = np.linspace(0, 1, num=300)\n\n        # Create figure\n        fig = make_subplots(\n                    rows=1,\n                    cols=3,\n                    # vertical_spacing=0.5,\n                    horizontal_spacing=0.02,\n                    column_widths=[20, 2, 3],\n                    subplot_titles=(\"t = \" + str(n_repetitions) + \" répétitions\",\"\",\"\")\n                )\n\n\n        for i in range(n_repetitions):\n            fig.add_trace(\n                    go.Scatter(\n                        mode='lines',\n                        line=dict(color=\"rgba(0,0,0,0.05)\", width=1),\n                        x=iterations,\n                        y=means_samples[i,:],\n                        ),\n                        row=1, col=1,\n            )\n        fig.add_trace(\n                go.Scatter(\n                    mode='lines',\n                    line=dict(dash=\"dash\", color=\"blue\", width=1),\n                    marker={},\n                    x=iterations,\n                    y=np.full((n_samples), p),\n                    name='p'),\n                    row=1, col=1,\n        )\n        fig.update_layout(\n            template=\"simple_white\",\n            showlegend=False,\n        )\n        fig.add_trace(\n                go.Scatter(\n                    mode='markers',\n                    marker=dict(color=\"rgba(0,0,0,0.05)\", size=4),\n                    x=np.zeros(n_samples),\n                    y=means_samples[:,-1],\n                ),\n                row=1, col=2,\n\n        )\n        y_hist, bins = np.histogram(means_samples[:,-1], bins=int(np.sqrt(n_repetitions)), density=True)\n        fig.add_trace(\n            go.Bar(x=y_hist, y=bins[:-1] + np.diff(bins)/2,\n                    opacity=0.75,\n                    marker_color = 'black',\n                    orientation='h',\n                    width=np.diff(bins),\n                    name=\"Tirages de moyennes empiriques\",\n                    ),\n                row=1, col=3,\n        )\n        fig.add_trace(\n            go.Scatter(x=norm.pdf(x_hist, p, np.sqrt(p*(1-p) / n_samples)),\n                       y=x_hist,\n                       mode='lines',\n                       line=dict(color=\"red\"),\n                       legendgroup='1',\n                       name=\"TCL\"\n                       ),\n                row=1, col=3,\n        )\n\n        fig.update_xaxes(range=[1, n_samples + 1])\n        fig.update_yaxes(range=[-.05, 1.05], row=1, col=1)\n        fig.update_yaxes(matches=\"y1\",visible = False,  row=1, col=2)\n        fig.update_xaxes(range=[-0.2, 0.2], visible = False, row=1, col=2)\n\n        fig.update_yaxes(matches=\"y1\", row=1, col=3, visible=False)\n        fig.update_xaxes(range=[0, 1.1 / np.sqrt(2*np.pi* p*(1-p) / n_samples)], row=1, col=3)\n        fig.update_xaxes(visible=False, row=1, col=3)\n\n\n\n        for trace in fig['data']:\n            print(trace)\n            if (trace['name'] != 'TCL') and (trace['name'] != 'p'):\n                trace['showlegend'] = False\n        fig.update_layout(\n        margin=dict(l=0, r=0, b=10, t=100),\n        )\n        fig.update_layout(\n            title=dict(text=\"Distribution de la moyenne empirique&lt;br&gt; (cas loi de Bernoulli)\", yanchor=\"top\", y=0.95),\n            title_x=0.5,\n            showlegend=True,\n\n        )\n        fig.update_layout(\n            legend=dict(\n                yanchor=\"top\",\n                y=0.99,\n                xanchor=\"left\",\n                x=0.89,\n                bgcolor=\"rgba(0,0,0,0)\",\n            )\n        )\n        fig['layout']['xaxis']['title']='Échantillons: n'\n\n        fig.update_layout(autosize=True)\n\n        return fig\n\n\napp = App(app_ui, server)"
  },
  {
    "objectID": "Slides/slides_th_asymptotique.html#pour-aller-plus-loin-vision-des-convolutions",
    "href": "Slides/slides_th_asymptotique.html#pour-aller-plus-loin-vision-des-convolutions",
    "title": "HAX603X: Modélisation stochastique",
    "section": "Pour aller plus loin: vision des convolutions",
    "text": "Pour aller plus loin: vision des convolutions\n\n\nNotation: Soient \\(f\\) et \\(g\\) définies sur \\(\\mathbb{R}\\) (intégrables au sens de Lebesgue).\n\nDéfinition 1 (Convolution) La convolution de \\(f\\) par \\(g\\) est la fonction \\(f*g\\) suivante: \\[\n\\begin{align}\nf*g:\n\\mathbb{R} &\\mapsto \\mathbb{R} \\nonumber\\\\\nx &\\to \\int_{-\\infty}^{+\\infty} f(x-y)g(y) dy \\enspace.\\nonumber\n\\end{align}\n\\]\n\n\n\n\n\n\n\n\nNote\n\n\nOn peut aussi obtenir \\(f*g(x)\\) en calculant \\(\\int_{\\mathbb{R}^2} f(u)g(v) {1\\hspace{-3.8pt} 1}_{u+v=x} du dv\\)."
  },
  {
    "objectID": "Slides/slides_th_asymptotique.html#somme-et-convolutions",
    "href": "Slides/slides_th_asymptotique.html#somme-et-convolutions",
    "title": "HAX603X: Modélisation stochastique",
    "section": "Somme et convolutions",
    "text": "Somme et convolutions\n\nThéorème 3 (Loi de la somme et convolutions) Soient \\(X\\) et \\(Y\\) des v.a. de densités \\(f\\) et \\(g\\) respectivement, la densité de \\(X+Y\\) est donnée par la convolution \\(f*g\\)."
  },
  {
    "objectID": "Slides/slides_th_asymptotique.html#convolution-et-tcl",
    "href": "Slides/slides_th_asymptotique.html#convolution-et-tcl",
    "title": "HAX603X: Modélisation stochastique",
    "section": "Convolution et TCL",
    "text": "Convolution et TCL\nPour \\(X_1, \\dots, X_n\\), i.i.d., de densité \\(f\\), on affiche la densité de \\(\\bar{X}_n\\).\n#| standalone: true\n#| viewerHeight: 550\nimport numpy as np\nfrom shiny import ui, render, App, reactive\nfrom shinywidgets import output_widget, register_widget\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objs as go\nfrom scipy import signal\n\n\napp_ui = ui.page_fluid(\n    ui.tags.head(\n        ui.tags.style(\"\"\"\n            .bslib-sidebar-layout &gt; .sidebar &gt; .sidebar-content {\n            display: flex;\n            flex-direction: column;\n            padding: 0.5rem;\n            }\n            .irs--shiny .irs-line {\n            top: 27px;\n            height: 2px;\n            }\n            .irs.irs--shiny .irs-bar {\n            top: 27px;\n            height: 2px;\n            }\n            .irs--shiny .irs-handle {\n            top: 23px;\n            width: 22px;\n            height: 10px;\n        \"\"\")\n    ),\n    ui.panel_title(\"TCL et convolutions\"),\n    ui.layout_sidebar(\n        ui.panel_sidebar(\n            ui.input_select(\n                \"loi\",\n                \"Loi sous-jacente\",\n                {'uniforme': 'Uniforme', 'laplace' : 'Laplace'},\n            ),\n            ui.input_slider(\n                \"n_iter\",\n                \"Échantillons: n\",\n                1,\n                10,\n                value=1,\n                step=1,\n                ticks=False,\n            ), width = 3\n    ),\n    ui.panel_main(\n        output_widget(\"my_widget\"), width = 9\n    )\n)\n)\n\n\nnnzeros = 10001\nx_min = -20\nx_max = 20\n\nx = np.linspace(x_min, x_max, nnzeros)\ny = np.zeros(nnzeros)\nmask = np.where(np.abs(x) &lt;= 0.5, 1, 0)\ny[mask == 1] = 1\ndelta = (x_max - x_min) / nnzeros\nvar = np.sum(y * x**2 *(delta)) - (np.sum(y * x * delta))**2\n\ndef convolve(signal_in, n_convolutions, delta):\n    output = np.zeros(len(signal_in))\n    if n_convolutions == 0:\n        return output\n    elif n_convolutions == 1:\n        return signal_in\n    else:\n        output = signal_in.copy()\n        for i in range(n_convolutions - 1):\n            output = signal.fftconvolve(\n                output * delta, signal_in, mode=\"same\"\n            )\n        return output\n\n\ndef server(input, output, session):\n\n    fig = go.FigureWidget()\n    fig.add_trace(\n        go.Scatter(\n            mode=\"lines\",\n            line=dict(color=\"black\", width=3),\n            x=[],\n            y=[],\n            name=r\"f*...*f\",\n        )\n    )\n\n    fig.add_trace(\n        go.Scatter(\n            x=x,\n            y=np.exp(-(x**2) / (2 * var)) / np.sqrt(2 * var * np.pi),\n            mode=\"lines\",\n            line=dict(dash=\"dash\", color=\"red\", width=2),\n            name=f\"Loi normale&lt;br&gt;(variance adéquate)\",\n        )\n    )\n    fig.update_xaxes(range=[-3, 3], position=0.)\n    fig.update_yaxes(range=[0, 2 * np.max(y)], position=0.5, showticklabels=False)\n\n\n    fig.update_layout(\n        template=\"simple_white\",\n        showlegend=True,\n        autosize=True,\n        title=dict(text=\"Densité : &lt;br&gt; moyenne de n variables aléatoires\", yanchor=\"top\", y=0.95),\n        title_x=0.5,\n    )\n    fig.update_layout(legend=dict(\n        yanchor=\"top\",\n        y=0.90,\n        xanchor=\"left\",\n        x=-0.1,\n        font=dict(size= 10)\n    ))\n    fig.update_layout(\n    height=250,\n    margin=dict(l=0, r=0, b=10, t=100),\n    )\n\n    register_widget(\"my_widget\", fig)\n\n    @reactive.Effect\n    def _():\n        if str(input.loi()) == 'uniforme':\n            y = np.zeros(nnzeros)\n            mask = np.where(np.abs(x) &lt;= 0.5, 1, 0)\n            y[mask == 1] = 1\n\n        else:\n            # else str(input.loi()) == 'laplace':\n            y=np.exp(-np.abs(x))/2\n            # y = np.zeros(nnzeros)\n            # mask = np.where(np.abs(x) &gt;= 0.5, 1, 0)\n            # y[mask == 1] = 0\n            # y = np.cos(np.pi * x) + 1\n            y = y / (np.sum(y) * delta)\n\n        var = np.sum(y * x**2 * delta) - (np.sum(y * x * delta))**2\n\n        y_display = convolve(y, input.n_iter(), delta)\n        # Update data in fig:\n        fig.data[0].x = x / np.sqrt(input.n_iter())\n        fig.data[0].y = y_display * np.sqrt(input.n_iter())\n        fig.data[1].y = np.exp(-(x**2) / (2 * var)) / np.sqrt(2 * var * np.pi)\n        fig.update_yaxes(range=[0, 2 * np.max(y)], position=0.5, showticklabels=False)\n\n\napp = App(app_ui, server)\n\n\n\n\n\n\n\nPour aller plus loin\n\n\nPour plus d’info sur les convolutions, voir la vidéo de 3Blue1Brown : Convolutions | Why X+Y in probability is a beautiful mess, 🇬🇧"
  },
  {
    "objectID": "Slides/slides_th_asymptotique.html#bibliographie",
    "href": "Slides/slides_th_asymptotique.html#bibliographie",
    "title": "HAX603X: Modélisation stochastique",
    "section": "Bibliographie",
    "text": "Bibliographie\n\n\n\n\n\nHAX603X: Modélisation stochastique"
  },
  {
    "objectID": "TP/TP1.html#test2",
    "href": "TP/TP1.html#test2",
    "title": "TP1:…",
    "section": "test2",
    "text": "test2",
    "crumbs": [
      "TP",
      "TP1:..."
    ]
  }
]