[
  {
    "objectID": "Courses/th_asymptotique.html",
    "href": "Courses/th_asymptotique.html",
    "title": "Théorèmes asymptotiques",
    "section": "",
    "text": "Le premier résultat fondamental en probabilités concerne le comportement asymptotique de la moyenne empirique: \n\\bar X_n = \\dfrac{X_1 + \\cdots + X_n}{n} \\enspace.\n quand on obsever n variables aléatoires i.i.d X_1,\\dots,X_n, ayant une espérance finie.\n\nThéorème 1 (Loi forte des grands nombres) \nSoit (X_n)_{n \\geq 1} une suite de variables aléatoires indépendantes et identiquement distribuées (i.i.d.) dans L^1(\\Omega, \\mathcal{F}, \\mathbb{P}). Notons \\mu = \\mathbb{E}[X_1]. Alors \\bar X_n converge vers \\mu presque sûrement : \n\\mathbb{P}\\bigg( \\dfrac{X_1 + \\cdots + X_n}{n} \\underset{n \\to \\infty}{\\longrightarrow} \\mu \\bigg) = 1\\,.\n\n\nInterprétation: Intuitivement, la probabilité d’un événement A correspond à la fréquence d’apparition de A quand on répète une expérience qui fait intervenir cet événement. Par exemple, si on dispose une pièce truquée, on estimera la probabilité d’apparition du côté pile en lançant la pièce un grand nombre de fois et en comptant le nombre de pile obtenu. La loi des grands nombres justifie a posteriori cette intuition : si X_1, \\ldots, X_n sont i.i.d. de loi de Bernoulli de paramètre p, alors \n    \\dfrac{X_1 + \\cdots + X_n}{n} \\xrightarrow[n \\to \\infty]{p.s.} p \\enspace.\n Le membre de gauche correspond au nombre empirique de pile obtenu, celui de droite à la valeur théorique.\nRemarque: Bien qu’intuitivement assez intuitif, ce théorème est difficile à démontrer. XXX TODO referece : Williams (en anglais) ou bien Ouvrard.\n#| standalone: true\n#| viewerHeight: 700\nimport numpy as np\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\nfrom shiny import ui, render, App, reactive\nfrom shinywidgets import output_widget, render_widget\n\n# n_samples = 50\n# step = 10\napp_ui = ui.page_fluid(\n    ui.panel_title(\"Loi des grands nombres: visualisation\"),\n    ui.input_action_button(\"seed\", \"Ré-échantillonner\",class_=\"btn-primary\"),\n    output_widget(\"my_widget\"),\n    ui.row(\n        ui.column(6,\n            ui.input_slider(\"p\", \"Paramètre de Bernoulli: p\", 0.01, 0.99, value=0.5, step=0.01)\n        ),\n        # ui.column(4, ui.input_slider(\"step\", \"n\", 1, n_samples, value=10)),\n        ui.column(6, ui.input_slider(\"n_samples\", \"Nombre de tirages aléatoires \", 2, 1000, value=15, step=1)\n        )\n    )\n)\n\n\ndef server(input, output, session):\n    seed = reactive.Value(42)\n\n    @reactive.Effect\n    @reactive.event(input.seed)\n    def _():\n        seed.set(np.random.randint(0, 1000))\n\n\n    @output\n    @render_widget\n    def my_widget():\n        # Create figure\n        rng = np.random.default_rng(seed())\n        p = input.p()\n        # step=input.step()\n        n_samples = input.n_samples()\n        iterations = np.arange(1, n_samples + 1)\n        samples = rng.binomial(1, p, size=n_samples)\n        means_samples = np.cumsum(samples) / np.arange(1, n_samples + 1)\n\n        fig = make_subplots(\n                    rows=2,\n                    cols=1,\n                    vertical_spacing=0.3,\n                    horizontal_spacing=0.04,\n                    subplot_titles=(\n                        f\"Moyenne empirique en fonction du nombre de tirages &lt;br&gt;(loi de Bernoulli)\",\n                        \"Tirages aléatoires &lt;span style='color:rgb(66, 139, 202)'&gt;bleu: 0&lt;/span&gt;, &lt;span style='color:rgb(255, 0, 0)'&gt;rouge: 1&lt;/span&gt; (seed=\"+ str(seed())+\")\",\n                    ),\n                    row_heights=[8, 1],\n                )\n\n        fig.add_trace(\n                go.Scatter(\n                    mode='lines',\n                    line=dict(color=\"black\", width=3),\n                    x=iterations,\n                    y=means_samples,\n                    name=r'Moyenne &lt;br&gt; empirique'),\n                    # name=r'$\\bar{X}_n$'),\n                    row=1, col=1,\n        )\n        fig.add_trace(\n                go.Scatter(\n                    mode='lines',\n                    line=dict(dash=\"dash\", color=\"black\", width=1),\n                    marker={},\n                    x=iterations,\n                    y=np.full((n_samples), p),\n                    # name=r'$p$'),\n                    name=r'p'),\n                    row=1, col=1,\n        )\n        fig.add_trace(\n                go.Heatmap(x=iterations + 0.5, z=[samples],\n                        # text=[list(samples.astype('str'))],\n                        # texttemplate=\"%{text}\",\n                        colorscale=[[0,'rgb(66, 139, 202)'], [1, 'rgb(255,0,0)']],\n                        showscale=False,\n                        # textfont={\"size\":10}\n                        ),\n                row=2, col=1,\n        )\n        fig.update_xaxes(range=[1, n_samples + 1])\n        fig.update_yaxes(range=[0, 1.1], row=1, col=1)\n        fig.update_xaxes(matches=\"x1\", row=2, col=1)\n        fig.update_yaxes(visible=False, row=2, col=1)\n        fig.update_xaxes(visible=False, row=2, col=1)\n\n\n        fig.update_layout(\n            template=\"simple_white\",\n            showlegend=True,\n        )\n        fig.update_layout(\n            legend=dict(\n            yanchor=\"top\",\n            y=0.99,\n            xanchor=\"left\",\n            x=0.75,\n            bgcolor='rgba(0,0,0,0)',\n            )\n        )\n        return fig\n\n\napp = App(app_ui, server)\n\nXXX TODO: phénomène intéressant en bougeant le paramètre p avec le reste fixé…les signaux générés sont très très proches, ce qui ne devrait pas être le cas a priori, saut structuration particulière de la génération."
  },
  {
    "objectID": "Courses/th_asymptotique.html#loi-des-grands-nombres",
    "href": "Courses/th_asymptotique.html#loi-des-grands-nombres",
    "title": "Théorèmes asymptotiques",
    "section": "",
    "text": "Le premier résultat fondamental en probabilités concerne le comportement asymptotique de la moyenne empirique: \n\\bar X_n = \\dfrac{X_1 + \\cdots + X_n}{n} \\enspace.\n quand on obsever n variables aléatoires i.i.d X_1,\\dots,X_n, ayant une espérance finie.\n\nThéorème 1 (Loi forte des grands nombres) \nSoit (X_n)_{n \\geq 1} une suite de variables aléatoires indépendantes et identiquement distribuées (i.i.d.) dans L^1(\\Omega, \\mathcal{F}, \\mathbb{P}). Notons \\mu = \\mathbb{E}[X_1]. Alors \\bar X_n converge vers \\mu presque sûrement : \n\\mathbb{P}\\bigg( \\dfrac{X_1 + \\cdots + X_n}{n} \\underset{n \\to \\infty}{\\longrightarrow} \\mu \\bigg) = 1\\,.\n\n\nInterprétation: Intuitivement, la probabilité d’un événement A correspond à la fréquence d’apparition de A quand on répète une expérience qui fait intervenir cet événement. Par exemple, si on dispose une pièce truquée, on estimera la probabilité d’apparition du côté pile en lançant la pièce un grand nombre de fois et en comptant le nombre de pile obtenu. La loi des grands nombres justifie a posteriori cette intuition : si X_1, \\ldots, X_n sont i.i.d. de loi de Bernoulli de paramètre p, alors \n    \\dfrac{X_1 + \\cdots + X_n}{n} \\xrightarrow[n \\to \\infty]{p.s.} p \\enspace.\n Le membre de gauche correspond au nombre empirique de pile obtenu, celui de droite à la valeur théorique.\nRemarque: Bien qu’intuitivement assez intuitif, ce théorème est difficile à démontrer. XXX TODO referece : Williams (en anglais) ou bien Ouvrard.\n#| standalone: true\n#| viewerHeight: 700\nimport numpy as np\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\nfrom shiny import ui, render, App, reactive\nfrom shinywidgets import output_widget, render_widget\n\n# n_samples = 50\n# step = 10\napp_ui = ui.page_fluid(\n    ui.panel_title(\"Loi des grands nombres: visualisation\"),\n    ui.input_action_button(\"seed\", \"Ré-échantillonner\",class_=\"btn-primary\"),\n    output_widget(\"my_widget\"),\n    ui.row(\n        ui.column(6,\n            ui.input_slider(\"p\", \"Paramètre de Bernoulli: p\", 0.01, 0.99, value=0.5, step=0.01)\n        ),\n        # ui.column(4, ui.input_slider(\"step\", \"n\", 1, n_samples, value=10)),\n        ui.column(6, ui.input_slider(\"n_samples\", \"Nombre de tirages aléatoires \", 2, 1000, value=15, step=1)\n        )\n    )\n)\n\n\ndef server(input, output, session):\n    seed = reactive.Value(42)\n\n    @reactive.Effect\n    @reactive.event(input.seed)\n    def _():\n        seed.set(np.random.randint(0, 1000))\n\n\n    @output\n    @render_widget\n    def my_widget():\n        # Create figure\n        rng = np.random.default_rng(seed())\n        p = input.p()\n        # step=input.step()\n        n_samples = input.n_samples()\n        iterations = np.arange(1, n_samples + 1)\n        samples = rng.binomial(1, p, size=n_samples)\n        means_samples = np.cumsum(samples) / np.arange(1, n_samples + 1)\n\n        fig = make_subplots(\n                    rows=2,\n                    cols=1,\n                    vertical_spacing=0.3,\n                    horizontal_spacing=0.04,\n                    subplot_titles=(\n                        f\"Moyenne empirique en fonction du nombre de tirages &lt;br&gt;(loi de Bernoulli)\",\n                        \"Tirages aléatoires &lt;span style='color:rgb(66, 139, 202)'&gt;bleu: 0&lt;/span&gt;, &lt;span style='color:rgb(255, 0, 0)'&gt;rouge: 1&lt;/span&gt; (seed=\"+ str(seed())+\")\",\n                    ),\n                    row_heights=[8, 1],\n                )\n\n        fig.add_trace(\n                go.Scatter(\n                    mode='lines',\n                    line=dict(color=\"black\", width=3),\n                    x=iterations,\n                    y=means_samples,\n                    name=r'Moyenne &lt;br&gt; empirique'),\n                    # name=r'$\\bar{X}_n$'),\n                    row=1, col=1,\n        )\n        fig.add_trace(\n                go.Scatter(\n                    mode='lines',\n                    line=dict(dash=\"dash\", color=\"black\", width=1),\n                    marker={},\n                    x=iterations,\n                    y=np.full((n_samples), p),\n                    # name=r'$p$'),\n                    name=r'p'),\n                    row=1, col=1,\n        )\n        fig.add_trace(\n                go.Heatmap(x=iterations + 0.5, z=[samples],\n                        # text=[list(samples.astype('str'))],\n                        # texttemplate=\"%{text}\",\n                        colorscale=[[0,'rgb(66, 139, 202)'], [1, 'rgb(255,0,0)']],\n                        showscale=False,\n                        # textfont={\"size\":10}\n                        ),\n                row=2, col=1,\n        )\n        fig.update_xaxes(range=[1, n_samples + 1])\n        fig.update_yaxes(range=[0, 1.1], row=1, col=1)\n        fig.update_xaxes(matches=\"x1\", row=2, col=1)\n        fig.update_yaxes(visible=False, row=2, col=1)\n        fig.update_xaxes(visible=False, row=2, col=1)\n\n\n        fig.update_layout(\n            template=\"simple_white\",\n            showlegend=True,\n        )\n        fig.update_layout(\n            legend=dict(\n            yanchor=\"top\",\n            y=0.99,\n            xanchor=\"left\",\n            x=0.75,\n            bgcolor='rgba(0,0,0,0)',\n            )\n        )\n        return fig\n\n\napp = App(app_ui, server)\n\nXXX TODO: phénomène intéressant en bougeant le paramètre p avec le reste fixé…les signaux générés sont très très proches, ce qui ne devrait pas être le cas a priori, saut structuration particulière de la génération."
  },
  {
    "objectID": "Courses/th_asymptotique.html#théorème-central-limite",
    "href": "Courses/th_asymptotique.html#théorème-central-limite",
    "title": "Théorèmes asymptotiques",
    "section": "Théorème central limite",
    "text": "Théorème central limite\nUne fois la loi des grands nombres établie, on peut se demander quel est l’ordre suivant dans le développement asymptotique de \\bar X_n - \\mu, ou de manière équivalente de S_n - n \\mu, où S_n = X_1 + \\cdots + X_n. Le théorème suivant répond à cette question, en donnant une convergence en loi d’une transformation affine de la moyenne empirique:\n\nThéorème 2 (Théorème central limite) Soit X_1, \\ldots, X_n une suite de variables aléatoires i.i.d de variance \\sigma^2 = {\\rm var}(X_1) \\in ]0, \\infty[. On note \\mu = \\mathbb{E}[X_1] leur espérance. Alors \n\\sqrt n \\left(\\frac{\\bar X_n - \\mu}{\\sigma} \\right) \\xrightarrow[n \\to +\\infty]{\\mathcal{L}} N\\enspace,\n où N suit une loi normale centrée réduite : N \\sim\\mathcal{N}(0,1).\n\nPreuve XXX TODO: donner une référence.\nEn termes de somme cumulée empirique, la convergence se réécrit\n\n    \\frac{S_n - n \\mu}{\\sqrt n \\sigma} \\xrightarrow[n \\to +\\infty]{\\mathcal{L}} N \\enspace.\n\nLes hypothèses de ce théorème sont plutôt faibles (il suffit de supposer une variance finie). Pourtant, le résultat est universel : la loi de départ peut être aussi farfelue que l’on veut, elle se rapprochera toujours asymptotiquement d’une loi normale.\nOn rappelle que la convergence en loi est équivalente à la convergence des fonctions de répartition en tout point de continuité de la limite. Ainsi, le théorème central limite se réécrit de la manière suivante : pour tout a &lt; b,\n\n    \\mathbb{P} \\bigg( a \\leq \\sqrt n \\left(\\frac{\\bar X_n - \\mu}{\\sigma} \\right) \\leq b\\bigg)\n    \\underset{n \\to \\infty}{\\longrightarrow}  \\dfrac{1}{\\sqrt{2\\pi}} \\int_a^b e^{-\\frac{x^2}{2}} \\, \\mathrm dx\\,.\n\n\nExemple 1 On considère des variables aléatoires X_1, \\ldots, X_n i.i.d. suivant une loi de Bernoulli de paramètre p \\in ]0,1[, dont l’espérance et la variance sont respectivemenbt p et p(1-p). Le théorème central limite donne alors \n    \\sqrt n \\left(\\frac{\\bar X_n - p}{p (1-p)} \\right) \\xrightarrow[n \\to +\\infty]{\\mathcal{L}} N\\,,\n avec N \\sim \\mathcal{N}(0,1). Cette convergence est illustrée en Figure XXX TODO image."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "HAX603X: Modélisation stochastique",
    "section": "",
    "text": "Aspects numériques de la modélisation aléatoire et statistiques (cours de Licence 3).\nAttention: site en construction…\n\n\n\nJoseph Salmon: joseph.salmon@umontpellier.fr,\nBenjamin Charlier: benjamin.charlier@umontpellier.fr\n\nCe cours est issu du travail antérieur de la part de:\n\nNicolas Meyer\nBenoîte de Saporta\n\n\n\n\nCours de mesure et intégration, analyse numérique…\n\n\n\n\n\n\n\n\n\n\n\nCCI\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(Data Science): Python Data Science Handbook, With Application to Understanding Data by J. Van DerPlas, 2016; videos: Reproducible Data Analysis in Jupyter"
  },
  {
    "objectID": "index.html#professeurs",
    "href": "index.html#professeurs",
    "title": "HAX603X: Modélisation stochastique",
    "section": "",
    "text": "Joseph Salmon: joseph.salmon@umontpellier.fr,\nBenjamin Charlier: benjamin.charlier@umontpellier.fr\n\nCe cours est issu du travail antérieur de la part de:\n\nNicolas Meyer\nBenoîte de Saporta"
  },
  {
    "objectID": "index.html#prérequis",
    "href": "index.html#prérequis",
    "title": "HAX603X: Modélisation stochastique",
    "section": "",
    "text": "Cours de mesure et intégration, analyse numérique…"
  },
  {
    "objectID": "index.html#modalité-de-contrôle-des-connaissances",
    "href": "index.html#modalité-de-contrôle-des-connaissances",
    "title": "HAX603X: Modélisation stochastique",
    "section": "",
    "text": "CCI"
  },
  {
    "objectID": "index.html#livres-et-ressources",
    "href": "index.html#livres-et-ressources",
    "title": "HAX603X: Modélisation stochastique",
    "section": "",
    "text": "(Data Science): Python Data Science Handbook, With Application to Understanding Data by J. Van DerPlas, 2016; videos: Reproducible Data Analysis in Jupyter"
  },
  {
    "objectID": "Courses/notations.html",
    "href": "Courses/notations.html",
    "title": "Notations et rappels",
    "section": "",
    "text": "On considère un espace probabilisé (\\Omega, {\\mathcal{F}}, \\mathbb{P}), composé d’un ensemble \\Omega, d’une tribu \\mathcal{F}, et d’une mesure de probabilité \\mathbb{P}.\nCette définition permet de transposer l’aléa qui provient de \\Omega dans l’espace E. L’hypothèse \\{X \\in B\\} \\in \\mathcal{F} assure que cet ensemble est bien un évènement et donc que l’on peut calculer sa probabilité.\nUne fois que l’aléa a été transposé de \\Omega vers E, on souhaite également transposer la probabilité \\mathbb{P} sur E. Ceci motive l’introduction de la notion de loi.\nLes propriétés de \\mathbb{P} assurent que \\mathbb{P}_X est bien une loi de probabilité sur l’espace mesurable (E, \\mathcal{E})."
  },
  {
    "objectID": "Courses/notations.html#loi-discrètes",
    "href": "Courses/notations.html#loi-discrètes",
    "title": "Notations et rappels",
    "section": "Loi discrètes",
    "text": "Loi discrètes\nLes variables aléatoires discrètes sont celles à valeurs dans un ensemble E discret, le plus souvent \\mathbb{N}, muni de la tribu pleine \\mathcal{F} = \\mathcal{P}(E).\n\nExemple 1 (Loi de Bernoulli) La loi la plus simple est la loi de Bernoulli de paramètre p \\in [0,1], définie sur \\{0,1\\} par \\mathbb{P}(X=1) = 1-\\mathbb{P}(X=0) = p qui modélise une expérience aléatoire à deux issues (succès = 1 et échec = 0).\n\n\nExemple 2 (Loi binomiale) En sommant des variables aléatoires indépendantes de loi de Bernoulli on obtient une loi binomiale : \\mathbb{P}(X=k) = \\binom{n}{k} p^k (1-p)^{n-k}, pour k \\in \\{0,\\ldots,n\\}, qui modélise le nombre de succès parmi n lancers.\n\n\nExemple 3 (Loi géométrique) En observant le nombre d’expériences nécessaires avant d’obtenir un succès, on obtient une loi géométrique : \\mathbb{P}(X=k) = p (1-p)^{k-1}, pour k \\geq 1.\n\n\nExemple 4 (Loi de Poisson) La loi de Poisson de paramètre \\lambda &gt; 0 est définie par \\mathbb{P}(X=k) = e^{-\\lambda} \\lambda^k / k!, pour k \\in \\mathbb{N}, et modélise les événements rares."
  },
  {
    "objectID": "Courses/notations.html#lois-continues",
    "href": "Courses/notations.html#lois-continues",
    "title": "Notations et rappels",
    "section": "Lois continues",
    "text": "Lois continues\nParmi les variables aléatoires réelles non discrètes, beaucoup peuvent se représenter avec une densité, c’est-à-dire qu’il existe une fonction mesurable f : \\mathbb{R} \\to [0, \\infty[ d’intégrale 1. La loi d’une telle variable aléatoire X est alors donnée pour tout A \\in \\mathcal{B}(\\mathbb{R}) par \n    \\mathbb{P}(X \\in A) = \\int_A f(x) \\, \\mathrm d x \\enspace.\n Les propriétés de l’intégrale de Lebesgue assure que cette formule définit bien une loi de probabilité.\n\nExemple 5 (Loi uniforme) La loi uniforme sur un ensemble B \\in \\mathcal{B}(\\mathbb{R}), s’obtient avec la densité définie par \nf(x) = {1\\hspace{-3.8pt} 1}_B(x) / \\lambda (B) \\enspace,\n où \\lambda (B) représente la mesure de Lebesgue de l’ensemble B. En particulier pour la loi uniforme sur le segment [0,1] on obtient la fonction suivante: \nf(x) = {1\\hspace{-3.8pt} 1}_{[0,1]}(x)\\enspace.\n Si une variable aléatoire U suit une telle loi on note U \\sim \\mathcal{U}([0,1]).\n\n\nExemple 6 (Loi exponentielle) La loi exponentielle de paramètre \\gamma &gt; 0 est obtenue avec la densité donnée par \nf(x) = \\gamma e^{-\\gamma x} {1\\hspace{-3.8pt} 1}_{\\mathbb{R}_+}(x)\\enspace.\n Si une variable aléatoire X suit cette loi on note X \\sim \\mathcal{Exp}(\\gamma).\n\n\nExemple 7 (Loi normale/gaussienne univariée) On obtient la loi normale de paramètre \\mu \\in \\mathbb{R} et \\sigma^2 &gt; 0 correspond à loi dont la densité est donnée par la fonction réelle: \nf(x) = \\frac{1}{\\sqrt{2 \\pi} \\sigma}e^{-\\frac{1}{2 \\sigma^2}(x-\\mu)^2} \\enspace.\n Si une variable aléatoire X suit une telle loi on note X \\sim \\mathcal{N}(\\mu,\\sigma^2), \\mu correspondant à l’espérance de la loi, et \\sigma^2 à sa variance. On nomme loi normale centrée réduite le cas correspondant à \\mu = 0 et \\sigma^2 = 1.\n\n\nExemple 8 (Loi normale multivariée) On peut étendre les lois normales au cas multi-dimensionnel. Fixons d\\in\\mathbb{N}^* un entier non nul. Pour un vecteur \\mu \\in \\mathbb{R}^d et une matrice symétrique-définie positive \\Sigma\\in \\mathbb{R^{d\\times d}}, la densité normale mutlivariée associée est donnée par la fonction: \nf(x) = \\frac{1}{{(2 \\pi)}^{\\frac{d}{2}} {\\rm det}(\\Sigma)} e^{-\\frac{1}{2}(x-\\mu)^\\top \\Sigma ^{-1}(x-\\mu)}\n Notons que \\mu est l’espérance de la loi et \\Sigma la matrice de variance-covariance."
  },
  {
    "objectID": "Courses/notations.html#fonction-de-répartition",
    "href": "Courses/notations.html#fonction-de-répartition",
    "title": "Notations et rappels",
    "section": "Fonction de répartition",
    "text": "Fonction de répartition\nLa notion de variable aléatoire n’est pas facile à manipuler puisqu’elle part d’un espace \\Omega dont on ne sait rien. On souhaite donc caractériser la loi d’une variable aléatoire en ne considérant que l’espace d’arrivée (E, \\mathcal{E}) .\nPlusieurs outils existent : la fonction de répartition (pour des variables aléatoires réelles), la fonction caractéristique (pour des variables aléatoires dans \\mathbb{R}^d), la fonction génératrice des moments (pour des variables aléatoires discrètes), etc. On se contente ici de la fonction de répartition qui nous sera utile pour simuler des variables aléatoires, ainsi que son inverse au sens de Levy.\n\nDéfinition 3 (Fonction de répartition 🇬🇧: cumulative distribution function) Soit X une variable aléatoire sur (\\mathbb{R}, \\mathcal{B}(\\mathbb{R})). La fonction de répartition de X est la fonction F_X définie sur \\mathbb{R} par \n    F_X(x) = \\mathbb{P}(X \\leq x) = \\mathbb{P}(X \\in ]-\\infty, x]) \\enspace.\n\n\nOn appelle quantile d’ordre p\\in (0,1), la quantité F_X^\\leftarrow(p). La médiane est égale à F_X^\\leftarrow(1/2), les premiers et troisièmes quartiles sont égaux à F_X^\\leftarrow(1/4) et F_X^\\leftarrow(3/4). Enfin, les déciles sont les quantiles F_X^\\leftarrow(k/10) pour k=1,\\dots, 9.\n\nExemple 9 (Cas discret) Soit (x_i)_{i \\in I} une suite ordonnée de réels, avec I \\subset \\mathbb{N}. Si X est une variable aléatoire discrète prenant les valeurs (x_i)_{i \\in I} et de loi (p_i = \\mathbb{P}(X=x_i))_{i \\in I}, alors \nF_X(x) = \\sum_{i \\in I} p_i {1\\hspace{-3.8pt} 1}_{[x_i, \\infty[}(x) \\enspace.\n\n\n\nExemple 10 (Cas continu) Si X est une variable aléatoire de densité f, alors \n    F_X(x) = \\int_{-\\infty}^x f(t) \\, \\mathrm dt \\enspace.\n\n\nLe graphe des fonctions de répartition des loi de Bernoulli, uniforme et normale sont représentées en Figure XXX. Notons que la fonction de répartition de la loi normale \\mathcal{N}(0,1), souvent notée \\Phi, n’admet pas d’expression explicite autre que \n\\Phi(x) = \\dfrac{1}{\\sqrt{2\\pi}} \\int_{-\\infty}^x e^{-\\frac{x^2}{2}}\\, \\mathrm d x\\enspace,\n Les valeurs numériques de \\Phi(x) étaient autrefois reportées dans des tables1.Par transformation affine, si X \\sim \\mathcal{N}(\\mu, \\sigma^2) — ce que l’on peut aussi écrire : X=\\mu + \\sigma Y, avec Y\\sim \\mathcal{N}(0,1) — alors sa fonction de répartition est donnée par F_X(x)=\\Phi((x-\\mu)/\\sigma).1 Wikipedia: loi normale\n\nProposition 1 Soit X une variable aléatoire de fonction de répartition F_X.\n\nF_X est une fonction croissante, de limite 0 en -\\infty et de limite 1 en +\\infty.\nF_X est continue à droite en tout point.\nPour tout x \\in \\mathbb{R}, on a \\mathbb{P}(X=x) = F_X(x) - F_X(x-), où F_X(x-) = \\lim_{\\epsilon \\to 0+} F_X(x- \\epsilon).\nSi X a pour densité f, alors F_X est dérivable \\lambda-presque partout de dérivée f.\n\n\nLa propriété 3. est utile dans le cas discret : les valeurs prises par X correspondent aux points de discontinuité de F_X et les probabilités associées correspondent à la hauteur du saut.\nLa propriété 4. donne le lien entre la fonction de répartition d’une variable aléatoire à densité et sa densité. On peut donc retrouver la loi de X à partir de sa fonction de répartition. Le théorème suivant généralise ce résultat à toute variable aléatoire réelle (pas nécessairement discrète ou à densité).\n\nThéorème 1 La fonction de répartition d’une variable aléatoire caractérise sa loi : deux variables aléatoires ont même loi si et seulement si elles ont même fonction de répartition.\n\nXXX source + proof???\nOn rappelle que la tribu des boréliens est engendrée par la famille d’ensembles \\{]-\\infty,x], x \\in \\mathbb{R}\\}. Le théorème précédent assure que si on connaît la mesure \\mathbb{P}_X sur cette famille d’ensemble alors on la connaît partout.\nXXX TODO: move this in correct part.\n\nExemple 11 (Loi exponentielle depuis une loi uniforme) On considère une variable aléatoire U de loi uniforme sur [0,1] et on pose X = -\\ln(1-U). Déterminons la loi de X en calculant sa fonction de répartition. Pour tout x \\in \\mathbb{R}, \n\\begin{align*}\nF_X(x) = & \\mathbb{P}(X \\leq x) \\\\\n       = & \\mathbb{P}(-\\ln(1-U) \\leq x) \\\\\n       = & \\mathbb{P}(U \\leq 1-e^{-x}) \\\\\n       = &\n    \\begin{cases}\n        0           & \\text{ si }x &lt; 0\\,,    \\\\\n        1 - e^{-x} & \\text{ si }x \\geq 0\\,,\n    \\end{cases}\n\\end{align*}\n\noù on a utilisé l’égalité \\mathbb{P}(U \\leq t) = t pour tout t \\in [0,1]. Ainsi la variable aléatoire X a la même fonction de répartition qu’une loi exponentielle de paramètre 1. On en conclut que X \\sim \\mathcal{Exp}(1). Notons que l’on peut aussi montrer que -\\ln(X)\\sim\\mathcal{E}(1), sachant que U et 1-U ont la même loi."
  },
  {
    "objectID": "Courses/notations.html#fonction-quantile-inverse-généralisée-à-gauche",
    "href": "Courses/notations.html#fonction-quantile-inverse-généralisée-à-gauche",
    "title": "Notations et rappels",
    "section": "Fonction quantile, inverse généralisée à gauche",
    "text": "Fonction quantile, inverse généralisée à gauche\nLa fonction de répartition étant une fonction croissante on peut donner un sens à son inverse généralisée de la manière suivante.\n\nDéfinition 4 (Fonction quantile/inverse de Levy 🇬🇧: quantile distribution function) Soit X une variable aléatoire sur (\\mathbb{R}, \\mathcal{B}(\\mathbb{R})). et F_X sa fonction de répartion. La fonction quantile associée F_X^\\leftarrow: ]0,1[\\rightarrow \\mathbb{R} est définie par \n  F_n^\\leftarrow(p)=  \\inf\\{ x\\colon F(x)\\geq p\\} \\enspace.\n\n\nDans le cas où la fonction de répartition F est bijective, alors l’inverse de la fonction de répartition coincide avec la fonction quantile."
  },
  {
    "objectID": "Courses/notations.html#visualisation-densité-fonction-de-répartition-quantiles-etc.",
    "href": "Courses/notations.html#visualisation-densité-fonction-de-répartition-quantiles-etc.",
    "title": "Notations et rappels",
    "section": "Visualisation: densité, fonction de répartition, quantiles, etc.",
    "text": "Visualisation: densité, fonction de répartition, quantiles, etc.\n\nCas des variables continues\n#| standalone: true\n#| viewerHeight: 830\nimport numpy as np\nfrom scipy import stats\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nfrom shiny import ui, render, App\nfrom shinywidgets import output_widget, render_widget\n\n\ndef keep_no_param_distribution():\n    distributions = stats._continuous_distns._distn_names\n    distributions_0 = []\n    for _, name in enumerate(distributions):\n        dist = getattr(stats, name)\n        if not dist.shapes or len(dist.shapes) == 0:\n            distributions_0.append(name)\n    distributions_0_val = [\n        getattr(stats.distributions, string) for string in distributions_0\n    ]\n    distributions_0_dict = dict(zip(distributions_0, distributions_0_val))\n    return distributions_0_dict\n\n\ndistributions_0_dict = keep_no_param_distribution()\n\nmu = 0\nsigma = 1\n\napp_ui = ui.page_fluid(\n    ui.div(\n        ui.input_slider(\"alpha\", \"Quantile\", 0.01, 0.99, value=0.5, step=0.01),\n        ui.input_slider(\"xrange\", \"x-range\", -10, 10, value=(-5, 5), step=0.2),\n        ui.input_select(\n            \"distrib\",\n            \"Distribution\",\n            list(distributions_0_dict.keys()),\n            selected='norm'\n        ),\n        class_=\"d-flex gap-3\",\n    ),\n    output_widget(\"my_widget\"),\n)\n\n\ndef server(input, output, session):\n    @output\n    @render_widget\n    def my_widget():\n        fig = make_subplots(\n            rows=3,\n            cols=2,\n            vertical_spacing=0.1,\n            horizontal_spacing=0.15,\n            subplot_titles=(\n                \"Fonction quantile\",\n                \"\",\n                \"\",\n                \"Fonction de répartition\",\n                \"\",\n                \"Densité et quantile\",\n            ),\n            column_widths=[0.2, 0.5],\n            row_heights=[0.35, 0.17, 0.17],\n        )\n\n        alpha = input.alpha()\n        distribution = distributions_0_dict[input.distrib()]\n        x = np.linspace(input.xrange()[0], input.xrange()[1], num=400)\n        cdf_data = distribution.cdf(x, loc=mu, scale=sigma)\n        pdf_data = distribution.pdf(x, loc=mu, scale=sigma)\n        q_alpha = distribution.ppf(alpha, loc=mu, scale=sigma)\n\n        fig.update_layout(autosize=True, height=700)\n\n        # Quantile plot\n        fig.add_trace(\n            go.Scatter(\n                x=cdf_data, y=x, mode=\"lines\", marker={\"color\": \"black\"}\n            ),\n            row=1,\n            col=1,\n        )\n        # Diagonal\n        fig.add_trace(\n            go.Scatter(\n                x=cdf_data, y=cdf_data, mode=\"lines\", marker={\"color\": \"black\"}\n            ),\n            row=2,\n            col=1,\n        )\n        # Cdf part\n        fig.add_trace(\n            go.Scatter(\n                x=x, y=cdf_data, mode=\"lines\", marker={\"color\": \"black\"}\n            ),\n            row=2,\n            col=2,\n        )\n        # pdf part\n        fig.add_scatter(\n            x=x[x &lt; q_alpha],\n            y=pdf_data[x &lt; q_alpha],\n            fill=\"tozeroy\",\n            mode=\"none\",\n            fillcolor=\"rgb(66, 139, 202)\",\n            row=3,\n            col=2,\n        )\n\n        fig.add_trace(\n            go.Scatter(\n                x=x, y=pdf_data, mode=\"lines\", marker={\"color\": \"black\"}\n            ),\n            row=3,\n            col=2,\n        )\n\n        # Dots\n        fig.add_trace(\n            go.Scatter(\n                x=[alpha],\n                y=[q_alpha],\n                mode=\"markers\",\n                marker={\"color\": \"rgb(66, 139, 202)\"},\n                marker_symbol=\"circle\",\n                marker_size=15,\n            ),\n            row=1,\n            col=1,\n        )\n\n        fig.add_trace(\n            go.Scatter(\n                x=[alpha],\n                y=[alpha],\n                mode=\"markers\",\n                marker={\"color\": \"rgb(66, 139, 202)\"},\n                marker_symbol=\"circle\",\n                marker_size=15,\n            ),\n            row=2,\n            col=1,\n        )\n\n        fig.add_trace(\n            go.Scatter(\n                x=[q_alpha],\n                y=[alpha],\n                mode=\"markers\",\n                marker={\"color\": \"rgb(66, 139, 202)\"},\n                marker_symbol=\"circle\",\n                marker_size=15,\n            ),\n            row=2,\n            col=2,\n        )\n\n        # Lines\n        fig.add_trace(\n            go.Scatter(\n                x=[alpha, alpha],\n                y=[x[0], q_alpha],\n                mode=\"lines\",\n                line=dict(dash=\"dash\", color=\"rgb(66, 139, 202)\")\n            ),\n            row=1,\n            col=1\n        )\n\n        fig.add_trace(\n            go.Scatter(\n                x=[alpha, alpha],\n                y=[alpha, 1.],\n                mode=\"lines\",\n                line=dict(dash=\"dash\", color=\"rgb(66, 139, 202)\")\n            ),\n            row=2,\n            col=1\n        )\n        fig.add_trace(\n            go.Scatter(\n                x=[alpha, 1],\n                y=[alpha, alpha],\n                mode=\"lines\",\n                line=dict(dash=\"dash\", color=\"rgb(66, 139, 202)\")\n            ),\n            row=2,\n            col=1\n        )\n        fig.add_trace(\n            go.Scatter(\n                x=[x[0], q_alpha],\n                y=[alpha, alpha],\n                mode=\"lines\",\n                line=dict(dash=\"dash\", color=\"rgb(66, 139, 202)\")\n            ),\n            row=2,\n            col=2\n        )\n        fig.add_trace(\n            go.Scatter(\n                x=[x[0], q_alpha],\n                y=[x[0], 0],\n                mode=\"lines\",\n                line=dict(dash=\"dash\", color=\"rgb(66, 139, 202)\")\n            ),\n            row=2,\n            col=2\n        )\n        # Axes ranges\n        fig.update_xaxes(range=[0, 1.], row=1, col=1)\n        fig.update_yaxes(matches=\"x6\", row=1, col=1)\n\n        fig.update_yaxes(range=[0, 1.], row=2, col=1)\n        fig.update_xaxes(matches=\"x1\", row=2, col=1)\n\n        fig.update_yaxes(rangemode=\"tozero\", row=3, col=2)\n        fig.update_xaxes(range=[x[0], x[-1]], row=3, col=2)\n\n        fig.update_xaxes(matches=\"x6\", row=2, col=2)\n        fig.update_yaxes(matches=\"y3\", row=2, col=2)\n\n        # Add dropdown\n        fig.update_layout(\n            showlegend=False,\n            template=\"simple_white\",\n        )\n        return fig\n\n\napp = App(app_ui, server)\n\n\n\nCas des variables discrètes\n#| standalone: true\n#| viewerHeight: 830\nimport numpy as np\nfrom scipy import stats\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nfrom shiny import ui, render, App\nfrom shinywidgets import output_widget, render_widget\n\n\ndef keep_no_param_distribution_disc():\n    distributions = stats._discrete_distns._distn_names\n    distributions_0 = []\n    for i, name in enumerate(distributions):\n        dist = getattr(stats, name)\n        if not dist.shapes or len(dist.shapes)==2 or len(dist.shapes)==1:\n            distributions_0.append(name)\n    distributions_0_val = [getattr(stats.distributions, string) for string in distributions_0 ]\n    distributions_0_dict = dict(zip(distributions_0, distributions_0_val))\n    return distributions_0_dict\n\ndef cdf_tool(x, dtype='int64'):\n    y = np.zeros(2*(len(x)), dtype=dtype)\n    y[::2]=x\n    y[1::2]=x\n    return y[1::], y[:-1], y\n\ndef pmf_tool(x, dtype='int64'):\n    y = np.zeros(2*(len(x)), dtype=dtype)\n    y[::2]=x\n    return y[1::], y[:-1], y\n\ndef insert_nones(my_list):\n    for i, val in enumerate(my_list):\n        if i % 3 == 2:\n            my_list.insert(i, None)\n    return my_list\n\ndistributions_0_dict = keep_no_param_distribution_disc()\n\napp_ui = ui.page_fluid(\n    ui.div(\n        ui.input_slider(\"alpha\", \"Quantile\", 0.01, 0.99, value=0.5, step=0.01),\n        ui.input_slider(\"xrange\", \"x-range\", -10, 10, value=(-5.5, 5.5), step=0.2),\n        ui.input_select(\n            \"distrib\",\n            \"Distribution\",\n            list(distributions_0_dict.keys()),\n            selected='poisson'\n        ),\n        class_=\"d-flex gap-3\",\n    ),\n    output_widget(\"my_widget\"),\n)\n\n\ndef server(input, output, session):\n    @output\n    @render_widget\n    def my_widget():\n        fig = make_subplots(\n            rows=3,\n            cols=2,\n            vertical_spacing=0.1,\n            horizontal_spacing=0.15,\n            subplot_titles=(\n                \"Fonction quantile\",\n                \"\",\n                \"\",\n                \"Fonction de répartition\",\n                \"\",\n                \"Fonction de masse et quantile\",\n            ),\n            column_widths=[0.2, 0.5],\n            row_heights=[0.35, 0.17, 0.17],\n        )\n\n\n        alpha = input.alpha()\n        # alpha=0.5\n\n        mu = 0.5  # Param needed for some distribution\n        if input.distrib()=='zipf':\n            mu = 2\n        distribution = distributions_0_dict[input.distrib()]\n        # distribution=distributions_0_dict['poisson']\n        x = np.arange(np.floor(input.xrange()[0]), np.ceil(input.xrange()[1]))\n        # x = np.arange(np.floor(-5.5), np.ceil(5.5))\n\n        cdf_data = distribution.cdf(x, mu)\n        pmf_data = distribution.pmf(x, mu)\n        q_alpha = distribution.ppf(alpha, mu)\n        support = pmf_data.nonzero()[0]\n        fig.update_layout(autosize=True, height=700)\n\n        # Quantile plot\n        new_x, new_y, new_z = cdf_tool(support)\n        _, _, new_pmf = pmf_tool(support)\n\n        fig.add_trace(\n            go.Scatter(\n                x=insert_nones(list(np.append(cdf_data[new_y[::-1]], distribution.cdf(x[0], mu)))),\n                y=insert_nones(list(np.append(x[new_x[::-1]], x[new_x[0]]))),\n                mode=\"lines\",\n                line=dict(color=\"black\")\n            ),\n            row=1,\n            col=1,\n        )\n        fig.add_trace(\n             go.Scatter(\n                x=cdf_data[support], y=x[support],\n                mode=\"markers\", marker={\"color\": \"black\"}\n            ),\n            row=1,\n            col=1,\n        )\n        # Diagonal\n        fig.add_trace(\n            go.Scatter(\n                x=cdf_data, y=cdf_data, mode=\"lines\", marker={\"color\": \"black\"}\n            ),\n            row=2,\n            col=1,\n        )\n        # Cdf part\n        fig.add_trace(\n            go.Scatter(\n                x=x[support], y=cdf_data[support],\n                mode=\"markers\", marker={\"color\": \"black\"}\n            ),\n            row=2,\n            col=2,\n        )\n        fig.add_trace(\n            go.Scatter(\n                x=insert_nones(list(np.append(np.insert(x[new_x], 0, [x[0], x[new_x[0]]]),x[-1]))),\n                y=insert_nones(list(np.append(np.insert(cdf_data[new_y], 0, [0,0]), cdf_data[-1]))),\n                mode=\"lines\",\n                line=dict(color=\"black\")\n            ),\n            row=2,\n            col=2\n        )\n\n        fig.add_trace(\n            go.Scatter(\n                x=x, y=pmf_data, mode=\"markers\", marker={\"color\": \"black\"}\n            ),\n            row=3,\n            col=2,\n        )\n        fig.add_trace(\n            go.Scatter(\n                x=x, y=pmf_data, mode=\"markers\", marker={\"color\": \"black\"}\n            ),\n            row=3,\n            col=2,\n        )\n        x_bar = insert_nones(list(x[new_z]))\n        y_bar = insert_nones(list(pmf_data[new_pmf]))\n        fig.add_trace(\n            go.Scatter(\n                x=x_bar,\n                y=y_bar,\n                mode=\"lines\",\n                line=dict(color=\"black\")\n            ),\n            row=3,\n            col=2\n        )\n        _,_, devil_x = cdf_tool(x[x&lt;=q_alpha])\n        _,_, devil_y = cdf_tool(pmf_data[x&lt;q_alpha], dtype='float64')\n\n        x_bar_blue = insert_nones(list(devil_x))\n        y_bar_blue = np.array(insert_nones(list(devil_y)))\n        y_bar_blue[::-3]=0.\n        y_bar_blue = list(y_bar_blue)\n        fig.add_trace(\n            go.Scatter(\n                x=x_bar_blue,\n                y=y_bar_blue,\n                mode=\"lines\",\n                line=dict(color=\"rgb(66, 139, 202)\")\n            ),\n            row=3,\n            col=2\n        )\n        # pdf part\n        fig.add_scatter(\n            x=x[x &lt;= q_alpha],\n            y=pmf_data[x &lt;= q_alpha],\n            mode=\"markers\",\n            marker={\"color\":\"rgb(66, 139, 202)\"},\n            row=3,\n            col=2,\n        )\n\n        # Dots\n        fig.add_trace(\n            go.Scatter(\n                x=[alpha],\n                y=[q_alpha],\n                mode=\"markers\",\n                marker={\"color\": \"rgb(66, 139, 202)\"},\n                marker_symbol=\"circle\",\n                marker_size=15,\n            ),\n            row=1,\n            col=1,\n        )\n\n        fig.add_trace(\n            go.Scatter(\n                x=[alpha],\n                y=[alpha],\n                mode=\"markers\",\n                marker={\"color\": \"rgb(66, 139, 202)\"},\n                marker_symbol=\"circle\",\n                marker_size=15,\n            ),\n            row=2,\n            col=1,\n        )\n\n        fig.add_trace(\n            go.Scatter(\n                x=[q_alpha],\n                y=[alpha],\n                mode=\"markers\",\n                marker={\"color\": \"rgb(66, 139, 202)\"},\n                marker_symbol=\"circle\",\n                marker_size=15,\n            ),\n            row=2,\n            col=2,\n        )\n\n        # Lines\n        fig.add_trace(\n            go.Scatter(\n                x=[alpha, alpha],\n                y=[x[0], q_alpha],\n                mode=\"lines\",\n                line=dict(dash=\"dash\", color=\"rgb(66, 139, 202)\")\n            ),\n            row=1,\n            col=1\n        )\n\n        fig.add_trace(\n            go.Scatter(\n                x=[alpha, alpha],\n                y=[alpha, 1.],\n                mode=\"lines\",\n                line=dict(dash=\"dash\", color=\"rgb(66, 139, 202)\")\n            ),\n            row=2,\n            col=1\n        )\n        fig.add_trace(\n            go.Scatter(\n                x=[alpha, 1],\n                y=[alpha, alpha],\n                mode=\"lines\",\n                line=dict(dash=\"dash\", color=\"rgb(66, 139, 202)\")\n            ),\n            row=2,\n            col=1\n        )\n        fig.add_trace(\n            go.Scatter(\n                x=[x[0], q_alpha],\n                y=[alpha, alpha],\n                mode=\"lines\",\n                line=dict(dash=\"dash\", color=\"rgb(66, 139, 202)\")\n            ),\n            row=2,\n            col=2\n        )\n        fig.add_trace(\n            go.Scatter(\n                x=[x[0], q_alpha],\n                y=[x[0], 0],\n                mode=\"lines\",\n                line=dict(dash=\"dash\", color=\"rgb(66, 139, 202)\")\n            ),\n            row=2,\n            col=2\n        )\n        # Axes ranges\n        fig.update_xaxes(range=[0, 1.05], row=1, col=1)\n        fig.update_yaxes(matches=\"x6\", row=1, col=1)\n\n        fig.update_yaxes(range=[0, 1.05], row=2, col=1)\n        fig.update_xaxes(matches=\"x1\", row=2, col=1)\n\n        fig.update_yaxes(rangemode=\"tozero\", row=3, col=2)\n        fig.update_xaxes(range=[x[0], x[-1]], row=3, col=2)\n\n        fig.update_xaxes(matches=\"x6\", row=2, col=2)\n        fig.update_yaxes(matches=\"y3\", row=2, col=2)\n\n        # Add dropdown\n        fig.update_layout(\n            showlegend=False,\n            template=\"simple_white\",\n        )\n        return fig\n\n\napp = App(app_ui, server)"
  }
]