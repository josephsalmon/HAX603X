[
  {
    "objectID": "Courses/th_asymptotique.html",
    "href": "Courses/th_asymptotique.html",
    "title": "Th√©or√®mes asymptotiques",
    "section": "",
    "text": "Le premier r√©sultat fondamental en probabilit√©s concerne le comportement asymptotique de la moyenne empirique: \n\\bar X_n = \\dfrac{X_1 + \\cdots + X_n}{n} \\enspace.\n quand on obsever n variables al√©atoires i.i.d X_1,\\dots,X_n, ayant une esp√©rance finie.\n\nTh√©or√®me 1 (Loi forte des grands nombres) \nSoit (X_n)_{n \\geq 1} une suite de variables al√©atoires ind√©pendantes et identiquement distribu√©es (i.i.d.) dans L^1(\\Omega, \\mathcal{F}, \\mathbb{P}). Notons \\mu = \\mathbb{E}[X_1]. Alors \\bar X_n converge vers \\mu presque s√ªrement : \n\\mathbb{P}\\bigg( \\dfrac{X_1 + \\cdots + X_n}{n} \\underset{n \\to \\infty}{\\longrightarrow} \\mu \\bigg) = 1\\,.\n\n\nInterpr√©tation: Intuitivement, la probabilit√© d‚Äôun √©v√©nement A correspond √† la fr√©quence d‚Äôapparition de A quand on r√©p√®te une exp√©rience qui fait intervenir cet √©v√©nement. Par exemple, si on dispose une pi√®ce truqu√©e, on estimera la probabilit√© d‚Äôapparition du c√¥t√© pile en lan√ßant la pi√®ce un grand nombre de fois et en comptant le nombre de pile obtenu. La loi des grands nombres justifie a posteriori cette intuition : si X_1, \\ldots, X_n sont i.i.d. de loi de Bernoulli de param√®tre p, alors \n    \\dfrac{X_1 + \\cdots + X_n}{n} \\xrightarrow[n \\to \\infty]{p.s.} p \\enspace.\n Le membre de gauche correspond au nombre empirique de pile obtenu, celui de droite √† la valeur th√©orique.\nRemarque: Bien qu‚Äôintuitivement assez intuitif, ce th√©or√®me est difficile √† d√©montrer. XXX TODO referece : Williams (en anglais) ou bien Ouvrard.\n#| standalone: true\n#| viewerHeight: 700\nimport numpy as np\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\nfrom shiny import ui, render, App, reactive\nfrom shinywidgets import output_widget, render_widget\n\n# n_samples = 50\n# step = 10\napp_ui = ui.page_fluid(\n    ui.panel_title(\"Loi des grands nombres: visualisation\"),\n    ui.input_action_button(\"seed\", \"R√©-√©chantillonner\",class_=\"btn-primary\"),\n    output_widget(\"my_widget\"),\n    ui.row(\n        ui.column(6,\n            ui.input_slider(\"p\", \"Param√®tre de Bernoulli: p\", 0.01, 0.99, value=0.5, step=0.01)\n        ),\n        # ui.column(4, ui.input_slider(\"step\", \"n\", 1, n_samples, value=10)),\n        ui.column(6, ui.input_slider(\"n_samples\", \"Nombre de tirages al√©atoires \", 2, 1000, value=15, step=1)\n        )\n    )\n)\n\n\ndef server(input, output, session):\n    seed = reactive.Value(42)\n\n    @reactive.Effect\n    @reactive.event(input.seed)\n    def _():\n        seed.set(np.random.randint(0, 1000))\n\n\n    @output\n    @render_widget\n    def my_widget():\n        # Create figure\n        rng = np.random.default_rng(seed())\n        p = input.p()\n        # step=input.step()\n        n_samples = input.n_samples()\n        iterations = np.arange(1, n_samples + 1)\n        samples = rng.binomial(1, p, size=n_samples)\n        means_samples = np.cumsum(samples) / np.arange(1, n_samples + 1)\n\n        fig = make_subplots(\n                    rows=2,\n                    cols=1,\n                    vertical_spacing=0.3,\n                    horizontal_spacing=0.04,\n                    subplot_titles=(\n                        f\"Moyenne empirique en fonction du nombre de tirages &lt;br&gt;(loi de Bernoulli)\",\n                        \"Tirages al√©atoires &lt;span style='color:rgb(66, 139, 202)'&gt;bleu: 0&lt;/span&gt;, &lt;span style='color:rgb(255, 0, 0)'&gt;rouge: 1&lt;/span&gt; (seed=\"+ str(seed())+\")\",\n                    ),\n                    row_heights=[8, 1],\n                )\n\n        fig.add_trace(\n                go.Scatter(\n                    mode='lines',\n                    line=dict(color=\"black\", width=3),\n                    x=iterations,\n                    y=means_samples,\n                    name=r'Moyenne &lt;br&gt; empirique'),\n                    # name=r'$\\bar{X}_n$'),\n                    row=1, col=1,\n        )\n        fig.add_trace(\n                go.Scatter(\n                    mode='lines',\n                    line=dict(dash=\"dash\", color=\"black\", width=1),\n                    marker={},\n                    x=iterations,\n                    y=np.full((n_samples), p),\n                    # name=r'$p$'),\n                    name=r'p'),\n                    row=1, col=1,\n        )\n        fig.add_trace(\n                go.Heatmap(x=iterations + 0.5, z=[samples],\n                        # text=[list(samples.astype('str'))],\n                        # texttemplate=\"%{text}\",\n                        colorscale=[[0,'rgb(66, 139, 202)'], [1, 'rgb(255,0,0)']],\n                        showscale=False,\n                        # textfont={\"size\":10}\n                        ),\n                row=2, col=1,\n        )\n        fig.update_xaxes(range=[1, n_samples + 1])\n        fig.update_yaxes(range=[0, 1.1], row=1, col=1)\n        fig.update_xaxes(matches=\"x1\", row=2, col=1)\n        fig.update_yaxes(visible=False, row=2, col=1)\n        fig.update_xaxes(visible=False, row=2, col=1)\n\n\n        fig.update_layout(\n            template=\"simple_white\",\n            showlegend=True,\n        )\n        fig.update_layout(\n            legend=dict(\n            yanchor=\"top\",\n            y=0.99,\n            xanchor=\"left\",\n            x=0.75,\n            bgcolor='rgba(0,0,0,0)',\n            )\n        )\n        return fig\n\n\napp = App(app_ui, server)\n\nXXX TODO: ph√©nom√®ne int√©ressant en bougeant le param√®tre p avec le reste fix√©‚Ä¶les signaux g√©n√©r√©s sont tr√®s tr√®s proches, ce qui ne devrait pas √™tre le cas a priori, saut structuration particuli√®re de la g√©n√©ration."
  },
  {
    "objectID": "Courses/th_asymptotique.html#loi-des-grands-nombres",
    "href": "Courses/th_asymptotique.html#loi-des-grands-nombres",
    "title": "Th√©or√®mes asymptotiques",
    "section": "",
    "text": "Le premier r√©sultat fondamental en probabilit√©s concerne le comportement asymptotique de la moyenne empirique: \n\\bar X_n = \\dfrac{X_1 + \\cdots + X_n}{n} \\enspace.\n quand on obsever n variables al√©atoires i.i.d X_1,\\dots,X_n, ayant une esp√©rance finie.\n\nTh√©or√®me 1 (Loi forte des grands nombres) \nSoit (X_n)_{n \\geq 1} une suite de variables al√©atoires ind√©pendantes et identiquement distribu√©es (i.i.d.) dans L^1(\\Omega, \\mathcal{F}, \\mathbb{P}). Notons \\mu = \\mathbb{E}[X_1]. Alors \\bar X_n converge vers \\mu presque s√ªrement : \n\\mathbb{P}\\bigg( \\dfrac{X_1 + \\cdots + X_n}{n} \\underset{n \\to \\infty}{\\longrightarrow} \\mu \\bigg) = 1\\,.\n\n\nInterpr√©tation: Intuitivement, la probabilit√© d‚Äôun √©v√©nement A correspond √† la fr√©quence d‚Äôapparition de A quand on r√©p√®te une exp√©rience qui fait intervenir cet √©v√©nement. Par exemple, si on dispose une pi√®ce truqu√©e, on estimera la probabilit√© d‚Äôapparition du c√¥t√© pile en lan√ßant la pi√®ce un grand nombre de fois et en comptant le nombre de pile obtenu. La loi des grands nombres justifie a posteriori cette intuition : si X_1, \\ldots, X_n sont i.i.d. de loi de Bernoulli de param√®tre p, alors \n    \\dfrac{X_1 + \\cdots + X_n}{n} \\xrightarrow[n \\to \\infty]{p.s.} p \\enspace.\n Le membre de gauche correspond au nombre empirique de pile obtenu, celui de droite √† la valeur th√©orique.\nRemarque: Bien qu‚Äôintuitivement assez intuitif, ce th√©or√®me est difficile √† d√©montrer. XXX TODO referece : Williams (en anglais) ou bien Ouvrard.\n#| standalone: true\n#| viewerHeight: 700\nimport numpy as np\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\nfrom shiny import ui, render, App, reactive\nfrom shinywidgets import output_widget, render_widget\n\n# n_samples = 50\n# step = 10\napp_ui = ui.page_fluid(\n    ui.panel_title(\"Loi des grands nombres: visualisation\"),\n    ui.input_action_button(\"seed\", \"R√©-√©chantillonner\",class_=\"btn-primary\"),\n    output_widget(\"my_widget\"),\n    ui.row(\n        ui.column(6,\n            ui.input_slider(\"p\", \"Param√®tre de Bernoulli: p\", 0.01, 0.99, value=0.5, step=0.01)\n        ),\n        # ui.column(4, ui.input_slider(\"step\", \"n\", 1, n_samples, value=10)),\n        ui.column(6, ui.input_slider(\"n_samples\", \"Nombre de tirages al√©atoires \", 2, 1000, value=15, step=1)\n        )\n    )\n)\n\n\ndef server(input, output, session):\n    seed = reactive.Value(42)\n\n    @reactive.Effect\n    @reactive.event(input.seed)\n    def _():\n        seed.set(np.random.randint(0, 1000))\n\n\n    @output\n    @render_widget\n    def my_widget():\n        # Create figure\n        rng = np.random.default_rng(seed())\n        p = input.p()\n        # step=input.step()\n        n_samples = input.n_samples()\n        iterations = np.arange(1, n_samples + 1)\n        samples = rng.binomial(1, p, size=n_samples)\n        means_samples = np.cumsum(samples) / np.arange(1, n_samples + 1)\n\n        fig = make_subplots(\n                    rows=2,\n                    cols=1,\n                    vertical_spacing=0.3,\n                    horizontal_spacing=0.04,\n                    subplot_titles=(\n                        f\"Moyenne empirique en fonction du nombre de tirages &lt;br&gt;(loi de Bernoulli)\",\n                        \"Tirages al√©atoires &lt;span style='color:rgb(66, 139, 202)'&gt;bleu: 0&lt;/span&gt;, &lt;span style='color:rgb(255, 0, 0)'&gt;rouge: 1&lt;/span&gt; (seed=\"+ str(seed())+\")\",\n                    ),\n                    row_heights=[8, 1],\n                )\n\n        fig.add_trace(\n                go.Scatter(\n                    mode='lines',\n                    line=dict(color=\"black\", width=3),\n                    x=iterations,\n                    y=means_samples,\n                    name=r'Moyenne &lt;br&gt; empirique'),\n                    # name=r'$\\bar{X}_n$'),\n                    row=1, col=1,\n        )\n        fig.add_trace(\n                go.Scatter(\n                    mode='lines',\n                    line=dict(dash=\"dash\", color=\"black\", width=1),\n                    marker={},\n                    x=iterations,\n                    y=np.full((n_samples), p),\n                    # name=r'$p$'),\n                    name=r'p'),\n                    row=1, col=1,\n        )\n        fig.add_trace(\n                go.Heatmap(x=iterations + 0.5, z=[samples],\n                        # text=[list(samples.astype('str'))],\n                        # texttemplate=\"%{text}\",\n                        colorscale=[[0,'rgb(66, 139, 202)'], [1, 'rgb(255,0,0)']],\n                        showscale=False,\n                        # textfont={\"size\":10}\n                        ),\n                row=2, col=1,\n        )\n        fig.update_xaxes(range=[1, n_samples + 1])\n        fig.update_yaxes(range=[0, 1.1], row=1, col=1)\n        fig.update_xaxes(matches=\"x1\", row=2, col=1)\n        fig.update_yaxes(visible=False, row=2, col=1)\n        fig.update_xaxes(visible=False, row=2, col=1)\n\n\n        fig.update_layout(\n            template=\"simple_white\",\n            showlegend=True,\n        )\n        fig.update_layout(\n            legend=dict(\n            yanchor=\"top\",\n            y=0.99,\n            xanchor=\"left\",\n            x=0.75,\n            bgcolor='rgba(0,0,0,0)',\n            )\n        )\n        return fig\n\n\napp = App(app_ui, server)\n\nXXX TODO: ph√©nom√®ne int√©ressant en bougeant le param√®tre p avec le reste fix√©‚Ä¶les signaux g√©n√©r√©s sont tr√®s tr√®s proches, ce qui ne devrait pas √™tre le cas a priori, saut structuration particuli√®re de la g√©n√©ration."
  },
  {
    "objectID": "Courses/th_asymptotique.html#th√©or√®me-central-limite",
    "href": "Courses/th_asymptotique.html#th√©or√®me-central-limite",
    "title": "Th√©or√®mes asymptotiques",
    "section": "Th√©or√®me central limite",
    "text": "Th√©or√®me central limite\nUne fois la loi des grands nombres √©tablie, on peut se demander quel est l‚Äôordre suivant dans le d√©veloppement asymptotique de \\bar X_n - \\mu, ou de mani√®re √©quivalente de S_n - n \\mu, o√π S_n = X_1 + \\cdots + X_n. Le th√©or√®me suivant r√©pond √† cette question, en donnant une convergence en loi d‚Äôune transformation affine de la moyenne empirique:\n\nTh√©or√®me 2 (Th√©or√®me central limite) Soit X_1, \\ldots, X_n une suite de variables al√©atoires i.i.d de variance \\sigma^2 = {\\rm var}(X_1) \\in ]0, \\infty[. On note \\mu = \\mathbb{E}[X_1] leur esp√©rance. Alors \n\\sqrt n \\left(\\frac{\\bar X_n - \\mu}{\\sigma} \\right) \\xrightarrow[n \\to +\\infty]{\\mathcal{L}} N\\enspace,\n o√π N suit une loi normale centr√©e r√©duite : N \\sim\\mathcal{N}(0,1).\n\nPreuve XXX TODO: donner une r√©f√©rence.\nEn termes de somme cumul√©e empirique, la convergence se r√©√©crit\n\n    \\frac{S_n - n \\mu}{\\sqrt n \\sigma} \\xrightarrow[n \\to +\\infty]{\\mathcal{L}} N \\enspace.\n\nLes hypoth√®ses de ce th√©or√®me sont plut√¥t faibles (il suffit de supposer une variance finie). Pourtant, le r√©sultat est universel : la loi de d√©part peut √™tre aussi farfelue que l‚Äôon veut, elle se rapprochera toujours asymptotiquement d‚Äôune loi normale.\nOn rappelle que la convergence en loi est √©quivalente √† la convergence des fonctions de r√©partition en tout point de continuit√© de la limite. Ainsi, le th√©or√®me central limite se r√©√©crit de la mani√®re suivante : pour tout a &lt; b,\n\n    \\mathbb{P} \\bigg( a \\leq \\sqrt n \\left(\\frac{\\bar X_n - \\mu}{\\sigma} \\right) \\leq b\\bigg)\n    \\underset{n \\to \\infty}{\\longrightarrow}  \\dfrac{1}{\\sqrt{2\\pi}} \\int_a^b e^{-\\frac{x^2}{2}} \\, \\mathrm dx\\,.\n\n\nExemple 1 On consid√®re des variables al√©atoires X_1, \\ldots, X_n i.i.d. suivant une loi de Bernoulli de param√®tre p \\in ]0,1[, dont l‚Äôesp√©rance et la variance sont respectivemenbt p et p(1-p). Le th√©or√®me central limite donne alors \n    \\sqrt n \\left(\\frac{\\bar X_n - p}{p (1-p)} \\right) \\xrightarrow[n \\to +\\infty]{\\mathcal{L}} N\\,,\n avec N \\sim \\mathcal{N}(0,1). Cette convergence est illustr√©e en Figure XXX TODO image."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "HAX603X: Mod√©lisation stochastique",
    "section": "",
    "text": "Aspects num√©riques de la mod√©lisation al√©atoire et statistiques (cours de Licence 3).\nAttention: site en construction‚Ä¶\n\n\n\nJoseph Salmon: joseph.salmon@umontpellier.fr,\nBenjamin Charlier: benjamin.charlier@umontpellier.fr\n\nCe cours est issu du travail ant√©rieur de la part de:\n\nNicolas Meyer\nBeno√Æte de Saporta\n\n\n\n\nCours de mesure et int√©gration, analyse num√©rique‚Ä¶\n\n\n\n\n\n\n\n\n\n\n\nCCI\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(Data Science): Python Data Science Handbook, With Application to Understanding Data by J. Van DerPlas, 2016; videos: Reproducible Data Analysis in Jupyter"
  },
  {
    "objectID": "index.html#professeurs",
    "href": "index.html#professeurs",
    "title": "HAX603X: Mod√©lisation stochastique",
    "section": "",
    "text": "Joseph Salmon: joseph.salmon@umontpellier.fr,\nBenjamin Charlier: benjamin.charlier@umontpellier.fr\n\nCe cours est issu du travail ant√©rieur de la part de:\n\nNicolas Meyer\nBeno√Æte de Saporta"
  },
  {
    "objectID": "index.html#pr√©requis",
    "href": "index.html#pr√©requis",
    "title": "HAX603X: Mod√©lisation stochastique",
    "section": "",
    "text": "Cours de mesure et int√©gration, analyse num√©rique‚Ä¶"
  },
  {
    "objectID": "index.html#modalit√©-de-contr√¥le-des-connaissances",
    "href": "index.html#modalit√©-de-contr√¥le-des-connaissances",
    "title": "HAX603X: Mod√©lisation stochastique",
    "section": "",
    "text": "CCI"
  },
  {
    "objectID": "index.html#livres-et-ressources",
    "href": "index.html#livres-et-ressources",
    "title": "HAX603X: Mod√©lisation stochastique",
    "section": "",
    "text": "(Data Science): Python Data Science Handbook, With Application to Understanding Data by J. Van DerPlas, 2016; videos: Reproducible Data Analysis in Jupyter"
  },
  {
    "objectID": "Courses/notations.html",
    "href": "Courses/notations.html",
    "title": "Notations et rappels",
    "section": "",
    "text": "On consid√®re un espace probabilis√© (\\Omega, {\\mathcal{F}}, \\mathbb{P}), compos√© d‚Äôun ensemble \\Omega, d‚Äôune tribu \\mathcal{F}, et d‚Äôune mesure de probabilit√© \\mathbb{P}.\nCette d√©finition permet de transposer l‚Äôal√©a qui provient de \\Omega dans l‚Äôespace E. L‚Äôhypoth√®se \\{X \\in B\\} \\in \\mathcal{F} assure que cet ensemble est bien un √©v√®nement et donc que l‚Äôon peut calculer sa probabilit√©.\nUne fois que l‚Äôal√©a a √©t√© transpos√© de \\Omega vers E, on souhaite √©galement transposer la probabilit√© \\mathbb{P} sur E. Ceci motive l‚Äôintroduction de la notion de loi.\nLes propri√©t√©s de \\mathbb{P} assurent que \\mathbb{P}_X est bien une loi de probabilit√© sur l‚Äôespace mesurable (E, \\mathcal{E})."
  },
  {
    "objectID": "Courses/notations.html#loi-discr√®tes",
    "href": "Courses/notations.html#loi-discr√®tes",
    "title": "Notations et rappels",
    "section": "Loi discr√®tes",
    "text": "Loi discr√®tes\nLes variables al√©atoires discr√®tes sont celles √† valeurs dans un ensemble E discret, le plus souvent \\mathbb{N}, muni de la tribu pleine \\mathcal{F} = \\mathcal{P}(E).\n\nExemple 1 (Loi de Bernoulli) La loi la plus simple est la loi de Bernoulli de param√®tre p \\in [0,1], d√©finie sur \\{0,1\\} par \\mathbb{P}(X=1) = 1-\\mathbb{P}(X=0) = p qui mod√©lise une exp√©rience al√©atoire √† deux issues (succ√®s = 1 et √©chec = 0).\n\n\nExemple 2 (Loi binomiale) En sommant des variables al√©atoires ind√©pendantes de loi de Bernoulli on obtient une loi binomiale : \\mathbb{P}(X=k) = \\binom{n}{k} p^k (1-p)^{n-k}, pour k \\in \\{0,\\ldots,n\\}, qui mod√©lise le nombre de succ√®s parmi n lancers.\n\n\nExemple 3 (Loi g√©om√©trique) En observant le nombre d‚Äôexp√©riences n√©cessaires avant d‚Äôobtenir un succ√®s, on obtient une loi g√©om√©trique : \\mathbb{P}(X=k) = p (1-p)^{k-1}, pour k \\geq 1.\n\n\nExemple 4 (Loi de Poisson) La loi de Poisson de param√®tre \\lambda &gt; 0 est d√©finie par \\mathbb{P}(X=k) = e^{-\\lambda} \\lambda^k / k!, pour k \\in \\mathbb{N}, et mod√©lise les √©v√©nements rares."
  },
  {
    "objectID": "Courses/notations.html#lois-continues",
    "href": "Courses/notations.html#lois-continues",
    "title": "Notations et rappels",
    "section": "Lois continues",
    "text": "Lois continues\nParmi les variables al√©atoires r√©elles non discr√®tes, beaucoup peuvent se repr√©senter avec une densit√©, c‚Äôest-√†-dire qu‚Äôil existe une fonction mesurable f : \\mathbb{R} \\to [0, \\infty[ d‚Äôint√©grale 1. La loi d‚Äôune telle variable al√©atoire X est alors donn√©e pour tout A \\in \\mathcal{B}(\\mathbb{R}) par \n    \\mathbb{P}(X \\in A) = \\int_A f(x) \\, \\mathrm d x \\enspace.\n Les propri√©t√©s de l‚Äôint√©grale de Lebesgue assure que cette formule d√©finit bien une loi de probabilit√©.\n\nExemple 5 (Loi uniforme) La loi uniforme sur un ensemble B \\in \\mathcal{B}(\\mathbb{R}), s‚Äôobtient avec la densit√© d√©finie par \nf(x) = {1\\hspace{-3.8pt} 1}_B(x) / \\lambda (B) \\enspace,\n o√π \\lambda (B) repr√©sente la mesure de Lebesgue de l‚Äôensemble B. En particulier pour la loi uniforme sur le segment [0,1] on obtient la fonction suivante: \nf(x) = {1\\hspace{-3.8pt} 1}_{[0,1]}(x)\\enspace.\n Si une variable al√©atoire U suit une telle loi on note U \\sim \\mathcal{U}([0,1]).\n\n\nExemple 6 (Loi exponentielle) La loi exponentielle de param√®tre \\gamma &gt; 0 est obtenue avec la densit√© donn√©e par \nf(x) = \\gamma e^{-\\gamma x} {1\\hspace{-3.8pt} 1}_{\\mathbb{R}_+}(x)\\enspace.\n Si une variable al√©atoire X suit cette loi on note X \\sim \\mathcal{Exp}(\\gamma).\n\n\nExemple 7 (Loi normale/gaussienne univari√©e) On obtient la loi normale de param√®tre \\mu \\in \\mathbb{R} et \\sigma^2 &gt; 0 correspond √† loi dont la densit√© est donn√©e par la fonction r√©elle: \nf(x) = \\frac{1}{\\sqrt{2 \\pi} \\sigma}e^{-\\frac{1}{2 \\sigma^2}(x-\\mu)^2} \\enspace.\n Si une variable al√©atoire X suit une telle loi on note X \\sim \\mathcal{N}(\\mu,\\sigma^2), \\mu correspondant √† l‚Äôesp√©rance de la loi, et \\sigma^2 √† sa variance. On nomme loi normale centr√©e r√©duite le cas correspondant √† \\mu = 0 et \\sigma^2 = 1.\n\n\nExemple 8 (Loi normale multivari√©e) On peut √©tendre les lois normales au cas multi-dimensionnel. Fixons d\\in\\mathbb{N}^* un entier non nul. Pour un vecteur \\mu \\in \\mathbb{R}^d et une matrice sym√©trique-d√©finie positive \\Sigma\\in \\mathbb{R^{d\\times d}}, la densit√© normale mutlivari√©e associ√©e est donn√©e par la fonction: \nf(x) = \\frac{1}{{(2 \\pi)}^{\\frac{d}{2}} {\\rm det}(\\Sigma)} e^{-\\frac{1}{2}(x-\\mu)^\\top \\Sigma ^{-1}(x-\\mu)}\n Notons que \\mu est l‚Äôesp√©rance de la loi et \\Sigma la matrice de variance-covariance."
  },
  {
    "objectID": "Courses/notations.html#fonction-de-r√©partition",
    "href": "Courses/notations.html#fonction-de-r√©partition",
    "title": "Notations et rappels",
    "section": "Fonction de r√©partition",
    "text": "Fonction de r√©partition\nLa notion de variable al√©atoire n‚Äôest pas facile √† manipuler puisqu‚Äôelle part d‚Äôun espace \\Omega dont on ne sait rien. On souhaite donc caract√©riser la loi d‚Äôune variable al√©atoire en ne consid√©rant que l‚Äôespace d‚Äôarriv√©e (E, \\mathcal{E}) .\nPlusieurs outils existent : la fonction de r√©partition (pour des variables al√©atoires r√©elles), la fonction caract√©ristique (pour des variables al√©atoires dans \\mathbb{R}^d), la fonction g√©n√©ratrice des moments (pour des variables al√©atoires discr√®tes), etc. On se contente ici de la fonction de r√©partition qui nous sera utile pour simuler des variables al√©atoires, ainsi que son inverse au sens de Levy.\n\nD√©finition 3 (Fonction de r√©partition üá¨üáß: cumulative distribution function) Soit X une variable al√©atoire sur (\\mathbb{R}, \\mathcal{B}(\\mathbb{R})). La fonction de r√©partition de X est la fonction F_X d√©finie sur \\mathbb{R} par \n    F_X(x) = \\mathbb{P}(X \\leq x) = \\mathbb{P}(X \\in ]-\\infty, x]) \\enspace.\n\n\nOn appelle quantile d‚Äôordre p\\in (0,1), la quantit√© F_X^\\leftarrow(p). La m√©diane est √©gale √† F_X^\\leftarrow(1/2), les premiers et troisi√®mes quartiles sont √©gaux √† F_X^\\leftarrow(1/4) et F_X^\\leftarrow(3/4). Enfin, les d√©ciles sont les quantiles F_X^\\leftarrow(k/10) pour k=1,\\dots, 9.\n\nExemple 9 (Cas discret) Soit (x_i)_{i \\in I} une suite ordonn√©e de r√©els, avec I \\subset \\mathbb{N}. Si X est une variable al√©atoire discr√®te prenant les valeurs (x_i)_{i \\in I} et de loi (p_i = \\mathbb{P}(X=x_i))_{i \\in I}, alors \nF_X(x) = \\sum_{i \\in I} p_i {1\\hspace{-3.8pt} 1}_{[x_i, \\infty[}(x) \\enspace.\n\n\n\nExemple 10 (Cas continu) Si X est une variable al√©atoire de densit√© f, alors \n    F_X(x) = \\int_{-\\infty}^x f(t) \\, \\mathrm dt \\enspace.\n\n\nLe graphe des fonctions de r√©partition des loi de Bernoulli, uniforme et normale sont repr√©sent√©es en Figure XXX. Notons que la fonction de r√©partition de la loi normale \\mathcal{N}(0,1), souvent not√©e \\Phi, n‚Äôadmet pas d‚Äôexpression explicite autre que \n\\Phi(x) = \\dfrac{1}{\\sqrt{2\\pi}} \\int_{-\\infty}^x e^{-\\frac{x^2}{2}}\\, \\mathrm d x\\enspace,\n Les valeurs num√©riques de \\Phi(x) √©taient autrefois report√©es dans des tables1.Par transformation affine, si X \\sim \\mathcal{N}(\\mu, \\sigma^2) ‚Äî ce que l‚Äôon peut aussi √©crire : X=\\mu + \\sigma Y, avec Y\\sim \\mathcal{N}(0,1) ‚Äî alors sa fonction de r√©partition est donn√©e par F_X(x)=\\Phi((x-\\mu)/\\sigma).1¬†Wikipedia: loi normale\n\nProposition 1 Soit X une variable al√©atoire de fonction de r√©partition F_X.\n\nF_X est une fonction croissante, de limite 0 en -\\infty et de limite 1 en +\\infty.\nF_X est continue √† droite en tout point.\nPour tout x \\in \\mathbb{R}, on a \\mathbb{P}(X=x) = F_X(x) - F_X(x-), o√π F_X(x-) = \\lim_{\\epsilon \\to 0+} F_X(x- \\epsilon).\nSi X a pour densit√© f, alors F_X est d√©rivable \\lambda-presque partout de d√©riv√©e f.\n\n\nLa propri√©t√© 3. est utile dans le cas discret : les valeurs prises par X correspondent aux points de discontinuit√© de F_X et les probabilit√©s associ√©es correspondent √† la hauteur du saut.\nLa propri√©t√© 4. donne le lien entre la fonction de r√©partition d‚Äôune variable al√©atoire √† densit√© et sa densit√©. On peut donc retrouver la loi de X √† partir de sa fonction de r√©partition. Le th√©or√®me suivant g√©n√©ralise ce r√©sultat √† toute variable al√©atoire r√©elle (pas n√©cessairement discr√®te ou √† densit√©).\n\nTh√©or√®me 1 La fonction de r√©partition d‚Äôune variable al√©atoire caract√©rise sa loi : deux variables al√©atoires ont m√™me loi si et seulement si elles ont m√™me fonction de r√©partition.\n\nXXX source + proof???\nOn rappelle que la tribu des bor√©liens est engendr√©e par la famille d‚Äôensembles \\{]-\\infty,x], x \\in \\mathbb{R}\\}. Le th√©or√®me pr√©c√©dent assure que si on conna√Æt la mesure \\mathbb{P}_X sur cette famille d‚Äôensemble alors on la conna√Æt partout.\nXXX TODO: move this in correct part.\n\nExemple 11 (Loi exponentielle depuis une loi uniforme) On consid√®re une variable al√©atoire U de loi uniforme sur [0,1] et on pose X = -\\ln(1-U). D√©terminons la loi de X en calculant sa fonction de r√©partition. Pour tout x \\in \\mathbb{R}, \n\\begin{align*}\nF_X(x) = & \\mathbb{P}(X \\leq x) \\\\\n       = & \\mathbb{P}(-\\ln(1-U) \\leq x) \\\\\n       = & \\mathbb{P}(U \\leq 1-e^{-x}) \\\\\n       = &\n    \\begin{cases}\n        0           & \\text{ si }x &lt; 0\\,,    \\\\\n        1 - e^{-x} & \\text{ si }x \\geq 0\\,,\n    \\end{cases}\n\\end{align*}\n\no√π on a utilis√© l‚Äô√©galit√© \\mathbb{P}(U \\leq t) = t pour tout t \\in [0,1]. Ainsi la variable al√©atoire X a la m√™me fonction de r√©partition qu‚Äôune loi exponentielle de param√®tre 1. On en conclut que X \\sim \\mathcal{Exp}(1). Notons que l‚Äôon peut aussi montrer que -\\ln(X)\\sim\\mathcal{E}(1), sachant que U et 1-U ont la m√™me loi."
  },
  {
    "objectID": "Courses/notations.html#fonction-quantile-inverse-g√©n√©ralis√©e-√†-gauche",
    "href": "Courses/notations.html#fonction-quantile-inverse-g√©n√©ralis√©e-√†-gauche",
    "title": "Notations et rappels",
    "section": "Fonction quantile, inverse g√©n√©ralis√©e √† gauche",
    "text": "Fonction quantile, inverse g√©n√©ralis√©e √† gauche\nLa fonction de r√©partition √©tant une fonction croissante on peut donner un sens √† son inverse g√©n√©ralis√©e de la mani√®re suivante.\n\nD√©finition 4 (Fonction quantile/inverse de Levy üá¨üáß: quantile distribution function) Soit X une variable al√©atoire sur (\\mathbb{R}, \\mathcal{B}(\\mathbb{R})). et F_X sa fonction de r√©partion. La fonction quantile associ√©e F_X^\\leftarrow: ]0,1[\\rightarrow \\mathbb{R} est d√©finie par \n  F_n^\\leftarrow(p)=  \\inf\\{ x\\colon F(x)\\geq p\\} \\enspace.\n\n\nDans le cas o√π la fonction de r√©partition F est bijective, alors l‚Äôinverse de la fonction de r√©partition coincide avec la fonction quantile."
  },
  {
    "objectID": "Courses/notations.html#visualisation-densit√©-fonction-de-r√©partition-quantiles-etc.",
    "href": "Courses/notations.html#visualisation-densit√©-fonction-de-r√©partition-quantiles-etc.",
    "title": "Notations et rappels",
    "section": "Visualisation: densit√©, fonction de r√©partition, quantiles, etc.",
    "text": "Visualisation: densit√©, fonction de r√©partition, quantiles, etc.\n\nCas des variables continues\n#| standalone: true\n#| viewerHeight: 830\nimport numpy as np\nfrom scipy import stats\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nfrom shiny import ui, render, App\nfrom shinywidgets import output_widget, render_widget\n\n\ndef keep_no_param_distribution():\n    distributions = stats._continuous_distns._distn_names\n    distributions_0 = []\n    for _, name in enumerate(distributions):\n        dist = getattr(stats, name)\n        if not dist.shapes or len(dist.shapes) == 0:\n            distributions_0.append(name)\n    distributions_0_val = [\n        getattr(stats.distributions, string) for string in distributions_0\n    ]\n    distributions_0_dict = dict(zip(distributions_0, distributions_0_val))\n    return distributions_0_dict\n\n\ndistributions_0_dict = keep_no_param_distribution()\n\nmu = 0\nsigma = 1\n\napp_ui = ui.page_fluid(\n    ui.div(\n        ui.input_slider(\"alpha\", \"Quantile\", 0.01, 0.99, value=0.5, step=0.01),\n        ui.input_slider(\"xrange\", \"x-range\", -10, 10, value=(-5, 5), step=0.2),\n        ui.input_select(\n            \"distrib\",\n            \"Distribution\",\n            list(distributions_0_dict.keys()),\n            selected='norm'\n        ),\n        class_=\"d-flex gap-3\",\n    ),\n    output_widget(\"my_widget\"),\n)\n\n\ndef server(input, output, session):\n    @output\n    @render_widget\n    def my_widget():\n        fig = make_subplots(\n            rows=3,\n            cols=2,\n            vertical_spacing=0.1,\n            horizontal_spacing=0.15,\n            subplot_titles=(\n                \"Fonction quantile\",\n                \"\",\n                \"\",\n                \"Fonction de r√©partition\",\n                \"\",\n                \"Densit√© et quantile\",\n            ),\n            column_widths=[0.2, 0.5],\n            row_heights=[0.35, 0.17, 0.17],\n        )\n\n        alpha = input.alpha()\n        distribution = distributions_0_dict[input.distrib()]\n        x = np.linspace(input.xrange()[0], input.xrange()[1], num=400)\n        cdf_data = distribution.cdf(x, loc=mu, scale=sigma)\n        pdf_data = distribution.pdf(x, loc=mu, scale=sigma)\n        q_alpha = distribution.ppf(alpha, loc=mu, scale=sigma)\n\n        fig.update_layout(autosize=True, height=700)\n\n        # Quantile plot\n        fig.add_trace(\n            go.Scatter(\n                x=cdf_data, y=x, mode=\"lines\", marker={\"color\": \"black\"}\n            ),\n            row=1,\n            col=1,\n        )\n        # Diagonal\n        fig.add_trace(\n            go.Scatter(\n                x=cdf_data, y=cdf_data, mode=\"lines\", marker={\"color\": \"black\"}\n            ),\n            row=2,\n            col=1,\n        )\n        # Cdf part\n        fig.add_trace(\n            go.Scatter(\n                x=x, y=cdf_data, mode=\"lines\", marker={\"color\": \"black\"}\n            ),\n            row=2,\n            col=2,\n        )\n        # pdf part\n        fig.add_scatter(\n            x=x[x &lt; q_alpha],\n            y=pdf_data[x &lt; q_alpha],\n            fill=\"tozeroy\",\n            mode=\"none\",\n            fillcolor=\"rgb(66, 139, 202)\",\n            row=3,\n            col=2,\n        )\n\n        fig.add_trace(\n            go.Scatter(\n                x=x, y=pdf_data, mode=\"lines\", marker={\"color\": \"black\"}\n            ),\n            row=3,\n            col=2,\n        )\n\n        # Dots\n        fig.add_trace(\n            go.Scatter(\n                x=[alpha],\n                y=[q_alpha],\n                mode=\"markers\",\n                marker={\"color\": \"rgb(66, 139, 202)\"},\n                marker_symbol=\"circle\",\n                marker_size=15,\n            ),\n            row=1,\n            col=1,\n        )\n\n        fig.add_trace(\n            go.Scatter(\n                x=[alpha],\n                y=[alpha],\n                mode=\"markers\",\n                marker={\"color\": \"rgb(66, 139, 202)\"},\n                marker_symbol=\"circle\",\n                marker_size=15,\n            ),\n            row=2,\n            col=1,\n        )\n\n        fig.add_trace(\n            go.Scatter(\n                x=[q_alpha],\n                y=[alpha],\n                mode=\"markers\",\n                marker={\"color\": \"rgb(66, 139, 202)\"},\n                marker_symbol=\"circle\",\n                marker_size=15,\n            ),\n            row=2,\n            col=2,\n        )\n\n        # Lines\n        fig.add_trace(\n            go.Scatter(\n                x=[alpha, alpha],\n                y=[x[0], q_alpha],\n                mode=\"lines\",\n                line=dict(dash=\"dash\", color=\"rgb(66, 139, 202)\")\n            ),\n            row=1,\n            col=1\n        )\n\n        fig.add_trace(\n            go.Scatter(\n                x=[alpha, alpha],\n                y=[alpha, 1.],\n                mode=\"lines\",\n                line=dict(dash=\"dash\", color=\"rgb(66, 139, 202)\")\n            ),\n            row=2,\n            col=1\n        )\n        fig.add_trace(\n            go.Scatter(\n                x=[alpha, 1],\n                y=[alpha, alpha],\n                mode=\"lines\",\n                line=dict(dash=\"dash\", color=\"rgb(66, 139, 202)\")\n            ),\n            row=2,\n            col=1\n        )\n        fig.add_trace(\n            go.Scatter(\n                x=[x[0], q_alpha],\n                y=[alpha, alpha],\n                mode=\"lines\",\n                line=dict(dash=\"dash\", color=\"rgb(66, 139, 202)\")\n            ),\n            row=2,\n            col=2\n        )\n        fig.add_trace(\n            go.Scatter(\n                x=[x[0], q_alpha],\n                y=[x[0], 0],\n                mode=\"lines\",\n                line=dict(dash=\"dash\", color=\"rgb(66, 139, 202)\")\n            ),\n            row=2,\n            col=2\n        )\n        # Axes ranges\n        fig.update_xaxes(range=[0, 1.], row=1, col=1)\n        fig.update_yaxes(matches=\"x6\", row=1, col=1)\n\n        fig.update_yaxes(range=[0, 1.], row=2, col=1)\n        fig.update_xaxes(matches=\"x1\", row=2, col=1)\n\n        fig.update_yaxes(rangemode=\"tozero\", row=3, col=2)\n        fig.update_xaxes(range=[x[0], x[-1]], row=3, col=2)\n\n        fig.update_xaxes(matches=\"x6\", row=2, col=2)\n        fig.update_yaxes(matches=\"y3\", row=2, col=2)\n\n        # Add dropdown\n        fig.update_layout(\n            showlegend=False,\n            template=\"simple_white\",\n        )\n        return fig\n\n\napp = App(app_ui, server)\n\n\n\nCas des variables discr√®tes\n#| standalone: true\n#| viewerHeight: 830\nimport numpy as np\nfrom scipy import stats\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nfrom shiny import ui, render, App\nfrom shinywidgets import output_widget, render_widget\n\n\ndef keep_no_param_distribution_disc():\n    distributions = stats._discrete_distns._distn_names\n    distributions_0 = []\n    for i, name in enumerate(distributions):\n        dist = getattr(stats, name)\n        if not dist.shapes or len(dist.shapes)==2 or len(dist.shapes)==1:\n            distributions_0.append(name)\n    distributions_0_val = [getattr(stats.distributions, string) for string in distributions_0 ]\n    distributions_0_dict = dict(zip(distributions_0, distributions_0_val))\n    return distributions_0_dict\n\ndef cdf_tool(x, dtype='int64'):\n    y = np.zeros(2*(len(x)), dtype=dtype)\n    y[::2]=x\n    y[1::2]=x\n    return y[1::], y[:-1], y\n\ndef pmf_tool(x, dtype='int64'):\n    y = np.zeros(2*(len(x)), dtype=dtype)\n    y[::2]=x\n    return y[1::], y[:-1], y\n\ndef insert_nones(my_list):\n    for i, val in enumerate(my_list):\n        if i % 3 == 2:\n            my_list.insert(i, None)\n    return my_list\n\ndistributions_0_dict = keep_no_param_distribution_disc()\n\napp_ui = ui.page_fluid(\n    ui.div(\n        ui.input_slider(\"alpha\", \"Quantile\", 0.01, 0.99, value=0.5, step=0.01),\n        ui.input_slider(\"xrange\", \"x-range\", -10, 10, value=(-5.5, 5.5), step=0.2),\n        ui.input_select(\n            \"distrib\",\n            \"Distribution\",\n            list(distributions_0_dict.keys()),\n            selected='poisson'\n        ),\n        class_=\"d-flex gap-3\",\n    ),\n    output_widget(\"my_widget\"),\n)\n\n\ndef server(input, output, session):\n    @output\n    @render_widget\n    def my_widget():\n        fig = make_subplots(\n            rows=3,\n            cols=2,\n            vertical_spacing=0.1,\n            horizontal_spacing=0.15,\n            subplot_titles=(\n                \"Fonction quantile\",\n                \"\",\n                \"\",\n                \"Fonction de r√©partition\",\n                \"\",\n                \"Fonction de masse et quantile\",\n            ),\n            column_widths=[0.2, 0.5],\n            row_heights=[0.35, 0.17, 0.17],\n        )\n\n\n        alpha = input.alpha()\n        # alpha=0.5\n\n        mu = 0.5  # Param needed for some distribution\n        if input.distrib()=='zipf':\n            mu = 2\n        distribution = distributions_0_dict[input.distrib()]\n        # distribution=distributions_0_dict['poisson']\n        x = np.arange(np.floor(input.xrange()[0]), np.ceil(input.xrange()[1]))\n        # x = np.arange(np.floor(-5.5), np.ceil(5.5))\n\n        cdf_data = distribution.cdf(x, mu)\n        pmf_data = distribution.pmf(x, mu)\n        q_alpha = distribution.ppf(alpha, mu)\n        support = pmf_data.nonzero()[0]\n        fig.update_layout(autosize=True, height=700)\n\n        # Quantile plot\n        new_x, new_y, new_z = cdf_tool(support)\n        _, _, new_pmf = pmf_tool(support)\n\n        fig.add_trace(\n            go.Scatter(\n                x=insert_nones(list(np.append(cdf_data[new_y[::-1]], distribution.cdf(x[0], mu)))),\n                y=insert_nones(list(np.append(x[new_x[::-1]], x[new_x[0]]))),\n                mode=\"lines\",\n                line=dict(color=\"black\")\n            ),\n            row=1,\n            col=1,\n        )\n        fig.add_trace(\n             go.Scatter(\n                x=cdf_data[support], y=x[support],\n                mode=\"markers\", marker={\"color\": \"black\"}\n            ),\n            row=1,\n            col=1,\n        )\n        # Diagonal\n        fig.add_trace(\n            go.Scatter(\n                x=cdf_data, y=cdf_data, mode=\"lines\", marker={\"color\": \"black\"}\n            ),\n            row=2,\n            col=1,\n        )\n        # Cdf part\n        fig.add_trace(\n            go.Scatter(\n                x=x[support], y=cdf_data[support],\n                mode=\"markers\", marker={\"color\": \"black\"}\n            ),\n            row=2,\n            col=2,\n        )\n        fig.add_trace(\n            go.Scatter(\n                x=insert_nones(list(np.append(np.insert(x[new_x], 0, [x[0], x[new_x[0]]]),x[-1]))),\n                y=insert_nones(list(np.append(np.insert(cdf_data[new_y], 0, [0,0]), cdf_data[-1]))),\n                mode=\"lines\",\n                line=dict(color=\"black\")\n            ),\n            row=2,\n            col=2\n        )\n\n        fig.add_trace(\n            go.Scatter(\n                x=x, y=pmf_data, mode=\"markers\", marker={\"color\": \"black\"}\n            ),\n            row=3,\n            col=2,\n        )\n        fig.add_trace(\n            go.Scatter(\n                x=x, y=pmf_data, mode=\"markers\", marker={\"color\": \"black\"}\n            ),\n            row=3,\n            col=2,\n        )\n        x_bar = insert_nones(list(x[new_z]))\n        y_bar = insert_nones(list(pmf_data[new_pmf]))\n        fig.add_trace(\n            go.Scatter(\n                x=x_bar,\n                y=y_bar,\n                mode=\"lines\",\n                line=dict(color=\"black\")\n            ),\n            row=3,\n            col=2\n        )\n        _,_, devil_x = cdf_tool(x[x&lt;=q_alpha])\n        _,_, devil_y = cdf_tool(pmf_data[x&lt;q_alpha], dtype='float64')\n\n        x_bar_blue = insert_nones(list(devil_x))\n        y_bar_blue = np.array(insert_nones(list(devil_y)))\n        y_bar_blue[::-3]=0.\n        y_bar_blue = list(y_bar_blue)\n        fig.add_trace(\n            go.Scatter(\n                x=x_bar_blue,\n                y=y_bar_blue,\n                mode=\"lines\",\n                line=dict(color=\"rgb(66, 139, 202)\")\n            ),\n            row=3,\n            col=2\n        )\n        # pdf part\n        fig.add_scatter(\n            x=x[x &lt;= q_alpha],\n            y=pmf_data[x &lt;= q_alpha],\n            mode=\"markers\",\n            marker={\"color\":\"rgb(66, 139, 202)\"},\n            row=3,\n            col=2,\n        )\n\n        # Dots\n        fig.add_trace(\n            go.Scatter(\n                x=[alpha],\n                y=[q_alpha],\n                mode=\"markers\",\n                marker={\"color\": \"rgb(66, 139, 202)\"},\n                marker_symbol=\"circle\",\n                marker_size=15,\n            ),\n            row=1,\n            col=1,\n        )\n\n        fig.add_trace(\n            go.Scatter(\n                x=[alpha],\n                y=[alpha],\n                mode=\"markers\",\n                marker={\"color\": \"rgb(66, 139, 202)\"},\n                marker_symbol=\"circle\",\n                marker_size=15,\n            ),\n            row=2,\n            col=1,\n        )\n\n        fig.add_trace(\n            go.Scatter(\n                x=[q_alpha],\n                y=[alpha],\n                mode=\"markers\",\n                marker={\"color\": \"rgb(66, 139, 202)\"},\n                marker_symbol=\"circle\",\n                marker_size=15,\n            ),\n            row=2,\n            col=2,\n        )\n\n        # Lines\n        fig.add_trace(\n            go.Scatter(\n                x=[alpha, alpha],\n                y=[x[0], q_alpha],\n                mode=\"lines\",\n                line=dict(dash=\"dash\", color=\"rgb(66, 139, 202)\")\n            ),\n            row=1,\n            col=1\n        )\n\n        fig.add_trace(\n            go.Scatter(\n                x=[alpha, alpha],\n                y=[alpha, 1.],\n                mode=\"lines\",\n                line=dict(dash=\"dash\", color=\"rgb(66, 139, 202)\")\n            ),\n            row=2,\n            col=1\n        )\n        fig.add_trace(\n            go.Scatter(\n                x=[alpha, 1],\n                y=[alpha, alpha],\n                mode=\"lines\",\n                line=dict(dash=\"dash\", color=\"rgb(66, 139, 202)\")\n            ),\n            row=2,\n            col=1\n        )\n        fig.add_trace(\n            go.Scatter(\n                x=[x[0], q_alpha],\n                y=[alpha, alpha],\n                mode=\"lines\",\n                line=dict(dash=\"dash\", color=\"rgb(66, 139, 202)\")\n            ),\n            row=2,\n            col=2\n        )\n        fig.add_trace(\n            go.Scatter(\n                x=[x[0], q_alpha],\n                y=[x[0], 0],\n                mode=\"lines\",\n                line=dict(dash=\"dash\", color=\"rgb(66, 139, 202)\")\n            ),\n            row=2,\n            col=2\n        )\n        # Axes ranges\n        fig.update_xaxes(range=[0, 1.05], row=1, col=1)\n        fig.update_yaxes(matches=\"x6\", row=1, col=1)\n\n        fig.update_yaxes(range=[0, 1.05], row=2, col=1)\n        fig.update_xaxes(matches=\"x1\", row=2, col=1)\n\n        fig.update_yaxes(rangemode=\"tozero\", row=3, col=2)\n        fig.update_xaxes(range=[x[0], x[-1]], row=3, col=2)\n\n        fig.update_xaxes(matches=\"x6\", row=2, col=2)\n        fig.update_yaxes(matches=\"y3\", row=2, col=2)\n\n        # Add dropdown\n        fig.update_layout(\n            showlegend=False,\n            template=\"simple_white\",\n        )\n        return fig\n\n\napp = App(app_ui, server)"
  }
]