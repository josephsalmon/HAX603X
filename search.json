[
  {
    "objectID": "trash/Slides/cours_intro.html",
    "href": "trash/Slides/cours_intro.html",
    "title": "HAX603X: Modélisation stochastique",
    "section": "",
    "text": "Retour au sommet"
  },
  {
    "objectID": "TP/TP1.html#test2",
    "href": "TP/TP1.html#test2",
    "title": "TP1:…",
    "section": "test2",
    "text": "test2",
    "crumbs": [
      "TP",
      "TP1:..."
    ]
  },
  {
    "objectID": "Courses/th_asymptotique.html",
    "href": "Courses/th_asymptotique.html",
    "title": "Théorèmes asymptotiques",
    "section": "",
    "text": "Le premier résultat fondamental en probabilités concerne le comportement asymptotique de la moyenne empirique: \n\\bar X_n = \\dfrac{X_1 + \\cdots + X_n}{n} \\enspace.\n quand on obsever n variables aléatoires i.i.d X_1,\\dots,X_n, ayant une espérance finie.\n\nThéorème 1 (Loi forte des grands nombres) \nSoit (X_n)_{n \\geq 1} une suite de variables aléatoires indépendantes et identiquement distribuées (i.i.d.) dans L^1(\\Omega, \\mathcal{F}, \\mathbb{P}). Notons \\mu = \\mathbb{E}[X_1]. Alors \\bar X_n converge vers \\mu presque sûrement : \n\\mathbb{P}\\bigg( \\dfrac{X_1 + \\cdots + X_n}{n} \\underset{n \\to \\infty}{\\longrightarrow} \\mu \\bigg) = 1\\,.\n\n\nInterprétation: Intuitivement, la probabilité d’un événement A correspond à la fréquence d’apparition de A quand on répète une expérience qui fait intervenir cet événement. Par exemple, si on dispose une pièce truquée, on estimera la probabilité d’apparition du côté pile en lançant la pièce un grand nombre de fois et en comptant le nombre de pile obtenu. La loi des grands nombres justifie a posteriori cette intuition : si X_1, \\ldots, X_n sont i.i.d. de loi de Bernoulli de paramètre p, alors \n    \\dfrac{X_1 + \\cdots + X_n}{n} \\xrightarrow[n \\to \\infty]{p.s.} p \\enspace.\n Le membre de gauche correspond au nombre empirique de pile obtenu, celui de droite à la valeur théorique.\nRemarque: Bien qu’assez intuitif, ce théorème est difficile à démontrer. XXX TODO reference : Williams (en anglais) ou bien Ouvrard.\n#| standalone: true\n#| viewerHeight: 550\nimport numpy as np\nfrom shiny import ui, render, App, reactive\nfrom shinywidgets import output_widget, register_widget\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objs as go\n\n\nn_init = 42\n\n\napp_ui = ui.page_fluid(\n    ui.tags.head(\n        ui.tags.style(\"\"\"\n            .bslib-sidebar-layout &gt; .sidebar &gt; .sidebar-content {\n            display: flex;\n            flex-direction: column;\n            padding: 0.5rem;\n            }\n            .irs--shiny .irs-line {\n            top: 27px;\n            height: 4px;\n            }\n            .irs.irs--shiny .irs-bar {\n            top: 27px;\n            height: 4px;\n            }\n            .irs--shiny .irs-handle {\n            top: 23px;\n            width: 22px;\n            height: 10px;\n        \"\"\")\n    ),\n    ui.panel_title(\"Loi des grands nombres\"),\n    ui.layout_sidebar(\n        ui.panel_sidebar(\n            ui.input_action_button(\"seed\", \"Nouveau tirage\", class_=\"btn-primary\"),\n            ui.input_slider( \"p\", \"Espérance: p\", 0.01, 0.99, value=0.35, step=0.01, ticks=False,\n            ),\n            ui.input_slider( \"n_samples\", \"Échantillons: n\", 2, 1000, value=15, step=1, ticks=False),\n        width=3),\n    ui.panel_main(output_widget(\"my_widget\"), width = 9)\n    )\n)\n\n\n\n\ndef server(input, output, session):\n    seed = reactive.Value(42)\n\n    @reactive.Effect\n    @reactive.event(input.seed)\n    def _():\n        seed.set(np.random.randint(0, 1000))\n\n    subplots = make_subplots(\n        rows=2,\n        cols=1,\n        vertical_spacing=0.45,\n        horizontal_spacing=0.04,\n        row_heights=[5, 1],\n        subplot_titles=(\n            f\"Moyenne empirique: loi de Bernoulli\",\n            \"Tirages aléatoires &lt;span style='color:rgb(66, 139, 202)'&gt;bleu: 0&lt;/span&gt;, &lt;span style='color:rgb(255, 0, 0)'&gt;rouge: 1&lt;/span&gt; (seed=\"\n            + f\"{n_init:03}\"\n            + \")\",\n        ),\n    )\n    fig = go.FigureWidget(subplots)\n    fig.add_trace(\n        go.Scatter(\n            mode=\"lines\",\n            line=dict(color=\"black\", width=3),\n            x=[],\n            y=[],\n            name=r\"Moyenne &lt;br&gt; empirique\",\n        ),\n        row=1,\n        col=1,\n    )\n    fig.add_trace(\n        go.Scatter(\n            mode=\"lines\",\n            line=dict(dash=\"dash\", color=\"black\", width=1),\n            marker={},\n            x=[],\n            y=[],\n            name=r\"p\",\n        ),\n        row=1,\n        col=1,\n    )\n    fig.add_trace(\n        go.Heatmap(\n            x=[],\n            z=[],\n            colorscale=[[0, \"rgb(66, 139, 202)\"], [1, \"rgb(255,0,0)\"]],\n            showscale=False,\n        ),\n        row=2,\n        col=1,\n    )\n\n    fig.update_yaxes(range=[0, 1.1], row=1, col=1)\n    fig.update_xaxes(matches=\"x1\", row=2, col=1)\n    fig.update_yaxes(visible=False, row=2, col=1)\n    fig.update_xaxes(visible=False, row=2, col=1)\n\n    fig.update_layout(\n        template=\"simple_white\",\n        showlegend=True,\n        xaxis_title=\"Échantillons: n\",\n    )\n    fig.update_layout(autosize=True)\n\n    fig.update_layout(\n        legend=dict(\n            yanchor=\"top\",\n            y=0.99,\n            xanchor=\"left\",\n            x=0.65,\n            bgcolor=\"rgba(0,0,0,0)\",\n        )\n    )\n    fig.update_layout(\n        margin=dict(l=0, r=0, b=10, t=70),\n    )\n\n    register_widget(\"my_widget\", fig)\n\n    @reactive.Effect\n    def _():\n        p = input.p()\n        n_samples = input.n_samples()\n\n        rng = np.random.default_rng(seed())\n        iterations = np.arange(1, n_samples + 1)\n        samples = rng.binomial(1, p, size=n_samples)\n        means_samples = np.cumsum(samples) / np.arange(1, n_samples + 1)\n\n        # Update data in fig:\n        fig.data[0].x = iterations\n        fig.data[0].y = means_samples\n\n        fig.data[1].x = iterations\n        fig.data[1].y = np.full((n_samples), p)\n\n        fig.data[2].x = iterations\n        fig.data[2].z = [samples]\n\n        fig.update_xaxes(range=[1, n_samples + 1])\n\n        # Update the subplot titles:\n        fig.layout.annotations[1].update(\n            text=f\"Tirages aléatoires (seed=\"\n            + f\"{seed():03}\"\n            + \") &lt;br&gt; &lt;span style='color:rgb(66, 139, 202)'&gt;bleu: 0&lt;/span&gt;, &lt;span style='color:rgb(255, 0, 0)'&gt;rouge: 1&lt;/span&gt; \"\n        )\n\n\napp = App(app_ui, server)\n\nXXX TODO: phénomène intéressant en bougeant le paramètre p avec le reste fixé…les signaux générés sont très très proches, ce qui ne devrait pas être le cas a priori, saut structuration particulière de la génération.",
    "crumbs": [
      "Cours",
      "Théorèmes asymptotiques"
    ]
  },
  {
    "objectID": "Courses/th_asymptotique.html#loi-des-grands-nombres",
    "href": "Courses/th_asymptotique.html#loi-des-grands-nombres",
    "title": "Théorèmes asymptotiques",
    "section": "",
    "text": "Le premier résultat fondamental en probabilités concerne le comportement asymptotique de la moyenne empirique: \n\\bar X_n = \\dfrac{X_1 + \\cdots + X_n}{n} \\enspace.\n quand on obsever n variables aléatoires i.i.d X_1,\\dots,X_n, ayant une espérance finie.\n\nThéorème 1 (Loi forte des grands nombres) \nSoit (X_n)_{n \\geq 1} une suite de variables aléatoires indépendantes et identiquement distribuées (i.i.d.) dans L^1(\\Omega, \\mathcal{F}, \\mathbb{P}). Notons \\mu = \\mathbb{E}[X_1]. Alors \\bar X_n converge vers \\mu presque sûrement : \n\\mathbb{P}\\bigg( \\dfrac{X_1 + \\cdots + X_n}{n} \\underset{n \\to \\infty}{\\longrightarrow} \\mu \\bigg) = 1\\,.\n\n\nInterprétation: Intuitivement, la probabilité d’un événement A correspond à la fréquence d’apparition de A quand on répète une expérience qui fait intervenir cet événement. Par exemple, si on dispose une pièce truquée, on estimera la probabilité d’apparition du côté pile en lançant la pièce un grand nombre de fois et en comptant le nombre de pile obtenu. La loi des grands nombres justifie a posteriori cette intuition : si X_1, \\ldots, X_n sont i.i.d. de loi de Bernoulli de paramètre p, alors \n    \\dfrac{X_1 + \\cdots + X_n}{n} \\xrightarrow[n \\to \\infty]{p.s.} p \\enspace.\n Le membre de gauche correspond au nombre empirique de pile obtenu, celui de droite à la valeur théorique.\nRemarque: Bien qu’assez intuitif, ce théorème est difficile à démontrer. XXX TODO reference : Williams (en anglais) ou bien Ouvrard.\n#| standalone: true\n#| viewerHeight: 550\nimport numpy as np\nfrom shiny import ui, render, App, reactive\nfrom shinywidgets import output_widget, register_widget\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objs as go\n\n\nn_init = 42\n\n\napp_ui = ui.page_fluid(\n    ui.tags.head(\n        ui.tags.style(\"\"\"\n            .bslib-sidebar-layout &gt; .sidebar &gt; .sidebar-content {\n            display: flex;\n            flex-direction: column;\n            padding: 0.5rem;\n            }\n            .irs--shiny .irs-line {\n            top: 27px;\n            height: 4px;\n            }\n            .irs.irs--shiny .irs-bar {\n            top: 27px;\n            height: 4px;\n            }\n            .irs--shiny .irs-handle {\n            top: 23px;\n            width: 22px;\n            height: 10px;\n        \"\"\")\n    ),\n    ui.panel_title(\"Loi des grands nombres\"),\n    ui.layout_sidebar(\n        ui.panel_sidebar(\n            ui.input_action_button(\"seed\", \"Nouveau tirage\", class_=\"btn-primary\"),\n            ui.input_slider( \"p\", \"Espérance: p\", 0.01, 0.99, value=0.35, step=0.01, ticks=False,\n            ),\n            ui.input_slider( \"n_samples\", \"Échantillons: n\", 2, 1000, value=15, step=1, ticks=False),\n        width=3),\n    ui.panel_main(output_widget(\"my_widget\"), width = 9)\n    )\n)\n\n\n\n\ndef server(input, output, session):\n    seed = reactive.Value(42)\n\n    @reactive.Effect\n    @reactive.event(input.seed)\n    def _():\n        seed.set(np.random.randint(0, 1000))\n\n    subplots = make_subplots(\n        rows=2,\n        cols=1,\n        vertical_spacing=0.45,\n        horizontal_spacing=0.04,\n        row_heights=[5, 1],\n        subplot_titles=(\n            f\"Moyenne empirique: loi de Bernoulli\",\n            \"Tirages aléatoires &lt;span style='color:rgb(66, 139, 202)'&gt;bleu: 0&lt;/span&gt;, &lt;span style='color:rgb(255, 0, 0)'&gt;rouge: 1&lt;/span&gt; (seed=\"\n            + f\"{n_init:03}\"\n            + \")\",\n        ),\n    )\n    fig = go.FigureWidget(subplots)\n    fig.add_trace(\n        go.Scatter(\n            mode=\"lines\",\n            line=dict(color=\"black\", width=3),\n            x=[],\n            y=[],\n            name=r\"Moyenne &lt;br&gt; empirique\",\n        ),\n        row=1,\n        col=1,\n    )\n    fig.add_trace(\n        go.Scatter(\n            mode=\"lines\",\n            line=dict(dash=\"dash\", color=\"black\", width=1),\n            marker={},\n            x=[],\n            y=[],\n            name=r\"p\",\n        ),\n        row=1,\n        col=1,\n    )\n    fig.add_trace(\n        go.Heatmap(\n            x=[],\n            z=[],\n            colorscale=[[0, \"rgb(66, 139, 202)\"], [1, \"rgb(255,0,0)\"]],\n            showscale=False,\n        ),\n        row=2,\n        col=1,\n    )\n\n    fig.update_yaxes(range=[0, 1.1], row=1, col=1)\n    fig.update_xaxes(matches=\"x1\", row=2, col=1)\n    fig.update_yaxes(visible=False, row=2, col=1)\n    fig.update_xaxes(visible=False, row=2, col=1)\n\n    fig.update_layout(\n        template=\"simple_white\",\n        showlegend=True,\n        xaxis_title=\"Échantillons: n\",\n    )\n    fig.update_layout(autosize=True)\n\n    fig.update_layout(\n        legend=dict(\n            yanchor=\"top\",\n            y=0.99,\n            xanchor=\"left\",\n            x=0.65,\n            bgcolor=\"rgba(0,0,0,0)\",\n        )\n    )\n    fig.update_layout(\n        margin=dict(l=0, r=0, b=10, t=70),\n    )\n\n    register_widget(\"my_widget\", fig)\n\n    @reactive.Effect\n    def _():\n        p = input.p()\n        n_samples = input.n_samples()\n\n        rng = np.random.default_rng(seed())\n        iterations = np.arange(1, n_samples + 1)\n        samples = rng.binomial(1, p, size=n_samples)\n        means_samples = np.cumsum(samples) / np.arange(1, n_samples + 1)\n\n        # Update data in fig:\n        fig.data[0].x = iterations\n        fig.data[0].y = means_samples\n\n        fig.data[1].x = iterations\n        fig.data[1].y = np.full((n_samples), p)\n\n        fig.data[2].x = iterations\n        fig.data[2].z = [samples]\n\n        fig.update_xaxes(range=[1, n_samples + 1])\n\n        # Update the subplot titles:\n        fig.layout.annotations[1].update(\n            text=f\"Tirages aléatoires (seed=\"\n            + f\"{seed():03}\"\n            + \") &lt;br&gt; &lt;span style='color:rgb(66, 139, 202)'&gt;bleu: 0&lt;/span&gt;, &lt;span style='color:rgb(255, 0, 0)'&gt;rouge: 1&lt;/span&gt; \"\n        )\n\n\napp = App(app_ui, server)\n\nXXX TODO: phénomène intéressant en bougeant le paramètre p avec le reste fixé…les signaux générés sont très très proches, ce qui ne devrait pas être le cas a priori, saut structuration particulière de la génération.",
    "crumbs": [
      "Cours",
      "Théorèmes asymptotiques"
    ]
  },
  {
    "objectID": "Courses/th_asymptotique.html#théorème-central-limite-tcl",
    "href": "Courses/th_asymptotique.html#théorème-central-limite-tcl",
    "title": "Théorèmes asymptotiques",
    "section": "Théorème central limite (TCL)",
    "text": "Théorème central limite (TCL)\nUne fois la loi des grands nombres établie, on peut se demander quel est l’ordre suivant dans le développement asymptotique de \\bar X_n - \\mu, ou de manière équivalente de S_n - n \\mu, où S_n = X_1 + \\cdots + X_n. Le théorème suivant répond à cette question, en donnant une convergence en loi d’une transformation affine de la moyenne empirique:\n\nThéorème 2 (Théorème central limite) Soit X_1, \\ldots, X_n une suite de variables aléatoires i.i.d de variance \\sigma^2 = {\\rm var}(X_1) \\in ]0, \\infty[. On note \\mu = \\mathbb{E}[X_1] leur espérance. Alors \n\\sqrt n \\left(\\tfrac{\\bar X_n - \\mu}{\\sigma} \\right) \\xrightarrow[n \\to +\\infty]{\\mathcal{L}} N\\enspace,\n où N suit une loi normale centrée réduite : N \\sim\\mathcal{N}(0,1).\n\nPreuve XXX TODO: donner une référence.\nOn peut interpréter ce théorème grossièrement de la façon suivante: la moyenne empirique de variables aléatoires i.i.d de variance \\sigma^2 se comporte asymptotiquement comme une loi normale \\mathcal{N}(\\mu, \\tfrac{\\sigma^2}{n}), ce que l’on écrit avec un abus de notation:\n\n\\bar X_n \\approx \\mathcal{N}(\\mu, \\frac{\\sigma^2}{n}) \\enspace.\n\nEn termes de somme cumulée empirique, la convergence se réécrit\n\n    \\tfrac{S_n - n \\mu}{\\sqrt n \\sigma} \\xrightarrow[n \\to +\\infty]{\\mathcal{L}} N \\enspace.\n\nLes hypothèses de ce théorème sont plutôt faibles (il suffit de supposer une variance finie). Pourtant, le résultat est universel : la loi de départ peut être aussi farfelue que l’on veut, elle se rapprochera toujours asymptotiquement d’une loi normale.\nOn rappelle que la convergence en loi est équivalente à la convergence des fonctions de répartition en tout point de continuité de la limite. Ainsi, le théorème central limite se réécrit de la manière suivante : pour tout a &lt; b, notons \\alpha_n=\\mathbb{P} \\left(\\bar X_n \\notin [ \\mu + \\tfrac{a \\sigma}{\\sqrt{n}},  \\mu + \\tfrac{ b \\sigma}{\\sqrt{n}}] \\right). Ainsi\n\n\\mathbb{P} \\left(\\tfrac{\\bar X_n - \\mu}{\\sigma}\\in [ \\tfrac{a}{\\sqrt{n}},\\tfrac{b}{\\sqrt{n}}] \\right)\\\\\n\\begin{align}\n    1-\\alpha_n& = \\mathbb{P} \\left(\\bar X_n \\in [ \\mu + \\tfrac{a \\sigma}{\\sqrt{n}}, \\mu + \\tfrac{ b \\sigma}{\\sqrt{n}}] \\right)\\nonumber\\\\\n    & =\n    \\mathbb{P} \\left(\\tfrac{\\bar X_n - \\mu}{\\sigma} \\in [ \\tfrac{a}{\\sqrt{n}},\\tfrac{b}{\\sqrt{n}}]\\right) \\nonumber\\\\\n    & =\n    \\mathbb{P} \\bigg( a \\leq \\sqrt n \\left(\\tfrac{\\bar X_n - \\mu}{\\sigma} \\right) \\leq b\\bigg) \\nonumber\\\\\n    & \\underset{n \\to \\infty}{\\longrightarrow}  \\int_a^b \\varphi(x) \\,  dx\\,. \\nonumber\\\\\n\\end{align}\n\noù l’on note \\varphi (resp. \\Phi) la densité (resp. la fonction de répartition) d’une loi normale centrée réduite, définie pour tout x\\in\\mathbb{R} par \\varphi(x)=\\tfrac{e^{-\\frac{x^2}{2}}}{\\sqrt{2\\pi}} (resp. \\Phi(x)= \\int_{-\\infty}^{x}\\varphi(u) du).\nDans le cas classique d’un intervalle de confiance à 95%, c’est-à-dire quand \\alpha_n=0.05, et en prenant un intervalle de confiance symétrique (alors a=-t et b=q) on obtient 1-\\alpha_n= \\int_{-q}^q \\varphi(x) \\, dx=\\Phi(q)-\\Phi(-q)=2 \\Phi(q)-1 \\implies \\boxed{q=\\Phi^{-1}(1-\\tfrac{\\alpha_n}{2})} et q est donc le quantile de niveau 1-\\tfrac{\\alpha_n}{2} de la loi normale centrée réduite. Numériquement on peut facilement évaluer q et vérifier que q\\approx 1.96 avec scipy:\n\nfrom scipy.stats import norm\nq = norm.ppf(1-0.05/2)\nprint(f\"Gaussienne centrée réduite,\\nQuantile de niveau (1-α/2):\\nq = {q:.2f}\")\n\nGaussienne centrée réduite,\nQuantile de niveau (1-α/2):\nq = 1.96\n\n\n\nExemple 1 (Loi de Bernoulli) On considère des variables aléatoires X_1, \\ldots, X_n i.i.d. suivant une loi de Bernoulli de paramètre p \\in ]0,1[, dont l’espérance et la variance sont respectivemenbt p et p(1-p). Le théorème central limite donne alors \n    \\sqrt n \\left(\\frac{\\bar X_n - p}{p (1-p)} \\right) \\xrightarrow[n \\to +\\infty]{\\mathcal{L}} N\\,,\n avec N \\sim \\mathcal{N}(0,1). Cette convergence est illustrée dans le widget ci-dessous. Le contexte est le suivant. On répète t fois le processus, qui consiste à afficher (\\bar{X}_k)_{k \\in [n]}, où les n variables aléatoires sont i.i.d. et suivent une loi de Bernoulli de paramètre p.\n\n#| standalone: true\n#| viewerHeight: 600\nimport numpy as np\nfrom scipy.stats import norm\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nfrom shiny import ui, render, App, reactive\nfrom shinywidgets import output_widget, render_widget\n\n\napp_ui = ui.page_fluid(\n    ui.tags.head(\n        ui.tags.style(\"\"\"\n            .bslib-sidebar-layout &gt; .sidebar &gt; .sidebar-content {\n            display: flex;\n            flex-direction: column;\n            padding: 0.1rem;\n            }\n            .irs--shiny .irs-line {\n            top: 27px;\n            height: 4px;\n            }\n            .irs.irs.irs--shiny .irs-bar {\n            top: 27px;\n            height: 4px;\n            }\n            .irs--shiny .irs-handle {\n            top: 23px;\n            width: 22px;\n            height: 10px;\n        \"\"\")\n    ),\n    ui.panel_title(\"TCL\"),\n    ui.layout_sidebar(\n        ui.panel_sidebar(\n            ui.input_action_button(\"seed\", \"Nouveau tirage\",class_=\"btn-primary\"),\n            ui.input_slider(\"p\", \"Espérance: p\", 0.01, 0.99, value=0.5, step=0.01, ticks=False),\n            ui.input_slider(\"n_samples\", \"Échantillons: n\", 1, 200, value=100, step=1, ticks=False),\n            ui.input_slider(\"n_repetitions\", \"Répétitions: t\", 1, 300, value=200, step=1, ticks=False),\n        width=3\n        ),\n    ui.panel_main(output_widget(\"my_widget\"), width = 9)\n    )\n)\n\n\n\ndef server(input, output, session):\n    seed = reactive.Value(42)\n\n\n    @reactive.Effect\n    @reactive.event(input.seed)\n    def _():\n        seed.set(np.random.randint(0, 1000))\n\n\n    @output\n    @render_widget\n    def my_widget():\n\n        rng = np.random.default_rng(seed())\n        p = input.p()\n        n_repetitions = input.n_repetitions()\n        n_samples = input.n_samples()\n        iterations = np.arange(1, n_samples + 1)\n        samples = rng.binomial(1, p, size=(n_repetitions, n_samples))\n        means_samples = np.cumsum(samples, axis=1) / np.arange(1, n_samples + 1)\n        x_hist = np.linspace(0, 1, num=300)\n\n        # Create figure\n        fig = make_subplots(\n                    rows=1,\n                    cols=3,\n                    # vertical_spacing=0.5,\n                    horizontal_spacing=0.02,\n                    column_widths=[20, 2, 3],\n                    subplot_titles=(\"t = \" + str(n_repetitions) + \" répétitions\",\"\",\"\")\n                )\n\n\n        for i in range(n_repetitions):\n            fig.add_trace(\n                    go.Scatter(\n                        mode='lines',\n                        line=dict(color=\"rgba(0,0,0,0.05)\", width=1),\n                        x=iterations,\n                        y=means_samples[i,:],\n                        ),\n                        row=1, col=1,\n            )\n        fig.add_trace(\n                go.Scatter(\n                    mode='lines',\n                    line=dict(dash=\"dash\", color=\"blue\", width=1),\n                    marker={},\n                    x=iterations,\n                    y=np.full((n_samples), p),\n                    name='p'),\n                    row=1, col=1,\n        )\n        fig.update_layout(\n            template=\"simple_white\",\n            showlegend=False,\n        )\n        fig.add_trace(\n                go.Scatter(\n                    mode='markers',\n                    marker=dict(color=\"rgba(0,0,0,0.05)\", size=4),\n                    x=np.zeros(n_samples),\n                    y=means_samples[:,-1],\n                ),\n                row=1, col=2,\n\n        )\n        y_hist, bins = np.histogram(means_samples[:,-1], bins=int(np.sqrt(n_repetitions)), density=True)\n        fig.add_trace(\n            go.Bar(x=y_hist, y=bins[:-1] + np.diff(bins)/2,\n                    opacity=0.75,\n                    marker_color = 'black',\n                    orientation='h',\n                    width=np.diff(bins),\n                    name=\"Tirages de moyennes empiriques\",\n                    ),\n                row=1, col=3,\n        )\n        fig.add_trace(\n            go.Scatter(x=norm.pdf(x_hist, p, np.sqrt(p*(1-p) / n_samples)),\n                       y=x_hist,\n                       mode='lines',\n                       line=dict(color=\"red\"),\n                       legendgroup='1',\n                       name=\"TCL\"\n                       ),\n                row=1, col=3,\n        )\n\n        fig.update_xaxes(range=[1, n_samples + 1])\n        fig.update_yaxes(range=[-.05, 1.05], row=1, col=1)\n        fig.update_yaxes(matches=\"y1\",visible = False,  row=1, col=2)\n        fig.update_xaxes(range=[-0.2, 0.2], visible = False, row=1, col=2)\n\n        fig.update_yaxes(matches=\"y1\", row=1, col=3, visible=False)\n        fig.update_xaxes(range=[0, 1.1 / np.sqrt(2*np.pi* p*(1-p) / n_samples)], row=1, col=3)\n        fig.update_xaxes(visible=False, row=1, col=3)\n\n\n\n        for trace in fig['data']:\n            print(trace)\n            if (trace['name'] != 'TCL') and (trace['name'] != 'p'):\n                trace['showlegend'] = False\n        fig.update_layout(\n        margin=dict(l=0, r=0, b=10, t=100),\n        )\n        fig.update_layout(\n            title=dict(text=\"Distribution de la moyenne empirique&lt;br&gt; (cas loi de Bernoulli)\", yanchor=\"top\", y=0.95),\n            title_x=0.5,\n            showlegend=True,\n\n        )\n        fig.update_layout(\n            legend=dict(\n                yanchor=\"top\",\n                y=0.99,\n                xanchor=\"left\",\n                x=0.89,\n                bgcolor=\"rgba(0,0,0,0)\",\n            )\n        )\n        fig['layout']['xaxis']['title']='Échantillons: n'\n\n        fig.update_layout(autosize=True)\n\n        return fig\n\n\napp = App(app_ui, server)\n\nUne autre illustration possible de la convergence donnée par le TCL est celle qui correspond au point de vue donnée par l’analyse. Pour cela supposons que l’on ait une suite de variables aléatoires réelles X_1, \\dots, X_n, i.i.d. dont la fonction de densité commune est notée par f.\nOn rappel quelques éléments de probabilités concernant les densités. Pour cela on rappelle la définition de la convolution deux fonctions. Pour cela prenons deux fonctions f et g définies sur \\mathbb{R} et qui sont intégrables au sens de Lebesgue. La convolution de f par g est alors la fonction f*g suivante:\n\n\\begin{align}\n\\mathbb{R} &\\mapsto \\mathbb{R} \\nonumber\\\\\nx &\\to \\int_{-\\infty}^{+\\infty} f(x-y)g(y) dy \\enspace.\\nonumber\n\\end{align}\n\n\n\n\n\n\n\nNote\n\n\n\nOn peut aussi obtenir f*g(x) en calculant \\int_{\\mathbb{R}^2} f(u)g(v) {1\\hspace{-3.8pt} 1}_{u+v=x} du dv.\n\n\n\nThéorème 3 (Loi de la somme et convolutions)  \n\n#| standalone: true\n#| viewerHeight: 550\nimport numpy as np\nfrom shiny import ui, render, App, reactive\nfrom shinywidgets import output_widget, register_widget\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objs as go\nfrom scipy import signal\n\n\napp_ui = ui.page_fluid(\n    ui.tags.head(\n        ui.tags.style(\"\"\"\n            .bslib-sidebar-layout &gt; .sidebar &gt; .sidebar-content {\n            display: flex;\n            flex-direction: column;\n            padding: 0.5rem;\n            }\n            .irs--shiny .irs-line {\n            top: 27px;\n            height: 2px;\n            }\n            .irs.irs--shiny .irs-bar {\n            top: 27px;\n            height: 2px;\n            }\n            .irs--shiny .irs-handle {\n            top: 23px;\n            width: 22px;\n            height: 10px;\n        \"\"\")\n    ),\n    ui.panel_title(\"TCL et convolutions\"),\n    ui.layout_sidebar(\n        ui.panel_sidebar(\n            ui.input_select(\n                \"loi\",\n                \"Loi sous-jacente\",\n                {'uniforme': 'Uniforme', 'laplace' : 'Laplace'},\n            ),\n            ui.input_slider(\n                \"n_iter\",\n                \"Échantillons: n\",\n                1,\n                10,\n                value=1,\n                step=1,\n                ticks=False,\n            ), width = 3\n    ),\n    ui.panel_main(\n        output_widget(\"my_widget\"), width = 9\n    )\n)\n)\n\n\nnnzeros = 10001\nx_min = -20\nx_max = 20\n\nx = np.linspace(x_min, x_max, nnzeros)\ny = np.zeros(nnzeros)\nmask = np.where(np.abs(x) &lt;= 0.5, 1, 0)\ny[mask == 1] = 1\ndelta = (x_max - x_min) / nnzeros\nvar = np.sum(y * x**2 *(delta)) - (np.sum(y * x * delta))**2\n\ndef convolve(signal_in, n_convolutions, delta):\n    output = np.zeros(len(signal_in))\n    if n_convolutions == 0:\n        return output\n    elif n_convolutions == 1:\n        return signal_in\n    else:\n        output = signal_in.copy()\n        for i in range(n_convolutions - 1):\n            output = signal.fftconvolve(\n                output * delta, signal_in, mode=\"same\"\n            )\n        return output\n\n\ndef server(input, output, session):\n\n    fig = go.FigureWidget()\n    fig.add_trace(\n        go.Scatter(\n            mode=\"lines\",\n            line=dict(color=\"black\", width=3),\n            x=[],\n            y=[],\n            name=r\"f*...*f\",\n        )\n    )\n\n    fig.add_trace(\n        go.Scatter(\n            x=x,\n            y=np.exp(-(x**2) / (2 * var)) / np.sqrt(2 * var * np.pi),\n            mode=\"lines\",\n            line=dict(dash=\"dash\", color=\"red\", width=2),\n            name=f\"Loi normale&lt;br&gt;(variance adéquate)\",\n        )\n    )\n    fig.update_xaxes(range=[-3, 3], position=0.)\n    fig.update_yaxes(range=[0, 2 * np.max(y)], position=0.5, showticklabels=False)\n\n\n    fig.update_layout(\n        template=\"simple_white\",\n        showlegend=True,\n        autosize=True,\n        title=dict(text=\"Densité : &lt;br&gt; moyenne de n variables aléatoires\", yanchor=\"top\", y=0.95),\n        title_x=0.5,\n    )\n    fig.update_layout(legend=dict(\n        yanchor=\"top\",\n        y=0.90,\n        xanchor=\"left\",\n        x=-0.1,\n        font=dict(size= 10)\n    ))\n    fig.update_layout(\n    height=250,\n    margin=dict(l=0, r=0, b=10, t=100),\n    )\n\n    register_widget(\"my_widget\", fig)\n\n    @reactive.Effect\n    def _():\n        if str(input.loi()) == 'uniforme':\n            y = np.zeros(nnzeros)\n            mask = np.where(np.abs(x) &lt;= 0.5, 1, 0)\n            y[mask == 1] = 1\n\n        else:\n            # else str(input.loi()) == 'laplace':\n            y=np.exp(-np.abs(x))/2\n            # y = np.zeros(nnzeros)\n            # mask = np.where(np.abs(x) &gt;= 0.5, 1, 0)\n            # y[mask == 1] = 0\n            # y = np.cos(np.pi * x) + 1\n            y = y / (np.sum(y) * delta)\n\n        var = np.sum(y * x**2 * delta) - (np.sum(y * x * delta))**2\n\n        y_display = convolve(y, input.n_iter(), delta)\n        # Update data in fig:\n        fig.data[0].x = x / np.sqrt(input.n_iter())\n        fig.data[0].y = y_display * np.sqrt(input.n_iter())\n        fig.data[1].y = np.exp(-(x**2) / (2 * var)) / np.sqrt(2 * var * np.pi)\n        fig.update_yaxes(range=[0, 2 * np.max(y)], position=0.5, showticklabels=False)\n\n\napp = App(app_ui, server)\n\nPour aller plus loins sur les convolutions, voir la vidéo de 3Blue1Brown à ce sujet: Convolutions | Why X+Y in probability is a beautiful mess",
    "crumbs": [
      "Cours",
      "Théorèmes asymptotiques"
    ]
  },
  {
    "objectID": "Courses/slides.html",
    "href": "Courses/slides.html",
    "title": "Slides",
    "section": "",
    "text": "Vous trouverez ci-dessous la listes des slides associés:\nCours introduction: plein écran\n\n\n\n\n Retour au sommet",
    "crumbs": [
      "Slides",
      "Slides"
    ]
  },
  {
    "objectID": "Courses/perspective_historique.html",
    "href": "Courses/perspective_historique.html",
    "title": "Perspectives historiques",
    "section": "",
    "text": "Nous allons présenter ici quelques éléments historiques sur les méthodes de Monte-Carlo, dont les prémisses remontent au XVIIIème siècle.",
    "crumbs": [
      "Cours",
      "Perspectives historiques"
    ]
  },
  {
    "objectID": "Courses/perspective_historique.html#laiguille-de-buffon",
    "href": "Courses/perspective_historique.html#laiguille-de-buffon",
    "title": "Perspectives historiques",
    "section": "L’aiguille de Buffon",
    "text": "L’aiguille de Buffon\nGeorges-Louis Leclerc, Comte de Buffon1 proposa en 1733 une méthode qui s’avéra être utile pour estimer la valeur de \\pi. On désigne de nos jours cette expérience sous le nom de l’aiguille de Buffon. C’est l’une des premières méthodes de Monte-Carlo référencée dans la littérature (la source du texte est disponible ici sur le site de la BNF ).\n1 Georges-Louis Leclerc, Comte de Buffon: (1707-1788) naturaliste, mathématicien et industriel français du siècle des Lumières La question initiale (simplifiée ici) posée par Buffon était la suivante: une aiguille de taille 1 tombe sur un parquet composé de lattes de largeur 1: quelle est alors la probabilité P que l’aiguille croise une ligne de la trame du parquet ?\nLe contexte original était dans celui d’un jeu à deux joueurs: un joueur parie sur le fait que l’aiguille croise une ligne de la trame du parquet, l’autre sur le fait que l’aiguille ne croise pas une ligne de la trame du parquet. L’enjeu est alors de calculer la probabilité de succès de chacun des joueurs, et de voir si le jeu est équilibré ou non.\nVoilà brièvement la question que s’est posée Buffon en 1733. La réponse est donnée par la formule suivante, qui montre que le jeu qu’il propose n’est pas équilibré:\n\nP = \\frac{2}{\\pi} \\approx 0.6366 \\enspace.\n Une preuve de ce résultat sera donnée ci-dessous.\nL’idée sous-jacente de Buffon est que si l’on répète cette expérience un grand nombre de fois, on peut approché la quantité P numériquement, par exemple en proposant un estimateur \\hat{P}_n qui compte la proportion de chevauchement après avoir fait n répétition des lancers. Pour estimer \\pi, il ne restera donc plus qu’à évaluer \\frac{2}{\\hat{P}_n}.\nOn peut faire cette expérience dans le monde réelle (c’est un peu long pour n grand!), mais on peut aussi utiliser une méthode numérique pour cela. Il s’agit alors de tirer aléatoire la position du centre de l’aiguille, puis de tirer aussi de manière aléatoire son angle de chute. On teste à la fin si l’aiguille croise une ligne de la trame du parquet ou non, et on recommence l’expérience un grand nombre de fois.\nCette méthode est donnée ci-dessous, avec un exemple interactif généré en Python.\n\nCode\nimport numpy as np\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\nrng = np.random.default_rng(44)\n\nn_samples = 200\nxmax = 14.499999\nxmin = -xmax\n\n\n# Create the needles\ncenters_x = rng.uniform(xmin, xmax, n_samples)\nangles = rng.uniform(0, 2 * np.pi, n_samples)\ncenters_y = rng.uniform(-2, 2, n_samples)\n\n# Compute the right borders of the needles\nborders_right = np.zeros((n_samples, 2))\nborders_right[:, 0] = centers_x + np.cos(angles) / 2\nborders_right[:, 1] = centers_y + np.sin(angles) / 2\n\n# Compute the left borders of the needles\nborders_left = np.zeros((n_samples, 2))\nborders_left[:, 0] = centers_x + np.cos(angles + np.pi) / 2\nborders_left[:, 1] = centers_y + np.sin(angles + np.pi) / 2\n\ncenters_x_round = np.round(centers_x)\noverlap = (borders_right[:, 0] - centers_x_round) * (\n    borders_left[:, 0] - centers_x_round\n) &lt; 0\noverlap = np.where(overlap, 1, 0)\nn_overlap = int(np.sum(overlap))\n\n\n# Check if the needles cross a line\nborders_red = np.empty((3 * n_overlap, 2), dtype=object)\nborders_red.fill(None)\nborders_red[::3, :] = borders_right[overlap == 1]\nborders_red[1::3, :] = borders_left[overlap == 1]\n\nborders_blue = np.empty((3 * (n_samples - n_overlap), 2), dtype=object)\nborders_blue.fill(None)\nborders_blue[::3, :] = borders_right[overlap == 0]\nborders_blue[1::3, :] = borders_left[overlap == 0]\n\noverlaps = np.empty((3 * n_samples), dtype=object)\noverlaps.fill(None)\noverlaps[::3] = overlap\noverlaps[1::3] = overlap\noverlaps[2::3] = overlap\n\nidx_red = np.cumsum(overlaps)\nidx_blue = np.cumsum(1 - overlaps)\n\n\n# Create subplots with 2 rows and 1 column with ratio x /  y  of 10\nfig = make_subplots(rows=2, cols=1, vertical_spacing=0.1, row_heights=[2, 1])\n\n# Use a loop to plot vertical lines equation \"y=c\" for integer values c in [-2, -1, 0, 1, 2]\nfor i in range(int(np.round(xmin)), int(np.round(xmax)) + 1):\n    fig.add_shape(\n        type=\"line\",\n        y0=-3,\n        x0=i,\n        y1=3,\n        x1=i,\n        line=dict(\n            color=\"black\",\n            width=2,\n        ),\n        row=1,\n        col=1,\n    )\n\ncolor = np.where(overlaps, 1.0, 0.0)\n\nn_samples_array = np.arange(1, n_samples + 1)\npi_estimate = 2 / (np.cumsum(overlap) / n_samples_array)\nt = n_samples\n\nfig.update_layout(\n    template=\"simple_white\",\n    xaxis=dict(range=[xmin, xmax], constrain=\"domain\", showgrid=False),\n    yaxis_scaleanchor=\"x\",\n    xaxis_visible=False,\n    yaxis_visible=False,\n)\n\nfor i in range(3, t):\n    fig.add_trace(\n        go.Scatter(\n            x=borders_red[: idx_red[3 * i] + 1, 0],\n            y=borders_red[: idx_red[3 * i] + 1, 1],\n            mode=\"lines\",\n            line=dict(width=2),\n            marker=dict(color=\"red\"),\n            name=\"Avec intersection\",\n            visible=False,\n        ),\n        row=1,\n        col=1,\n    )\n    fig.add_trace(\n        go.Scatter(\n            x=borders_blue[: idx_blue[3 * i] + 1, 0],\n            y=borders_blue[: idx_blue[3 * i] + 1, 1],\n            mode=\"lines\",\n            line=dict(width=2),\n            marker=dict(color=\"darkblue\"),\n            name=\"Sans intersection\",\n            visible=False,\n        ),\n        row=1,\n        col=1,\n    )\n\n    fig.add_trace(\n        go.Scatter(\n            x=n_samples_array[:i],\n            y=pi_estimate[:i],\n            mode=\"lines\",\n            line=dict(width=1),\n            marker=dict(color=\"red\"),\n            showlegend=False,\n            visible=False,\n        ),\n        row=2,\n        col=1,\n    )\n\nfig.add_annotation(\n    dict(\n        x=1.25,\n        y=0.14,\n        xref=\"paper\",\n        yref=\"paper\",\n        text=\"Estimation de pi\",\n        showarrow=False,\n        font=dict(color=\"red\"),\n    )\n)\n\nfig.add_annotation(\n    dict(x=-0.04, y=0.19, xref=\"paper\", yref=\"paper\", text=\"pi\", showarrow=False)\n)\n\nfig.update_xaxes(title_text=\"Nombre d'aiguilles tirées\", row=2, col=1)\n\nfig.update_layout(\n    template=\"none\",\n    xaxis2=dict(showgrid=True, zeroline=True, zerolinewidth=1, range=[0, n_samples]),\n    yaxis2=dict(showgrid=True, zeroline=True, zerolinewidth=1, range=[0, 6]),\n)\n# plot a dash line at y=pi\nfig.add_shape(\n    type=\"line\",\n    y0=np.pi,\n    x0=0,\n    y1=np.pi,\n    x1=n_samples,\n    line=dict(\n        color=\"black\",\n        width=1,\n        dash=\"dashdot\",\n    ),\n    row=2,\n    col=1,\n)\n\n\nfig.data[10 * 3].visible = True\nfig.data[10 * 3 + 1].visible = True\nfig.data[10 * 3 + 2].visible = True\n\n\nsteps = []\nfor i in range(len(fig.data) // 3):\n    step = dict(\n        label=str(i + 4),\n        method=\"update\",\n        args=[\n            {\"visible\": [False] * len(fig.data)},\n            {\n                \"title\": \"Estimation avec \"\n                + str(i + 4)\n                + f\" aiguilles: pi = {pi_estimate[i]:.4f}\"\n            },\n        ],\n    )\n    step[\"args\"][0][\"visible\"][3 * i] = True\n    step[\"args\"][0][\"visible\"][3 * i + 1] = True\n    step[\"args\"][0][\"visible\"][3 * i + 2] = True\n\n    steps.append(step)\n\nslider = dict(\n    active=6,\n    currentvalue={\"prefix\": \"Nombre d'aiguilles: \"},\n    pad={\"t\": 50},\n    y=-0.32,\n    steps=steps,\n)\n\nfig.update_layout(sliders=[slider])\nfig.show()\n\n\n\n\n                                                \n\n\n\nOn va fournir ici le calcul de la probabilité P. Pour cela on aura besoin de quelques éléments décrits dans le dessin ci-dessous.\n\nx : distance entre le centre de l’aiguille et la ligne de la trame du parquet la plus proche\n\\theta : angle entre l’aiguille et la ligne de la trame du parquet la plus proche\n1 : longueur de l’aiguille (et donc la demi longueur est \\frac{1}{2})\n\\frac{1}{2}\\sin(\\theta) : distance entre l’extrémité de l’aiguille et la ligne de la trame du parquet la plus proche\n\n\n\n\n\n\n\n\nSans croisement\n\n\n\n\n\nAvec croisement\n\n\n\n\n\nFigure 1: Configuration sans croisement (à gauche) et avec croisement (à droite) de l’aiguille avec une ligne de la trame du parquet.\n\n\n\nAvec les éléments ci-dessus, on voit qu’il y a chevauchement si et seulement si: \\frac{1}{2}\\sin(\\theta) \\geq x.\nMaintenant par des arguments de symétrie on voit qu’on peut se restreindre à \\theta \\in [0, \\frac{\\pi}{2}], et à x \\in [0, \\frac{1}{2}]. Les lois de générations des variables aléatoires X et \\Theta sont les suivantes:\n\nX \\sim \\mathcal{U}([0, \\frac{1}{2}]), de densité f_X(x) = 2 {1\\hspace{-3.8pt} 1}_{[0, \\frac{1}{2}]}(x)\n\\Theta \\sim \\mathcal{U}([0, \\frac{\\pi}{2}]) de densité f_\\Theta(\\theta) = \\frac{2}{\\pi} {1\\hspace{-3.8pt} 1}_{[0, \\frac{\\pi}{2}]}(\\theta)\n\nDe plus on suppose que X et \\Theta sont indépendantes.\nMaintenant pour calculer la probabilité P on procède comme suit: \n\\begin{align*}\nP\n& = \\mathbb{P}\\left(\\frac{1}{2}\\sin(\\Theta) \\geq X\\right) \\\\\n& = \\int_{\\mathbb{R}^2} {1\\hspace{-3.8pt} 1}_{\\{\\frac{1}{2}\\sin(\\theta) \\geq x\\}} f_{\\Theta}(\\theta) f_X(x) d\\theta dx  \\quad (\\text{par indépendance})\\\\\n& = \\int_{\\mathbb{R}} \\int_{\\mathbb{R}}\n{1\\hspace{-3.8pt} 1}_{\\{\\frac{1}{2}\\sin(\\theta) \\geq x\\}} \\frac{2}{\\pi} {1\\hspace{-3.8pt} 1}_{[0, \\frac{\\pi}{2}]}(\\theta) \\cdot 2 {1\\hspace{-3.8pt} 1}_{[0, \\frac{1}{2}]}(x) d\\theta dx \\\\\n& = \\int_{0}^{\\frac{\\pi}{2}} \\int_{0}^{\\frac{1}{2}\\sin(\\theta)} \\frac{4}{\\pi} dx  d\\theta \\\\\n& = \\frac{4}{\\pi} \\int_{0}^{\\frac{\\pi}{2}} {\\frac{1}{2}\\sin(\\theta)}  d\\theta\\\\  \n& = \\frac{2}{\\pi} \\Big[ -\\cos(\\theta)\\Big]_{0}^{\\frac{\\pi}{2}} \\\\\n& = \\frac{2}{\\pi} \\enspace.\n\\end{align*}\n\n\n\n\n\n\n\nExercice: rendre le jeu équilibré?\n\n\n\nEn reprenant le même type de raisonnement que ci-dessus, trouver la distance entre les lattes du parquet qui rend le jeu équilibrer entre les deux joueurs introduit par Buffon (l’un pariant sur le fait que l’aiguille croise une ligne de la trame du parquet, l’autre pariant sur le fait que l’aiguille ne croise pas une ligne de la trame du parquet).",
    "crumbs": [
      "Cours",
      "Perspectives historiques"
    ]
  },
  {
    "objectID": "Courses/perspective_historique.html#méthode-de-monte-carlo",
    "href": "Courses/perspective_historique.html#méthode-de-monte-carlo",
    "title": "Perspectives historiques",
    "section": "Méthode de Monte-Carlo",
    "text": "Méthode de Monte-Carlo\nLa méthode de Monte-Carlo, est une méthode de calcul numérique qui consiste à utiliser des nombres aléatoires pour résoudre des problèmes déterministes. Elle est utilisée dans de nombreux domaines, comme la physique, la chimie, la biologie, la finance, ou encore l’apprentissage automatique. Cette méthode basée sur la loi des grands nombres a été mis au point à Los Alamos, dans le cadre du projet Manhattan (dont l’objectif était le développement du nucléaire civil et militaire) par un groupe de scientifiques dont les plus connus sont: John von Neumann2, Nicholas Metropolis3 ou encore Stanisław Ulam4\n2 John von Neumann: (1903-1957) mathématicien et physicien américano-hongrois, un des pères de l’informatique. 3 Nicholas Metropolis: (1915-1999) 4 Stanisław Ulam: (1909-1984) Dans le cadre du projet Manhattan, il s’agissait de calculer des intégrales de manière numérique pour modéliser l’évolution de particules, en utilisant des nombres aléatoires.\nEckhardt (1987) donne un bref aperçu historique, et mentionne les premières description de la méthode du rejet et de la méthode de l’inversion dans des lettres entre Von Neumann et Ulam datant de 1947. Ulam aurait une l’idée d’utiliser de telles méthodes pour résoudre le jeu du solitaire lors d’un séjour à l’hôpital en 1946, et éviter ainsi de faire des calculs combinatoires fastidieux. Rapidement, la possibilité d’appliquer cette approche pour des calculs en physique mathématiques (diffusion des neutrons notamment) lui serait apparue prometteuse. Le développement de l’informatique naissante allait permettre une mise en oeuvre pratique de ces idées, et c’est ainsi que la méthode de Monte-Carlo est née. Le nom Monte-Carlo est lui venu du besoin de confidentialité du projet, et provient du nom de la ville de Monte-Carlo, connue pour ses jeux de hasard, où l’oncle de Stanisław Ulam aimait se rendre pour assouvir sa soif de jeu. Ce serait N. Metropolis qui aurait proposé ce nom (cf. Metropolis 1987):\n\nEckhardt, R. 1987. « Stan Ulam, John Von Neumann, and the Monte Carlo Method ». Los Alamos Science, nᵒ 15: 131‑37.\n\nMetropolis, Nicholas. 1987. « The beginning of the Monte Carlo method ». Los Alamos Science, nᵒ 15: 125‑30.\nIt was at that time that I suggested an obvious name for the statistical method—a suggestion not unrelated to the fact that Stan had an uncle who would borrow money from relatives because he “just had to go to Monte Carlo”.",
    "crumbs": [
      "Cours",
      "Perspectives historiques"
    ]
  },
  {
    "objectID": "Courses/perspective_historique.html#hasting-metropolis",
    "href": "Courses/perspective_historique.html#hasting-metropolis",
    "title": "Perspectives historiques",
    "section": "Hasting Metropolis",
    "text": "Hasting Metropolis",
    "crumbs": [
      "Cours",
      "Perspectives historiques"
    ]
  },
  {
    "objectID": "Courses/perspective_historique.html#culture-populaire-monty-hall-game-show",
    "href": "Courses/perspective_historique.html#culture-populaire-monty-hall-game-show",
    "title": "Perspectives historiques",
    "section": "Culture populaire “Monty Hall game show”",
    "text": "Culture populaire “Monty Hall game show”",
    "crumbs": [
      "Cours",
      "Perspectives historiques"
    ]
  },
  {
    "objectID": "Courses/matterjs-inverse-vizu.html",
    "href": "Courses/matterjs-inverse-vizu.html",
    "title": "inverse-vizu",
    "section": "",
    "text": "viewof dist = Inputs.select(['normal','cauchy','laplace','bimodal'], {value: \"bimodal\", label: \"Distribution type\"})\nviewof replay = html`&lt;button&gt;replay`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ninvcdfboard = require(await FileAttachment(\"../inverse-vizu/dist/invcdfboard.umd.cjs\").url())\n\n{\n  replay\n  const canvas = DOM.canvas(500, 500);\n  const galton = invcdfboard.galton(canvas,dist);\n  \n  return html`${galton.canvas}`\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Retour au sommet"
  },
  {
    "objectID": "Courses/loi_normale1D.html",
    "href": "Courses/loi_normale1D.html",
    "title": "Loi normale (1D)",
    "section": "",
    "text": "On considère ici \\mathbb{R}^d muni du produit scalaire euclidien \\langle \\cdot, \\cdot \\rangle et de la norme euclidienne \\|\\cdot\\| associée.",
    "crumbs": [
      "Cours",
      "Loi normale (1D)"
    ]
  },
  {
    "objectID": "Courses/loi_normale1D.html#la-loi-normale",
    "href": "Courses/loi_normale1D.html#la-loi-normale",
    "title": "Loi normale (1D)",
    "section": "La loi normale",
    "text": "La loi normale\n\nDéfinitions et propriétés\nOn rappelle que la loi normale de paramètres \\mu \\in \\mathbb{R} et \\nu &gt; 0 a une densité donnée pour tout x \\in \\mathbb{R} par\n\n    \\varphi_{\\mu, \\nu}(x)=\\frac{1}{\\sqrt{2 \\pi \\nu}}\\exp\\Big(-\\frac{(x-\\mu)^2}{2\\nu}\\Big)\\enspace.\n\nOn note X \\sim \\mathcal{N}(\\mu, \\nu), si X est une variable aléatoire ayant pour densité \\varphi_{\\mu, \\nu}. Notons que si X \\sim \\mathcal{N}(\\mu,\\nu), alors X a pour espérance \\mu et pour variance \\nu. Le cas particulier \\mu=0 et \\nu=1 correspond à une variable aléatoire dite centrée réduite.\nLa loi normale vérifie la propriété de stabilité par transformation affine : si X \\sim \\mathcal{N}(\\mu, \\nu) et si (a,b) \\in \\mathbb{R}^* \\times \\mathbb{R}, alors la variable aléatoire a X + b suit une loi normale \\mathcal{N}(a\\mu + b, a^2 \\nu). On peut donc facilement passer d’une loi normale centrée réduite à une loi normale quelconque via une transformation affine :\n\nsi X \\sim \\mathcal{N}(0,1), alors \\sqrt{\\nu} X + \\mu \\sim \\mathcal{N}(\\mu, \\nu),\nsi X \\sim \\mathcal{N}(\\mu, \\nu), alors (X-\\mu)/\\sqrt{\\nu} \\sim \\mathcal{N}(0,1).\n\nAinsi, savoir simuler une loi normale centrée réduite, permet de simuler n’importe quelle loi normale.\nRappelons enfin que la fonction caractéristique d’une variable aléatoire X \\sim \\mathcal{N}(\\mu, \\nu) est donnée pour tout t \\in \\mathbb{R} par \n\\begin{align*}\n\\phi_{\\mu, \\nu}(t) & \\triangleq \\mathbb{E}(e^{i t X})  \\\\\n\\phi_X(t) & = \\exp\\Big( i \\mu t - \\dfrac{\\nu t^2}{2}\\Big)\\enspace.\n\\end{align*}\n\n\n\nSimulation d’une loi normale\n\n\n\n\n\n\nUne mauvaise piste pour simuler une loi normale\n\n\n\nOn peut simuler une loi normale à partir de variables aléatoires uniformes U_1, \\ldots, U_n iid en appliquant le théorème central limite à \n    \\dfrac{U_1 + \\cdots + U_n - n/2}{\\sqrt{n/12}}\\,.\n Cependant, cette méthode ne donne qu’une approximation d’une loi normale. Par ailleurs, la vitesse de convergence étant relativement lente (de l’ordre de \\sqrt n), il faudra simuler beaucoup de variables aléatoires uniformes pour avoir une approximation correcte, ce qui demande un temps de calcul assez élevé.\n\n\n\nChangement de variables\nLe théorème suivant permet de passer de la loi d’un couple (X,Y) à celle de (U,V) = \\phi(X,Y), où \\phi est un C^1-difféomorphisme, c’est-à-dire une application bijective dont la réciproque est également de classe C^1.\nPour cela rappelons que la jacobienne de \\phi^{-1} correspond à la matrice (application linéaire) des dérivées partielles. Ainsi, si \\phi(x,y) = (u,v) \\iff (x,y) = \\phi^{-1}(u,v), alors \n{\\rm{Jac}}~\\phi^{-1} (u,v)\n=\n\\begin{pmatrix}\n  \\frac{\\partial x}{\\partial u} & \\frac{\\partial x}{\\partial v}    \\\\\n  \\frac{\\partial y}{\\partial u} & \\frac{\\partial y}{\\partial v}\n\\end{pmatrix} \\enspace.\n\n\nThéorème 1 (Caractérisation de la loi d’une variable aléatoire réelle) XXX source\nSoit (X,Y) un vecteur aléatoire de densité f_{(X,Y)} définie sur l’ouvert A \\subset \\mathbb{R}^2 et \\phi : A \\to B \\subset \\mathbb{R}^2 un C^1-difféomorphisme. Le vecteur aléatoire (U,V)=\\phi(X,Y) admet alors pour densité f_{(U,V)} définie sur B pour tout (u,v) \\in \\mathbb{R}^2 par \n  f_{(U,V)} (u,v)\n  = f_{(X,Y)} (\\phi^{-1}(u,v)) |\\det ({\\rm{Jac}}~\\phi^{-1} (u,v))| {1\\hspace{-3.8pt} 1}_B(u,v)\\enspace.\n\n\nOn a énoncé le résultat en dimension 2 par simplicité. Il s’étend bien évidemment à une dimension d quelconque. En particulier, pour d=1, on retrouve le changement de variable classique dans le cas de l’intégration d’une fonction à valeurs réelles.\nDémonstration.\nOn rappelle que la loi de (U,V) est caractérisée par les quantités \\mathbb{E}[h(U,V)] pour tout h : \\mathbb{R}^2 \\to \\mathbb{R} mesurable bornée. On considère donc une telle fonction h et on applique la formule de transfert : \n  \\mathbb{E}[h(U,V)]\n  =\\mathbb{E}[h(\\phi(X,Y))]\n  = \\int_{\\mathbb{R}^2} h(\\phi(x,y)) f_{(X,Y)}(x,y) \\, d x d y\n  = \\int_{A} h(\\phi(x,y)) f_{(X,Y)}(x,y) \\, d x d y\\enspace.\n On applique alors la formule du changement de variables vu en théorie de l’intégration avec (u,v) = \\phi(x,y) \\iff \\phi^{-1}(u,v) = (x,y) : \n  \\int_{A} h(\\phi(x,y)) f_{(X,Y)}(x,y) \\, d x d y\n  = \\int_{B} h(u,v)) f_{(X,Y)}(\\phi^{-1}(u,v)) |\\det ({\\rm{Jac}}~\\phi^{-1} (u,v))| \\, d u d v\\,.\n On conclut alors que \n\\mathbb{E}[h(U,V)]\n= \\int_{\\mathbb{R}^2} h(u,v) f_{(X,Y)}(\\phi^{-1}(u,v)) |\\det ({\\rm{Jac}}~\\phi^{-1} (u,v))| {1\\hspace{-3.8pt} 1}_B(u,v)\\, d u d v\\,,\n ce qui donne le résultat voulu.\n\n\n\n\n\n□",
    "crumbs": [
      "Cours",
      "Loi normale (1D)"
    ]
  },
  {
    "objectID": "Courses/matterjs-inverse-vizu (copy).html",
    "href": "Courses/matterjs-inverse-vizu (copy).html",
    "title": "inverse-vizu",
    "section": "",
    "text": "viewof dist = Inputs.select(['normal','cauchy','laplace','bimodal'], {value: \"bimodal\", label: \"Distribution type\"})\nviewof replay = html`&lt;button&gt;replay`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ninvcdfboard = {\n  // const response = await fetch(\"https://raw.githubusercontent.com/fradav/matter-playground/main/dist/invcdfboard.umd.cjs\");\n  const response = await fetch(\"/home/jsalmon/Documents/OpenSource/matter-playground/dist/invcdfboard.umd.cjs\");\n\n\nconst blob = await response.blob();\n  const url = URL.createObjectURL(blob);\n  return require(url).catch(() =&gt; window.invcdfboard);\n}\n\n// Or load from local file\n// invcdfboard = require(await FileAttachment(\"invcdfboard.umd.cjs\").url())\n\n{\n  replay\n  const canvas = DOM.canvas(500, 500);\n  const galton = invcdfboard.galton(canvas,dist);\n  \n  return html`${galton.canvas}`\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Retour au sommet"
  },
  {
    "objectID": "Courses/notations.html",
    "href": "Courses/notations.html",
    "title": "Notations et rappels",
    "section": "",
    "text": "On considère un espace probabilisé (\\Omega, {\\mathcal{F}}, \\mathbb{P}), composé d’un ensemble \\Omega, d’une tribu \\mathcal{F}, et d’une mesure de probabilité \\mathbb{P}.\nCette définition permet de transposer l’aléa qui provient de \\Omega dans l’espace E. L’hypothèse \\{X \\in B\\} \\in \\mathcal{F} assure que cet ensemble est bien un évènement et donc que l’on peut calculer sa probabilité.\nUne fois que l’aléa a été transposé de \\Omega vers E, on souhaite également transposer la probabilité \\mathbb{P} sur E. Ceci motive l’introduction de la notion de loi.\nLes propriétés de \\mathbb{P} assurent que \\mathbb{P}_X est bien une loi de probabilité sur l’espace mesurable (E, \\mathcal{E}).",
    "crumbs": [
      "Cours",
      "Notations et rappels"
    ]
  },
  {
    "objectID": "Courses/notations.html#loi-discrètes",
    "href": "Courses/notations.html#loi-discrètes",
    "title": "Notations et rappels",
    "section": "Loi discrètes",
    "text": "Loi discrètes\nLes variables aléatoires discrètes sont celles à valeurs dans un ensemble E discret, le plus souvent \\mathbb{N}, muni de la tribu pleine \\mathcal{F} = \\mathcal{P}(E).\n\nExemple 1 (Loi de Bernoulli) La loi la plus simple est la loi de Bernoulli de paramètre p \\in [0,1], définie sur \\{0,1\\} par \\mathbb{P}(X=1) = 1-\\mathbb{P}(X=0) = p qui modélise une expérience aléatoire à deux issues (succès = 1 et échec = 0).\n\n\nExemple 2 (Loi binomiale) En sommant des variables aléatoires indépendantes de loi de Bernoulli on obtient une loi binomiale : \\mathbb{P}(X=k) = \\binom{n}{k} p^k (1-p)^{n-k}, pour k \\in \\{0,\\ldots,n\\}, qui modélise le nombre de succès parmi n lancers.\n\n\nExemple 3 (Loi géométrique) En observant le nombre d’expériences nécessaires avant d’obtenir un succès, on obtient une loi géométrique : \\mathbb{P}(X=k) = p (1-p)^{k-1}, pour k \\geq 1.\n\n\nExemple 4 (Loi de Poisson) La loi de Poisson de paramètre \\lambda &gt; 0 est définie par \\mathbb{P}(X=k) = e^{-\\lambda} \\lambda^k / k!, pour k \\in \\mathbb{N}, et modélise les événements rares.",
    "crumbs": [
      "Cours",
      "Notations et rappels"
    ]
  },
  {
    "objectID": "Courses/notations.html#lois-continues",
    "href": "Courses/notations.html#lois-continues",
    "title": "Notations et rappels",
    "section": "Lois continues",
    "text": "Lois continues\nParmi les variables aléatoires réelles non discrètes, beaucoup peuvent se représenter avec une densité, c’est-à-dire qu’il existe une fonction mesurable f : \\mathbb{R} \\to [0, \\infty[ d’intégrale 1. La loi d’une telle variable aléatoire X est alors donnée pour tout A \\in \\mathcal{B}(\\mathbb{R}) par \n    \\mathbb{P}(X \\in A) = \\int_A f(x) \\, \\mathrm d x \\enspace.\n Les propriétés de l’intégrale de Lebesgue assure que cette formule définit bien une loi de probabilité.\n\nExemple 5 (Loi uniforme) La loi uniforme sur un ensemble B \\in \\mathcal{B}(\\mathbb{R}), s’obtient avec la densité définie par \nf(x) = {1\\hspace{-3.8pt} 1}_B(x) / \\lambda (B) \\enspace,\n où \\lambda (B) représente la mesure de Lebesgue de l’ensemble B. En particulier pour la loi uniforme sur le segment [0,1] on obtient la fonction suivante: \nf(x) = {1\\hspace{-3.8pt} 1}_{[0,1]}(x)\\enspace.\n Si une variable aléatoire U suit une telle loi on note U \\sim \\mathcal{U}([0,1]).\n\n\nExemple 6 (Loi exponentielle) La loi exponentielle de paramètre \\gamma &gt; 0 est obtenue avec la densité donnée par \nf(x) = \\gamma e^{-\\gamma x} {1\\hspace{-3.8pt} 1}_{\\mathbb{R}_+}(x)\\enspace.\n Si une variable aléatoire X suit cette loi on note X \\sim \\mathcal{Exp}(\\gamma).\n\n\nExemple 7 (Loi normale/gaussienne univariée) On obtient la loi normale de paramètre \\mu \\in \\mathbb{R} et \\sigma^2 &gt; 0 correspond à loi dont la densité est donnée par la fonction réelle: \nf(x) = \\frac{1}{\\sqrt{2 \\pi} \\sigma}e^{-\\frac{1}{2 \\sigma^2}(x-\\mu)^2} \\enspace.\n Si une variable aléatoire X suit une telle loi on note X \\sim \\mathcal{N}(\\mu,\\sigma^2), \\mu correspondant à l’espérance de la loi, et \\sigma^2 à sa variance. On nomme loi normale centrée réduite le cas correspondant à \\mu = 0 et \\sigma^2 = 1.\n\n\nExemple 8 (Loi normale multivariée) On peut étendre les lois normales au cas multi-dimensionnel. Fixons d\\in\\mathbb{N}^* un entier non nul. Pour un vecteur \\mu \\in \\mathbb{R}^d et une matrice symétrique-définie positive \\Sigma\\in \\mathbb{R^{d\\times d}}, la densité normale mutlivariée associée est donnée par la fonction: \nf(x) = \\frac{1}{{(2 \\pi)}^{\\frac{d}{2}} {\\rm det}(\\Sigma)} e^{-\\frac{1}{2}(x-\\mu)^\\top \\Sigma ^{-1}(x-\\mu)}\n Notons que \\mu est l’espérance de la loi et \\Sigma la matrice de variance-covariance.",
    "crumbs": [
      "Cours",
      "Notations et rappels"
    ]
  },
  {
    "objectID": "Courses/notations.html#fonction-de-répartition",
    "href": "Courses/notations.html#fonction-de-répartition",
    "title": "Notations et rappels",
    "section": "Fonction de répartition",
    "text": "Fonction de répartition\nLa notion de variable aléatoire n’est pas facile à manipuler puisqu’elle part d’un espace \\Omega dont on ne sait rien. On souhaite donc caractériser la loi d’une variable aléatoire en ne considérant que l’espace d’arrivée (E, \\mathcal{E}) .\nPlusieurs outils existent : la fonction de répartition (pour des variables aléatoires réelles), la fonction caractéristique (pour des variables aléatoires dans \\mathbb{R}^d), la fonction génératrice des moments (pour des variables aléatoires discrètes), etc. On se contente ici de la fonction de répartition qui nous sera utile pour simuler des variables aléatoires, ainsi que son inverse au sens de Levy.\n\nDéfinition 3 (Fonction de répartition 🇬🇧: cumulative distribution function) \nSoit X une variable aléatoire sur (\\mathbb{R}, \\mathcal{B}(\\mathbb{R})). La fonction de répartition de X est la fonction F_X définie sur \\mathbb{R} par \n\\begin{align*}\n    F_X(x) & = \\mathbb{P}(X \\leq x)\\\\\n           & = \\mathbb{P}(X \\in ]-\\infty, x]) \\enspace.\n\\end{align*}\n\n\nOn appelle quantile d’ordre p\\in (0,1), la quantité F_X^\\leftarrow(p). La médiane est égale à F_X^\\leftarrow(1/2), les premiers et troisièmes quartiles sont égaux à F_X^\\leftarrow(1/4) et F_X^\\leftarrow(3/4). Enfin, les déciles sont les quantiles F_X^\\leftarrow(k/10) pour k=1,\\dots, 9.\n\nExemple 9 (Cas discret) Soit (x_i)_{i \\in I} une suite ordonnée de réels, avec I \\subset \\mathbb{N}. Si X est une variable aléatoire discrète prenant les valeurs (x_i)_{i \\in I} et de loi (p_i = \\mathbb{P}(X=x_i))_{i \\in I}, alors \n    F_X(x) = \\sum_{i \\in I} p_i {1\\hspace{-3.8pt} 1}_{[x_i, \\infty[}(x) \\enspace.\n\n\n\nExemple 10 (Cas continu) Si X est une variable aléatoire de densité f, alors \n    F_X(x) = \\int_{-\\infty}^x f(t) \\, \\mathrm dt \\enspace.\n\n\nLe graphe des fonctions de répartition des loi de Bernoulli, uniforme et normale sont représentées en Figure XXX. Notons que la fonction de répartition de la loi normale \\mathcal{N}(0,1), souvent notée \\Phi, n’admet pas d’expression explicite autre que \n\\Phi(x) = \\dfrac{1}{\\sqrt{2\\pi}} \\int_{-\\infty}^x e^{-\\frac{x^2}{2}}\\, \\mathrm d x\\enspace,\n Les valeurs numériques de \\Phi(x) étaient autrefois reportées dans des tables1. Par transformation affine, si X \\sim \\mathcal{N}(\\mu, \\sigma^2) — ce que l’on peut aussi écrire : X=\\mu + \\sigma Y, avec Y\\sim \\mathcal{N}(0,1) — alors sa fonction de répartition est donnée par F_X(x)=\\Phi((x-\\mu)/\\sigma).\n1 Wikipedia: loi normale\nProposition 1 (Propriétés de la fonction de répartition) Soit X une variable aléatoire de fonction de répartition F_X.\n\nF_X est une fonction croissante, de limite 0 en -\\infty et de limite 1 en +\\infty.\nF_X est continue à droite en tout point.\nPour tout x \\in \\mathbb{R}, on a \\mathbb{P}(X=x) = F_X(x) - F_X(x-), où F_X(x-) = \\lim_{\\epsilon \\to 0+} F_X(x- \\epsilon).\nSi X a pour densité f, alors F_X est dérivable \\lambda-presque partout de dérivée f.\n\n\nLa propriété 3. est utile dans le cas discret : les valeurs prises par X correspondent aux points de discontinuité de F_X et les probabilités associées correspondent à la hauteur du saut.\nLa propriété 4. donne le lien entre la fonction de répartition d’une variable aléatoire à densité et sa densité. On peut donc retrouver la loi de X à partir de sa fonction de répartition. Le théorème suivant généralise ce résultat à toute variable aléatoire réelle (pas nécessairement discrète ou à densité).\n\nThéorème 1 (Caractérisation de la loi d’une variable aléatoire réelle) La fonction de répartition d’une variable aléatoire caractérise sa loi : deux variables aléatoires ont même loi si et seulement si elles ont même fonction de répartition.\n\nXXX source + proof???\nOn rappelle que la tribu des boréliens est engendrée par la famille d’ensembles \\{]-\\infty,x], x \\in \\mathbb{R}\\}. Le théorème précédent assure que si on connaît la mesure \\mathbb{P}_X sur cette famille d’ensemble alors on la connaît partout.\nXXX TODO: move this in correct part.\n\nExemple 11 (Loi exponentielle depuis une loi uniforme) On considère une variable aléatoire U de loi uniforme sur [0,1] et on pose X = -\\ln(1-U). Déterminons la loi de X en calculant sa fonction de répartition. Pour tout x \\in \\mathbb{R}, \n\\begin{align*}\nF_X(x) = & \\mathbb{P}(X \\leq x) \\\\\n       = & \\mathbb{P}(-\\ln(1-U) \\leq x) \\\\\n       = & \\mathbb{P}(U \\leq 1-e^{-x}) \\\\\n       = &\n    \\begin{cases}\n        0           & \\text{ si }x &lt; 0\\,,    \\\\\n        1 - e^{-x} & \\text{ si }x \\geq 0\\,,\n    \\end{cases}\n\\end{align*}\n\noù on a utilisé l’égalité \\mathbb{P}(U \\leq t) = t pour tout t \\in [0,1]. Ainsi la variable aléatoire X a la même fonction de répartition qu’une loi exponentielle de paramètre 1. On en conclut que X \\sim \\mathcal{Exp}(1). Notons que l’on peut aussi montrer que -\\ln(X)\\sim\\mathcal{E}(1), sachant que U et 1-U ont la même loi.",
    "crumbs": [
      "Cours",
      "Notations et rappels"
    ]
  },
  {
    "objectID": "Courses/notations.html#fonction-quantile-inverse-généralisée-à-gauche",
    "href": "Courses/notations.html#fonction-quantile-inverse-généralisée-à-gauche",
    "title": "Notations et rappels",
    "section": "Fonction quantile, inverse généralisée à gauche",
    "text": "Fonction quantile, inverse généralisée à gauche\nLa fonction de répartition étant une fonction croissante on peut donner un sens à son inverse généralisée de la manière suivante.\n\nDéfinition 4 (Fonction quantile/ inverse généralisée 🇬🇧: quantile distribution function) \nSoit X une variable aléatoire sur (\\mathbb{R}, \\mathcal{B}(\\mathbb{R})) et F_X sa fonction de répartion. La fonction quantile associée F_X^\\leftarrow: ]0,1[\\rightarrow \\mathbb{R} est définie par \n  F_X^\\leftarrow(p)=  \\inf\\{ x \\in \\mathbb{R} \\colon F_X(x)\\geq p\\} \\enspace.\n\n\nOn parle parfois aussi d’inverse au sens de Levy pour cette inverse généralisée.\nDans le cas où la fonction de répartition F est bijective, alors l’inverse de la fonction de répartition coincide avec la fonction quantile.",
    "crumbs": [
      "Cours",
      "Notations et rappels"
    ]
  },
  {
    "objectID": "Courses/notations.html#visualisation-densité-fonction-de-répartition-quantiles-etc.",
    "href": "Courses/notations.html#visualisation-densité-fonction-de-répartition-quantiles-etc.",
    "title": "Notations et rappels",
    "section": "Visualisation: densité, fonction de répartition, quantiles, etc.",
    "text": "Visualisation: densité, fonction de répartition, quantiles, etc.\n\nCas des variables continues\n\nObservablePython / Shiny\n\n\n\nPlotly = require('plotly.js-dist');\ndists = require( 'https://cdn.jsdelivr.net/gh/stdlib-js/stats-base-dists@umd/browser.js' );\n// see source here: https://github.com/stdlib-js/stats-base-dists/tree/umd\n// continuous case\njstatPDFs = () =&gt; {\n  const distributions = Object.keys(dists);\n  // Get in continuousDistributions the distributions whose pdf, cdf and quantile are defined\n    const continuousDistributions = distributions.filter(name =&gt; dists[name].pdf && dists[name].cdf && dists[name].quantile);\n  return continuousDistributions\n};\noutput = jstatPDFs();\n\nexcludedPDFs = [];\npdfNames_unsorted = output.filter(name =&gt; !excludedPDFs.includes(name));\npdfNames=pdfNames_unsorted.toSorted();\nviewof inputs = Inputs.form([\n      Inputs.range([-10, 10], {value: 0.1, step: 0.001, label: tex`\\mu`}),\n      Inputs.range([0.01, 5], {value: 1.01, step: 0.001, label: tex`\\sigma`}),\n      Inputs.range([0.001, 0.999], {value: 0.75, step: 0.001, label:tex`\\alpha`}),\n    ]);\n\nviewof distrib_name = Inputs.select(pdfNames, {value: \"normal\", label: \"Distribution\"});\n\n\n{\n  const x = d3.range(-5, 5, 0.01);\n  const pdf = x.map(x =&gt; dists[distrib_name].pdf(x, mu, sigma));\n  const cdf = x.map(x =&gt; dists[distrib_name].cdf(x, mu, sigma));\n  const inv = x.map(x =&gt; dists[distrib_name].quantile(x, mu, sigma));\n  const quantile = dists[distrib_name].quantile(alpha, mu, sigma);\n  const filteredX = x.filter(coord =&gt; coord &lt;= quantile);\n  const filteredPdf = pdf.filter((_, i) =&gt; x[i] &lt;= quantile);\n  const filteredCdf = cdf.filter(coord =&gt; coord &lt;= quantile);\n\n\n{\nvar trace1 = {\n      type: \"scatter\",\n      name: 'Quantile',\n      x : cdf,\n      y : x,\n      line: {color: 'black'},\n      xaxis: 'x1',\n      yaxis: 'y3',\n\n\n};\n\nvar trace12 = {\n        x : [alpha, alpha],\n        y : [x[0], quantile],\n        mode: 'lines',\n        line: {\n            dash: 'dash',\n            width: 1,\n            color: ' #428BCA',\n            marks: {\n                size: 0,\n                color: ' #428BCA',\n            }\n\n          },\n      xaxis: 'x1',\n      yaxis: 'y3',\n\n\n};\n\n\n\nvar trace13 = {\n    x: [0, alpha],\n    y: [quantile, quantile],\n    mode: 'scatter',\n    line: {\n        dash: 'dash',\n        width: 1,\n        color: ' #428BCA',\n        marks: {\n            size: 0,\n        }\n    },\n    xaxis: 'x1',\n    yaxis: 'y3'\n}\n\n\nvar trace2 = {\nx: cdf,\ny: cdf,\ntype: 'scatter',\nname: 'identity',\nline: {color: 'black'},\nxaxis: 'x1',\nyaxis: 'y2'\n\n};\n\n\nvar trace21 = {\n        x : [alpha, 1],\n        y : [alpha, alpha],\n        mode: 'lines',\n        line: {\n            dash: 'dash',\n            width: 1,\n            color: ' #428BCA',\n            marks: {\n                size: 0,\n                color: ' #428BCA',\n            }\n\n          },\n      xaxis: 'x1',\n      yaxis: 'y2'\n}\n\nvar trace22 = {\n        x : [alpha, alpha],\n        y : [alpha, 1],\n        mode: 'lines',\n        line: {\n            dash: 'dash',\n            width: 1,\n            color: ' #428BCA',\n            marks: {\n                size: 0,\n                color: ' #428BCA',\n            }\n\n          },\n      xaxis: 'x1',\n      yaxis: 'y2'\n}\n\n\n\nvar trace23 = {\n    x: [alpha],\n    y: [alpha],\n    mode: 'scatter',\n    line: {\n        dash: 'dash',\n        width: 1,\n        color: ' #428BCA',\n        marks: {\n            size: 0,\n        }\n    },\n    xaxis: 'x1',\n    yaxis: 'y2'\n}\n\n\nvar trace31 = {\n      type: \"scatter\",\n      mode: \"lines\",\n      name: 'PDF2',\n      x: x,\n      y: cdf,\n      line: {color: 'black'},\n      xaxis: 'x2',\n      yaxis: 'y2',\n};\n\nvar trace32 = {\n        x : filteredX,\n        y : filteredX.map(x =&gt; alpha),\n        mode: 'lines',\n        line: {\n            dash: 'dash',\n            width: 1,\n            color: ' #428BCA',\n            marks: {\n                size: 0,\n                color: ' #428BCA',\n            }\n          },\n      xaxis: 'x2',\n      yaxis: 'y2'\n}\n\nvar trace33 = {\n        x : [quantile, quantile],\n        y : [0, alpha],\n        mode: 'lines',\n        line: {\n            dash: 'dash',\n            width: 1,\n            color: ' #428BCA',\n            marks: {\n                size: 0,\n                color: ' #428BCA',\n            }\n          },\n\n      xaxis: 'x2',\n      yaxis: 'y2'\n}\n\nvar trace34 = {\n    x: [quantile],\n    y: [alpha],\n    mode: 'scatter',\n    line: {\n        dash: 'dash',\n        width: 1,\n        color: ' #428BCA',\n        marks: {\n            size: 0,\n        }\n    },\n    xaxis: 'x2',\n    yaxis: 'y2'\n}\n\n\nvar trace41 = {\n\n  type: \"scatter\",\n  name: 'Quantile2',\n  fill: 'tozeroy',\n  x : filteredX,\n  y : filteredPdf,\n  opacity: 0.9,\n  line: {color: ' #428BCA'},\n  xaxis: 'x2',\n  yaxis: 'y1'\n\n};\n\n\nvar trace42 = {\n\n  type: \"scatter\",\n  mode: \"lines\",\n  name: 'PDF2',\n  x: x,\n  y: pdf,\n  line: {color: 'black'},\n  xaxis: 'x2',\n  yaxis: 'y1'\n\n};\n\nvar data = [\n  trace1,\n  trace12, trace13,\n  trace2, trace21, trace22, trace23,\n  trace31, trace32, trace33, trace34,\n  trace41, trace42];\n\n\nvar layout = {\n\n  title: 'Distribution et quantile',\n  xaxis: {\n    domain: [0, 0.32],\n    anchor: 'y1'\n  },\n  yaxis: {\n    domain: [0, 0.24],\n    anchor: 'x1'\n\n  },\n  xaxis2: {\n    domain: [0.35, 1],\n    anchor: 'y'\n  },\n\n  yaxis2: {\n    domain: [0.26, 0.49],\n    anchor: 'x1'\n  },\n\n  yaxis3: {\n    domain: [0.5, 1],\n    anchor: 'x1'\n  },\n\n// legend offset\n  showlegend: false,\n  height: 680,\n  annotations: [\n\n    {\n      x: 1/4,\n      y: quantile - (x[0]-x.slice(-1))/20,\n      xref: 'x1',\n      yref: 'y3',\n      text: 'q= ' + quantile.toFixed(2),\n      font: {\n        size: 12,\n        color: '#428BCA',\n\n      },\n      showarrow: false,\n      arrowhead: 0,\n      ax: 25,\n      ay: -10,\n\n    },\n    {\n      x: 1/4,\n      y: alpha,\n      xref: 'x1',\n      yref: 'y2',\n      text: 'alpha= ' + alpha.toFixed(2),\n      font: {\n        size: 12,\n        color: '#428BCA',\n\n      },\n      showarrow: false,\n      arrowhead: 0,\n      ax: 25,\n      ay: -10,\n\n    },\n    {\n        text: \"Fonction quantile\",\n      font: {\n      size: 15,\n      color: 'black',\n    },\n    showarrow: false,\n    align: 'center',\n    x: -0.01,\n    y: 1.05,\n    xref: 'paper',\n    yref: 'paper',\n    },\n    {\n      text: \"Fonction de répartition\",\n      font: {\n      size: 15,\n      color: 'black',\n            },\n    showarrow: false,\n    align: 'center',\n    x: 0.65,\n    y: 0.52,\n    xref: 'paper',\n    yref: 'paper',\n    },\n    {\n      text: \"Fonction de densité\",\n      font: {\n      size: 15,\n      color: 'black',\n            },\n    showarrow: false,\n    align: 'center',\n    x: 0.65,\n    y: 0.225,\n    xref: 'paper',\n    yref: 'paper',\n    },\n  ]\n\n};\n// XXX: TODO: put the xticks labels on the middle plot for x and on the right plot for y\n    var config = {responsive: true}\n    const div = DOM.element('div');\n    Plotly.newPlot(div, data, layout, config);\n    return div;\n  }\n\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmu = inputs[0];\nsigma = inputs[1];\nalpha = inputs[2]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#| standalone: true\n#| viewerHeight: 830\nimport numpy as np\nfrom scipy import stats\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nfrom shiny import ui, render, App\nfrom shinywidgets import output_widget, render_widget\n\n\ndef keep_no_param_distribution():\n    distributions = stats._continuous_distns._distn_names\n    distributions_0 = []\n    for _, name in enumerate(distributions):\n        dist = getattr(stats, name)\n        if not dist.shapes or len(dist.shapes) == 0:\n            distributions_0.append(name)\n    distributions_0_val = [\n        getattr(stats.distributions, string) for string in distributions_0\n    ]\n    distributions_0_dict = dict(zip(distributions_0, distributions_0_val))\n    return distributions_0_dict\n\n\ndistributions_0_dict = keep_no_param_distribution()\n\nmu = 0\nsigma = 1\n\napp_ui = ui.page_fluid(\n    ui.div(\n        ui.input_slider(\"alpha\", \"Quantile\", 0.01, 0.99, value=0.5, step=0.01),\n        ui.input_slider(\"xrange\", \"x-range\", -10, 10, value=(-5, 5), step=0.2),\n        ui.input_select(\n            \"distrib\",\n            \"Distribution\",\n            list(distributions_0_dict.keys()),\n            selected='norm'\n        ),\n        class_=\"d-flex gap-3\",\n    ),\n    output_widget(\"my_widget\"),\n)\n\n\ndef server(input, output, session):\n    @output\n    @render_widget\n    def my_widget():\n        fig = make_subplots(\n            rows=3,\n            cols=2,\n            vertical_spacing=0.1,\n            horizontal_spacing=0.15,\n            subplot_titles=(\n                \"Fonction quantile\",\n                \"\",\n                \"\",\n                \"Fonction de répartition\",\n                \"\",\n                \"Densité et quantile\",\n            ),\n            column_widths=[0.2, 0.5],\n            row_heights=[0.35, 0.17, 0.17],\n        )\n\n        alpha = input.alpha()\n        distribution = distributions_0_dict[input.distrib()]\n        x = np.linspace(input.xrange()[0], input.xrange()[1], num=400)\n        cdf_data = distribution.cdf(x, loc=mu, scale=sigma)\n        pdf_data = distribution.pdf(x, loc=mu, scale=sigma)\n        q_alpha = distribution.ppf(alpha, loc=mu, scale=sigma)\n\n        fig.update_layout(autosize=True, height=700)\n\n        # Quantile plot\n        fig.add_trace(\n            go.Scatter(\n                x=cdf_data, y=x, mode=\"lines\", marker={\"color\": \"black\"}\n            ),\n            row=1,\n            col=1,\n        )\n        # Diagonal\n        fig.add_trace(\n            go.Scatter(\n                x=cdf_data, y=cdf_data, mode=\"lines\", marker={\"color\": \"black\"}\n            ),\n            row=2,\n            col=1,\n        )\n        # Cdf part\n        fig.add_trace(\n            go.Scatter(\n                x=x, y=cdf_data, mode=\"lines\", marker={\"color\": \"black\"}\n            ),\n            row=2,\n            col=2,\n        )\n        # pdf part\n        fig.add_scatter(\n            x=x[x &lt; q_alpha],\n            y=pdf_data[x &lt; q_alpha],\n            fill=\"tozeroy\",\n            mode=\"none\",\n            fillcolor=\"rgb(66, 139, 202)\",\n            row=3,\n            col=2,\n        )\n\n        fig.add_trace(\n            go.Scatter(\n                x=x, y=pdf_data, mode=\"lines\", marker={\"color\": \"black\"}\n            ),\n            row=3,\n            col=2,\n        )\n\n        # Dots\n        fig.add_trace(\n            go.Scatter(\n                x=[alpha],\n                y=[q_alpha],\n                mode=\"markers\",\n                marker={\"color\": \"rgb(66, 139, 202)\"},\n                marker_symbol=\"x\",\n                marker_size=8,\n            ),\n            row=1,\n            col=1,\n        )\n\n        fig.add_trace(\n            go.Scatter(\n                x=[alpha],\n                y=[alpha],\n                mode=\"markers\",\n                marker={\"color\": \"rgb(66, 139, 202)\"},\n                marker_symbol=\"x\",\n                marker_size=8,\n            ),\n            row=2,\n            col=1,\n        )\n\n        fig.add_trace(\n            go.Scatter(\n                x=[q_alpha],\n                y=[alpha],\n                mode=\"markers\",\n                marker={\"color\": \"rgb(66, 139, 202)\"},\n                marker_symbol=\"x\",\n                marker_size=8,\n            ),\n            row=2,\n            col=2,\n        )\n\n        # Lines\n        fig.add_trace(\n            go.Scatter(\n                x=[alpha, alpha],\n                y=[x[0], q_alpha],\n                mode=\"lines\",\n                line=dict(dash=\"dash\", color=\"rgb(66, 139, 202)\")\n            ),\n            row=1,\n            col=1\n        )\n\n        fig.add_trace(\n            go.Scatter(\n                x=[alpha, alpha],\n                y=[alpha, 1.],\n                mode=\"lines\",\n                line=dict(dash=\"dash\", color=\"rgb(66, 139, 202)\")\n            ),\n            row=2,\n            col=1\n        )\n        fig.add_trace(\n            go.Scatter(\n                x=[alpha, 1],\n                y=[alpha, alpha],\n                mode=\"lines\",\n                line=dict(dash=\"dash\", color=\"rgb(66, 139, 202)\")\n            ),\n            row=2,\n            col=1\n        )\n        fig.add_trace(\n            go.Scatter(\n                x=[x[0], q_alpha],\n                y=[alpha, alpha],\n                mode=\"lines\",\n                line=dict(dash=\"dash\", color=\"rgb(66, 139, 202)\")\n            ),\n            row=2,\n            col=2\n        )\n        fig.add_trace(\n            go.Scatter(\n                x=[x[0], q_alpha],\n                y=[x[0], 0],\n                mode=\"lines\",\n                line=dict(dash=\"dash\", color=\"rgb(66, 139, 202)\")\n            ),\n            row=2,\n            col=2\n        )\n        # Axes ranges\n        fig.update_xaxes(range=[0, 1.], row=1, col=1)\n        fig.update_yaxes(matches=\"x6\", row=1, col=1)\n\n        fig.update_yaxes(range=[0, 1.], row=2, col=1)\n        fig.update_xaxes(matches=\"x1\", row=2, col=1)\n\n        fig.update_yaxes(rangemode=\"tozero\", row=3, col=2)\n        fig.update_xaxes(range=[x[0], x[-1]], row=3, col=2)\n\n        fig.update_xaxes(matches=\"x6\", row=2, col=2)\n        fig.update_yaxes(matches=\"y3\", row=2, col=2)\n\n        # Add dropdown\n        fig.update_layout(\n            showlegend=False,\n            template=\"simple_white\",\n        )\n        return fig\n\n\napp = App(app_ui, server)\n\n\n\n\n\n\nCas des variables discrètes\n\nObservablePython / Shiny\n\n\n\ndiscretePDFs = () =&gt; {\n  const distributions = Object.keys(dists);\n  // Get in continuousDistributions the distributions whose pdf, cdf and quantile are defined\n    const continuousDistributions = distributions.filter(name =&gt; dists[name].pmf && dists[name].cdf && dists[name].quantile);\n  return continuousDistributions\n};\noutput_discr = discretePDFs();\n\npmfNames_unsorted = output_discr.filter(name =&gt; !excludedPDFs.includes(name));\npmfNames=pmfNames_unsorted.toSorted();\nviewof inputs_disc = Inputs.form([\n      Inputs.range([-10, 10], {value: 0.1, step: 0.001, label: tex`\\mu `}),\n      Inputs.range([0.01, 5], {value: 1.01, step: 0.001, label: tex`\\sigma `}),\n      Inputs.range([0.001, 0.999], {value: 0.75, step: 0.001, label:tex`\\alpha`}),\n    ]);\n\nviewof distrib_name_discr = Inputs.select(pmfNames, {value: \"normal\", label: \"Distribution\"});\n\n\n{\n  const z = d3.range(-5, 5, 0.01);\n  const pmf = z.map(z =&gt; dists[distrib_name_discr].pmf(z, mu_disc, sigma_disc));\n  const cdf = z.map(z =&gt; dists[distrib_name_discr].cdf(z, mu_disc, sigma_disc));\n  const inv = z.map(z =&gt; dists[distrib_name_discr].quantile(z, mu_disc, sigma_disc));\n  const quantile = dists[distrib_name_discr].quantile(alpha_disc, mu_disc, sigma_disc);\n  const filteredX = z.filter(coord =&gt; coord &lt;= quantile);\n  const filteredPmf = pmf.filter((_, i) =&gt; z[i] &lt;= quantile);\n  const filteredCdf = cdf.filter(coord =&gt; coord &lt;= quantile);\n\n\n{\nvar trace1 = {\n      type: \"scatter\",\n      name: 'Quantile',\n      x : cdf,\n      y : z,\n      line: {color: 'black'},\n      xaxis: 'x1',\n      yaxis: 'y3',\n\n\n};\n\nvar trace12 = {\n        x : [alpha_disc, alpha_disc],\n        y : [z[0], quantile],\n        mode: 'lines',\n        line: {\n            dash: 'dash',\n            width: 1,\n            color: ' #428BCA',\n            marks: {\n                size: 0,\n                color: ' #428BCA',\n            }\n\n          },\n      xaxis: 'x1',\n      yaxis: 'y3',\n\n\n};\n\n\n\nvar trace13 = {\n    x: [0, alpha_disc],\n    y: [quantile, quantile],\n    mode: 'scatter',\n    line: {\n        dash: 'dash',\n        width: 1,\n        color: ' #428BCA',\n        marks: {\n            size: 0,\n        }\n    },\n    xaxis: 'x1',\n    yaxis: 'y3'\n}\n\n\nvar trace2 = {\nx: cdf,\ny: cdf,\ntype: 'scatter',\nname: 'identity',\nline: {color: 'black'},\nxaxis: 'x1',\nyaxis: 'y2'\n\n};\n\n\nvar trace21 = {\n        x : [alpha_disc, 1],\n        y : [alpha_disc, alpha_disc],\n        mode: 'lines',\n        line: {\n            dash: 'dash',\n            width: 1,\n            color: ' #428BCA',\n            marks: {\n                size: 0,\n                color: ' #428BCA',\n            }\n\n          },\n      xaxis: 'x1',\n      yaxis: 'y2'\n}\n\nvar trace22 = {\n        x : [alpha_disc, alpha_disc],\n        y : [alpha_disc, 1],\n        mode: 'lines',\n        line: {\n            dash: 'dash',\n            width: 1,\n            color: ' #428BCA',\n            marks: {\n                size: 0,\n                color: ' #428BCA',\n            }\n\n          },\n      xaxis: 'x1',\n      yaxis: 'y2'\n}\n\nvar trace23 = {\n    x: [alpha_disc],\n    y: [alpha_disc],\n    mode: 'scatter',\n    line: {\n        dash: 'dash',\n        width: 1,\n        color: ' #428BCA',\n        marks: {\n            size: 0,\n        }\n    },\n    xaxis: 'x1',\n    yaxis: 'y2'\n}\n\n\nvar trace31 = {\n      type: \"scatter\",\n      mode: \"lines\",\n      name: 'PDF2',\n      x: z,\n      y: cdf,\n      line: {color: 'black'},\n      xaxis: 'x2',\n      yaxis: 'y2',\n};\n\nvar trace32 = {\n        x : filteredX,\n        y : filteredX.map(z =&gt; alpha_disc),\n        mode: 'lines',\n        line: {\n            dash: 'dash',\n            width: 1,\n            color: ' #428BCA',\n            marks: {\n                size: 0,\n                color: ' #428BCA',\n            }\n          },\n      xaxis: 'x2',\n      yaxis: 'y2'\n}\n\nvar trace33 = {\n        x : [quantile, quantile],\n        y : [0, alpha_disc],\n        mode: 'lines',\n        line: {\n            dash: 'dash',\n            width: 1,\n            color: ' #428BCA',\n            marks: {\n                size: 0,\n                color: ' #428BCA',\n            }\n          },\n\n      xaxis: 'x2',\n      yaxis: 'y2'\n}\n\nvar trace34 = {\n    x: [quantile],\n    y: [alpha],\n    mode: 'scatter',\n    line: {\n        dash: 'dash',\n        width: 1,\n        color: ' #428BCA',\n        marks: {\n            size: 0,\n        }\n    },\n    xaxis: 'x2',\n    yaxis: 'y2'\n}\n\n\nvar trace41 = {\n\n  type: \"scatter\",\n  name: 'Quantile2',\n  fill: 'tozeroy',\n  x : filteredX,\n  y : filteredPmf,\n  opacity: 0.9,\n  line: {color: ' #428BCA'},\n  xaxis: 'x2',\n  yaxis: 'y1'\n\n};\n\n\nvar trace42 = {\n\n  type: \"scatter\",\n  mode: \"lines\",\n  name: 'PDF2',\n  x: z,\n  y: pmf,\n  line: {color: 'black'},\n  xaxis: 'x2',\n  yaxis: 'y1'\n\n};\n\nvar data = [\n  trace1,\n  trace12, trace13,\n  trace2, trace21, trace22, trace23,\n  trace31, trace32, trace33, trace34,\n  trace41, trace42];\n\n\nvar layout = {\n\n  title: 'Distribution et quantile',\n  xaxis: {\n    domain: [0, 0.32],\n    anchor: 'y1'\n  },\n  yaxis: {\n    domain: [0, 0.24],\n    anchor: 'x1'\n\n  },\n  xaxis2: {\n    domain: [0.35, 1],\n    anchor: 'y'\n  },\n\n  yaxis2: {\n    domain: [0.26, 0.49],\n    anchor: 'x1'\n  },\n\n\n\n  yaxis3: {\n    domain: [0.5, 1],\n    anchor: 'x1'\n\n  },\n\n  showlegend: false,\n  height: 680,\n\n\n  annotations: [\n\n    {\n      x: 1/4,\n      y: quantile - (z[0]-z.slice(-1))/20,\n      xref: 'x1',\n      yref: 'y3',\n      text: 'q=' + quantile.toFixed(2),\n      font: {\n        size: 12,\n        color: '#428BCA',\n\n      },\n      showarrow: false,\n      arrowhead: 0,\n      ax: 25,\n      ay: -10,\n\n    },\n    {\n      x: 1/4,\n      y: alpha_disc,\n      xref: 'x1',\n      yref: 'y2',\n      text: 'alpha=' + alpha_disc.toFixed(2),\n      font: {\n        size: 12,\n        color: '#428BCA',\n\n      },\n      showarrow: false,\n      arrowhead: 0,\n      ax: 25,\n      ay: -10,\n\n    },\n    {\n        text: \"Fonction quantile\",\n      font: {\n      size: 15,\n      color: 'black',\n    },\n    showarrow: false,\n    align: 'center',\n    x: -0.01,\n    y: 1.05,\n    xref: 'paper',\n    yref: 'paper',\n    },\n    {\n      text: \"Fonction de répartition\",\n      font: {\n      size: 15,\n      color: 'black',\n            },\n    showarrow: false,\n    align: 'center',\n    x: 0.65,\n    y: 0.52,\n    xref: 'paper',\n    yref: 'paper',\n    },\n    {\n      text: \"Fonction de densité\",\n      font: {\n      size: 15,\n      color: 'black',\n            },\n    showarrow: false,\n    align: 'center',\n    x: 0.65,\n    y: 0.225,\n    xref: 'paper',\n    yref: 'paper',\n    },\n  ]\n\n};\n// XXX: TODO: put the xticks labels on the middle plot for x and on the right plot for y\n\n    var config = {responsive: true}\n    const div = DOM.element('div');\n    Plotly.newPlot(div, data, layout, config);\n    return div;\n  }\n\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmu_disc = inputs_disc[0];\nsigma_disc = inputs_disc[1];\nalpha_disc = inputs_disc[2];\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#| standalone: true\n#| viewerHeight: 830\nimport numpy as np\nfrom scipy import stats\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nfrom shiny import ui, render, App\nfrom shinywidgets import output_widget, render_widget\n\n\ndef keep_no_param_distribution_disc():\n    distributions = stats._discrete_distns._distn_names\n    distributions_0 = [name for name in distributions if not getattr(stats, name).shapes or len(getattr(stats, name).shapes) in [1, 2]]\n    distributions_0_val = [getattr(stats.distributions, string) for string in distributions_0]\n    distributions_0_dict = dict(zip(distributions_0, distributions_0_val))\n    return distributions_0_dict\n\ndef cdf_tool(x, dtype='int64'):\n    y = np.zeros(2*(len(x)), dtype=dtype)\n    y[::2]=x\n    y[1::2]=x\n    return y[1::], y[:-1], y\n\ndef pmf_tool(x, dtype='int64'):\n    y = np.zeros(2*(len(x)), dtype=dtype)\n    y[::2]=x\n    return y[1::], y[:-1], y\n\ndef insert_nones(my_list):\n    for i, val in enumerate(my_list):\n        if i % 3 == 2:\n            my_list.insert(i, None)\n    return my_list\n\ndistributions_0_dict = keep_no_param_distribution_disc()\n\napp_ui = ui.page_fluid(\n    ui.div(\n        ui.input_slider(\"alpha\", \"Quantile\", 0.01, 0.99, value=0.5, step=0.01),\n        ui.input_slider(\"xrange\", \"x-range\", -10, 10, value=(-5.5, 5.5), step=0.2),\n        ui.input_select(\n            \"distrib\",\n            \"Distribution\",\n            list(distributions_0_dict.keys()),\n            selected='poisson'\n        ),\n        class_=\"d-flex gap-3\",\n    ),\n    output_widget(\"my_widget\"),\n)\n\n\ndef server(input, output, session):\n    @output\n    @render_widget\n    def my_widget():\n        fig = make_subplots(\n            rows=3,\n            cols=2,\n            vertical_spacing=0.1,\n            horizontal_spacing=0.15,\n            subplot_titles=(\n                \"Fonction quantile\",\n                \"\",\n                \"\",\n                \"Fonction de répartition\",\n                \"\",\n                \"Fonction de masse et quantile\",\n            ),\n            column_widths=[0.2, 0.5],\n            row_heights=[0.35, 0.17, 0.17],\n        )\n\n\n        alpha = input.alpha()\n        # alpha=0.5\n\n        mu = 0.5  # Param needed for some distribution\n        if input.distrib()=='zipf':\n            mu = 2\n        distribution = distributions_0_dict[input.distrib()]\n        # distribution=distributions_0_dict['poisson']\n        x = np.arange(np.floor(input.xrange()[0]), np.ceil(input.xrange()[1]))\n        # x = np.arange(np.floor(-5.5), np.ceil(5.5))\n\n        cdf_data = distribution.cdf(x, mu)\n        pmf_data = distribution.pmf(x, mu)\n        q_alpha = distribution.ppf(alpha, mu)\n        support = pmf_data.nonzero()[0]\n        fig.update_layout(autosize=True, height=700)\n\n        # Quantile plot\n        new_x, new_y, new_z = cdf_tool(support)\n        _, _, new_pmf = pmf_tool(support)\n\n        fig.add_trace(\n            go.Scatter(\n                x=insert_nones(list(np.append(cdf_data[new_y[::-1]], distribution.cdf(x[0], mu)))),\n                y=insert_nones(list(np.append(x[new_x[::-1]], x[new_x[0]]))),\n                mode=\"lines\",\n                line=dict(color=\"black\")\n            ),\n            row=1,\n            col=1,\n        )\n        fig.add_trace(\n             go.Scatter(\n                x=cdf_data[support], y=x[support],\n                mode=\"markers\", marker={\"color\": \"black\"}\n            ),\n            row=1,\n            col=1,\n        )\n        # Diagonal\n        fig.add_trace(\n            go.Scatter(\n                x=cdf_data, y=cdf_data, mode=\"lines\", marker={\"color\": \"black\"}\n            ),\n            row=2,\n            col=1,\n        )\n        # Cdf part\n        fig.add_trace(\n            go.Scatter(\n                x=x[support], y=cdf_data[support],\n                mode=\"markers\", marker={\"color\": \"black\"}\n            ),\n            row=2,\n            col=2,\n        )\n        fig.add_trace(\n            go.Scatter(\n                x=insert_nones(list(np.append(np.insert(x[new_x], 0, [x[0], x[new_x[0]]]),x[-1]))),\n                y=insert_nones(list(np.append(np.insert(cdf_data[new_y], 0, [0,0]), cdf_data[-1]))),\n                mode=\"lines\",\n                line=dict(color=\"black\")\n            ),\n            row=2,\n            col=2\n        )\n\n        fig.add_trace(\n            go.Scatter(\n                x=x, y=pmf_data, mode=\"markers\", marker={\"color\": \"black\"}\n            ),\n            row=3,\n            col=2,\n        )\n        fig.add_trace(\n            go.Scatter(\n                x=x, y=pmf_data, mode=\"markers\", marker={\"color\": \"black\"}\n            ),\n            row=3,\n            col=2,\n        )\n        x_bar = insert_nones(list(x[new_z]))\n        y_bar = insert_nones(list(pmf_data[new_pmf]))\n        fig.add_trace(\n            go.Scatter(\n                x=x_bar,\n                y=y_bar,\n                mode=\"lines\",\n                line=dict(color=\"black\")\n            ),\n            row=3,\n            col=2\n        )\n        _,_, devil_x = cdf_tool(x[x&lt;=q_alpha])\n        _,_, devil_y = cdf_tool(pmf_data[x&lt;q_alpha], dtype='float64')\n\n        x_bar_blue = insert_nones(list(devil_x))\n        y_bar_blue = np.array(insert_nones(list(devil_y)))\n        y_bar_blue[::-3]=0.\n        y_bar_blue = list(y_bar_blue)\n        fig.add_trace(\n            go.Scatter(\n                x=x_bar_blue,\n                y=y_bar_blue,\n                mode=\"lines\",\n                line=dict(color=\"rgb(66, 139, 202)\")\n            ),\n            row=3,\n            col=2\n        )\n        # pdf part\n        fig.add_scatter(\n            x=x[x &lt;= q_alpha],\n            y=pmf_data[x &lt;= q_alpha],\n            mode=\"markers\",\n            marker={\"color\":\"rgb(66, 139, 202)\"},\n            row=3,\n            col=2,\n        )\n\n        # Dots\n        fig.add_trace(\n            go.Scatter(\n                x=[alpha],\n                y=[q_alpha],\n                mode=\"markers\",\n                marker={\"color\": \"rgb(66, 139, 202)\"},\n                marker_symbol=\"x\",\n                marker_size=8,\n            ),\n            row=1,\n            col=1,\n        )\n\n        fig.add_trace(\n            go.Scatter(\n                x=[alpha],\n                y=[alpha],\n                mode=\"markers\",\n                marker={\"color\": \"rgb(66, 139, 202)\"},\n                marker_symbol=\"x\",\n                marker_size=8,\n            ),\n            row=2,\n            col=1,\n        )\n\n        fig.add_trace(\n            go.Scatter(\n                x=[q_alpha],\n                y=[alpha],\n                mode=\"markers\",\n                marker={\"color\": \"rgb(66, 139, 202)\"},\n                marker_symbol=\"x\",\n                marker_size=8,\n            ),\n            row=2,\n            col=2,\n        )\n\n        # Lines\n        fig.add_trace(\n            go.Scatter(\n                x=[alpha, alpha],\n                y=[x[0], q_alpha],\n                mode=\"lines\",\n                line=dict(dash=\"dash\", color=\"rgb(66, 139, 202)\")\n            ),\n            row=1,\n            col=1\n        )\n\n        fig.add_trace(\n            go.Scatter(\n                x=[alpha, alpha],\n                y=[alpha, 1.],\n                mode=\"lines\",\n                line=dict(dash=\"dash\", color=\"rgb(66, 139, 202)\")\n            ),\n            row=2,\n            col=1\n        )\n        fig.add_trace(\n            go.Scatter(\n                x=[alpha, 1],\n                y=[alpha, alpha],\n                mode=\"lines\",\n                line=dict(dash=\"dash\", color=\"rgb(66, 139, 202)\")\n            ),\n            row=2,\n            col=1\n        )\n        fig.add_trace(\n            go.Scatter(\n                x=[x[0], q_alpha],\n                y=[alpha, alpha],\n                mode=\"lines\",\n                line=dict(dash=\"dash\", color=\"rgb(66, 139, 202)\")\n            ),\n            row=2,\n            col=2\n        )\n        fig.add_trace(\n            go.Scatter(\n                x=[x[0], q_alpha],\n                y=[x[0], 0],\n                mode=\"lines\",\n                line=dict(dash=\"dash\", color=\"rgb(66, 139, 202)\")\n            ),\n            row=2,\n            col=2\n        )\n        # Axes ranges\n        fig.update_xaxes(range=[0, 1.05], row=1, col=1)\n        fig.update_yaxes(matches=\"x6\", row=1, col=1)\n\n        fig.update_yaxes(range=[0, 1.05], row=2, col=1)\n        fig.update_xaxes(matches=\"x1\", row=2, col=1)\n\n        fig.update_yaxes(rangemode=\"tozero\", row=3, col=2)\n        fig.update_xaxes(range=[x[0], x[-1]], row=3, col=2)\n\n        fig.update_xaxes(matches=\"x6\", row=2, col=2)\n        fig.update_yaxes(matches=\"y3\", row=2, col=2)\n\n        # Add dropdown\n        fig.update_layout(\n            showlegend=False,\n            template=\"simple_white\",\n        )\n        return fig\n\n\napp = App(app_ui, server)",
    "crumbs": [
      "Cours",
      "Notations et rappels"
    ]
  },
  {
    "objectID": "Courses/simulation.html",
    "href": "Courses/simulation.html",
    "title": "Simulation",
    "section": "",
    "text": "Dans ce chapitre on se demande comment simuler en pratique des variables aléatoires i.i.d. L’idée est de commencer par le cas de variables aléatoires de loi uniforme et d’en déduire les autres lois.",
    "crumbs": [
      "Cours",
      "Simulation"
    ]
  },
  {
    "objectID": "Courses/simulation.html#variables-aléatoires-uniformes",
    "href": "Courses/simulation.html#variables-aléatoires-uniformes",
    "title": "Simulation",
    "section": "Variables aléatoires uniformes",
    "text": "Variables aléatoires uniformes\nOn rappelle qu’une variable aléatoire U suit une loi uniforme sur [0,1], noté \\mathcal{U}([0,1]) si sa fonction de répartition F_U est donnée par \nF_U(x)\n=\n\\begin{cases}\n    0, & \\text{si }x &lt; 0\\,,        \\\\\n    x, & \\text{si }x \\in [0,1]\\,,  \\\\\n    1, & \\text{si }x &gt; 1\\,.        \\\\\n\\end{cases}\n\n\n\n\n\n                                                \n\n\nFigure 1: Fonction de répartition de la loi uniforme\n\n\n\n\nL’objectif est de simuler sur machine une suite U_1, \\ldots, U_n de variables aléatoires i.i.d. de loi \\mathcal{U}([0,1]). Plusieurs problèmes apparaissent alors :\n\nUne machine est déterministe.\nLes nombres entre 0 et 1 donnés par la machine sont de la forme k/2^p, pour k \\in \\{0, \\ldots, 2^{p-1}\\}. On ne pourra donc jamais générer des nombres qui ne sont pas de cette forme.\nVérifier qu’une suite est bien i.i.d. est un problème difficile.\n\n\nDéfinition 1 (Générateur de nombres pseudo-aléatoires) \nUn générateur de nombres pseudo-aléatoires (🇬🇧: Pseudo Random Number Generator, PRNG), est un algorithme déterministe récursif qui renvoie une suite U_1, \\ldots, U_n dans [0,1] qui a un “comportement similaire” à une suite i.i.d. de loi \\mathcal{U}([0,1]). Pour être plus rigoureux, ces nombres sont en fait des nombres entiers générés uniformément sur un certain interval. Dans un second une transformation simple permet d’obtenir des nombres flottants (🇬🇧: floats) entre 0 et 1.\n\n\n\n\n\n\n\nPour aller plus loin\n\n\n\nParfois il est utile d’aller chercher dans le code source certaines information pour savoir comment les fonctions sont codées dans les packages que l’on utiliser. Par exemple, pour numpy que l’on utilise fréquement, on peut voir l’opération choisie ici: Random: int -&gt; float en numpy.\n\n\nUn tel algorithme se construit de la manière suivante :\n\nOn part d’une graine (🇬🇧: seed) U_0 qui détermine la première valeur de manière la plus arbitraire possible.\nLa procédure récursive s’écrit U_{n+1} = f(U_n), où f est une transformation déterministe, de sorte que U_{n+1} est le plus indépendant possible de U_1, \\dots·, U_n.\n\n\nLa fonction f est déterministe et prend ses valeurs dans un ensemble fini, donc l’algorithme est périodique. Le but est donc d’avoir la plus grande période possible.\nNotons qu’une fois que la graine est fixée, alors l’algorithme donne toujours les mêmes valeurs. Fixer la graine peut donc être très utile pour répéter des simulations dans des conditions identiques et ainsi repérer des erreurs.\n\n\n\n\n\n\n\nExercice: bug ou feature?\n\n\n\nReprendre les widgets du chapitre Théorèmes asymptotiques et faites varier doucement le paramètre p (de Bernoulli). Que constatez-vous? Proposer une explication potentielle.\n\n\n\nGénérateur congruentiel linéaire\nLa plupart des PRNG s’appuient sur des résultats arithmétiques. Un des plus connus est celui appelé Générateur congruentiel linéaire (🇬🇧 Linear congruential generator, LCG). Il est défini comme suit: on construit récursivement une suite d’entiers X_i via la congruence \n  X_{n+1} = a X_n + b \\quad \\text{mod } m \\enspace,\n où a,b,m sont des entiers bien choisis pour que la suite obtenue ait de bonnes propriétés. Il suffit alors de considérer X_n/m. Par exemple, la fonction rand sur scilab utilise cette congruence avec m=2^{31}, a=843\\; 314\\; 861, et b=453\\; 816\\; 693.\n\n\nGénérateurs alternatifs\nLes langages Python et R utilisent par défaut le générateur Mersenne-Twister qui s’appuie sur la multiplication vectorielle, mais d’autres générateurs sont aussi disponibles. Ce générateur a pour période m =2^{19937}-1, nombre qu’on peut raisonnablement considérer comme grand.\nPour numpy la méthode par défaut est PCG64 (cf. documentation de numpy), qui dispose de meilleures garanties statistiques (Voir le site https://www.pcg-random.org pour cela).\n\n\nUsage en numpy\nOn suppose désormais disposer d’un générateur pseudo-aléatoire sur [0,1]. En numpy depuis la version 1.17, une bonne manière d’utiliser des éléments aléatoires est d’utiliser un générateur que l’on définit soi-même:\n\nseed = 12345  # Toujours être conscient qu'une graine existe\nrng = np.random.default_rng(seed)  #\nprint(rng.random())  ##  un tirage uniforme sur [0,1]\nprint(rng.random(size=5))  ## cinq tirages uniformes sur [0,1]\nprint(rng.random(size=(3, 2)))  ## matrice 3x2, à entrées unif. sur [0,1]\n\n0.22733602246716966\n[0.31675834 0.79736546 0.67625467 0.39110955 0.33281393]\n[[0.59830875 0.18673419]\n [0.67275604 0.94180287]\n [0.24824571 0.94888115]]\n\n\nDans la suite on va voir comment générer d’autres lois à partir de la loi uniforme, mais il est clair que les logiciels modernes propose un large éventail de distribution classique (gaussienne, exponentielle, etc.). Une liste exhaustive est donnée ici pour numpy.\n\n\n\n\n\n\nPour aller plus loin\n\n\n\nUne excellent discussion sur les bonnes pratiques aléatoires en numpy, et l’usage de np.random.default_rng est donnée dans ce blog post d’Albert Thomas.\n\n\n\n\nPropriété de la loi uniforme\nOn verra souvent apparaître la variable aléatoire 1-U où U \\sim \\mathcal{U}([0,1]). Il se trouve que 1-U suit aussi une loi uniforme sur [0,1] comme le montre le calcul de sa fonction de répartition. Ainsi pour tout x \\in [0,1] on obtient \n\\begin{align*}\n\\mathbb{P}(1-U \\leq x) & = \\mathbb{P}(U \\geq 1-x),\\\\\n                       & = 1-(1-x), \\\\\n                       & = x\\,.\n\\end{align*}\n On peut démontrer facilement la même relation pour x&lt;0 et x&gt;1, d’où le résultat.",
    "crumbs": [
      "Cours",
      "Simulation"
    ]
  },
  {
    "objectID": "Courses/simulation.html#méthode-dinversion",
    "href": "Courses/simulation.html#méthode-dinversion",
    "title": "Simulation",
    "section": "Méthode d’inversion",
    "text": "Méthode d’inversion\nL’idée de la méthode d’inversion repose sur le résultat suivant : si F est une fonction de répartition bijective et U \\sim\\mathcal{U}([0,1]), alors la variable aléatoire F^{-1}(U) a pour fonction de répartition F. C’est une conséquence de la suite d’égalités : \n\\begin{align}\n  \\mathbb{P}( F^{-1}(U) \\leq x ) & = \\mathbb{P}( U \\leq F(x) ) \\\\\n                                 & = F(x)\\,,\n\\end{align}\n\\tag{1}\noù la deuxième égalité résulte de la bijectivité de F. Ainsi, si F est facilement inversible, on peut simuler une variable aléatoire X de loi F en simulant une variable aléatoire uniforme U et en posant X = F^{-1}(U).\n\nExemple 1 (Simulation d’une loi exponentielle) On rappelle que la loi exponentielle de paramètre \\lambda &gt; 0 a pour densité \nf_{\\lambda}(x) = \\lambda e^{-\\lambda x} {1\\hspace{-3.8pt} 1}_{\\mathbb{R}_+}(x)\\enspace.\n et donc pour fonction de répartition \nF_{\\lambda}(x) = 1 - e^{-\\lambda x} {1\\hspace{-3.8pt} 1}_{\\mathbb{R}_+}(x)\\enspace.\n On vérifie que F_{\\lambda} est bijective de \\mathbb{R}_+ dans ]0,1[ et que son inverse est donnée pour tout u \\in ]0,1[ par \nF_{\\lambda}^{-1}(u) = -\\frac{1}{\\lambda} \\log(1-u)\\enspace.\n\n\nMalheureusement, la fonction F n’est pas toujours inversible (penser aux lois discrètes) c’est donc pourquoi on utilise l’inverse l’inverse généralisée ou fonction quantile introduite dans la section Notations: \n\n  F^\\leftarrow(p)=  \\inf\\{ x \\in \\mathbb{R} \\colon F(x)\\geq p\\} \\enspace.\n\nInterprétation: Définir l’inverse d’une fonction de répartition F revient à résoudre l’équation F(x) = \\alpha d’inconnue x pour un \\alpha fixé. Si F n’est pas bijective, deux problèmes apparaissent :\n\nl’équation n’a aucune solution ce qui revient à dire que F n’est pas surjectif (graphiquement, F présente des sauts) ;\nl’équation a plusieurs solutions ce qui revient à dire que F n’est pas injective (graphiquement cela se matérialise par un plateau à la hauteur \\alpha). Un exemple classique est celui où F est la fonction de répartition d’une variable aléatoire discrète.\n\nLe passage à l’inéquation F(x) \\geq u permet de contourner la non-surjectivité : on ne regarde non plus les droites horizontales y=u mais la région \\{y \\geq \\alpha\\}. Le choix de l’\\inf dans la définition de F^{\\leftarrow} permet de contourner la non-injectivité : vu qu’il y a possiblement plusieurs x tels que F(x) \\geq u, on choisit le “premier”. sCes considérations sont illustrées en Figure Figure 2.\n\n\n\n\n                                                \n\n\nFigure 2\n\n\n\n\nRemarques additionnelles:\n\nLa fonction F étant croissante, la quantité F^\\leftarrow(u) correspond au premier instant où F dépasse \\alpha. Si F est bijective (ce qui équivaut dans ce cas à strictement croissante et injective), alors F^\\leftarrow = F^{-1}.\nLa fonction F^\\leftarrow n’est rien d’autre que la fonction quantile : si 0 &lt; \\alpha &lt; 1, q_{1-\\alpha} = F^\\leftarrow(1-\\alpha) est le quantile d’ordre (1-\\alpha) de F. Par exemple, F^\\leftarrow(1/2) correspond à la médiane.\nNotons que si u=0, on peut alors naturellement poser F^{\\leftarrow}(0) = -\\infty. De même, avec la convention la convention \\inf \\emptyset = +\\infty, on peut alors étendre la définition de F^\\leftarrow à u=1 (mais F^\\leftarrow(1) n’est pas toujours égal à \\infty, voir les exemples ci-dessous).\n\n\nDéfinition 2 (Loi à support fini) \nSoit X une variable aléatoire discrète prenant uniquement les valeurs x_1 &lt; \\dots &lt; x_r avec probabilité p_1, \\dots, p_r, et tel que p_1 + \\dots + p_r=1. On vérifie que pour tout u \\in ]0,1[, \n        F^\\leftarrow(u) =\n        \\begin{cases}\n            x_1 & \\text{si } 0 &lt; u \\leq p_1\\,,                  \\\\\n            x_2 & \\text{si } p_1 &lt; u \\leq p_1+p_2\\,,            \\\\\n                & \\vdots                                        \\\\\n            x_r & \\text{si }  p_1 + \\dots + p_{r-1} &lt; u &lt; 1\\,.\n        \\end{cases}\n\nSur cet exemple, on peut prolonger la définition de F^\\leftarrow à u=1 en posant F^\\leftarrow(1) = x_r. L’inverse généralisée se réécrit alors sous la forme \n        F^\\leftarrow(u) = \\sum_{k=1}^r x_k {1\\hspace{-3.8pt} 1}_{p_1 + \\dots + p_{k-1} &lt; u \\leq p_1 + \\dots + p_k}\\enspace,\n où on a posé p_0=0. Cette expression s’étend directement au cas où X prend un nombre infini dénombrable de valeurs (la somme devient alors une série).\n\nXXX TODO: donner un example discret et montrer F^\\leftarrow.\nLa méthode est illustré ci-dessous pour quelques lois intéressantes:\n\nviewof dist = Inputs.select(['normal','cauchy','laplace','bimodal'], {value: \"bimodal\", label: \"Distribution type\"})\nviewof replay = html`&lt;button&gt;replay`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ninvcdfboard = require(await FileAttachment(\"../inverse-vizu/dist/invcdfboard.umd.cjs\").url())\n\n{\n  replay\n  const canvas = DOM.canvas(500, 500);\n  const galton = invcdfboard.galton(canvas,dist);\n  \n  return html`${galton.canvas}`\n}",
    "crumbs": [
      "Cours",
      "Simulation"
    ]
  },
  {
    "objectID": "Courses/simulation.html#méthode-de-rejet",
    "href": "Courses/simulation.html#méthode-de-rejet",
    "title": "Simulation",
    "section": "Méthode de rejet",
    "text": "Méthode de rejet\nL’idée de la méthode de rejet est la suivante. On souhaite simuler une variable aléatoire X de densité f, appelée loi cible, mais f est trop compliquée pour que la simulation puisse se faire directement. On dispose cependant d’une autre densité g possédant les propriétés suivantes :\n\non sait simuler Y de loi g,\nil existe m &gt; 0 tel que f(x) \\leq m \\cdot g(x),\non sait évaluer le rapport d’acceptation r(x) = \\frac{f(x)}{mg(x)}.\n\nRemarquons d’ores et déjà que la constante m est nécessairement plus grande que 1 car \n    1 = \\int_\\mathbb{R} f(x) \\, \\mathrm dx \\leq m \\int_\\mathbb{R} g(x)\\, \\mathrm dx = m\\,.\n\nL’idée est alors de considérer deux suites iid de variables aléatoires indépendantes entre elles :\n\n(Y_n)_{n \\geq 1} de loi g,\n(U_n)_{n \\geq 1} de loi uniforme sur [0,1].\n\nEn pratique, Y_n correspond à une proposition et U_n permettra de décider si on accepte la proposition ou non. Si oui, alors on conserve Y_n, sinon on simule Y_{n+1}. Le rapport d’acceptation, c’est-à-dire la proportion de Y_n acceptées, correspond à r(x).\nAutrement dit, pour simuler X de densité f, il suffit de simuler Y de densité g et U uniforme jusqu’à ce que U \\leq r(Y) (voir Figure ??? pour une illustration). La proposition suivante assure que cette méthode donne bien le résultat voulu.\n\nProposition 1 (Méthode de rejet) \nSoit T = \\inf \\{n \\geq 1 : U_n \\leq r(Y_n)\\} le premier instant où le tirage est accepté. Alors :\n\nT suit une loi géométrique de paramètre 1/m,\nla variable aléatoire X = Y_T a pour densité f et est indépendante de T.\n\n\nDémonstration:\nIl s’agit d’étudier la loi du couple (X,T). Pour x \\in \\mathbb{R} et n \\in \\mathbb{N}, on écrit \\mathbb{P}(X \\leq x, T=n)= \\mathbb{P}(U_1 &gt; r(Y_1), \\dots, U_{n-1} &gt; r(Y_{n-1}), U_n \\leq r(Y_n), Y_n \\leq x). Les n tirages étant iid, on obtient \n    \\mathbb{P}(X \\leq x, T=n) = \\mathbb{P}(U_1 &gt; r(Y_1))^{n-1} \\mathbb{P}(U_n \\leq r(Y_n), Y_n \\leq x)\\,.\n\nConcernant le premier terme, les variables aléatoires Y_1 et U_1 sont indépendantes donc leur loi jointe correspond au produit des densités : \\begin{align*}\n        \\mathbb{P}(U_1 &gt; r(Y_1)\n         & = \\mathbb{P}((U_1, Y_1) \\in \\{(u,y) \\in \\mathbb{R}^2 : u &gt; r(y)\\})                             \\\\\n         & = \\int_{\\mathbb{R}^2} {1\\hspace{-3.8pt} 1}_{\\{u &gt; r(y)\\}} ({1\\hspace{-3.8pt} 1}_{[0,1]}(u) g(y)) \\, \\mathrm du \\mathrm dy  \\\\\n         & = \\int_\\mathbb{R} \\bigg( \\int_0^1 {1\\hspace{-3.8pt} 1}_{\\{u &gt; r(y)\\}} \\, \\mathrm du\\bigg) g(y)\\, \\mathrm d y \\\\\n         & =  \\int_\\mathbb{R} (1-r(y)) \\, g(y)\\, \\mathrm d y\\,,\n    \\end{align*}\n ce qui se réécrit, comme f et g sont des densités et que r(y) = f(y)/(m \\cdot g(y)): \\begin{align*}\n        \\mathbb{P}(U_1 &gt; r(Y_1))\n        & = \\int_\\mathbb{R} g(y)\\, \\mathrm d y - \\int_\\mathbb{R} \\dfrac{f(y)}{m}\\, \\mathrm dy \\\\\n        & = 1 - \\dfrac{1}{m}\\,.\n    \\end{align*}\n Le deuxième terme se calcule de manière analogue : \n\\begin{align*}\n    \\mathbb{P}(U_n \\leq r(Y_n), Y_n \\leq x)\n        & = \\int_{\\mathbb{R}^2} {1\\hspace{-3.8pt} 1}_{\\{u \\leq r(y)\\}} {1\\hspace{-3.8pt} 1}_{\\{y \\leq x\\}} ({1\\hspace{-3.8pt} 1}_{[0,1]}(u) g(y)) \\, \\mathrm du \\mathrm dy       \\\\\n        & = \\int_\\mathbb{R} \\bigg( \\int_0^1  {1\\hspace{-3.8pt} 1}_{\\{u \\leq r(y)\\}} \\, \\mathrm du\\bigg) {1\\hspace{-3.8pt} 1}_{\\{y \\leq x\\}}  g(y)\\, \\mathrm d y\\,,\n\\end{align*}\n c’est-à-dire \n\\begin{align*}\n        \\mathbb{P}(U_n \\leq r(Y_n), Y_n \\leq x)\n        & = \\int_\\mathbb{R} r(y) {1\\hspace{-3.8pt} 1}_{\\{y \\leq x\\}}  g(y)\\, \\mathrm d y \\\\\n        & = \\int_{-\\infty}^x \\dfrac{f(y)}{m}\\, \\mathrm d y \\\\\n        & = \\dfrac{F(x)}{m}\\,,\n\\end{align*}\n où F est la fonction de répartition de la loi de densité f. On peut ainsi conclure que \n    \\mathbb{P}(X \\leq x, T=n)\n    =\n    \\bigg(1 - \\dfrac{1}{m}\\bigg)^{n-1} \\dfrac{F(x)}{m}\\,.\n Il ne reste plus qu’à étudier les lois marginales. D’une part, par continuité monotone croissante, \n    \\mathbb{P}(T=n)\n    = \\lim_{q \\to \\infty} \\mathbb{P}(X \\in ]-\\infty, q], T=n)\\,,\n ce qui donne \n    \\mathbb{P}(T=n)\n    = \\lim_{q \\to \\infty} \\bigg(1 - \\dfrac{1}{m}\\bigg)^{n-1} \\dfrac{F(q)}{m}\n    = \\bigg(1 - \\dfrac{1}{m}\\bigg)^{n-1} \\dfrac{1}{m}\\,.\n On en déduit que T suit une loi géométrique de paramètre 1/m. D’autre part, par \\sigma-additivité, \n    \\mathbb{P}(X \\leq x)\n    = \\mathbb{P}(X \\leq x, T \\in \\mathbb{N}^*)\n    = \\sum_{n=1}^\\infty \\mathbb{P}(X \\leq x, T=n)\\,,\n ce qui donne \n\\begin{align*}\n    \\mathbb{P}(X \\leq x)\n    & = \\sum_{n=1}^\\infty \\bigg(1 - \\dfrac{1}{m}\\bigg)^{n-1} \\dfrac{F(x)}{m}\\\\\n    & = \\dfrac{1}{1-(1-1/m)} \\dfrac{F(x)}{m}\\\\\n    & = F(x)\\,,\n\\end{align*}\n ce qui prouve que X a pour loi F.\nEnfin, la loi du couple (X,T) est égale au produit des lois \n    \\mathbb{P}(X \\leq x, T=n)\n    = \\bigg(1 - \\dfrac{1}{m}\\bigg)^{n-1} \\dfrac{F(x)}{m}\n    = \\mathbb{P}(T=n) \\mathbb{P}(X \\leq x)\\,,\n\n\n\nce qui prouve l’indépendance de X et T.\n\n\n□\n\n\n\ndef accept_reject(n, f, g, g_sampler, m):\n    \"\"\"\n    n: number of samples\n    f: target density\n    g: proposal density\n    m: constant such that f(x) &lt;= m*g(x)\n    \"\"\"\n    x_samples = np.zeros(n)\n    u_samples = np.zeros(n)\n    accepted = np.zeros(n)\n    n_accepted = 0\n    while n_accepted &lt; n:\n        x = g_sampler()\n        u = np.random.uniform()\n        alpha = u * m * g(x)\n        u_samples [n_accepted] = alpha\n        x_samples[n_accepted] = x\n        if  alpha &lt;= f(x):\n            accepted[n_accepted] = 1\n        n_accepted += 1\n    return x_samples, u_samples, accepted\n\n\n\n\n\n\n\nEn pratique…\n\n\n\nOn simule U_1 et Y_1. Si U_1 \\leq r(Y_1) c’est gagné, on pose X=Y_1. Sinon, on simule U_2 et Y_2 et on teste à nouveau l’inégalité U_2 \\leq r(Y_2). Et ainsi de suite. Comme T suit une loi géométrique de paramètre 1/m, son espérance vaut m : il faut en moyenne m tentatives pour obtenir une simulation de la loi de densité f. L’objectif est alors de choisir un couple (g, m) de sorte que m soit le plus proche possible de 1.\n\n\n\nExemple 2 (Rejet d’une loi polynomiale) Donnons un exemple jouet (on étudiera des exemples plus pertinents en TD). On considère la densité f(x) = 4x^3 {1\\hspace{-3.8pt} 1}_{[0,1]}(x). Comme f est majorée par 4, on peut choisir pour g la densité de la loi uniforme sur [0,1] et m=4. Alors, r(x) =f(x) / (mg(x)) = x^3, pour x \\in [0,1]. On simule donc (Y_1, U_1) et on teste si U_1 \\leq Y_1^3, etc.\nBien évidemment, on privilégiera ici une simulation via F^\\leftarrow qui permet de générer des variables aléatoires de loi f plus rapidement.\n\n\n\n\n                                                \n\n\nFigure 3: Visualisation des zones d’acceptations/rejet (g uniforme)\n\n\n\n\nNous pouvons facilement améliorer la proportion de point acceptés en proposant par exemple g définie par g(x) = x {1\\hspace{-3.8pt} 1}_{[0, 1]}(x).\n\n\n\n\n                                                \n\n\nFigure 4: Visualisation des zones d’acceptations/rejet (g triangulaire)\n\n\n\n\n\n\n\n\n\n\n\n\n\nmd`Le taux d'acceptation est passé de **${ratio1}** à\n**${ratio2}** en utilisant une loi triangulaire au lieu d'une loi uniforme.`\n\n\n\n\n\n\n\n\n\nExemple 3 (Rejet d’une loi de densité d’Andrews) Considérons la densité d’Andrews définie par f(x) = \\frac{1}{S} \\frac{\\sin(\\pi\\cdot x)}{\\pi \\cdot x} {1\\hspace{-3.8pt} 1}_{[-1,1]}(x), avec S = \\int_{-1}^{1}\\frac{\\sin(\\pi\\cdot x)}{\\pi \\cdot x}dx. Dans ce contexte, on ne connait pas la valeur exacte de S, et on va donc utiliser la méthode de rejet pour simuler des variables aléatoires de loi f sans cette information. On peut l’adapter en adaptant le test de la manière suivante: si l’on prend m=2/S et g(x) = \\frac{1}{2} {1\\hspace{-3.8pt} 1}_{[-1,1]}(x), on observe que tester u\\leq \\frac{f(x)}{m \\cdot g(x)} est équivalent à tester u \\leq r(x)=\\frac{1}{S} \\frac{\\sin(\\pi\\cdot x)}{\\pi \\cdot x} \\cdot \\frac{1}{\\frac{2}{S} g(x)} = \\frac{\\sin(\\pi\\cdot x)}{\\pi \\cdot x} \\cdot \\frac{1}{2 \\cdot g(x)}, ce qui peut se faire sans connaissance de S. De plus on peut vérifier que g(x) = \\frac{1}{2} {1\\hspace{-3.8pt} 1}_{[-1,1]}(x) définit une densité et que f(x) \\leq m \\cdot g(x) pour tout x\\in \\mathbb{R}.\n\nn = 10000\ng = lambda x: np.ones_like(x) / 2\ng_sampler = lambda: 2 * np.random.uniform() - 1\nm = 2\n\nx_samples, u_samples, accepted = accept_reject(n, np.sinc, g, g_sampler, m)\nratio = np.sum(accepted) / n\n# Note: https://stackoverflow.com/questions/70804891/how-to-vectorize-a-function-in-python-that-includes-a-limit-e-g-sinx-x\n\n\n\n\n\n\nOn peut approcher numériquement la valeur exacte de S en utilisant une méthode de calcul approchée, ce qui permet de comparer ici notre méthode de rejet avec la densité sous jacente:\n\nfrom scipy import integrate\nS = integrate.quad(np.sinc, -1, 1)[0]\nprint(f\"En utilisant la méthode de rejet, on trouve que S = {S:.3f}\")\n\nEn utilisant la méthode de rejet, on trouve que S = 1.179\n\n\nEnfin, on peut visualiser la qualité l’approximation de la densité par la méthode de rejet en comparant la densité approchée (avec un histogramme) avec la densité exacte:\n\nfig = go.Figure()\nfig.add_trace(\n    go.Histogram(\n        x=x_samples[accepted == 1], histnorm=\"probability density\", name=\"Échantillons\"\n    )\n)\n\n# Plot the density\nx = np.linspace(-1, 1, 100)\nfig.add_trace(\n    go.Scatter(\n        x=x,\n        y=np.sinc(x) / S,\n        mode=\"lines\",\n        line=dict(color=\"black\", dash=\"dash\"),\n        name=\"Densité\",\n    )\n)\n\nfig.update_layout(template=\"simple_white\", showlegend=True)\n\n\n\n                                                \n\n\nFigure 5: Méthode de rejet pour simuler une loi de densité de type Andrews, sans connaissance de la valeur exacte de la constante de normalisation.\n\n\n\n\n\nmd`Dans cet example, le taux d'acceptation est ici de **${ratio}**.`\n\n\n\n\n\n\n\n\nCas mutlidimensionnel\nCommençons par un cas de dimension deux.\nPour cela on va utiliser la méthode de rejet pour simuler une loi de densité f sur \\mathbb{R}^2. En particulier, un exemple classique est de tirer des points dans le disque unité, c’est-à-dire de simuler une loi uniforme sur le disque unité. Pour cela, on va utiliser la méthode de rejet avec g la densité de la loi uniforme sur le carré [-1,1]^2, et m=1 fonctionne.\nMais prenons un autre exemple, à savoir tirer des points uniformément dans la surface délimité par une cardioïde. Pour cela, on va utiliser la méthode de rejet avec g la densité de la loi uniforme sur le carré [-2,2]^2, et m=1 fonctionne.\n\n\n\n\n                                                \n\n\nFigure 6: Méthode de rejet pour simuler une loi uniforme sur une surface délimitée par une cardioïde.\n\n\n\n\n4.88\n\n\n\n\n\n\n                                                \n\n\nFigure 7: Méthode de rejet pour simuler une loi uniforme sur un disque unité.\n\n\n\n\n3.1879999999999997\n\n\n\n\n\n\n\n\nEXERCISE loi uniforme sur un cylindre\n\n\n\nProposer une méthode pour simuler une loi uniforme sur un cylindre de rayon 1 et de hauteur 10.",
    "crumbs": [
      "Cours",
      "Simulation"
    ]
  },
  {
    "objectID": "Courses/simulation.html#autres-méthodes",
    "href": "Courses/simulation.html#autres-méthodes",
    "title": "Simulation",
    "section": "Autres méthodes",
    "text": "Autres méthodes\n\nSommation de variables aléatoires\nPour simuler une variable aléatoire de loi binomiale \\mathcal{B}(n,p), on peut utiliser la méthode d’inversion. Cependant, cela nécessite le calcul de l’inverse généralisée de F, donc de coefficients binomiaux et de puissances de p et 1-p. À la place, on utilisera plutôt la relation bien connue suivante : si X_1, \\ldots, X_n est une suite iid de variables aléatoires de loi de Bernoulli de paramètre p, alors \n    X = X_1 + \\cdots + X_n \\sim \\mathcal{B}(n,p)\\,.\n\nPour simuler des variables aléatoires de Bernoulli, on utilise la méthode d’inversion (voir Exemple ). Ainsi, si U_1, \\ldots, U_n sont des variables aléatoires iid de loi uniforme sur [0,1], alors \n    \\sum_{i=1}^n {1\\hspace{-3.8pt} 1}_{\\{U_i \\leq p\\}} \\sim \\mathcal{B}(n,p)\\,.\n\n\n\nLoi de Poisson\nRappelons qu’une variable aléatoire X suit une loi de Poisson de paramètre \\lambda &gt; 0, notée X \\sim \\mathcal{P}(\\lambda) si \n    \\mathbb{P}(X = k) = e^{-\\lambda} \\dfrac{\\lambda^k}{k!}\\,, \\quad k \\in \\mathbb{N}^*\\,.\n Une méthode pour simuler une variable aléatoire de loi de Poisson est donnée par la proposition suivante.\n\nProposition 2 (Génération de v.a. de loi de Poisson) \nSoit (E_n)_{n \\geq 1} des variables aléatoires iid de loi exponentielle de paramètre \\lambda &gt; 0. On pose S_k = E_1 + \\cdots + E_k. Alors \n    \\mathbb{P}(S_n \\leq 1 &lt; S_{n+1}) =  e^{-\\lambda} \\dfrac{\\lambda^n}{n!}\\,, \\quad n \\in \\mathbb{N}^*\\,.\n Ainsi, la variable aléatoire T définie par \n    T := \\sup \\{n \\in \\mathbb{N}^* : E_1 + \\cdots + E_n \\leq 1\\}\n suit une loi de Poisson de paramètre \\lambda : T \\sim \\mathcal{P}(\\lambda).\n\nLa preuve repose sur le lemme suivant.\n\nLemme 1 (Loi de Erlang) \nSoit n variables aléatoires E_1, \\dots, E_n iid de loi exponentielle de paramètre \\lambda &gt;0. La somme E_1+\\dots+E_n suit une loi d’Erlang de paramètres (n,\\lambda), donnée par la fonction de répartition \n    F_{n,\\lambda}(t) = 1 - \\sum_{k=0}^{n-1} e^{-\\lambda t} \\frac{(\\lambda t)^k}{k!}\\,.\n\n\nDémonstration:\nOn montre le résultat pour n=2. La généralisation à k quelconque se fait par récurrence. Soit t &gt; 0, et f_{\\lambda}(x)={1\\hspace{-3.8pt} 1}_{\\{x \\geq 0 \\}} \\lambda e^{-\\lambda x} la densité d’une loi exponentielle de paramètre \\lambda. Les variables aléatoires E_1 et E_2 étant indépendantes et suivant des lois exponentielles de paramètre \\lambda_1 et \\lambda_2, on a \n\\begin{align*}\n    \\mathbb{P}(E_1+E_2 \\leq t)\n        & = \\int_{\\mathbb{R}^2} {1\\hspace{-3.8pt} 1}_{\\{x_1 + x_2 \\leq t\\}} f_{\\lambda}(x_1) f_{\\lambda}(x_2)\\, \\mathrm d x_1 \\mathrm d x_2 \\\\\n        & = \\int_{\\mathbb{R}^2} {1\\hspace{-3.8pt} 1}_{\\{x_1 + x_2 \\leq t\\}} \\lambda^2 e^{-\\lambda (x_1+x_2)} {1\\hspace{-3.8pt} 1}_{\\{x_1 \\geq 0\\}} {1\\hspace{-3.8pt} 1}_{\\{x_2 \\geq 0\\}}\\, \\mathrm d x_1 \\mathrm d x_2 \\\\\n        & = \\int_{\\mathbb{R}^2} {1\\hspace{-3.8pt} 1}_{\\{0 \\leq x_1 \\leq t\\}} {1\\hspace{-3.8pt} 1}_{\\{0 \\leq x_2 \\leq t-x_1\\}} \\lambda^2 e^{-\\lambda x_1} e^{-\\lambda x_2}\\, \\mathrm d x_1 \\mathrm d x_2             \\\\\n        & = \\int_0^t \\lambda e^{-\\lambda x_1} \\bigg(\\int _0^{t-x_1} \\lambda e^{-\\lambda x_2}\\, \\mathrm d x_2\\bigg)  \\mathrm d x_1\\,.\n\\end{align*}\n La première intégrale se calcule alors facilement : \n    \\int _0^{t-x_1} \\lambda e^{-\\lambda x_2}\\, \\mathrm d x_2 = 1 - e^{-\\lambda(t-x_1)}\\,.\n On obtient alors \n\\begin{align*}\n    \\mathbb{P}(E_1+E_2 \\leq t)\n   & = \\int_0^t \\lambda e^{-\\lambda x_1}dx_1 -  \\int_0^t e^{-\\lambda t} \\mathrm d x_1\\\\\n   & = 1 - e^{-\\lambda t} - \\lambda t e^{-\\lambda t}\\,.\n\\end{align*}\n Si t&lt;0, alors comme les E_i ne prennent que des valeurs positives on trouve \\mathbb{P}(E_1 + E_2 \\leq t) = 0. Ceci prouve le résultat pour n=2.\n\n\n\n\n\n□\n\n\nOn peut désormais prouver le résultat de la Proposition Proposition 2.\nDémonstration:\nPour n \\in \\mathbb{N}^*, on décompose la probabilité \\mathbb{P}(S_n \\leq 1 &lt; S_{n+1}) via \n\\begin{align*}\n    \\mathbb{P}(S_n \\leq 1 &lt; S_{n+1})\n    & = \\mathbb{P}(\\{S_n \\leq 1\\} \\setminus \\{S_{n+1} \\leq 1\\})\\\\\n    & = \\mathbb{P}(S_n \\leq 1) - \\mathbb{P}(S_{n+1} \\leq 1)\\,.\n\\end{align*}\n Le lemme précédent donne \n    \\mathbb{P}(S_n \\leq 1) = 1 - \\sum_{n=0}^{n-1} e^{-\\lambda} \\dfrac{\\lambda^k}{k!}\n et \n    \\mathbb{P}(S_{n+1} \\leq 1) = 1 - \\sum_{k=0}^{n} e^{-\\lambda} \\dfrac{\\lambda^k}{k!}\\,.\n On obtient alors le résultat souhaité : \n    \\mathbb{P}(S_n \\leq 1 &lt; S_{n+1})\n    = e^{-\\lambda} \\dfrac{\\lambda^n}{n!}\\,.\n\nOn conclut la preuve de la proposition en remarquant que \n    \\mathbb{P}(T=n) = \\mathbb{P}(S_n \\leq 1 &lt; S_{n+1})\\,.\n\n\n\n\n\n\n□\n\n\nLa simulation d’une variable aléatoire de Poisson repose donc sur la simulation de lois exponentielles qui se fait via la méthode d’inversion, comme vu dans Exemple 1. En pratique, on simule E_1 et on teste si E_1 &gt; 1. Si oui, on pose alors T=0. Si non, on simule E_2 et on teste si E_1 + E_2 &gt; 1. Si oui, on pose T=1. Sinon on continue la procédure.\n\n\nBibliographie et pour aller plus loin\n\nGenerating Random Floating-Point Numbers by Dividing Integers: a Case Study par Frédéric Goualard\nGenerating Pseudo-random Floating-Point Values par Allen Downey.",
    "crumbs": [
      "Cours",
      "Simulation"
    ]
  },
  {
    "objectID": "Courses/slides_intro.html#historical-perspective-timeline",
    "href": "Courses/slides_intro.html#historical-perspective-timeline",
    "title": "HAX603X: Modélisation stochastique",
    "section": "Historical perspective timeline",
    "text": "Historical perspective timeline\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2010s: Deep Learning"
  },
  {
    "objectID": "Courses/slides_intro.html#ai-domains",
    "href": "Courses/slides_intro.html#ai-domains",
    "title": "HAX603X: Modélisation stochastique",
    "section": "AI domains",
    "text": "AI domains\n\n\nHAX603X: Modélisation stochastique"
  },
  {
    "objectID": "TD/TD1.html#test2",
    "href": "TD/TD1.html#test2",
    "title": "TD1:…",
    "section": "test2",
    "text": "test2",
    "crumbs": [
      "TD",
      "TD1:..."
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Présentation",
    "section": "",
    "text": "Aspects numériques de la modélisation aléatoire et statistiques (cours de Licence 3).\nAttention: site en construction…\n\n\n\nJoseph Salmon: joseph.salmon@umontpellier.fr,\nBenjamin Charlier: benjamin.charlier@umontpellier.fr\n\nCe cours est issu du travail antérieur de la part de:\n\nNicolas Meyer\nBenoîte de Saporta et de l’aide de François-David Collin.\n\n\n\n\nCours de mesure et intégration, analyse numérique…\n\n\n\n\n\n\n\n\n\n\n\nCCI\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction à Python Cours de Python 🇫🇷\nData Science: Python Data Science Handbook, With Application to Understanding Data by J. Van DerPlas, 2016; 🇬🇧  videos: Reproducible Data Analysis in Jupyter\nMath for journalists by Naël Shiab 🇬🇧\n\n\n\n\n\nSoftware Dev. for datascience by J. Salmon and B. Charlier, 🇬🇧\nChaîne de Markov: Markov Chains by Ethan N. Epperly 🇬🇧\nAdvanced Data Analysis from an Elementary Point of View by Cosma Shalizi; 🇬🇧\nMaximum likelihood by numerical optimization 🇬🇧",
    "crumbs": [
      "Présentation"
    ]
  },
  {
    "objectID": "index.html#professeurs",
    "href": "index.html#professeurs",
    "title": "Présentation",
    "section": "",
    "text": "Joseph Salmon: joseph.salmon@umontpellier.fr,\nBenjamin Charlier: benjamin.charlier@umontpellier.fr\n\nCe cours est issu du travail antérieur de la part de:\n\nNicolas Meyer\nBenoîte de Saporta et de l’aide de François-David Collin.",
    "crumbs": [
      "Présentation"
    ]
  },
  {
    "objectID": "index.html#prérequis",
    "href": "index.html#prérequis",
    "title": "Présentation",
    "section": "",
    "text": "Cours de mesure et intégration, analyse numérique…",
    "crumbs": [
      "Présentation"
    ]
  },
  {
    "objectID": "index.html#modalité-de-contrôle-des-connaissances",
    "href": "index.html#modalité-de-contrôle-des-connaissances",
    "title": "Présentation",
    "section": "",
    "text": "CCI",
    "crumbs": [
      "Présentation"
    ]
  },
  {
    "objectID": "index.html#livres-et-ressources",
    "href": "index.html#livres-et-ressources",
    "title": "Présentation",
    "section": "",
    "text": "Introduction à Python Cours de Python 🇫🇷\nData Science: Python Data Science Handbook, With Application to Understanding Data by J. Van DerPlas, 2016; 🇬🇧  videos: Reproducible Data Analysis in Jupyter\nMath for journalists by Naël Shiab 🇬🇧\n\n\n\n\n\nSoftware Dev. for datascience by J. Salmon and B. Charlier, 🇬🇧\nChaîne de Markov: Markov Chains by Ethan N. Epperly 🇬🇧\nAdvanced Data Analysis from an Elementary Point of View by Cosma Shalizi; 🇬🇧\nMaximum likelihood by numerical optimization 🇬🇧",
    "crumbs": [
      "Présentation"
    ]
  }
]