[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "HAX603X: Mod√©lisation stochastique",
    "section": "",
    "text": "Aspects num√©riques de la mod√©lisation al√©atoire et statistiques (cours de Licence 3). Les sections en haut de page regroupe le contenus p√©dagogique : les slides seront pr√©sent√©s en cours, et pour aller un peu plus loin le ‚Äúpoly‚Äù est disponible en format ‚Äúhtml‚Äù sur la page Cours.\n\n\n\n\n\n\nAvertissement\n\n\n\nSite en construction‚Ä¶(un peu de patience donc)\n\n\n\n\n\nJoseph Salmon: joseph.salmon@umontpellier.fr,\nBenjamin Charlier: benjamin.charlier@umontpellier.fr\n\nCe cours est issu du travail ant√©rieur de la part de:\n\nNicolas Meyer\nBeno√Æte de Saporta et de l‚Äôaide de Fran√ßois-David Collin.\n\n\n\n\n\nBases de probabilit√©s (en particulier ‚ÄúHAX506X- Th√©orie des Probabilit√©s‚Äù): probabilit√©, densit√©, esp√©rance, fonction de r√©partition, mesure, int√©gration, analyse num√©rique √©l√©mentaire, etc. (Foata et Fuchs 1996; Barbe et Ledoux 2006; Ouvrard 2007, 2008)\nProgrammation √©l√©mentaire (en Python): if ‚Ä¶ then‚Ä¶ else ‚Ä¶, for, while, fonctions, etc. HLMA310 - Logiciels scientifiques, (Courant et al. 2013), Cours de Python: Univ. Paris Diderot\n\n\nFoata, D., et A. Fuchs. 1996. Calcul des probabilit√©s: cours et exercices corrig√©s. Masson.\n\nBarbe, Philippe, et Michel Ledoux. 2006. Probabilit√©s.\n\nOuvrard, J.-Y. 2007. Probabilit√©s : Tome 2, Licence - CAPES. 2·µâ √©d. Enseignement des math√©matiques. Cassini.\n\n‚Äî‚Äî‚Äî. 2008. Probabilit√©s : Tome 1, Licence - CAPES. 2·µâ √©d. Enseignement des math√©matiques. Cassini.\n\n\n\n\nG√©n√©rer l‚Äôal√©a\n\ng√©n√©rateurs pseudo-al√©atoires\nillustrations num√©riques et visualisation en Python (loi des grands nombres, th√©or√®me central limite)\nsimulations de variables al√©atoires¬†(m√©thode de l‚Äôinverse, m√©thode du rejet, cas sp√©cifiques, etc.)\n\nM√©thode de Monte-Carlo¬†\n\nm√©thode de Monte-Carlo pour le calcul approch√© d‚Äôune int√©grale\nr√©duction de la variance¬†: variables antith√©tiques, variables de contr√¥le, √©chantillonnage pr√©f√©rentiel.\n\nCompl√©ments\n\nvecteurs gaussiens et lien avec les lois usuelles de la statistique inf√©rentielle (student, chi2)\nconstruction d‚Äôintervalles de confiance.\nmarche al√©atoire simple, etc.\n\n\n\n\n\n\nTP not√©s : Rendu = fichier Python .py unique\n\nTP not√© 1 : rendre en fin de session\nTP not√© 2 : rendre en fin de session\n\nCC : devoir sur table d‚Äôune heure\nCoefficients:\n\nNote Session 1 : (40% CC + 30% TP 1 + 30% TP 2)\nNote Session 2 : (30% CC + 35% TP 1 + 35% TP 2)\n\n\n\n\n\nMoodle: HAX603X Mod√©lisation Stochastique\n\n\n\n\n\n\nIntroduction √† Python Cours de Python üá´üá∑\nHLMA310 - Logiciels scientifiques üá´üá∑\nManuel d‚Äôalgorithmique en Python (Courant et al. 2013) üá´üá∑\nData Science: Python Data Science Handbook, With Application to Understanding Data by J. Van DerPlas, 2016; üá¨üáß  videos: Reproducible Data Analysis in Jupyter\nMath for journalists by Na√´l Shiab üá¨üáß\n\n\nCourant, J., M. de Falco, S. Gonnord, J.-C. Filli√¢tre, S. Conchon, G. Dowek, et B. Wack. 2013. Informatique pour tous en classes pr√©paratoires aux grandes √©coles: Manuel d‚Äôalgorithmique et programmation structur√©e avec Python. Eyrolles.\n\n\n\n\nSoftware Dev. for datascience by J. Salmon and B. Charlier, üá¨üáß\nMonte Carlo Methods and Applications by Keenan Crane üá¨üáß\nCha√Æne de Markov: Markov Chains by Ethan N. Epperly üá¨üáß\nAdvanced Data Analysis from an Elementary Point of View by Cosma Shalizi; üá¨üáß\nMaximum likelihood by numerical optimization üá¨üáß\nConditionnement, martingales et autres preuves de la loi des grands nombres: (Williams 1991) üá¨üáß\n\n\n\n\nWilliams, D. 1991. Probability with martingales. Cambridge Mathematical Textbooks. Cambridge: Cambridge University Press.",
    "crumbs": [
      "HAX603X: Mod√©lisation stochastique"
    ]
  },
  {
    "objectID": "index.html#enseignants",
    "href": "index.html#enseignants",
    "title": "HAX603X: Mod√©lisation stochastique",
    "section": "",
    "text": "Joseph Salmon: joseph.salmon@umontpellier.fr,\nBenjamin Charlier: benjamin.charlier@umontpellier.fr\n\nCe cours est issu du travail ant√©rieur de la part de:\n\nNicolas Meyer\nBeno√Æte de Saporta et de l‚Äôaide de Fran√ßois-David Collin.",
    "crumbs": [
      "HAX603X: Mod√©lisation stochastique"
    ]
  },
  {
    "objectID": "index.html#pr√©requis",
    "href": "index.html#pr√©requis",
    "title": "HAX603X: Mod√©lisation stochastique",
    "section": "",
    "text": "Bases de probabilit√©s (en particulier ‚ÄúHAX506X- Th√©orie des Probabilit√©s‚Äù): probabilit√©, densit√©, esp√©rance, fonction de r√©partition, mesure, int√©gration, analyse num√©rique √©l√©mentaire, etc. (Foata et Fuchs 1996; Barbe et Ledoux 2006; Ouvrard 2007, 2008)\nProgrammation √©l√©mentaire (en Python): if ‚Ä¶ then‚Ä¶ else ‚Ä¶, for, while, fonctions, etc. HLMA310 - Logiciels scientifiques, (Courant et al. 2013), Cours de Python: Univ. Paris Diderot\n\n\nFoata, D., et A. Fuchs. 1996. Calcul des probabilit√©s: cours et exercices corrig√©s. Masson.\n\nBarbe, Philippe, et Michel Ledoux. 2006. Probabilit√©s.\n\nOuvrard, J.-Y. 2007. Probabilit√©s : Tome 2, Licence - CAPES. 2·µâ √©d. Enseignement des math√©matiques. Cassini.\n\n‚Äî‚Äî‚Äî. 2008. Probabilit√©s : Tome 1, Licence - CAPES. 2·µâ √©d. Enseignement des math√©matiques. Cassini.",
    "crumbs": [
      "HAX603X: Mod√©lisation stochastique"
    ]
  },
  {
    "objectID": "index.html#description-du-cours",
    "href": "index.html#description-du-cours",
    "title": "HAX603X: Mod√©lisation stochastique",
    "section": "",
    "text": "G√©n√©rer l‚Äôal√©a\n\ng√©n√©rateurs pseudo-al√©atoires\nillustrations num√©riques et visualisation en Python (loi des grands nombres, th√©or√®me central limite)\nsimulations de variables al√©atoires¬†(m√©thode de l‚Äôinverse, m√©thode du rejet, cas sp√©cifiques, etc.)\n\nM√©thode de Monte-Carlo¬†\n\nm√©thode de Monte-Carlo pour le calcul approch√© d‚Äôune int√©grale\nr√©duction de la variance¬†: variables antith√©tiques, variables de contr√¥le, √©chantillonnage pr√©f√©rentiel.\n\nCompl√©ments\n\nvecteurs gaussiens et lien avec les lois usuelles de la statistique inf√©rentielle (student, chi2)\nconstruction d‚Äôintervalles de confiance.\nmarche al√©atoire simple, etc.",
    "crumbs": [
      "HAX603X: Mod√©lisation stochastique"
    ]
  },
  {
    "objectID": "index.html#modalit√©-de-contr√¥le-des-connaissances",
    "href": "index.html#modalit√©-de-contr√¥le-des-connaissances",
    "title": "HAX603X: Mod√©lisation stochastique",
    "section": "",
    "text": "TP not√©s : Rendu = fichier Python .py unique\n\nTP not√© 1 : rendre en fin de session\nTP not√© 2 : rendre en fin de session\n\nCC : devoir sur table d‚Äôune heure\nCoefficients:\n\nNote Session 1 : (40% CC + 30% TP 1 + 30% TP 2)\nNote Session 2 : (30% CC + 35% TP 1 + 35% TP 2)",
    "crumbs": [
      "HAX603X: Mod√©lisation stochastique"
    ]
  },
  {
    "objectID": "index.html#moodle",
    "href": "index.html#moodle",
    "title": "HAX603X: Mod√©lisation stochastique",
    "section": "",
    "text": "Moodle: HAX603X Mod√©lisation Stochastique",
    "crumbs": [
      "HAX603X: Mod√©lisation stochastique"
    ]
  },
  {
    "objectID": "index.html#ressources-suppl√©mentaires",
    "href": "index.html#ressources-suppl√©mentaires",
    "title": "HAX603X: Mod√©lisation stochastique",
    "section": "",
    "text": "Introduction √† Python Cours de Python üá´üá∑\nHLMA310 - Logiciels scientifiques üá´üá∑\nManuel d‚Äôalgorithmique en Python (Courant et al. 2013) üá´üá∑\nData Science: Python Data Science Handbook, With Application to Understanding Data by J. Van DerPlas, 2016; üá¨üáß  videos: Reproducible Data Analysis in Jupyter\nMath for journalists by Na√´l Shiab üá¨üáß\n\n\nCourant, J., M. de Falco, S. Gonnord, J.-C. Filli√¢tre, S. Conchon, G. Dowek, et B. Wack. 2013. Informatique pour tous en classes pr√©paratoires aux grandes √©coles: Manuel d‚Äôalgorithmique et programmation structur√©e avec Python. Eyrolles.\n\n\n\n\nSoftware Dev. for datascience by J. Salmon and B. Charlier, üá¨üáß\nMonte Carlo Methods and Applications by Keenan Crane üá¨üáß\nCha√Æne de Markov: Markov Chains by Ethan N. Epperly üá¨üáß\nAdvanced Data Analysis from an Elementary Point of View by Cosma Shalizi; üá¨üáß\nMaximum likelihood by numerical optimization üá¨üáß\nConditionnement, martingales et autres preuves de la loi des grands nombres: (Williams 1991) üá¨üáß\n\n\n\n\nWilliams, D. 1991. Probability with martingales. Cambridge Mathematical Textbooks. Cambridge: Cambridge University Press.",
    "crumbs": [
      "HAX603X: Mod√©lisation stochastique"
    ]
  },
  {
    "objectID": "TP/TP3.html",
    "href": "TP/TP3.html",
    "title": "TP3: Simulation de variables al√©atoires",
    "section": "",
    "text": "Objectifs de ce TP\n\n\n\n\nUtiliser les g√©n√©rateurs al√©atoires en Python et numpy pour g√©n√©rer des √©chantillons de lois non triviales.\nR√©diger un compte-rendu sous Quarto pour pr√©senter ses r√©sultats de TP.",
    "crumbs": [
      "TP",
      "TP3: Simulation de variables al√©atoires"
    ]
  },
  {
    "objectID": "TP/TP3.html#m√©thode-dinversion-loi-exponentielle-et-loi-de-cauchy",
    "href": "TP/TP3.html#m√©thode-dinversion-loi-exponentielle-et-loi-de-cauchy",
    "title": "TP3: Simulation de variables al√©atoires",
    "section": "M√©thode d‚Äôinversion : loi exponentielle et loi de Cauchy",
    "text": "M√©thode d‚Äôinversion : loi exponentielle et loi de Cauchy\n\nRepr√©senter graphiquement la fonction de r√©partition d‚Äôune loi exponentielle (on pourra se restreindre aux r√©els positifs).\n\n\n\n\n\n\n\n\n\n\n\n\n√âcrire une fonction expo qui prend en argument un entier n et un param√®tre \\lambda &gt; 0 et qui donne en sortie un √©chantillon de taille n de loi \\mathcal{E}(\\lambda). On utilisera la m√©thode d‚Äôinversion vue en cours.\nRepr√©senter graphiquement l‚Äôhistogramme d‚Äôun tel √©chantillon pour n=10^2, n=10^3, puis n=10^4, et pour \\lambda = 1/2, 1, 4. Superposer √† chaque fois le graphe de la densit√© de \\mathcal{E}(\\lambda).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nAttention! les conventions pour le param√®tre de la loi exponentielle ne sont pas les m√™me que dans votre cours‚Ä¶\n\n\n\nIllustrer graphiquement la loi des grands nombres avec \\lambda = 1/2, 1, 4. On tracera en particulier la droite d‚Äô√©quation y=\\mathbb E[X], o√π X \\sim \\mathcal{E}(\\lambda).\n M√™me exercice avec la loi de Cauchy. Que remarque-t-on √† la question 4 ?",
    "crumbs": [
      "TP",
      "TP3: Simulation de variables al√©atoires"
    ]
  },
  {
    "objectID": "TP/TP3.html#lois-discr√®tes",
    "href": "TP/TP3.html#lois-discr√®tes",
    "title": "TP3: Simulation de variables al√©atoires",
    "section": "Lois discr√®tes",
    "text": "Lois discr√®tes\n\nLoi de Bernoulli\n\n√âcrire une fonction bernou qui prend en argument un entier n et un param√®tre p \\in ]0,1[ et qui donne en sortie un √©chantillon de taille n de loi \\mathcal{B}(p).\n\n\n\n\n\n\n\n[1 0 1 1 0 1 1 1 0 0]\n\n\n\n\nIllustrer graphiquement la loi des grands nombres pour un √©chantillon de taille n=10^3 et diff√©rentes valeurs de p (on pourra superposer les graphes).\n\n\n\nLoi g√©om√©trique\n\nRappeler les deux m√©thodes vues en TD pour simuler une loi g√©om√©trique.\n√âcrire une fonction geo_bernou qui prend en argument un entier n et un param√®tre p \\in ]0,1[ et qui renvoie en sortie un √©chantillon de taille n de loi \\mathcal{G}(p) en se basant sur la fonction bernou.\n\n# Example usage\nn = 12\np = 0.3\ngeometric_samples = geo_bernou(n, p)\nprint(geometric_samples)\n\n\n\n[ 1.  2.  7.  5.  1.  1.  3.  2. 13.  3.  5.  1.]\n\n\n\n\n√âcrire une fonction geo_expo qui prend en argument un entier n et un param√®tre p \\in ]0,1[ et qui renvoie en sortie un √©chantillon de taille n de loi \\mathcal{G}(p) en se basant sur la fonction expo.\nLe module time contient la fonction process_time() qui permet de mesurer le temps √©coul√© entre deux appels. Le code suivant affiche le temps pass√© √† √©valuer\n\nimport time\n\nt0 = time.process_time()\n\n\"\"\" code chunck to be timed \"\"\"\n\nt1 = time.process_time()\n\nprint(\"Time elaped  when running code chunk:\", t1 - t0)\n\n\n\nUtiliser cette fonction pour comparer la dur√©e de simulation des deux m√©thodes.\n\n\n\n\n\n\nNote\n\n\n\nEn pratique, les temps d‚Äô√©x√©cution peuvent varier suivant la charge d‚Äôutilisation de la machine (c‚Äôest un phenom√®ne al√©atoire). Pour donner une meilleur approximation de tzemps r√©el d‚Äô√©x√©cution, on r√©p√®te plusieurs fois la mesure et on affiche le temps moyen.",
    "crumbs": [
      "TP",
      "TP3: Simulation de variables al√©atoires"
    ]
  },
  {
    "objectID": "TP/TP3.html#m√©thode-de-rejet-et-loi-beta",
    "href": "TP/TP3.html#m√©thode-de-rejet-et-loi-beta",
    "title": "TP3: Simulation de variables al√©atoires",
    "section": "M√©thode de rejet et loi Beta",
    "text": "M√©thode de rejet et loi Beta\nOn rappelle que la loi de Beta de param√®tres \\alpha, \\beta &gt; 0, not√©e \\text{Beta}(\\alpha, \\beta), est donn√©e par la densit√© \n    f_{\\alpha, \\beta}(x)\n    = \\dfrac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha) \\Gamma(\\beta)} x^{\\alpha-1} (1-x)^{\\beta-1}\\,,\n    \\quad x \\in [0,1]\\,.\n La fonction \\Gamma s‚Äôobtient sur avec .\n\nRepr√©senter la densit√© pour diff√©rentes valeurs de \\alpha et \\beta pour visualiser cette loi. On pourra utiliser la fonction scipy.special.gamma.\n\n\n\n\n\n\n\n\n\n\n\n\n√Ä l‚Äôaide de la m√©thode de rejet vue en TD, construire une fonction loi_beta qui g√©n√®re n variables al√©atoires de loi \\text{Beta}(\\alpha,\\beta).\n\n# Example usage\nn = 12\nalpha, beta = 2, 3\n\nbeta_samples = loi_beta(n, alpha, beta)\nprint(beta_samples)\n\n\n\n[0.61582429 0.50112604 0.50748761 0.45232163 0.77487079 0.40568319\n 0.50203394 0.3483966  0.39499544 0.36214897 0.31623463 0.27488559]\n\n\n\n\nIllustrer graphiquement votre r√©sultat en repr√©sentant l‚Äôhistogramme pour n=10^3 et diff√©rentes valeurs de \\alpha, \\beta. On superposera √† chaque fois la densit√© ad√©quate.",
    "crumbs": [
      "TP",
      "TP3: Simulation de variables al√©atoires"
    ]
  },
  {
    "objectID": "TP/TP3.html#loi-sur-le-disque",
    "href": "TP/TP3.html#loi-sur-le-disque",
    "title": "TP3: Simulation de variables al√©atoires",
    "section": "Loi sur le disque",
    "text": "Loi sur le disque\n\n√âcrire une fonction unif_disque bas√©e sur une m√©thode de rejet qui g√©n√®re n uniformes sur le disque unit√© √† partir de variables al√©atoires uniformes ind√©pendantes sur [0,1]. Utiliser cette fonction pour repr√©senter graphiquement un √©chantillon de n=100 uniformes sur le disque.\n\n\n\n\n\n\n\n\n\n\n\n\n√âcrire une fonction unif_disque2, bas√©e sur la fonction pr√©c√©dente, qui donne en sortie √† la fois les points dans le disque mais √©galement, dans une matrice s√©par√©e, les points rejet√©s. Repr√©senter graphiquement un √©chantillon en utilisant une couleur pour les points accept√©s (dans le disque) et une autre pour les points rejet√©s (√† l‚Äôext√©rieur).\n\n# Example usage\nx_accepted, x_rejected = unif_disque2(100)\nprint(\"Reject ratio :\", len(x_rejected) / (len(x_rejected) + len(x_accepted)))\n\n\n\nReject ratio : 0.1935483870967742",
    "crumbs": [
      "TP",
      "TP3: Simulation de variables al√©atoires"
    ]
  },
  {
    "objectID": "TP/TP1.html",
    "href": "TP/TP1.html",
    "title": "TP1: Prise en main de Python",
    "section": "",
    "text": "Objectifs de ce TP\n\n\n\n\nUtiliser les op√©rateurs classiques en Python (+,-,*,/,**,@), etc., savoir cr√©er une fonction, g√©n√©rer un graphique clair et lisible\nQu‚Äôest-ce que la pr√©cision de calcul ? Comment utiliser de une visualisation pour mieux comprendre un th√©or√®me ou une fonction ?\nComprendre au mieux comment utiliser les fonctions al√©atoires (principalement les g√©n√©rateurs) en numpy.",
    "crumbs": [
      "TP",
      "TP1: Prise en main de Python"
    ]
  },
  {
    "objectID": "TP/TP1.html#prise-en-main-de-python",
    "href": "TP/TP1.html#prise-en-main-de-python",
    "title": "TP1: Prise en main de Python",
    "section": "Prise en main de Python",
    "text": "Prise en main de Python\nPython est un langage ouvert qui permet de manipuler des donn√©es, faire des analyses statistiques, tracer des graphes, et bien d‚Äôautres choses encore. Il est distribu√© gratuitement et vous pouvez le t√©l√©charger et l‚Äôinstaller sur une machine personnelle. Dans ce premier TP, on pr√©sente les bases de Python.\nPour plus de d√©tails on pourra consulter les ouvrages:\n\nIntroduction √† Python Cours de Python üá´üá∑\nHLMA310 - Logiciels scientifiques üá´üá∑\nManuel d‚Äôalgorithmique en Python (Courant et al. 2013) üá´üá∑\n\n\nCourant, J., M. de Falco, S. Gonnord, J.-C. Filli√¢tre, S. Conchon, G. Dowek, et B. Wack. 2013. Informatique pour tous en classes pr√©paratoires aux grandes √©coles: Manuel d‚Äôalgorithmique et programmation structur√©e avec Python. Eyrolles.\n\nInstallation de Python\nTout est d√©j√† install√© sur les ordinateurs de l‚Äôuniversit√©. Cette section n‚Äôest utile que si vous souhaitez utiliser votre propre machine.\nLe conseil principal, est d‚Äôinstaller VSCode et d‚Äôutiliser l‚Äôextension Python associ√©e. Pour Python, privil√©gier Conda (ou Mamba) pour installer les packages, voir par exemple: installer-anaconda.\n\n\nL‚Äôenvironnement de travail VSCode / VSCodium\nOn travaillera sous VSCodium (une variante de VSCode) sur les machines de l‚ÄôUniversit√©, un √©diteur de texte qui permet de travailler avec Python, mais aussi avec LaTeX, Markdown, R, etc.\n\nLancer l‚Äôapplication VSCodium, par exemple en cliquant sur l‚Äôic√¥ne ‚ÄúApplication Menu‚Äù en haut √† gauche de votre √©cran, puis en tapant ‚ÄúVSCodium‚Äù dans la barre de recherche. La d√©marche √† suivre est visible dans la vid√©o ci-dessous:\n\n\n\n\nSi besoin (√† ne faire qu‚Äôune fois), il vous faut installer l‚Äôextension ‚Äúhttps://open-vsx.org/extension/ms-python/python‚Äù. Pour cela, il y a plusieurs strat√©gies. La plus simple consiste √† cliquer sur le menu d‚Äôinstallation et chercher l‚Äôapplication Python ‚ÄúPython, extension for Visual Studio Code‚Äù, propos√©e par Microsoft, Intellisense, (attention il y a beaucoup, choisir la bonne, avec plusieurs millions d‚Äô√©toiles et de t√©l√©chargements):\n\n\nRemarque: Une alternative est d‚Äôaller dans le menu ‚ÄúView/Command Palette‚Äù (accessible avec ctrl + shift + p), et taper ‚ÄúExtensions : install extensions‚Äù et installer l‚Äôextension ‚ÄúPython, extension for Visual Studio Code‚Äù (propos√©e par Microsoft). Au besoin, il faudra recharger (reload) VSCodium. Si vous avez d√©j√† install√© l‚Äôextension Python sur votre machine personnelle, vous pouvez passer cette √©tape.\nLa m√™me op√©ration devra √™tre faite pour installer l‚Äôextension ‚Äúhttps://open-vsx.org/extension/ms-toolsai/jupyter‚Äù (propos√©e par Microsoft,) qui nous permettra de manipuler des fen√™tres interactives:\n\n\n\n\nPremiers pas\n\nCr√©er un nouveau fichier dans VSCodium intitul√© HAX603X_tp1.py, et sauvegarder le dans un dossier HAX603X.\nDans ce fichier, copier-coller le code de la bo√Æte suivante. On pourra alors lancer des cellules de code en tapant sur shift + enter dans une cellule d√©limit√©e par les symboles \\# \\%\\%. On peut aussi lancer la cellule en cliquant sur le bouton ‚Äúrun cell‚Äù dans VSCodium (ou clique droit puis une option de type ‚Äúrun cell‚Äù ou ‚Äúrun all cell‚Äù).\n\n\n# %%\n# D√©but de cellule\nprint(1 + 3)  # commentaire en ligne\n# %%\n# Une autre cellule\nprint(2**3)  # commentaire en ligne\n\n4\n8\n\n\n\n\n\nCliquer dans VSCodium sur la version de ‚ÄúPython‚Äù en bas de votre √©cran et choisir sur les machines de l‚Äô√©cole l‚Äôenvironnement ‚Äòdatascience‚Äô (version: 3.10.6 au 20/01/2024). Si vous travaillez sur votre machine personnelle, choisissez un environnement de base, ou bien cr√©er un environnement conda qui vous conviendra, par exemple avec Miniconda1.\nV√©rifier que maintenant vous pouvez lancer une cellule, par exemple en tapant crtl + enter, ou bien en cliquant sur le bouton ‚Äúrun cell‚Äù.\n\n1¬†Installer un environnement de d√©veloppement Python avec Conda\n\nL‚Äôenvironnement de travail\nVous voyez appara√Ætre plusieurs fen√™tres :\n\nla console (√† droite), avec les environnement et l‚Äôhistorique (en haut √† droite)\nla fen√™tre de texte (√† gauche)\n\nLa console permet d‚Äôex√©cuter des instructions ou commandes. C‚Äôest ici que vous donnez vos instructions et que s‚Äôaffichent les r√©sultats demand√©s. La fen√™tre d‚Äôenvironnement et d‚Äôhistorique recense l‚Äôhistorique des commandes et les variables qui ont √©t√© d√©finies. Enfin, la fen√™tre de texte permet d‚Äô√©crire du texte, des commentaires, bref les fichiers que vous conserverez.\nUne mani√®re simple de garder traces de vos calculs/instructions est de les √©crire dans un fichier texte (ici HAX603X_tp1.py), et de les d√©limiter par des symboles \\# \\%\\% (voir ci-dessus), et de les lancer en tapant shift + enter dans une cellule d√©limit√©e par les symboles \\# \\%\\%.\nUne premi√®re utilisation basique de Python concerne les calculs. Vous pouvez entrer toutes les op√©rations classiques : addition +, soustraction -, multiplication *, division /, puissance **, etc. Les fonctions usuelles sont √©galement d√©j√† programm√©es en Python, mais n√©cessite le chargement du package numpy : exponentielle, logarithme, fonctions trigonom√©triques, racine carr√©e, etc.\nPour cela il suffit de taper import numpy as np dans une cellule de code, puis d‚Äôutiliser les fonctions de numpy comme suit par exemple:\nimport numpy as np\n\nprint(np.exp(1))\nprint(np.log(2))\nprint(np.sin(np.pi))\n\n\n\n2.718281828459045\n0.6931471805599453\n1.2246467991473532e-16\n\n\n\n\nQuestion : fonctions math√©matiques\nEntrez quelques op√©rations de base pour vous familiariser avec les instructions sur Python. Faire de m√™me avec les fonctions np.exp, np.log, np.sin, np.cos, np.tan, np.sqrt, np.abs,np.round. Entrer les instructions 1/0 et np.sqrt(-2). Que constatez-vous ?\nOn remarquera qu‚Äôon peut utiliser le symbole np.inf pour repr√©senter l‚Äôinfini. Par ailleurs, si un r√©sultat n‚Äôest pas possible (par exemple en tapant np.sqrt(-2) ou np.inf - np.inf), alors on obtient nan qui signifie Not a Number.\nIl faut se souvenir que les calculs num√©riques ne sont pas toujours exacts du fait de la discr√©tisation des nombres sur machine. Taper par exemple np.sin(0), np.sin(2*np.pi) et np.sin(np.pi*10**16). Voir aussi les diff√©rences entre:\n\nprint(0.6, 0.3 + 0.2 + 0.1)\nprint(0.6, 0.1 + 0.2 + 0.3)\n\n0.6 0.6\n0.6 0.6000000000000001\n\n\n\n\n\n\n\n\nPour aller plus loin\n\n\n\nOn pourra consulter https://0.30000000000000004.com/ pour plus de d√©tails sur les ce type de ph√©nom√®nes.\n\n\n\n\n\nAide en Python\nOn peut utiliser l‚Äôaide de base de python avec les commandes help(la-fonction) ou ?la-fonction. L‚Äôaide en ligne est aussi conseill√©e, surtout pour la cr√©ation de graphiques avec matplotlib pour avoir plus de d√©tails et des galleries de visualisation.\n\n\nR√©pertoire de travail\nLe r√©pertoire de travail (üá¨üáß: working directory) est le r√©pertoire par d√©faut, c‚Äôest-√†-dire le r√©pertoire qui s‚Äôouvre quand vous cliquez sur le bouton pour enregistrer un fichier. La commande pour conna√Ætre le r√©pertoire de travail actuel est getcwd du package os:\n\nimport os\nprint(os.getcwd())\n\n/home/jsalmon/Documents/Mes_cours/Montpellier/HAX603X/HAX603X/TP\n\n\nPour changer le r√©pertoire de travail, on pourra utiliser la commande os.chdir avec un nom de r√©pertoire (valide) entre guillemets, par exemple sous Linux la commande suivante permet de remonter d‚Äôun cran dans l‚Äôarborescence des r√©pertoires:\n\nos.chdir('../')\n\nSi l‚Äôon ferme la fen√™tre interactive (√† droite), alors ex√©cuter une cellule lancera une nouvelle fen√™tre interactive dans le r√©pertoire de travail qui correspond au fichier courant que l‚Äôon √©dite (ici le fichier HAX603X_tp1.py).\n\n\nCr√©ation et affectation de variables\nPour cr√©er des objets, il suffit d‚Äôutiliser la commande =.\n\nQuestion : variables\nCr√©er une variable x qui contient la valeur 12. Effectuer des calculs du type x+3, x**4, 4*x pour v√©rifier que tout se passe comme pr√©vu.\nEn pratique on donnera des noms d‚Äôobjets pertinents, par exemple\n\ndistance = 105  # en km\ntemps = 2  # en heures\nvitesse = distance/temps  # en km/h\n\nOn remarquera que lorsque l‚Äôon cr√©e des objets, ils sont stock√©es dans l‚Äôenvironnement de travail (chercher l‚Äôonglet variables de la fen√™tre interactive).",
    "crumbs": [
      "TP",
      "TP1: Prise en main de Python"
    ]
  },
  {
    "objectID": "TP/TP1.html#numpy-et-calcul-scientifique-en-python",
    "href": "TP/TP1.html#numpy-et-calcul-scientifique-en-python",
    "title": "TP1: Prise en main de Python",
    "section": "numpy et calcul scientifique en Python",
    "text": "numpy et calcul scientifique en Python\nnumpy est l‚Äôoutil de base en Python pour faire du calcul vectoriel et matriciel.\n\nVecteurs en numpy\nPour cr√©er un vecteur, la commande de base est la fonction np.array:\n\nv = np.array([1, 2, 3])\nprint(v)\n\n[1 2 3]\n\n\nEnsuite, on peut concat√©ner des vecteurs, les multiplier par une constante, leur ajouter une constante, les √©lever √† une certaine puissance, etc. Si on manipule deux vecteurs, on prendra garde √† leur taille.\n\nQuestion : Vecteurs\nCr√©ez un vecteur v1 compos√© des r√©els 7, 8, 3, un vecteur v2 compos√© des r√©els -0.5, 120, -12, et un vecteur v3 compos√© des r√©els 0, 1, 0, 1. Testez les commandes suivantes : v1-7, v2**4, 10*v3, v1+v2, v1*v2, v1/v2, v1+v3.\nIl existe un grand nombre de fonctions math√©matiques qui s‚Äôappliquent directement sur un vecteur : np.sum(), np.prod(), len(), np.min(), np.max(), np.nanmax(), np.argmax(), np.mean(), np.median(), np.var(), np.std().\nEnfin on peut aussi utiliser des fonctions de tri, partiel ou non: np.sort(), np.argsort(), np.partition(), np.argpartition() on pourra consulter l‚Äôaide en ligne pour plus de d√©tails: https://numpy.org/doc/stable/reference/routines.sort.html, et les teser sur le vecteur v2 par exemple.\n\n\nQuestion : Op√©rations sur les vecteurs\nCr√©ez un vecteur de taille 5 et appliquez-lui les fonctions pr√©c√©dentes. Si vous ne comprenez pas la sortie (utiliser l‚Äôaide avec ? ou la documentation en ligne).\n\n\n\n\n\n\nPour aller plus loin: vectorisation\n\n\n\nVous pourrez consulter les commandes d√©crites visuellement ici pour cr√©er des vecteurs et/ou des matrices classiques.\n\n\n\n\n\nSuites r√©guli√®res\nUne autre mani√®re de cr√©er des vecteurs consiste √† cr√©er des suites r√©guli√®res :\n\nLa commande np.arange(n1, n2) cr√©e un vecteur de r√©els partant de n1 et croissant d‚Äôune unit√© pour arriver √† n2 (exclu). On peut changer le pas en ajoutant un argument optionnel np.arange(n1, n2, step=pas). Ainsi,\n\nnp.arange(0, 10, step=2)\n\narray([0, 2, 4, 6, 8])\n\n\nLa commande np.tile() permet de r√©p√©ter un vecteur un nombre de fois fix√©.\n\n\nQuestion: arange et tile\nEx√©cutez les commandes suivantes et essayer d‚Äôanalyser les sorties :\n\nnp.arange(9, 13)\nnp.arange(3, -8, step=-1)\nnp.arange(9, 13, step=2)\nnp.arange(9, 13, step=3)\nnp.tile(np.arange(9, 13, step=3), (4, 1))\n\narray([[ 9, 12],\n       [ 9, 12],\n       [ 9, 12],\n       [ 9, 12]])\n\n\nEnfin, pour extraire la valeur d‚Äôindice i d‚Äôun vecteur x, on tapera x[i] (avec la convention que Python commence √† √©num√©rer √† 0). Plus g√©n√©ralement, pour extraire les valeurs associ√©es aux indices 3, 4 et 7, on tapera x[[3,4,7]]. Le v√©rifier sur un vecteur de taille 10. On peut aussi extraire des sous parties de vecteurs, par exemple x[3:7] pour extraire les valeurs d‚Äôindice 3, 4, 5 et 6, ou bien x[3:] pour extraire les valeurs d‚Äôindice 3, 4, 5, etc. jusqu‚Äô√† la fin.\n\n\n\nMatrices en numpy\nLa fonction np.shape permet de conna√Ætre la taille d‚Äôun vecteur ou d‚Äôune matrice. On regardera son comportement sur les vecteurs notamment.\n\nQuestion : op√©rations √©l√©mentaires\nManipulez les op√©rations classiques sur des matrices (arrays) de numpy (si vous √™tes d√©j√† habitu√© √† numpy vous pouvez continuer)\nOp√©rations termes √† termes:\n\n# Somme de deux vecteurs\nA = np.array([1.0, 2, 3])\nB = np.array([-1, -2, -3.0])\n\n# Attribuer √† la variable C la somme de A et B\nsum_A_B = ...  # XXX TODO\n\nnp.testing.assert_allclose(np.zeros((3,)), sum_A_B)\nprint(\"it worked\")\n\n# Le produit terme √† terme avec *\nprod_A_B = ...  # XXX TODO\n\nnp.testing.assert_allclose(np.array([-1.0, -4, -9]), prod_A_B)\nprint(\"it worked\")\n\n# Remarque: la m√™me chose fonctionne terme √† terme avec \\, ** (puissance)\nnp.testing.assert_allclose(np.array([1.0, 4, 9]), A ** 2)\nprint(\"it worked: even for powers\")\n\nLe produit scalaire (ou matriciel) est l‚Äôop√©rateur @. V√©rifiez que pour la matrice J ci-dessous J^3 = Id de deux fa√ßons. Pour cela on pourra aussi utiliser la puissance matricielle avec np.linalg.matrix_power:\n\nJ = np.array([[0, 0, 1.0], [1.0, 0, 0], [0, 1.0, 0]])\n\nI3 = np.eye(3)\n\nnp.testing.assert_allclose(I3, ...)  # XXX TODO\nprint(\"it worked: method 1\")\nnp.testing.assert_allclose(I3, ...)  # XXX TODO\nprint(\"it worked: method 2\")\n\n\n\nQuestion : r√©solution de syst√®mes lin√©aires\nPour r√©soudre le syst√®me de la forme Ax=b en math√©matiques, la formule explicite est x=A^{-1}b (dans le cas o√π A est inversible).\n\n\n\n\n\n\nImportant\n\n\n\nEn pratique vous n‚Äôutiliserez (presque) jamais l‚Äôinversion de matrice ! En effet, on n‚Äôinverse JAMAIS JAMAIS (!) une matrice sans une tr√®s bonne raison. La plupart du temps il existe des m√©thodes plus rapides pour r√©soudre un syst√®me num√©riquement !\n\n\n\nprint(f\"L'inverse de la matrice: \\n {J} \\n est \\n {np.linalg.inv(J)}\")\n\nn = 20  # XXX TODO: tester avec n=100\nJbig = np.roll(np.eye(n), -1, axis=1)  # matrice de permutation de taille n\nprint(Jbig)\n\nb = np.arange(n)\nprint(b)\n\n# on peut transposer une matrice facilement de 2 mani√®res:\nprint(Jbig)\nprint(Jbig.T)\nprint(np.transpose(Jbig))\n\nComparons niveau temps d‚Äôexecution l‚Äôinversion explicite vs.¬†l‚Äôutilisation d‚Äôun solveur de syst√®me lin√©aire tel que np.linalg.solve:\n\nimport time\n# R√©solution de syst√®me par une m√©thode naive: inversion de matrice\nt0 = time.perf_counter()  # XXX TODO\ny1 = ... @ b\ntiming_naive = time.perf_counter() - t0\nprint(\n    f\"Temps pour r√©soudre un syst√®me avec la formule math√©matique: {timing_naive - t0:.4f} s.\"\n)\n\n# R√©solution de syst√®me par une m√©thode adapt√©e: fonctions d√©di√©e de `numpy``\nt0 = time.perf_counter()\ny2 = ...\ntiminig_optimized = time.perf_counter()\nprint(\n    f\"Temps pour r√©soudre un syst√®me avec la formule math√©matique: {timing_optimized:.4f} s.\\nC'est donc {timing_naive / timing_optimized} fois plus rapide d'utiliser la seconde formulation\"\n)\n\nnp.testing.assert_allclose(y1, y2)\nprint(\"Les deux m√©thodes trouvent le m√™me r√©sultat\")\n\n\n\n\n\n\n\nAstuce\n\n\n\nPour des comparaisons d‚Äôefficacit√© temporelle plus pouss√©es on pourra utiliser le package timeit2 ou voir la discussion ici: https://superfastpython.com/time-time-vs-time-perf_counter/.\n\n\n2¬†lien vers la documentation de timeit\n\nQuestion d√©coupage (üá¨üáß: slicing)\nLe d√©coupage permet d‚Äôextraire des √©l√©ments selon un crit√®re (position, condition, etc.). La notation : signifie ‚Äútout le monde‚Äù, et l‚Äôindexation commence en 0. Pour partir de la fin, il est possible de mettre le signe - devant le nombre: ainsi -1 renvoie donc au dernier √©l√©ment. Enfin, on peut extraire des sous suites d‚Äôindices pair ou impair, par exemple x[::2] pour extraire les valeurs d‚Äôindice pair, ou bien x[1::2] pour extraire les valeurs d‚Äôindice impair de x. Enfin on peut aussi utiliser le signe - pour partir de la fin, par exemple x[-1] pour extraire la derni√®re valeur, ou bien x[-2] pour extraire l‚Äôavant-derni√®re valeur.\n\nprint(f\"The first column is {J[:, 0]}\")\n\n# Afficher la deuxi√®me ligne de J\nprint(f\"The second row is {...}\")  # XXX TODO\n\nMettre √† z√©ro une ligne sur 2 de la matrice identit√© de taille 5\\times 5\n\nC = np.eye(5, 5)\nC[...,...] = 0  # mettre √† z√©ro une ligne sur deux. # XXX TODO",
    "crumbs": [
      "TP",
      "TP1: Prise en main de Python"
    ]
  },
  {
    "objectID": "TP/TP1.html#visualisation-dune-figure",
    "href": "TP/TP1.html#visualisation-dune-figure",
    "title": "TP1: Prise en main de Python",
    "section": "Visualisation d‚Äôune figure",
    "text": "Visualisation d‚Äôune figure\nPour lancer une figure on peut utiliser la package matplotlib. Un exemple utilisant le package numpy pour cr√©er une figure simple est donn√© ci-dessous, dans la Figure¬†1.\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = np.cos(2 * np.pi * r)\n\nfig, ax = plt.subplots()\nax.plot(r,theta)\nax.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\nFigure¬†1: Une figure simple.",
    "crumbs": [
      "TP",
      "TP1: Prise en main de Python"
    ]
  },
  {
    "objectID": "TP/TP1.html#aspects-al√©atoires",
    "href": "TP/TP1.html#aspects-al√©atoires",
    "title": "TP1: Prise en main de Python",
    "section": "Aspects al√©atoires",
    "text": "Aspects al√©atoires\nLe module random de numpy permet d‚Äôutiliser l‚Äôal√©atoire et des lois usuelles en Python. On cr√©e d‚Äôabord un g√©n√©rateur qui nous permettra ensuite d‚Äôappeler les lois voulues comme suite:\n\nimport numpy as np  # package de calcul scientifique\nimport matplotlib.pyplot as plt  # package graphique\n\ngenerateur = np.random.default_rng()\ngenerateur.normal()\n\n-0.8369253715602726\n\n\n\nQuestion : Matrices al√©atoires\nCr√©er une matrice de taille 4\\times 5 dont les entr√©es sont i.i.d de loi de Laplace d‚Äôesp√©rance 0 et de variance 2. Lancer plusieurs fois le code et observez les changements. On pourra s‚Äôaider de l‚Äôaide en ligne si besoin.\n\ngenerateur = np.random.default_rng()\nM = ...  # XXX TODO\nprint(M)\n\n\n\nQuestion : Reproduire des r√©sultats\nPour reproduire des r√©sultats ou d√©bugger un code, il est utile de ‚Äúfiger‚Äù l‚Äôal√©atoire. On utilise pour cela une graine (üá´üá∑ seed) dans la cr√©ation du g√©n√©rateur. Fixez la graine √† 0 dans default_rng() et lancez une g√©n√©ration al√©atoire. Commenter.\n\nrng = np.random.default_rng(0)\nrng.normal()\nrng2 = np.random.default_rng(...)\nrng.normal()\n\n\n\nQuestion : afficher un histogramme\nAvec plt.subplot, cr√©er 3 histogrammes de 100 tirages al√©atoires de distributions suivantes:\n\nloi gaussienne (centr√©e-r√©duite)\nloi de Cauchy\nloi de Laplace.\n\nOn utilisera les m√™mes param√®tres de centrage et d‚Äô√©chelle pour les trois lois.\n\nn_samples = 10000\nX = np.empty([n_samples, 3])\nX[:, 0] = ...\nX[:, 1] = ...\nX[:, 2] = ...\n\nlois = [\"Loi de Gauss\", \"Loi de Laplace\", \"Loi de Cauchy\"]\n\nfig_hist, ax = plt.subplots(3, 1, figsize=(3, 3))\n\nfor i, name in enumerate(lois):\n    ax[i].hist(..., bins=100, density=True)\n    ax[i].set_title(name)\n\nplt.tight_layout()\nplt.show()\n\nDe mani√®re compl√©mentaire le module scipy.stats permet d‚Äôutiliser des lois usuelles, et de faire des tests statistiques. On pourra consulter la documentation en ligne pour plus de d√©tails: https://docs.scipy.org/doc/scipy/reference/stats.html.\n\n\n\n\n\n\nPour aller plus loin.\n\n\n\nLa plupart des lois usuelles sont disponibles, cf.¬†la documentation; vous pourrez en manipuler avec des widgets ici.",
    "crumbs": [
      "TP",
      "TP1: Prise en main de Python"
    ]
  },
  {
    "objectID": "Slides/slides_th_asymptotique.html#loi-forte-des-grands-nombres",
    "href": "Slides/slides_th_asymptotique.html#loi-forte-des-grands-nombres",
    "title": "Th√©or√®mes asymptotiques",
    "section": "Loi forte des grands nombres",
    "text": "Loi forte des grands nombres\n\nR√©sultat fondamental: concerne le comportement asymptotique de la moyenne empirique: \\[\n\\bar X_n = \\dfrac{X_1 + \\cdots + X_n}{n} \\enspace.\n\\] quand on observe \\(n\\) variables al√©atoires i.i.d \\(X_1,\\dots,X_n\\), ayant une esp√©rance finie.\n\n\nTh√©or√®me 1 (Loi forte des grands nombres) \nSoit \\((X_n)_{n \\geq 1}\\) une suite de variables al√©atoires ind√©pendantes et identiquement distribu√©es (i.i.d.) dans \\(L^1(\\Omega, \\mathcal{F}, \\mathbb{P})\\). Notons \\(\\mu = \\mathbb{E}[X_1]\\). Alors \\(\\bar X_n\\) converge vers \\(\\mu\\) presque s√ªrement : \\[\n\\mathbb{P}\\bigg( \\dfrac{X_1 + \\cdots + X_n}{n} \\underset{n \\to \\infty}{\\longrightarrow} \\mu \\bigg) = 1\\,.\n\\]",
    "crumbs": [
      "Slides",
      "Th√©or√®mes asymptotiques"
    ]
  },
  {
    "objectID": "Slides/slides_th_asymptotique.html#interpr√©tation",
    "href": "Slides/slides_th_asymptotique.html#interpr√©tation",
    "title": "Th√©or√®mes asymptotiques",
    "section": "Interpr√©tation",
    "text": "Interpr√©tation\nIntuitivement, la probabilit√© d‚Äôun √©v√©nement \\(A\\) correspond √† la fr√©quence d‚Äôapparition de \\(A\\) quand on r√©p√®te une exp√©rience qui fait intervenir cet √©v√©nement.\n\nExemple 1 (Cas Bernouilli: pile ou face) La probabilit√© d‚Äôapparition du c√¥t√© pile (not√© \\(p\\)) peut-√™tre estim√©e en lan√ßant la pi√®ce un grand nombre de fois et en comptant le nombre de pile obtenu.\nLa loi des grands nombres justifie cette intuition : si \\(X_1, \\ldots, X_n\\) sont i.i.d. de loi de Bernoulli de param√®tre \\(p\\), alors \\[\n    \\dfrac{X_1 + \\cdots + X_n}{n} \\xrightarrow[n \\to \\infty]{p.s.} p =\\mathbb{E}(X_1)\n\\]\n\nMembre de gauche : la fr√©quence empirique de piles\nMembre de droite : la fr√©quence th√©orique de piles",
    "crumbs": [
      "Slides",
      "Th√©or√®mes asymptotiques"
    ]
  },
  {
    "objectID": "Slides/slides_th_asymptotique.html#visualisation-de-lexemple-du-pile-ou-face",
    "href": "Slides/slides_th_asymptotique.html#visualisation-de-lexemple-du-pile-ou-face",
    "title": "Th√©or√®mes asymptotiques",
    "section": "Visualisation de l‚Äôexemple du pile ou face",
    "text": "Visualisation de l‚Äôexemple du pile ou face\n#| echo: false\n#| standalone: true\n#| viewerHeight: 550\nimport numpy as np\nfrom shiny import ui, render, App, reactive\nfrom shinywidgets import output_widget, register_widget\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objs as go\n\n\nn_init = 42\n\n\napp_ui = ui.page_fluid(\n    ui.tags.head(\n        ui.tags.style(\"\"\"\n            .bslib-sidebar-layout &gt; .sidebar &gt; .sidebar-content {\n            display: flex;\n            flex-direction: column;\n            padding: 0.5rem;\n            }\n            .irs--shiny .irs-line {\n            top: 27px;\n            height: 4px;\n            }\n            .irs.irs--shiny .irs-bar {\n            top: 27px;\n            height: 4px;\n            }\n            .irs--shiny .irs-handle {\n            top: 23px;\n            width: 22px;\n            height: 10px;\n        \"\"\")\n    ),\n    ui.panel_title(\"Loi des grands nombres\"),\n    ui.layout_sidebar(\n        ui.panel_sidebar(\n            ui.input_action_button(\"seed\", \"Nouveau tirage\", class_=\"btn-primary\"),\n            ui.input_slider( \"p\", \"Esp√©rance: p\", 0.01, 0.99, value=0.35, step=0.01, ticks=False,\n            ),\n            ui.input_slider( \"n_samples\", \"√âchantillons: n\", 2, 1000, value=15, step=1, ticks=False),\n        width=3),\n    ui.panel_main(output_widget(\"my_widget\"), width = 9)\n    )\n)\n\n\n\n\ndef server(input, output, session):\n    seed = reactive.Value(42)\n\n    @reactive.Effect\n    @reactive.event(input.seed)\n    def _():\n        seed.set(np.random.randint(0, 1000))\n\n    subplots = make_subplots(\n        rows=2,\n        cols=1,\n        vertical_spacing=0.45,\n        horizontal_spacing=0.04,\n        row_heights=[5, 1],\n        subplot_titles=(\n            f\"Moyenne empirique: loi de Bernoulli\",\n            \"Tirages al√©atoires &lt;span style='color:rgb(66, 139, 202)'&gt;bleu: 0&lt;/span&gt;, &lt;span style='color:rgb(255, 0, 0)'&gt;rouge: 1&lt;/span&gt; (seed=\"\n            + f\"{n_init:03}\"\n            + \")\",\n        ),\n    )\n    fig = go.FigureWidget(subplots)\n    fig.add_trace(\n        go.Scatter(\n            mode=\"lines\",\n            line=dict(color=\"black\", width=3),\n            x=[],\n            y=[],\n            name=r\"Moyenne &lt;br&gt; empirique\",\n        ),\n        row=1,\n        col=1,\n    )\n    fig.add_trace(\n        go.Scatter(\n            mode=\"lines\",\n            line=dict(dash=\"dash\", color=\"black\", width=1),\n            marker={},\n            x=[],\n            y=[],\n            name=r\"p\",\n        ),\n        row=1,\n        col=1,\n    )\n    fig.add_trace(\n        go.Heatmap(\n            x=[],\n            z=[],\n            colorscale=[[0, \"rgb(66, 139, 202)\"], [1, \"rgb(255,0,0)\"]],\n            showscale=False,\n        ),\n        row=2,\n        col=1,\n    )\n\n    fig.update_yaxes(range=[0, 1.1], row=1, col=1)\n    fig.update_xaxes(matches=\"x1\", row=2, col=1)\n    fig.update_yaxes(visible=False, row=2, col=1)\n    fig.update_xaxes(visible=False, row=2, col=1)\n\n    fig.update_layout(\n        template=\"simple_white\",\n        showlegend=True,\n        xaxis_title=\"√âchantillons: n\",\n    )\n    fig.update_layout(autosize=True)\n\n    fig.update_layout(\n        legend=dict(\n            yanchor=\"top\",\n            y=1.18,\n            x=0.85,\n            bgcolor=\"rgba(0,0,0,0)\",\n        )\n    )\n    fig.update_layout(\n        margin=dict(l=0, r=0, b=10, t=70),\n    )\n\n    register_widget(\"my_widget\", fig)\n\n    @reactive.Effect\n    def _():\n        p = input.p()\n        n_samples = input.n_samples()\n\n        rng = np.random.default_rng(seed())\n        iterations = np.arange(1, n_samples + 1)\n        samples = rng.binomial(1, p, size=n_samples)\n        means_samples = np.cumsum(samples) / np.arange(1, n_samples + 1)\n\n        # Update data in fig:\n        fig.data[0].x = iterations\n        fig.data[0].y = means_samples\n\n        fig.data[1].x = iterations\n        fig.data[1].y = np.full((n_samples), p)\n\n        fig.data[2].x = iterations\n        fig.data[2].z = [samples]\n\n        fig.update_xaxes(range=[1, n_samples + 1])\n\n        # Update the subplot titles:\n        fig.layout.annotations[1].update(\n            text=f\"Tirages al√©atoires (seed=\"\n            + f\"{seed():03}\"\n            + \") &lt;br&gt; &lt;span style='color:rgb(66, 139, 202)'&gt;bleu: 0&lt;/span&gt;, &lt;span style='color:rgb(255, 0, 0)'&gt;rouge: 1&lt;/span&gt; \"\n        )\n\n\napp = App(app_ui, server)\n\nPour aller plus loin:\nQuand \\(p\\) varie (\\(n\\) fix√©), les signaux g√©n√©r√©s sont tr√®s tr√®s proches, ce qui ne devrait pas √™tre le cas sans structuration particuli√®re de la g√©n√©ration. L‚Äôal√©a est imparfait (structure sous-jacente)!",
    "crumbs": [
      "Slides",
      "Th√©or√®mes asymptotiques"
    ]
  },
  {
    "objectID": "Slides/slides_th_asymptotique.html#au-del√†-de-la-loi-des-grands-nombres",
    "href": "Slides/slides_th_asymptotique.html#au-del√†-de-la-loi-des-grands-nombres",
    "title": "Th√©or√®mes asymptotiques",
    "section": "Au del√† de la loi des grands nombres",
    "text": "Au del√† de la loi des grands nombres\n\n1er ordre d‚Äôapproximation de la convergence de \\(\\bar{X}_n\\): loi des grands nombres\n2√®me ordre d‚Äôapproximation: th√©or√®me central limite\n\nEnjeu: quantifier les variations de \\(\\bar X_n - \\mu\\)\nR√©ponse: th√©or√®me central limite (TCL), avec la convergence en loi d‚Äôune transformation affine de la moyenne empirique",
    "crumbs": [
      "Slides",
      "Th√©or√®mes asymptotiques"
    ]
  },
  {
    "objectID": "Slides/slides_th_asymptotique.html#th√©or√®me-central-limite-1",
    "href": "Slides/slides_th_asymptotique.html#th√©or√®me-central-limite-1",
    "title": "Th√©or√®mes asymptotiques",
    "section": "Th√©or√®me central limite",
    "text": "Th√©or√®me central limite\n\nTh√©or√®me 2 (Th√©or√®me central limite) Soit \\(X_1, \\ldots, X_n\\) une suite de variables al√©atoires i.i.d de variance \\(\\sigma^2 = {\\rm var}(X_1) \\in ]0, \\infty[\\). On note \\(\\mu = \\mathbb{E}[X_1]\\) leur esp√©rance. Alors \\[\n\\sqrt n \\left(\\tfrac{\\bar X_n - \\mu}{\\sigma} \\right) \\xrightarrow[n \\to +\\infty]{\\mathcal{L}} N\\enspace,\n\\] o√π \\(N\\) suit une loi normale centr√©e r√©duite : \\(N \\sim\\mathcal{N}(0,1)\\).\n\nInterpr√©tation: la moyenne empirique de v.a. i.i.d de variance \\(\\sigma^2\\) se comporte asymptotiquement comme une loi normale \\(\\mathcal{N}(\\mu, \\tfrac{\\sigma^2}{n})\\): \\(\\quad \\bar X_n \\approx \\mathcal{N}(\\mu, \\frac{\\sigma^2}{n})\\).\n\n\n\n\n\n\nNote\n\n\nHypoth√®ses du th√©or√®me plut√¥t faibles: variance finie uniquement",
    "crumbs": [
      "Slides",
      "Th√©or√®mes asymptotiques"
    ]
  },
  {
    "objectID": "Slides/slides_th_asymptotique.html#formulation-de-la-convergence",
    "href": "Slides/slides_th_asymptotique.html#formulation-de-la-convergence",
    "title": "Th√©or√®mes asymptotiques",
    "section": "Formulation de la convergence",
    "text": "Formulation de la convergence\nConvergence en loi \\(\\iff\\) convergence des fonctions de r√©partition (aux pts de continuit√©)\nNotations:\n\n\\(\\varphi\\) : la densit√© d‚Äôune loi normale centr√©e r√©duite \\(\\varphi(x) = \\tfrac{e^{-\\frac{x^2}{2}}}{\\sqrt{2\\pi}}\\)\n\\(\\Phi\\) : la fonction de r√©partition d‚Äôune loi normale centr√©e r√©duite \\(\\Phi(x) = \\displaystyle \\int_{-\\infty}^{x}\\varphi(u) du\\)\n\n\nR√©-√©criture du TCL: pour tout \\(a &lt; b\\) on a alors\n\\[\n\\begin{align}\n    \\mathbb{P} \\left(\\bar X_n \\in [ \\mu + \\tfrac{a \\sigma}{\\sqrt{n}}, \\mu + \\tfrac{ b \\sigma}{\\sqrt{n}}] \\right)\n    & =\n    \\mathbb{P} \\left(\\tfrac{\\bar X_n - \\mu}{\\sigma} \\in [ \\tfrac{a}{\\sqrt{n}},\\tfrac{b}{\\sqrt{n}}]\\right) \\nonumber\\\\\n    & \\class{fragment}{{} = \\mathbb{P} \\bigg( a \\leq \\sqrt n \\left(\\tfrac{\\bar X_n - \\mu}{\\sigma} \\right) \\leq b\\bigg) \\nonumber}\n    \\\\\n    & \\class{fragment}{{} \\underset{n \\to \\infty}{\\longrightarrow}  \\int_a^b \\varphi(x) \\,  dx = \\Phi(b) - \\Phi(a) \\nonumber}\\\\\n\\end{align}\n\\]",
    "crumbs": [
      "Slides",
      "Th√©or√®mes asymptotiques"
    ]
  },
  {
    "objectID": "Slides/slides_th_asymptotique.html#lien-intervalle-de-confiance-et-tcl",
    "href": "Slides/slides_th_asymptotique.html#lien-intervalle-de-confiance-et-tcl",
    "title": "Th√©or√®mes asymptotiques",
    "section": "Lien intervalle de confiance et TCL",
    "text": "Lien intervalle de confiance et TCL\n\n\nQuestion: comment choisir \\(a\\) et \\(b\\) pour obtenir un intervalle de confiance √† 95% pour \\(\\mu\\)?\nNotation: \\(\\alpha_n=\\mathbb{P} \\left(\\bar X_n \\notin [ \\mu + \\tfrac{a \\sigma}{\\sqrt{n}}, \\mu + \\tfrac{ b \\sigma}{\\sqrt{n}}] \\right)\\), on cherche \\(a, b\\) tels que \\(\\alpha_n \\approx 0.05\\).\nSimplification: choix d‚Äôun intervalle sym√©trique autour de \\(\\mu \\implies q=a=-b\\) \\[\n\\begin{align}\n& 1-\\alpha_n \\approx \\int_{-q}^q \\varphi(x) \\,  dx=\\Phi(q)-\\Phi(-q)=2 \\Phi(q)-1 \\\\\n\\implies & \\boxed{q\\approx\\Phi^{-1}(1-\\tfrac{\\alpha_n}{2})}\n\\end{align}\n\\]\nInterpretation: \\(q\\) est (approx.) le quantile de niveau \\(1-\\tfrac{\\alpha_n}{2}\\) de la loi normale centr√©e r√©duite\nNum√©riquement: on peut facilement √©valuer \\(q\\) et v√©rifier que \\(q\\approx 1.96\\) avec scipy\n\n\n\n\nfrom scipy.stats import norm  # import du module \"norm\" de scipy.stats\nq = norm.ppf((1-0.05/2))      # Calcul du quantile (en: Percent point function) de niveau 1-0.05/2\nprint(f\"{q:.2f}\")             # Affichage √† 2 d√©cimales\n\n1.96",
    "crumbs": [
      "Slides",
      "Th√©or√®mes asymptotiques"
    ]
  },
  {
    "objectID": "Slides/slides_th_asymptotique.html#visualization-du-tcl",
    "href": "Slides/slides_th_asymptotique.html#visualization-du-tcl",
    "title": "Th√©or√®mes asymptotiques",
    "section": "Visualization du TCL",
    "text": "Visualization du TCL\n#| standalone: true\n#| viewerHeight: 600\nimport numpy as np\nfrom scipy.stats import norm\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nfrom shiny import ui, render, App, reactive\nfrom shinywidgets import output_widget, render_widget\n\n\napp_ui = ui.page_fluid(\n    ui.tags.head(\n        ui.tags.style(\"\"\"\n            .bslib-sidebar-layout &gt; .sidebar &gt; .sidebar-content {\n            display: flex;\n            flex-direction: column;\n            padding: 0.1rem;\n            }\n            .irs--shiny .irs-line {\n            top: 27px;\n            height: 4px;\n            }\n            .irs.irs.irs--shiny .irs-bar {\n            top: 27px;\n            height: 4px;\n            }\n            .irs--shiny .irs-handle {\n            top: 23px;\n            width: 22px;\n            height: 10px;\n        \"\"\")\n    ),\n    ui.panel_title(\"TCL\"),\n    ui.layout_sidebar(\n        ui.panel_sidebar(\n            ui.input_action_button(\"seed\", \"Nouveau tirage\",class_=\"btn-primary\"),\n            ui.input_slider(\"p\", \"Esp√©rance: p\", 0.01, 0.99, value=0.5, step=0.01, ticks=False),\n            ui.input_slider(\"n_samples\", \"√âchantillons: n\", 1, 200, value=100, step=1, ticks=False),\n            ui.input_slider(\"n_repetitions\", \"R√©p√©titions: t\", 1, 300, value=200, step=1, ticks=False),\n        width=3\n        ),\n    ui.panel_main(output_widget(\"my_widget\"), width = 9)\n    )\n)\n\n\n\ndef server(input, output, session):\n    seed = reactive.Value(42)\n\n\n    @reactive.Effect\n    @reactive.event(input.seed)\n    def _():\n        seed.set(np.random.randint(0, 1000))\n\n\n    @output\n    @render_widget\n    def my_widget():\n\n        rng = np.random.default_rng(seed())\n        p = input.p()\n        n_repetitions = input.n_repetitions()\n        n_samples = input.n_samples()\n        iterations = np.arange(1, n_samples + 1)\n        samples = rng.binomial(1, p, size=(n_repetitions, n_samples))\n        means_samples = np.cumsum(samples, axis=1) / np.arange(1, n_samples + 1)\n        x_hist = np.linspace(0, 1, num=300)\n\n        # Create figure\n        fig = make_subplots(\n                    rows=1,\n                    cols=3,\n                    # vertical_spacing=0.5,\n                    horizontal_spacing=0.02,\n                    column_widths=[20, 2, 3],\n                    subplot_titles=(\"t = \" + str(n_repetitions) + \" r√©p√©titions\",\"\",\"\")\n                )\n\n\n        for i in range(n_repetitions):\n            fig.add_trace(\n                    go.Scatter(\n                        mode='lines',\n                        line=dict(color=\"rgba(0,0,0,0.05)\", width=1),\n                        x=iterations,\n                        y=means_samples[i,:],\n                        ),\n                        row=1, col=1,\n            )\n        fig.add_trace(\n                go.Scatter(\n                    mode='lines',\n                    line=dict(dash=\"dash\", color=\"blue\", width=1),\n                    marker={},\n                    x=iterations,\n                    y=np.full((n_samples), p),\n                    name='p'),\n                    row=1, col=1,\n        )\n        fig.update_layout(\n            template=\"simple_white\",\n            showlegend=False,\n        )\n        fig.add_trace(\n                go.Scatter(\n                    mode='markers',\n                    marker=dict(color=\"rgba(0,0,0,0.05)\", size=4),\n                    x=np.zeros(n_samples),\n                    y=means_samples[:,-1],\n                ),\n                row=1, col=2,\n\n        )\n        y_hist, bins = np.histogram(means_samples[:,-1], bins=int(np.sqrt(n_repetitions)), density=True)\n        fig.add_trace(\n            go.Bar(x=y_hist, y=bins[:-1] + np.diff(bins)/2,\n                    opacity=0.75,\n                    marker_color = 'black',\n                    orientation='h',\n                    width=np.diff(bins),\n                    name=\"Tirages de moyennes empiriques\",\n                    ),\n                row=1, col=3,\n        )\n        fig.add_trace(\n            go.Scatter(x=norm.pdf(x_hist, p, np.sqrt(p*(1-p) / n_samples)),\n                       y=x_hist,\n                       mode='lines',\n                       line=dict(color=\"red\"),\n                       legendgroup='1',\n                       name=\"TCL\"\n                       ),\n                row=1, col=3,\n        )\n\n        fig.update_xaxes(range=[1, n_samples + 1])\n        fig.update_yaxes(range=[-.05, 1.05], row=1, col=1)\n        fig.update_yaxes(matches=\"y1\",visible = False,  row=1, col=2)\n        fig.update_xaxes(range=[-0.2, 0.2], visible = False, row=1, col=2)\n\n        fig.update_yaxes(matches=\"y1\", row=1, col=3, visible=False)\n        fig.update_xaxes(range=[0, 1.1 / np.sqrt(2*np.pi* p*(1-p) / n_samples)], row=1, col=3)\n        fig.update_xaxes(visible=False, row=1, col=3)\n\n\n\n        for trace in fig['data']:\n            print(trace)\n            if (trace['name'] != 'TCL') and (trace['name'] != 'p'):\n                trace['showlegend'] = False\n        fig.update_layout(\n        margin=dict(l=0, r=0, b=10, t=100),\n        )\n        fig.update_layout(\n            title=dict(text=\"Distribution de la moyenne empirique&lt;br&gt; (cas loi de Bernoulli)\", yanchor=\"top\", y=0.95),\n            title_x=0.5,\n            showlegend=True,\n\n        )\n        fig.update_layout(\n            legend=dict(\n                yanchor=\"top\",\n                y=1.24,\n                xanchor=\"left\",\n                x=0.85,\n                bgcolor=\"rgba(0,0,0,0)\",\n            )\n        )\n        fig['layout']['xaxis']['title']='√âchantillons: n'\n\n        fig.update_layout(autosize=True)\n\n        return fig\n\n\napp = App(app_ui, server)",
    "crumbs": [
      "Slides",
      "Th√©or√®mes asymptotiques"
    ]
  },
  {
    "objectID": "Slides/slides_th_asymptotique.html#pour-aller-plus-loin-vision-des-convolutions",
    "href": "Slides/slides_th_asymptotique.html#pour-aller-plus-loin-vision-des-convolutions",
    "title": "Th√©or√®mes asymptotiques",
    "section": "Pour aller plus loin: vision des convolutions",
    "text": "Pour aller plus loin: vision des convolutions\n\n\nNotation: Soient \\(f\\) et \\(g\\) d√©finies sur \\(\\mathbb{R}\\) (int√©grables au sens de Lebesgue).\n\nD√©finition 1 (Convolution) La convolution de \\(f\\) par \\(g\\) est la fonction \\(f*g\\) suivante: \\[\n\\begin{align}\nf*g:\n\\mathbb{R} &\\mapsto \\mathbb{R} \\nonumber\\\\\nx &\\to \\int_{-\\infty}^{+\\infty} f(x-y)g(y) dy \\enspace.\\nonumber\n\\end{align}\n\\]\n\n\n\n\n\n\n\n\nNote\n\n\nOn peut aussi obtenir \\(f*g(x)\\) en calculant \\(\\int_{\\mathbb{R}^2} f(u)g(v) {1\\hspace{-3.8pt} 1}_{u+v=x} du dv\\).",
    "crumbs": [
      "Slides",
      "Th√©or√®mes asymptotiques"
    ]
  },
  {
    "objectID": "Slides/slides_th_asymptotique.html#somme-et-convolutions",
    "href": "Slides/slides_th_asymptotique.html#somme-et-convolutions",
    "title": "Th√©or√®mes asymptotiques",
    "section": "Somme et convolutions",
    "text": "Somme et convolutions\n\nTh√©or√®me 3 (Loi de la somme et convolutions) Soient \\(X\\) et \\(Y\\) des v.a. ind√©pendantes de densit√©s respectives \\(f\\) et \\(g\\), alors la densit√© de \\(X+Y\\) est donn√©e par la convolution \\(f*g\\).\n\nRappel: pour un scalaire \\(\\alpha\\neq 0\\), la densit√© de \\(\\alpha X\\) est donn√©e par la fonction \\(x \\mapsto \\frac{1}{|\\alpha|} \\cdot f(\\frac{x}{\\alpha})\\).\n\nCorollaire 1 (Loi de la moyenne) Soient \\(X_1,\\dots,X_n\\) des v.a. i.i.d. de densit√© \\(f\\), la densit√© de \\(\\bar{X}_n\\) est donn√©e par la fonction \\(x \\mapsto n \\cdot [f*\\dots*f](n \\cdot x)\\).",
    "crumbs": [
      "Slides",
      "Th√©or√®mes asymptotiques"
    ]
  },
  {
    "objectID": "Slides/slides_th_asymptotique.html#convolution-et-tcl",
    "href": "Slides/slides_th_asymptotique.html#convolution-et-tcl",
    "title": "Th√©or√®mes asymptotiques",
    "section": "Convolution et TCL",
    "text": "Convolution et TCL\nPour \\(X_1, \\dots, X_n\\), i.i.d., de densit√© \\(f\\), on affiche la loi de \\(\\bar{X}_n\\) (√† une constante pr√®s)\n#| standalone: true\n#| viewerHeight: 550\nimport numpy as np\nfrom shiny import ui, render, App, reactive\nfrom shinywidgets import output_widget, register_widget\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objs as go\nfrom scipy import signal\n\n\napp_ui = ui.page_fluid(\n    ui.tags.head(\n        ui.tags.style(\"\"\"\n            .bslib-sidebar-layout &gt; .sidebar &gt; .sidebar-content {\n            display: flex;\n            flex-direction: column;\n            padding: 0.5rem;\n            }\n            .irs--shiny .irs-line {\n            top: 27px;\n            height: 2px;\n            }\n            .irs.irs--shiny .irs-bar {\n            top: 27px;\n            height: 2px;\n            }\n            .irs--shiny .irs-handle {\n            top: 23px;\n            width: 22px;\n            height: 10px;\n        \"\"\")\n    ),\n    ui.panel_title(\"TCL et convolutions\"),\n    ui.layout_sidebar(\n        ui.panel_sidebar(\n            ui.input_select(\n                \"loi\",\n                \"Loi sous-jacente\",\n                {'uniforme': 'Uniforme', 'laplace' : 'Laplace'},\n            ),\n            ui.input_slider(\n                \"n_iter\",\n                \"√âchantillons: n\",\n                1,\n                10,\n                value=1,\n                step=1,\n                ticks=False,\n            ), width = 3\n    ),\n    ui.panel_main(\n        output_widget(\"my_widget\"), width = 9\n    )\n)\n)\n\n\nnnzeros = 10001\nx_min = -20\nx_max = 20\n\nx = np.linspace(x_min, x_max, nnzeros)\ny = np.zeros(nnzeros)\nmask = np.where(np.abs(x) &lt;= 0.5, 1, 0)\ny[mask == 1] = 1\ndelta = (x_max - x_min) / nnzeros\nvar = np.sum(y * x**2 *(delta)) - (np.sum(y * x * delta))**2\n\ndef convolve(signal_in, n_convolutions, delta):\n    output = np.zeros(len(signal_in))\n    if n_convolutions == 0:\n        return output\n    elif n_convolutions == 1:\n        return signal_in\n    else:\n        output = signal_in.copy()\n        for i in range(n_convolutions - 1):\n            output = signal.fftconvolve(\n                output * delta, signal_in, mode=\"same\"\n            )\n        return output\n\n\ndef server(input, output, session):\n\n    fig = go.FigureWidget()\n    fig.add_trace(\n        go.Scatter(\n            mode=\"lines\",\n            line=dict(color=\"black\", width=3),\n            x=[],\n            y=[],\n            name=\"loi de de la moyenne empirique&lt;br&gt;(variance ad√©quate)\",\n        )\n    )\n\n    fig.add_trace(\n        go.Scatter(\n            x=x,\n            y=np.exp(-(x**2) / (2 * var)) / np.sqrt(2 * var * np.pi),\n            mode=\"lines\",\n            line=dict(dash=\"dash\", color=\"red\", width=2),\n            name=f\"Loi normale&lt;br&gt;(variance ad√©quate)\",\n        )\n    )\n    fig.update_xaxes(range=[-3, 3], position=0.)\n    fig.update_yaxes(range=[0, 2 * np.max(y)], position=0.5, showticklabels=False)\n\n\n    fig.update_layout(\n        template=\"simple_white\",\n        showlegend=True,\n        autosize=True,\n        title=dict(text=\"Densit√© : &lt;br&gt; moyenne de n variables al√©atoires\", yanchor=\"top\", y=0.95),\n        title_x=0.5,\n    )\n    fig.update_layout(legend=dict(\n        yanchor=\"top\",\n        y=0.95,\n        xanchor=\"left\",\n        x=0.8,\n        font=dict(size= 18)\n    ))\n    fig.update_layout(\n    height=250,\n    margin=dict(l=0, r=0, b=10, t=100),\n    )\n\n    register_widget(\"my_widget\", fig)\n\n    @reactive.Effect\n    def _():\n        if str(input.loi()) == 'uniforme':\n            y = np.zeros(nnzeros)\n            mask = np.where(np.abs(x) &lt;= 0.5, 1, 0)\n            y[mask == 1] = 1\n\n        else:\n            y=np.exp(-np.abs(x)) / 2\n            y = y / (np.sum(y) * delta)\n\n        var = np.sum(y * x**2 * delta) - (np.sum(y * x * delta))**2\n\n        y_display = convolve(y, input.n_iter(), delta)\n        # Update data in fig:\n        fig.data[0].x = x / np.sqrt(input.n_iter())\n        fig.data[0].y = y_display * np.sqrt(input.n_iter())\n        fig.data[1].y = np.exp(-(x**2) / (2 * var)) / np.sqrt(2 * var * np.pi)\n        fig.update_yaxes(range=[0, 2 * np.max(y)], position=0.5, showticklabels=False)\n\n\napp = App(app_ui, server)\n\n\n\n\n\n\n\nPour aller plus loin\n\n\nPour plus d‚Äôinfo sur les convolutions, voir la vid√©o de 3Blue1Brown : Convolutions | Why X+Y in probability is a beautiful mess, üá¨üáß\n\n\n\n\n\n\nTh√©or√®mes asymptotiques",
    "crumbs": [
      "Slides",
      "Th√©or√®mes asymptotiques"
    ]
  },
  {
    "objectID": "Slides/slides_notations_premiers_pas.html#notation-et-rappels-de-probabilit√©s",
    "href": "Slides/slides_notations_premiers_pas.html#notation-et-rappels-de-probabilit√©s",
    "title": "Notations et premiers pas",
    "section": "Notation et rappels de probabilit√©s",
    "text": "Notation et rappels de probabilit√©s\n\n\nEspace probabilis√©: \\((\\Omega, {\\mathcal{F}}, \\mathbb{P})\\)\n\ncompos√© d‚Äôun ensemble: \\(\\Omega\\)\nd‚Äôune tribu: \\(\\mathcal{F}\\)\nd‚Äôune mesure de probabilit√©: \\(\\mathbb{P}\\)\n\n\n\n\n\nD√©finition 1 (Variable al√©atoire, v.a.) Soit \\((E, \\mathcal{E})\\) un espace mesurable. Une variable al√©atoire est une application mesurable \\[\n    \\begin{array}{ccccc}\n        X & : & \\Omega & \\to     & E            \\\\\n            &   & \\omega & \\mapsto & X(\\omega)\\,.\n    \\end{array}\n\\] Ainsi \\(\\{\\omega \\in \\Omega : X(\\omega) \\in B\\} =  X^{-1}(B) = \\{X \\in B\\} \\in \\mathcal{F}, \\forall B \\in \\mathcal{E}\\)",
    "crumbs": [
      "Slides",
      "Notations et premiers pas"
    ]
  },
  {
    "objectID": "Slides/slides_notations_premiers_pas.html#loi-dune-variable-al√©atoire-unidimensionnelle",
    "href": "Slides/slides_notations_premiers_pas.html#loi-dune-variable-al√©atoire-unidimensionnelle",
    "title": "Notations et premiers pas",
    "section": "Loi d‚Äôune variable al√©atoire (unidimensionnelle)",
    "text": "Loi d‚Äôune variable al√©atoire (unidimensionnelle)\n\nD√©finition 2 (Loi d‚Äôune variable al√©atoire) \nSoit \\(X : (\\Omega, \\mathcal{F}, \\mathbb{P}) \\to (E, \\mathcal{E})\\) une v.a. On appelle loi de \\(X\\) la mesure de probabilit√© sur \\((E, \\mathcal{E})\\) d√©finie par \\[\n        \\begin{array}{ccccc}\n            \\mathbb{P}_X & : & \\mathcal{E} & \\to     & [0,1]          \\\\\n                 &   & B           & \\mapsto & \\mathbb{P}(X \\in B) \\enspace.\n        \\end{array}\n\\]\n\n\n\n\n\n\n\n\nNote\n\n\nLes propri√©t√©s de \\(\\mathbb{P}\\) assurent que \\(\\mathbb{P}_X\\) est bien une mesure de probabilit√© sur l‚Äôespace mesurable \\((E, \\mathcal{E})\\)",
    "crumbs": [
      "Slides",
      "Notations et premiers pas"
    ]
  },
  {
    "objectID": "Slides/slides_notations_premiers_pas.html#lois-discr√®tes",
    "href": "Slides/slides_notations_premiers_pas.html#lois-discr√®tes",
    "title": "Notations et premiers pas",
    "section": "Lois discr√®tes",
    "text": "Lois discr√®tes\nLes variables al√©atoires discr√®tes sont celles √† valeurs dans un ensemble \\(E\\) discret, le plus souvent \\(\\mathbb{N}\\), muni de la tribu pleine \\(\\mathcal{F} = \\mathcal{P}(E)\\).\n\nExemple 1 (Loi de Bernoulli) Soit un param√®tre \\(p \\in [0,1]\\), et \\(E=\\{0,1\\}\\), alors la loi de Bernouilli est donn√©e par \\(\\mathbb{P}(X=1)=1-\\mathbb{P}(X=0) = p\\).\nNotation: \\(\\quad\\) \\(X \\sim \\mathcal{B}(p)\\)\n\nExemple physique:  loi d‚Äôun tirage de pile ou face, de biais \\(p\\)\n\nExemple 2 (Loi binomiale) Soient \\(p \\in [0,1]\\) (biais) et \\(n \\in \\mathbb{N}^*\\) (nombre de tirages) alors la loi Binomiale est donn√©e par \\(\\mathbb{P}(X=k) = \\binom{n}{k} p^k (1-p)^{n-k}\\), pour \\(k \\in E=\\{0,\\dots,n\\}\\).\nNotation: \\(\\quad\\) \\(X \\sim \\mathcal{B}(n,p)\\)\n\nExemple physique:      loi du nombre de succ√®s obtenus lors de \\(n\\) r√©p√©titions ind√©pendantes d‚Äôune exp√©rience al√©atoire de Bernoulli de param√®tre \\(p\\)",
    "crumbs": [
      "Slides",
      "Notations et premiers pas"
    ]
  },
  {
    "objectID": "Slides/slides_notations_premiers_pas.html#lois-discr√®tes-ii",
    "href": "Slides/slides_notations_premiers_pas.html#lois-discr√®tes-ii",
    "title": "Notations et premiers pas",
    "section": "Lois discr√®tes (II)",
    "text": "Lois discr√®tes (II)\n\nExemple 3 (Loi g√©om√©trique) Soient \\(p \\in [0,1]\\) (biais), alors la loi g√©om√©trique est donn√©e par \\(\\mathbb{P}(X=k) = p (1-p)^{k-1}\\), pour \\(k \\in E=\\mathbb{N}^*\\).\nNotation: \\(\\quad\\) \\(X \\sim \\mathcal{G}(p)\\)\n\n\nExemple physique:     \n\nloi du nombre tirage n√©cessaire avant d‚Äôobtenir un succ√®s obtenus en r√©p√©tant ind√©pendamment des exp√©riences al√©atoires de Bernoulli de param√®tre \\(p\\)\n\n\n\n\nExemple 4 (Loi de Poisson) Pour \\(\\lambda&gt;0\\), la loi de Poisson de param√®tre \\(\\lambda\\) est d√©finie par \\(\\mathbb{P}(X=k) = e^{-\\lambda} \\lambda^k / k!\\), pour tout \\(k \\in E=\\mathbb{N}\\).\nNotation: \\(\\quad\\) \\(X \\sim \\mathcal{P}(\\lambda)\\)\n\nExemple physique: comportement du nombre d‚Äô√©v√©nements se produisant avec une fr√©quence connue, et ind√©pendamment du temps √©coul√© depuis l‚Äô√©v√©nement pr√©c√©dent (e.g., nombre de clients dans une file d‚Äôattente, nombre de mutations dans un g√®ne, etc.)",
    "crumbs": [
      "Slides",
      "Notations et premiers pas"
    ]
  },
  {
    "objectID": "Slides/slides_notations_premiers_pas.html#lois-continues",
    "href": "Slides/slides_notations_premiers_pas.html#lois-continues",
    "title": "Notations et premiers pas",
    "section": "Lois continues",
    "text": "Lois continues\nLoi d‚Äôune v.a. admettant une fonction de densit√©, c‚Äôest-√†-dire qu‚Äôil existe une fonction mesurable \\(f : \\mathbb{R} \\to [0, \\infty[\\) d‚Äôint√©grale \\(1\\), telle que pour tout \\(A \\in \\mathcal{B}(\\mathbb{R})\\) \\[\n    \\mathbb{P}(X \\in A) = \\int_A f(x) dx \\enspace.\n\\]\n\n\n\n\n\n\nNote\n\n\nLes propri√©t√©s de l‚Äôint√©grale de Lebesgue assurent que cette formule d√©finit bien une loi de probabilit√©.\n\n\n\n\nEsp√©rance: \\(\\mathbb{E}(X) = \\displaystyle\\int_{\\mathbb{R}} x f(x) dx\\)\nVariance: \\(\\mathbb{V}(X) = \\mathbb{E}((X-\\mathbb{E}(X))^2) = \\displaystyle\\int_{\\mathbb{R}} (x-\\mathbb{E}(X))^2 f(x) dx\\)",
    "crumbs": [
      "Slides",
      "Notations et premiers pas"
    ]
  },
  {
    "objectID": "Slides/slides_notations_premiers_pas.html#lois-continues-usuelles",
    "href": "Slides/slides_notations_premiers_pas.html#lois-continues-usuelles",
    "title": "Notations et premiers pas",
    "section": "Lois continues usuelles",
    "text": "Lois continues usuelles\n\nExemple 5 (Loi uniforme) La loi uniforme sur un ensemble \\(B \\in \\mathcal{B}(\\mathbb{R})\\), s‚Äôobtient avec la densit√© d√©finie par \\[\nf(x) = {1\\hspace{-3.8pt} 1}_B(x) / \\lambda (B) \\enspace,\n\\] o√π \\(\\lambda (B)\\) repr√©sente la mesure de Lebesgue de l‚Äôensemble \\(B\\).\n\nCas particulier: pour la loi uniforme sur \\([0,1]\\), on obtient la fonction suivante: \\[\nf(x) = {1\\hspace{-3.8pt} 1}_{[0,1]}(x)\\enspace.\n\\] Notation: \\(\\quad\\) \\(X \\sim \\mathcal{U}([0,1])\\)\n\n\n\n\n\n\nNote\n\n\nUne telle loi est caract√©ris√©e ainsi : tous les intervalles de m√™me longueur inclus dans le support de la loi ont la m√™me probabilit√©.",
    "crumbs": [
      "Slides",
      "Notations et premiers pas"
    ]
  },
  {
    "objectID": "Slides/slides_notations_premiers_pas.html#lois-continues-usuelles-ii",
    "href": "Slides/slides_notations_premiers_pas.html#lois-continues-usuelles-ii",
    "title": "Notations et premiers pas",
    "section": "Lois continues usuelles (II)",
    "text": "Lois continues usuelles (II)\n\n\nExemple 6 (Loi exponentielle) La loi exponentielle de param√®tre \\(\\gamma &gt; 0\\) est obtenue avec la densit√© donn√©e par \\[\nf(x) = \\gamma e^{-\\gamma x} {1\\hspace{-3.8pt} 1}_{\\mathbb{R}_+}(x)\\enspace.\n\\] Notation: \\(\\quad X \\sim \\mathcal{Exp}(\\gamma)\\)\n\n\n\n\nProposition 1 (Absence de m√©moire) La loi exponentielle mod√©lise la dur√©e de vie d‚Äôun ph√©nom√®ne sans m√©moire (ou sans vieillissement), c‚Äôest-√†-dire que pour tout \\(s,t&gt;0\\), on a \\[\n\\mathbb{P}(X&gt;t+s | X&gt;t)=\\mathbb{P}(X&gt;s) \\enspace.\n\\]",
    "crumbs": [
      "Slides",
      "Notations et premiers pas"
    ]
  },
  {
    "objectID": "Slides/slides_notations_premiers_pas.html#lois-continues-usuelles-iii",
    "href": "Slides/slides_notations_premiers_pas.html#lois-continues-usuelles-iii",
    "title": "Notations et premiers pas",
    "section": "Lois continues usuelles (III)",
    "text": "Lois continues usuelles (III)\n\nExemple 7 (Loi normale/gaussienne univari√©e) Pour des param√®tres \\(\\mu \\in \\mathbb{R}\\) (esp√©rance) et \\(\\sigma^2 &gt; 0\\) (variance), la loi normale associ√©e correspond √† la fonction de densit√© : \\[\nf(x) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}}e^{-\\frac{1}{2 \\sigma^2}(x-\\mu)^2} \\enspace.\n\\] Notation: \\(X \\sim \\mathcal{N}(\\mu,\\sigma^2)\\),\n\nOn nomme loi normale centr√©e r√©duite le cas canonique: \\(\\mu = 0, \\sigma^2 = 1\\).\nSi \\(X\\sim \\mathcal{N}(\\mu,\\sigma^2)\\), alors l‚Äôesp√©rance et la variance de \\(X\\) valent \\(\\mathbb{E}(X) = \\mu\\) et \\(\\mathbb{V}(X) = \\sigma^2\\).\n\n\n\n\n\n\n\nNote\n\n\nLes lois normales sont omnipr√©sente gr√¢ce au th√©or√®me central limite.",
    "crumbs": [
      "Slides",
      "Notations et premiers pas"
    ]
  },
  {
    "objectID": "Slides/slides_notations_premiers_pas.html#enjeu-de-la-fonction-de-r√©partition",
    "href": "Slides/slides_notations_premiers_pas.html#enjeu-de-la-fonction-de-r√©partition",
    "title": "Notations et premiers pas",
    "section": "Enjeu de la fonction de r√©partition",
    "text": "Enjeu de la fonction de r√©partition\n\nEnjeux: caract√©riser la loi d‚Äôune v.a. en ne consid√©rant que l‚Äôespace d‚Äôarriv√©e \\((E, \\mathcal{E})\\)\n\n\nOutils:\n\n\nla fonction de r√©partition (v.a. r√©elles),\nla fonction caract√©ristique (v.a. dans \\(\\mathbb{R}^d\\)), en gros la transform√©e de Fourier de la loi!\nla fonction g√©n√©ratrice des moments (v.a. discr√®tes)\netc.",
    "crumbs": [
      "Slides",
      "Notations et premiers pas"
    ]
  },
  {
    "objectID": "Slides/slides_notations_premiers_pas.html#fonction-de-r√©partition",
    "href": "Slides/slides_notations_premiers_pas.html#fonction-de-r√©partition",
    "title": "Notations et premiers pas",
    "section": "Fonction de r√©partition",
    "text": "Fonction de r√©partition\n\nD√©finition 3 (Fonction de r√©partition üá¨üáß: cumulative distribution function) \nSoit \\(X\\) une variable al√©atoire sur \\((\\mathbb{R}, \\mathcal{B}(\\mathbb{R}))\\). La fonction de r√©partition de \\(X\\) est la fonction \\(F_X\\) d√©finie sur \\(\\mathbb{R}\\) par \\[\n\\begin{align*}\n     F_X(x) & = \\mathbb{P}(X \\leq x)\\\\\n     & \\class{fragment}{{} = \\mathbb{P}(X \\in ]-\\infty, x])}\n\\end{align*}\n\\]",
    "crumbs": [
      "Slides",
      "Notations et premiers pas"
    ]
  },
  {
    "objectID": "Slides/slides_notations_premiers_pas.html#propri√©t√©-√©l√©mentaire-de-la-fonction-de-r√©partition",
    "href": "Slides/slides_notations_premiers_pas.html#propri√©t√©-√©l√©mentaire-de-la-fonction-de-r√©partition",
    "title": "Notations et premiers pas",
    "section": "Propri√©t√© √©l√©mentaire de la fonction de r√©partition",
    "text": "Propri√©t√© √©l√©mentaire de la fonction de r√©partition\n\nProposition 2 (Propri√©t√©s √©l√©mentaires) Soit \\(X\\) une v.a. de fonction de r√©partition \\(F_X\\).\n\n\n\\(F_X\\) est une fonction croissante, de limite \\(0\\) en \\(-\\infty\\) et de limite \\(1\\) en \\(+\\infty\\).\n\\(F_X\\) est continue √† droite en tout point.\n\\(\\forall x \\in \\mathbb{R}\\), on a \\(\\mathbb{P}(X=x) = F_X(x) - \\lim_{\\epsilon \\to 0+}F_X(x- \\epsilon)\\).\nSi \\(X\\) a pour densit√© \\(f\\), alors \\(F_X\\) est d√©rivable \\(\\lambda\\)-presque partout de d√©riv√©e \\(f\\).\n\n\n\n\nD√©monstration: voir par exemple (Barbe et Ledoux 2006)\n\n\n\n\n\n\n\n\nNote\n\n\n\nProp. 1. et 2. : \\(F_X\\) est c√†dl√†g (continue √† droite, limite √† gauche).\nProp 3. (cas discret): les valeurs prises par \\(X\\) correspondent aux discontinuit√©s de \\(F_X\\), les probabilit√©s, √† la hauteur du saut.\nProp. 4. (cas continu): le lien entre la fonction de r√©partition et densit√©.",
    "crumbs": [
      "Slides",
      "Notations et premiers pas"
    ]
  },
  {
    "objectID": "Slides/slides_notations_premiers_pas.html#fonction-de-r√©partition-et-caract√©risation-de-la-loi",
    "href": "Slides/slides_notations_premiers_pas.html#fonction-de-r√©partition-et-caract√©risation-de-la-loi",
    "title": "Notations et premiers pas",
    "section": "Fonction de r√©partition et caract√©risation de la loi",
    "text": "Fonction de r√©partition et caract√©risation de la loi\n\nTh√©or√®me 1 (Caract√©risation de la loi d‚Äôune variable al√©atoire r√©elle) \nLa fonction de r√©partition d‚Äôune variable al√©atoire caract√©rise sa loi : deux variables al√©atoires ont m√™me loi si et seulement si elles ont m√™me fonction de r√©partition.\n\n\n\nD√©monstration: voir Wikipedia \nRappel: la tribu des bor√©liens est engendr√©e par la famille d‚Äôensembles \\(\\{]-\\infty,x], x \\in \\mathbb{R}\\}\\) \nInterpr√©tation: conna√Ætre \\(\\mathbb{P}_X\\) sur cette famille d‚Äôensembles \\(\\implies\\) la conna√Ætre partout",
    "crumbs": [
      "Slides",
      "Notations et premiers pas"
    ]
  },
  {
    "objectID": "Slides/slides_notations_premiers_pas.html#fonction-de-r√©partition-cas-discret",
    "href": "Slides/slides_notations_premiers_pas.html#fonction-de-r√©partition-cas-discret",
    "title": "Notations et premiers pas",
    "section": "Fonction de r√©partition: cas discret",
    "text": "Fonction de r√©partition: cas discret\n Dans le cas d‚Äôune loi discr√®te, la fonction de r√©partition est une fonction en escalier, constante par morceaux, et croissante.   \n\nExemple 8 (Cas discret) Soit \\((x_i)_{i \\in I}\\) une suite ordonn√©e de r√©els, avec \\(I \\subset \\mathbb{N}\\). Si \\(X\\) est une variable al√©atoire discr√®te prenant les valeurs \\((x_i)_{i \\in I}\\) et de loi \\((p_i = \\mathbb{P}(X=x_i))_{i \\in I}\\), alors \\[\n\\forall x \\in \\mathbb{R}, \\quad F_X(x) = \\sum_{i \\in I} p_i {1\\hspace{-3.8pt} 1}_{[x_i, \\infty[}(x) \\enspace.\n\\]",
    "crumbs": [
      "Slides",
      "Notations et premiers pas"
    ]
  },
  {
    "objectID": "Slides/slides_notations_premiers_pas.html#fonction-de-r√©partition-cas-continu",
    "href": "Slides/slides_notations_premiers_pas.html#fonction-de-r√©partition-cas-continu",
    "title": "Notations et premiers pas",
    "section": "Fonction de r√©partition: cas continu",
    "text": "Fonction de r√©partition: cas continu\n\n\n\n\nExemple 9 (Cas continu) Si \\(X\\) est une variable al√©atoire de densit√© \\(f\\), alors \\[\n\\forall x \\in \\mathbb{R}, \\quad F_X(x) = \\int_{-\\infty}^x f(t) \\, \\mathrm dt \\enspace.\n\\]\n\nVocabulaire: densit√© (üá¨üáß: probability density function)",
    "crumbs": [
      "Slides",
      "Notations et premiers pas"
    ]
  },
  {
    "objectID": "Slides/slides_notations_premiers_pas.html#loi-normale",
    "href": "Slides/slides_notations_premiers_pas.html#loi-normale",
    "title": "Notations et premiers pas",
    "section": "Loi normale",
    "text": "Loi normale\nCas de la loi normale centr√©e r√©duite, \\(X \\sim \\mathcal{N}(0,1)\\): \\(F_X=\\Phi\\), avec \\(\\Phi\\) d√©finie par \\[\nF_X(x) \\triangleq \\Phi(x) = \\dfrac{1}{\\sqrt{2\\pi}} \\int_{-\\infty}^x e^{-\\frac{t^2}{2}}\\, \\mathrm d t\\enspace,\n\\]\n\n\n\n\n\n\nNote\n\n\nL‚Äôint√©grale ne peut √™tre obtenue √† partir d‚Äôune formule ferm√©e1. Autrefois, les valeurs de \\(\\Phi(x)\\) √©taient report√©es dans des tables2.\n\n\n\n\n\nTransformation affine: si \\(X \\sim \\mathcal{N}(\\mu, \\sigma^2)\\) ‚Äî i.e., \\(X=\\mu + \\sigma Y\\), avec \\(Y\\sim \\mathcal{N}(0,1)\\) ‚Äî alors \\[\nF_X(x)=\\Phi\\left(\\frac{x-\\mu}{\\sigma}\\right)\n\\]\n\n\nWikipedia: Th√©or√®me de LiouvilleWikipedia: loi normale",
    "crumbs": [
      "Slides",
      "Notations et premiers pas"
    ]
  },
  {
    "objectID": "Slides/slides_notations_premiers_pas.html#fonction-quantile-inverse-g√©n√©ralis√©e-√†-gauche",
    "href": "Slides/slides_notations_premiers_pas.html#fonction-quantile-inverse-g√©n√©ralis√©e-√†-gauche",
    "title": "Notations et premiers pas",
    "section": "Fonction quantile, inverse g√©n√©ralis√©e √† gauche",
    "text": "Fonction quantile, inverse g√©n√©ralis√©e √† gauche\n\nD√©finition 4 (Fonction quantile/ inverse g√©n√©ralis√©e üá¨üáß: quantile distribution function) \nSoit \\(X\\) une variable al√©atoire sur \\((\\mathbb{R}, \\mathcal{B}(\\mathbb{R}))\\) et \\(F_X\\) sa fonction de r√©partion. La fonction quantile associ√©e \\(F_X^\\leftarrow: ]0,1[ \\rightarrow \\mathbb{R}\\) est d√©finie par \\[\n  F_X^\\leftarrow(p) = \\inf\\{ x \\in \\mathbb{R} : F_X(x)\\geq p\\} \\enspace.\n\\]\n\n\n\\(F_X\\) est bijective \\(\\implies\\) \\(F^{-1}=F_X^\\leftarrow\\)\n\nVocabulaire:\n\nla fonction quantile s‚Äôappelle aussi inverse de Levy ou inverse g√©n√©ralis√©e (√† gauche)\nm√©diane : \\(F_X^\\leftarrow(1/2)\\)\npremier/troisi√®me quartile: \\(F_X^\\leftarrow(1/4), F_X^\\leftarrow(3/4)\\)\nd√©ciles : \\(F_X^\\leftarrow(k/10)\\) pour \\(k=1,\\dots, 9\\)",
    "crumbs": [
      "Slides",
      "Notations et premiers pas"
    ]
  },
  {
    "objectID": "Slides/slides_notations_premiers_pas.html#quantiles-cas-continu",
    "href": "Slides/slides_notations_premiers_pas.html#quantiles-cas-continu",
    "title": "Notations et premiers pas",
    "section": "Quantiles: cas continu",
    "text": "Quantiles: cas continu\n\n\n\nPlotly = require('plotly.js-dist');\ndists = require( 'https://cdn.jsdelivr.net/gh/stdlib-js/stats-base-dists@umd/browser.js' );\n// see source here: https://github.com/stdlib-js/stats-base-dists/tree/umd\n// continuous case\njstatPDFs = () =&gt; {\n  const distributions = Object.keys(dists);\n  // Get in continuousDistributions the distributions whose pdf, cdf and quantile are defined\n    const continuousDistributions = distributions.filter(name =&gt; dists[name].pdf && dists[name].cdf && dists[name].quantile);\n  return continuousDistributions\n};\noutput = jstatPDFs();\n\nexcludedPDFs = [];\npdfNames_unsorted = output.filter(name =&gt; !excludedPDFs.includes(name));\npdfNames=pdfNames_unsorted.toSorted();\nviewof inputs = Inputs.form([\n      Inputs.range([-10, 10], {value: 0.1, step: 0.001, label: tex`\\mu`, width: 500}),\n      Inputs.range([0.01, 5], {value: 1.01, step: 0.001, label: tex`\\sigma`, width: 500}),\n      Inputs.range([0.001, 0.999], {value: 0.75, step: 0.001, label:tex`\\alpha`, width: 500}),\n    ]);\n\n\nviewof distrib_name = Inputs.select(pdfNames, {value: \"normal\", label: \"Loi\", width: 500});\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n{\n  const x = d3.range(-5, 5, 0.01);\n  const pdf = x.map(x =&gt; dists[distrib_name].pdf(x, mu, sigma));\n  const cdf = x.map(x =&gt; dists[distrib_name].cdf(x, mu, sigma));\n  const inv = x.map(x =&gt; dists[distrib_name].quantile(x, mu, sigma));\n  const quantile = dists[distrib_name].quantile(alpha, mu, sigma);\n  const filteredX = x.filter(coord =&gt; coord &lt; quantile);\n  const filteredPdf = pdf.filter((_, i) =&gt; x[i] &lt;= quantile);\n  const filteredCdf = cdf.filter(coord =&gt; coord &lt;= quantile);\n\n\n{\nvar trace1 = {\n      type: \"scatter\",\n      name: 'Quantile',\n      x: cdf,\n      y: x,\n      line: {color: 'black'},\n      xaxis: 'x1',\n      yaxis: 'y3',\n\n\n};\n\nvar trace12 = {\n        x : [alpha, alpha],\n        y : [x[0], quantile],\n        mode: 'lines',\n        line: {\n            dash: 'dash',\n            width: 1,\n            color: ' #428BCA',\n            marks: {\n                size: 0,\n                color: ' #428BCA',\n            }\n\n          },\n      xaxis: 'x1',\n      yaxis: 'y3',\n\n\n};\n\n\n\nvar trace13 = {\n    x: [0, alpha],\n    y: [quantile, quantile],\n    mode: 'scatter',\n    line: {\n        dash: 'dash',\n        width: 1,\n        color: ' #428BCA',\n        marks: {\n            size: 0,\n        }\n    },\n    xaxis: 'x1',\n    yaxis: 'y3'\n}\n\n\nvar trace2 = {\nx: cdf,\ny: cdf,\ntype: 'scatter',\nname: 'identity',\nline: {color: 'black'},\nxaxis: 'x1',\nyaxis: 'y2'\n\n};\n\n\nvar trace21 = {\n        x : [alpha, 1],\n        y : [alpha, alpha],\n        mode: 'lines',\n        line: {\n            dash: 'dash',\n            width: 1,\n            color: ' #428BCA',\n            marks: {\n                size: 0,\n                color: ' #428BCA',\n            }\n\n          },\n      xaxis: 'x1',\n      yaxis: 'y2'\n}\n\nvar trace22 = {\n        x : [alpha, alpha],\n        y : [alpha, 1],\n        mode: 'lines',\n        line: {\n            dash: 'dash',\n            width: 1,\n            color: ' #428BCA',\n            marks: {\n                size: 0,\n                color: ' #428BCA',\n            }\n\n          },\n      xaxis: 'x1',\n      yaxis: 'y2'\n}\n\n\n\nvar trace23 = {\n    x: [alpha],\n    y: [alpha],\n    mode: 'scatter',\n    line: {\n        dash: 'dash',\n        width: 1,\n        color: ' #428BCA',\n        marks: {\n            size: 0,\n        }\n    },\n    xaxis: 'x1',\n    yaxis: 'y2'\n}\n\n\nvar trace31 = {\n      type: \"scatter\",\n      mode: \"lines\",\n      name: 'PDF2',\n      x: x,\n      y: cdf,\n      line: {color: 'black'},\n      xaxis: 'x2',\n      yaxis: 'y2',\n};\n\nvar trace32 = {\n        x: filteredX,\n        y: filteredX.map(x =&gt; alpha),\n        mode: 'lines',\n        line: {\n            dash: 'dash',\n            width: 1,\n            color: ' #428BCA',\n            marks: {\n                size: 0,\n                color: ' #428BCA',\n            }\n          },\n      xaxis: 'x2',\n      yaxis: 'y2'\n}\n\nvar trace33 = {\n        x : [quantile, quantile],\n        y : [0, alpha],\n        mode: 'lines',\n        line: {\n            dash: 'dash',\n            width: 1,\n            color: ' #428BCA',\n            marks: {\n                size: 0,\n                color: ' #428BCA',\n            }\n          },\n\n      xaxis: 'x2',\n      yaxis: 'y2'\n}\n\nvar trace34 = {\n    x: [quantile],\n    y: [alpha],\n    mode: 'scatter',\n    line: {\n        dash: 'dash',\n        width: 1,\n        color: ' #428BCA',\n        marks: {\n            size: 0,\n        }\n    },\n    xaxis: 'x2',\n    yaxis: 'y2'\n}\n\n\nvar trace41 = {\n\n  type: \"scatter\",\n  name: 'Quantile2',\n  fill: 'tozeroy',\n  x: filteredX,\n  y: filteredPdf,\n  opacity: 0.9,\n  line: {color: ' #428BCA'},\n  xaxis: 'x2',\n  yaxis: 'y1'\n\n};\n\n\nvar trace42 = {\n\n  type: \"scatter\",\n  mode: \"lines\",\n  name: 'PDF2',\n  x: x,\n  y: pdf,\n  line: {color: 'black'},\n  xaxis: 'x2',\n  yaxis: 'y1'\n\n};\n\nvar data = [\n  trace1,\n  trace12, trace13,\n  trace2, trace21, trace22, trace23,\n  trace31, trace32, trace33, trace34,\n  trace41, trace42];\n\n\nvar layout = {\n\n  title: 'Distribution et quantile',\n  xaxis: {\n    domain: [0, 0.32],\n    anchor: 'y1'\n  },\n  yaxis: {\n    domain: [0, 0.24],\n    anchor: 'x1'\n\n  },\n  xaxis2: {\n    domain: [0.35, 1],\n    anchor: 'y'\n  },\n\n  yaxis2: {\n    domain: [0.26, 0.49],\n    anchor: 'x1'\n  },\n\n  yaxis3: {\n    domain: [0.5, 1],\n    anchor: 'x1'\n  },\n\n// legend offset\n  showlegend: false,\n  height: 680,\n  annotations: [\n\n    {\n      x: 1/4,\n      y: quantile - (x[0]-x.slice(-1))/20,\n      xref: 'x1',\n      yref: 'y3',\n      text: 'q= ' + quantile.toFixed(2),\n      font: {\n        size: 12,\n        color: '#428BCA',\n\n      },\n      showarrow: false,\n      arrowhead: 0,\n      ax: 25,\n      ay: -10,\n\n    },\n    {\n      x: 1/4,\n      y: alpha,\n      xref: 'x1',\n      yref: 'y2',\n      text: 'alpha= ' + alpha.toFixed(2),\n      font: {\n        size: 12,\n        color: '#428BCA',\n\n      },\n      showarrow: false,\n      arrowhead: 0,\n      ax: 25,\n      ay: -10,\n\n    },\n    {\n        text: \"Fonction quantile\",\n      font: {\n      size: 15,\n      color: 'black',\n    },\n    showarrow: false,\n    align: 'center',\n    x: -0.01,\n    y: 1.05,\n    xref: 'paper',\n    yref: 'paper',\n    },\n    {\n      text: \"Fonction de r√©partition\",\n      font: {\n      size: 15,\n      color: 'black',\n            },\n    showarrow: false,\n    align: 'center',\n    x: 0.65,\n    y: 0.52,\n    xref: 'paper',\n    yref: 'paper',\n    },\n    {\n      text: \"Fonction de densit√©\",\n      font: {\n      size: 15,\n      color: 'black',\n            },\n    showarrow: false,\n    align: 'center',\n    x: 0.65,\n    y: 0.225,\n    xref: 'paper',\n    yref: 'paper',\n    },\n  ]\n\n};\n// XXX: TODO: put the xticks labels on the middle plot for x and on the right plot for y\n    var config = {responsive: true}\n    const div = DOM.element('div');\n    Plotly.newPlot(div, data, layout, config);\n    return div;\n  }\n\n}\n\n\n\n\n\n\n\nmu = inputs[0];\nsigma = inputs[1];\nalpha = inputs[2]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote: voir aussi Notations et rappels",
    "crumbs": [
      "Slides",
      "Notations et premiers pas"
    ]
  },
  {
    "objectID": "Slides/slides_notations_premiers_pas.html#quantiles-cas-discret",
    "href": "Slides/slides_notations_premiers_pas.html#quantiles-cas-discret",
    "title": "Notations et premiers pas",
    "section": "Quantiles: cas discret",
    "text": "Quantiles: cas discret\n\n\n\ndiscretePDFs = () =&gt; {\n  const distributions = Object.keys(dists);\n  // Get in continuousDistributions the distributions whose pdf, cdf and quantile are defined\n    const continuousDistributions = distributions.filter(name =&gt; dists[name].pmf && dists[name].cdf && dists[name].quantile);\n  return continuousDistributions\n};\noutput_discr = discretePDFs();\n\npmfNames_unsorted = output_discr.filter(name =&gt; !excludedPDFs.includes(name));\npmfNames=pmfNames_unsorted.toSorted();\nviewof inputs_disc = Inputs.form([\n      Inputs.range([-10, 10], {value: 0.1, step: 0.001, label: tex`\\mu `, width: 500}),\n      Inputs.range([0.01, 5], {value: 1.01, step: 0.001, label: tex`\\sigma `, width: 500}),\n      Inputs.range([0.001, 0.999], {value: 0.75, step: 0.001, label: tex`\\alpha`, width: 500}),\n    ]);\n\nviewof distrib_name_discr = Inputs.select(pmfNames, {value: \"normal\", label: \"Loi\", width: 500});\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n{\n  const z = d3.range(-5, 5, 0.01);\n  const pmf = z.map(z =&gt; dists[distrib_name_discr].pmf(z, mu_disc, sigma_disc));\n  const cdf = z.map(z =&gt; dists[distrib_name_discr].cdf(z, mu_disc, sigma_disc));\n  const inv = z.map(z =&gt; dists[distrib_name_discr].quantile(z, mu_disc, sigma_disc));\n  const quantile = dists[distrib_name_discr].quantile(alpha_disc, mu_disc, sigma_disc);\n  const filteredX = z.filter(coord =&gt; coord &lt;= quantile);\n  const filteredPmf = pmf.filter((_, i) =&gt; z[i] &lt;= quantile);\n  const filteredCdf = cdf.filter(coord =&gt; coord &lt;= quantile);\n\n\n{\nvar trace1 = {\n      type: \"scatter\",\n      name: 'Quantile',\n      x: cdf,\n      y: z,\n      line: {color: 'black'},\n      xaxis: 'x1',\n      yaxis: 'y3',\n\n\n};\n\nvar trace12 = {\n        x : [alpha_disc, alpha_disc],\n        y : [z[0], quantile],\n        mode: 'lines',\n        line: {\n            dash: 'dash',\n            width: 1,\n            color: ' #428BCA',\n            marks: {\n                size: 0,\n                color: ' #428BCA',\n            }\n\n          },\n      xaxis: 'x1',\n      yaxis: 'y3',\n\n\n};\n\n\n\nvar trace13 = {\n    x: [0, alpha_disc],\n    y: [quantile, quantile],\n    mode: 'scatter',\n    line: {\n        dash: 'dash',\n        width: 1,\n        color: ' #428BCA',\n        marks: {\n            size: 0,\n        }\n    },\n    xaxis: 'x1',\n    yaxis: 'y3'\n}\n\n\nvar trace2 = {\nx: cdf,\ny: cdf,\ntype: 'scatter',\nname: 'identity',\nline: {color: 'black'},\nxaxis: 'x1',\nyaxis: 'y2'\n\n};\n\n\nvar trace21 = {\n        x : [alpha_disc, 1],\n        y : [alpha_disc, alpha_disc],\n        mode: 'lines',\n        line: {\n            dash: 'dash',\n            width: 1,\n            color: ' #428BCA',\n            marks: {\n                size: 0,\n                color: ' #428BCA',\n            }\n\n          },\n      xaxis: 'x1',\n      yaxis: 'y2'\n}\n\nvar trace22 = {\n        x : [alpha_disc, alpha_disc],\n        y : [alpha_disc, 1],\n        mode: 'lines',\n        line: {\n            dash: 'dash',\n            width: 1,\n            color: ' #428BCA',\n            marks: {\n                size: 0,\n                color: ' #428BCA',\n            }\n\n          },\n      xaxis: 'x1',\n      yaxis: 'y2'\n}\n\nvar trace23 = {\n    x: [alpha_disc],\n    y: [alpha_disc],\n    mode: 'scatter',\n    line: {\n        dash: 'dash',\n        width: 1,\n        color: ' #428BCA',\n        marks: {\n            size: 0,\n        }\n    },\n    xaxis: 'x1',\n    yaxis: 'y2'\n}\n\n\nvar trace31 = {\n      type: \"scatter\",\n      mode: \"lines\",\n      name: 'PDF2',\n      x: z,\n      y: cdf,\n      line: {color: 'black'},\n      xaxis: 'x2',\n      yaxis: 'y2',\n};\n\nvar trace32 = {\n        x: filteredX,\n        y: filteredX.map(z =&gt; alpha_disc),\n        mode: 'lines',\n        line: {\n            dash: 'dash',\n            width: 1,\n            color: ' #428BCA',\n            marks: {\n                size: 0,\n                color: ' #428BCA',\n            }\n          },\n      xaxis: 'x2',\n      yaxis: 'y2'\n}\n\nvar trace33 = {\n        x : [quantile, quantile],\n        y : [0, alpha_disc],\n        mode: 'lines',\n        line: {\n            dash: 'dash',\n            width: 1,\n            color: ' #428BCA',\n            marks: {\n                size: 0,\n                color: ' #428BCA',\n            }\n          },\n\n      xaxis: 'x2',\n      yaxis: 'y2'\n}\n\nvar trace34 = {\n    x: [quantile],\n    y: [alpha_disc],\n    mode: 'scatter',\n    line: {\n        dash: 'dash',\n        width: 1,\n        color: ' #428BCA',\n        marks: {\n            size: 0,\n        }\n    },\n    xaxis: 'x2',\n    yaxis: 'y2'\n}\n\n\nvar trace41 = {\n\n  type: \"scatter\",\n  name: 'Quantile2',\n  fill: 'tozeroy',\n  x : filteredX,\n  y : filteredPmf,\n  opacity: 0.9,\n  line: {color: ' #428BCA'},\n  xaxis: 'x2',\n  yaxis: 'y1'\n\n};\n\n\nvar trace42 = {\n\n  type: \"scatter\",\n  mode: \"lines\",\n  name: 'PDF2',\n  x: z,\n  y: pmf,\n  line: {color: 'black'},\n  xaxis: 'x2',\n  yaxis: 'y1'\n\n};\n\nvar data = [\n  trace1,\n  trace12, trace13,\n  trace2, trace21, trace22, trace23,\n  trace31, trace32, trace33, trace34,\n  trace41, trace42];\n\n\nvar layout = {\n\n  title: 'Distribution et quantile',\n  xaxis: {\n    domain: [0, 0.32],\n    anchor: 'y1'\n  },\n  yaxis: {\n    domain: [0, 0.24],\n    anchor: 'x1'\n\n  },\n  xaxis2: {\n    domain: [0.35, 1],\n    anchor: 'y'\n  },\n\n  yaxis2: {\n    domain: [0.26, 0.49],\n    anchor: 'x1'\n  },\n\n\n\n  yaxis3: {\n    domain: [0.5, 1],\n    anchor: 'x1'\n\n  },\n\n  showlegend: false,\n  height: 680,\n\n\n  annotations: [\n\n    {\n      x: 1/4,\n      y: quantile - (z[0]-z.slice(-1))/20,\n      xref: 'x1',\n      yref: 'y3',\n      text: 'q=' + quantile.toFixed(2),\n      font: {\n        size: 12,\n        color: '#428BCA',\n\n      },\n      showarrow: false,\n      arrowhead: 0,\n      ax: 25,\n      ay: -10,\n\n    },\n    {\n      x: 1/4,\n      y: alpha_disc,\n      xref: 'x1',\n      yref: 'y2',\n      text: 'alpha=' + alpha_disc.toFixed(2),\n      font: {\n        size: 12,\n        color: '#428BCA',\n\n      },\n      showarrow: false,\n      arrowhead: 0,\n      ax: 25,\n      ay: -10,\n\n    },\n    {\n        text: \"Fonction quantile\",\n      font: {\n      size: 15,\n      color: 'black',\n    },\n    showarrow: false,\n    align: 'center',\n    x: -0.01,\n    y: 1.05,\n    xref: 'paper',\n    yref: 'paper',\n    },\n    {\n      text: \"Fonction de r√©partition\",\n      font: {\n      size: 15,\n      color: 'black',\n            },\n    showarrow: false,\n    align: 'center',\n    x: 0.65,\n    y: 0.52,\n    xref: 'paper',\n    yref: 'paper',\n    },\n    {\n      text: \"Fonction de densit√©\",\n      font: {\n      size: 15,\n      color: 'black',\n            },\n    showarrow: false,\n    align: 'center',\n    x: 0.65,\n    y: 0.225,\n    xref: 'paper',\n    yref: 'paper',\n    },\n  ]\n\n};\n// XXX: TODO: put the xticks labels on the middle plot for x and on the right plot for y\n\n    var config = {responsive: true}\n    const div = DOM.element('div');\n    Plotly.newPlot(div, data, layout, config);\n    return div;\n  }\n\n}\n\n\n\n\n\n\n\nmu_disc = inputs_disc[0];\nsigma_disc = inputs_disc[1];\nalpha_disc = inputs_disc[2];\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote: voir aussi Notations et rappels",
    "crumbs": [
      "Slides",
      "Notations et premiers pas"
    ]
  },
  {
    "objectID": "Slides/slides_notations_premiers_pas.html#bibliographie",
    "href": "Slides/slides_notations_premiers_pas.html#bibliographie",
    "title": "Notations et premiers pas",
    "section": "Bibliographie",
    "text": "Bibliographie\n\n\nBarbe, Philippe, et Michel Ledoux. 2006. Probabilit√©s.\n\n\n\n\n\nNotations et premiers pas",
    "crumbs": [
      "Slides",
      "Notations et premiers pas"
    ]
  },
  {
    "objectID": "Slides/slides_loi_normale1D.html#rappel-concernant-la-loi-normale",
    "href": "Slides/slides_loi_normale1D.html#rappel-concernant-la-loi-normale",
    "title": "Loi normale: cas univari√©",
    "section": "Rappel concernant la loi normale",
    "text": "Rappel concernant la loi normale\n\n\nPour \\(\\mu \\in \\mathbb{R}\\) et \\(\\nu &gt; 0\\), on note \\(X \\sim \\mathcal{N}(\\mu, \\nu)\\), si \\(X\\) est une variable al√©atoire ayant pour densit√© \\(\\varphi_{\\mu, \\nu}\\):\n\\[\n\\forall x \\in \\mathbb{R}, \\quad \\varphi_{\\mu, \\nu}(x)=\\frac{1}{\\sqrt{2 \\pi \\nu}}\\exp\\Big(-\\frac{(x-\\mu)^2}{2\\nu}\\Big)\\enspace.\n\\]\n\n\nEsp√©rance: \\(X\\) a pour esp√©rance \\(\\mu\\), \\(\\mathbb{E}(X)=\\mu\\),\nVariance: \\(X\\) a pour variance \\(\\nu\\), \\(\\mathbb{V}(X)=\\nu\\).\nCas particulier \\(\\mu=0\\) et \\(\\nu=1\\) correspond √† une variable al√©atoire dite centr√©e r√©duite.\n\n\n\n\nOn parle aussi de loi gaussienne, en hommage au math√©maticien Carl Friedrich Gauss, le prince des math√©maticiens: (1777-1855) math√©maticien, astronome et physicien n√© √† Brunswick, directeur de l‚Äôobservatoire de G√∂ttingen de 1807 jusqu‚Äô√† sa mort en 1855",
    "crumbs": [
      "Slides",
      "Loi normale: cas univari√©"
    ]
  },
  {
    "objectID": "Slides/slides_loi_normale1D.html#visualisation-de-la-densit√©-de-la-loi-normale",
    "href": "Slides/slides_loi_normale1D.html#visualisation-de-la-densit√©-de-la-loi-normale",
    "title": "Loi normale: cas univari√©",
    "section": "Visualisation de la densit√© de la loi normale",
    "text": "Visualisation de la densit√© de la loi normale\nvoir https://josephsalmon.github.io/HAX603X/Courses/loi_normale1D.html",
    "crumbs": [
      "Slides",
      "Loi normale: cas univari√©"
    ]
  },
  {
    "objectID": "Slides/slides_loi_normale1D.html#propri√©t√©s-de-la-loi-normale",
    "href": "Slides/slides_loi_normale1D.html#propri√©t√©s-de-la-loi-normale",
    "title": "Loi normale: cas univari√©",
    "section": "Propri√©t√©s de la loi normale",
    "text": "Propri√©t√©s de la loi normale\n\n\nstabilit√© par transformation affine :\nsi \\(X \\sim \\mathcal{N}(\\mu, \\nu)\\) et si \\((\\alpha,\\beta) \\in \\mathbb{R}^* \\times \\mathbb{R}\\), alors \\(\\alpha X + \\beta \\sim \\mathcal{N}(\\alpha\\mu + \\beta, \\alpha^2 \\nu)\\).\n\nsi \\(X \\sim \\mathcal{N}(0,1)\\), alors \\(\\sqrt{\\nu} X + \\mu \\sim \\mathcal{N}(\\mu, \\nu)\\),\nsi \\(X \\sim \\mathcal{N}(\\mu, \\nu)\\), alors \\((X-\\mu)/\\sqrt{\\nu} \\sim \\mathcal{N}(0,1)\\).\n\n\n\n\n\nCons√©quence: pour simuler selon une loi normale, il suffit de savoir le faire pour le cas centr√©-r√©duit, puis d‚Äôutiliser la propri√©t√© ci-dessus",
    "crumbs": [
      "Slides",
      "Loi normale: cas univari√©"
    ]
  },
  {
    "objectID": "Slides/slides_loi_normale1D.html#fonction-caract√©ristique",
    "href": "Slides/slides_loi_normale1D.html#fonction-caract√©ristique",
    "title": "Loi normale: cas univari√©",
    "section": "Fonction caract√©ristique",
    "text": "Fonction caract√©ristique\n\nProposition 1 (Fonction caract√©ristique de la loi normale) La fonction caract√©ristique d‚Äôune variable al√©atoire \\(X \\sim \\mathcal{N}(\\mu, \\nu)\\) est donn√©e pour tout \\(t \\in \\mathbb{R}\\) par \\[\n\\begin{align*}\n\\phi_{\\mu,\\nu}(t) & \\triangleq \\mathbb{E}(e^{i t X}) = \\exp\\Big( i \\mu t - \\frac{\\nu t^2}{2}\\Big)\\enspace.\n\\end{align*}\n\\]\n\nCas particulier: si \\(X \\sim \\mathcal{N}(0,1)\\), alors \\(\\phi_{0,1}(t) = \\exp\\Big( - \\frac{t^2}{2}\\Big)\\)\n\n√âl√©ments de preuve: pour tout \\(z \\in \\mathbb{R}\\), on calcule la transform√©e de Laplace, puis on l‚Äô√©tend ensuite sur \\(\\mathbb{C}\\), et on l‚Äôinstancie pour \\(z=it\\).\n\n\n\\[\n\\begin{align*}\n\\class{fragment}{{}\\mathbb{E}[e^{zX}]}&\n\\class{fragment}{{}=\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^\\infty e^{-\\frac12x^2}e^{zx}\\,dx} \\class{fragment}{{}= \\frac{e^{\\frac12z^2}}{\\sqrt{2\\pi}}\\int_{-\\infty}^\\infty e^{-\\frac12(x-z)^2}\\,dx}\\\\\n&\\class{fragment}{{}=\\frac{e^{\\frac12z^2}}{\\sqrt{2\\pi}}\\int_{-\\infty}^\\infty e^{-\\frac12y^2}\\,dy}\\class{fragment}{{}\n=e^{\\frac12z^2}}\n\\end{align*}\n\\]",
    "crumbs": [
      "Slides",
      "Loi normale: cas univari√©"
    ]
  },
  {
    "objectID": "Slides/slides_loi_normale1D.html#id√©e-na√Øves-pour-simuler-une-loi-normale",
    "href": "Slides/slides_loi_normale1D.html#id√©e-na√Øves-pour-simuler-une-loi-normale",
    "title": "Loi normale: cas univari√©",
    "section": "Id√©e na√Øves pour simuler une loi normale",
    "text": "Id√©e na√Øves pour simuler une loi normale\n\nM√©thode de l‚Äôinverse: besoin d‚Äôun calcul de la fonction de r√©partition de la loi normale, qui n‚Äôa pas de forme analytique simple (analyse num√©rique, m√©thode co√ªteuse).\nTCL: tirer \\(U_1, \\dots, U_n\\) i.i.d. et uniforme sur \\([0,1]\\), puis poser \\[\n  \\sqrt{n}\\frac{(\\bar{U}_n - 1/2)}{\\sqrt{1/12}}\n\\] Limite: seulement une approximation, et convergence relativement lente (co√ªt √©lev√©)\nAlternatives: n√©cessite op√©rations de changement de variables",
    "crumbs": [
      "Slides",
      "Loi normale: cas univari√©"
    ]
  },
  {
    "objectID": "Slides/slides_loi_normale1D.html#changement-de-variables-en-dimension-2",
    "href": "Slides/slides_loi_normale1D.html#changement-de-variables-en-dimension-2",
    "title": "Loi normale: cas univari√©",
    "section": "Changement de variables en dimension 2",
    "text": "Changement de variables en dimension 2\nSoit \\(\\phi\\) un \\(C^1\\)-diff√©omorphisme de \\(\\mathbb{R}^2\\) (bijection dont la r√©ciproque est √©galement de classe \\(C^1\\))\n\nRappel: la jacobienne de \\(\\phi^{-1}\\) correspond √† la matrice (application lin√©aire) des d√©riv√©es partielles. Ainsi, si \\(\\phi(x,y) = (u,v) \\iff (x,y) = \\phi^{-1}(u,v)\\), alors \\[\n\\begin{align*}\n{\\rm{J}}_{\\phi^{-1}}: (u,v) & \\mapsto\n\\begin{pmatrix}\n  \\frac{\\partial x}{\\partial u} & \\frac{\\partial x}{\\partial v}    \\\\\n  \\frac{\\partial y}{\\partial u} & \\frac{\\partial y}{\\partial v}\n\\end{pmatrix} \\in \\mathbb{R}^{2\\times 2}\n\\end{align*}\n\\]\n\n\n\nTh√©or√®me 1 (Caract√©risation de la loi d‚Äôune variable al√©atoire r√©elle) Soit \\((X,Y)\\) un vecteur al√©atoire de densit√© \\(f_{(X,Y)}\\) d√©finie sur l‚Äôouvert \\(A \\subset \\mathbb{R}^2\\) et \\(\\phi : A \\to B \\subset \\mathbb{R}^2\\) un \\(C^1\\)-diff√©omorphisme. Le vecteur al√©atoire \\((U,V)=\\phi(X,Y)\\) admet alors pour densit√© \\(f_{(U,V)}\\) d√©finie sur \\(B\\) pour tout \\((u,v) \\in \\mathbb{R}^2\\) par \\[\n\\begin{align*}\n    (u,v) & \\mapsto\n    f_{(X,Y)} (\\phi^{-1}(u,v)) |\\det ({\\rm{J}}_{\\phi^{-1}} (u,v))| {1\\hspace{-3.8pt} 1}_B(u,v)\n\\end{align*}\n\\]\n\nRemarque: le r√©sultat s‚Äô√©tend facilement en dimension sup√©rieure",
    "crumbs": [
      "Slides",
      "Loi normale: cas univari√©"
    ]
  },
  {
    "objectID": "Slides/slides_loi_normale1D.html#preuve",
    "href": "Slides/slides_loi_normale1D.html#preuve",
    "title": "Loi normale: cas univari√©",
    "section": "Preuve",
    "text": "Preuve\nRappel: la loi de \\((U,V)\\) est caract√©ris√©e par \\(\\mathbb{E}[h(U,V)]\\) pour tout \\(h\\) mesurable born√©e.\n\nSoit un tel \\(h\\) et on applique la formule de transfert : \\[\n\\begin{align*}\n  \\mathbb{E}[h(U,V)] & = \\mathbb{E}[h(\\phi(X,Y))] = \\int_{\\mathbb{R}^2} h(\\phi(x,y)) f_{(X,Y)}(x,y) \\, dx dy \\\\\n& = \\int_{A} h(\\phi(x,y)) f_{(X,Y)}(x,y) \\, d x d y\\enspace.\n\\end{align*}\n\\]\n\n\nOn applique alors la formule du changement de variables \\((u,v) = \\phi(x,y) \\iff \\phi^{-1}(u,v) = (x,y)\\) : \\[\n\\begin{align*}\n   \\mathbb{E}[h(U,V)] &\n  = \\!\\int_{B}  \\!\\!\\! h(u,v) f_{(X,Y)}(\\phi^{-1}(u,v)) |\\det ({\\rm{J}}_{\\phi^{-1}} (u,v))| \\, d u d v\\\\\n  & = \\!\\int_{\\mathbb{R}^2} \\!\\!\\!\\! h(u,v) f_{(X,Y)}(\\phi^{-1}(u,v)) |\\det ({\\rm{J}}_{\\phi^{-1}} (u,v))| {1\\hspace{-3.8pt} 1}_B(u,v)\\, d u d v .\n\\end{align*}\n\\] ce qui donne le r√©sultat voulu.",
    "crumbs": [
      "Slides",
      "Loi normale: cas univari√©"
    ]
  },
  {
    "objectID": "Slides/slides_loi_normale1D.html#m√©thode-de-box-m√ºller",
    "href": "Slides/slides_loi_normale1D.html#m√©thode-de-box-m√ºller",
    "title": "Loi normale: cas univari√©",
    "section": "M√©thode de Box-M√ºller",
    "text": "M√©thode de Box-M√ºller\nL‚Äôalgorithme de Box-M√ºller est le suivant: si \\(U\\) et \\(V\\) sont des v.a. ind√©pendantes de loi uniforme sur \\([0,1]\\) et qu‚Äôon d√©finit \\(X\\) et \\(Y\\) par \\[\n\\begin{cases}\n  X = \\sqrt{-2 \\log(U)} \\cos(2\\pi V)\\\\\n  Y = \\sqrt{-2 \\log(U)} \\sin(2\\pi V)\\,.\n\\end{cases}\n\\] alors \\(X\\) et \\(Y\\) des variables al√©atoires gaussiennes centr√©es r√©duites ind√©pendantes.",
    "crumbs": [
      "Slides",
      "Loi normale: cas univari√©"
    ]
  },
  {
    "objectID": "Slides/slides_loi_normale1D.html#preuve-de-la-m√©thode-de-box-m√ºller",
    "href": "Slides/slides_loi_normale1D.html#preuve-de-la-m√©thode-de-box-m√ºller",
    "title": "Loi normale: cas univari√©",
    "section": "Preuve de la m√©thode de Box-M√ºller",
    "text": "Preuve de la m√©thode de Box-M√ºller\n\\[\n    \\begin{array}{ccccc}\n        \\phi^{-1} & : & ]0, \\infty[ \\times ]0, 2\\pi[ & \\to     & &\\mathbb{R}^2 \\setminus ([0,\\infty[ \\times \\{0\\})  \\\\\n                  &   & ( r , \\theta)                   & \\mapsto && (r \\cos(\\theta) , r \\sin(\\theta))  \\\\\n        \\phi & : & \\mathbb{R}^2 \\setminus ([0,\\infty[ \\times \\{0\\}) & \\to  && ]0, \\infty[ \\times ]0, 2\\pi[                                                       \\\\\n             &   & ( x, y )                                            & \\mapsto && (\\sqrt{x^2+y^2} , 2 \\arctan \\Big( \\frac{y}{x+\\sqrt{x^2+y^2}} \\Big)\n    \\end{array}\n\\]\n\nTh√©or√®me 2 (M√©thode de Box-M√ºller) Soit \\(X\\) et \\(Y\\) deux v.a. ind√©pendantes \\(X,Y \\sim \\mathcal{N}(0,1)\\). Le couple de variables al√©atoires polaires \\((R, \\Theta) = \\phi(X,Y)\\) a pour densit√© \\[\n            f_{R, \\Theta}(r,\\theta)\n            = \\Big( r \\cdot e^{-\\tfrac{r^2}{2}} {1\\hspace{-3.8pt} 1}_{]0, \\infty[}(r) \\Big) \\bigg(\\frac{{1\\hspace{-3.8pt} 1}_{]0, 2 \\pi[}(\\theta)}{2 \\pi} \\bigg)\\,.\n\\] \\(R\\) et \\(\\Theta\\) sont ind√©pendantes, \\(\\Theta \\sim \\mathcal{U}(]0, 2\\pi[)\\), \\(R\\) suit une loi de Rayleigh de densit√© \\[\n    f_R(r) =  r \\cdot e^{-r^2/2} {1\\hspace{-3.8pt} 1}_{]0, \\infty[}(r)\\,, \\quad r &gt; 0\\,.\n\\]",
    "crumbs": [
      "Slides",
      "Loi normale: cas univari√©"
    ]
  },
  {
    "objectID": "Slides/slides_loi_normale1D.html#preuve-de-la-m√©thode-de-box-m√ºller-suite",
    "href": "Slides/slides_loi_normale1D.html#preuve-de-la-m√©thode-de-box-m√ºller-suite",
    "title": "Loi normale: cas univari√©",
    "section": "Preuve de la m√©thode de Box-M√ºller (suite)",
    "text": "Preuve de la m√©thode de Box-M√ºller (suite)\n\nLemme 1 (Simulation selon la loi de Rayleigh) Si \\(U\\) est une variable al√©atoire de loi uniforme sur \\(]0,1[\\), alors \\(\\sqrt{-2 \\log(U)}\\) suit une loi de Rayleigh.\n\n\nPreuve: Pour tout \\(x &gt; 0\\), \\(F_R(x)=\\mathbb{P}(R\\leq x) = 1-\\exp(-\\tfrac{x^2}{2})\\), et donc pour tout \\(q \\in ]0,1[, F_R^{^\\leftarrow}(q)=\\sqrt{-2\\log(1-q)}\\). Ainsi par la m√©thode de l‚Äôinverse, \\(\\sqrt{-2\\log(1-U)}\\) suit est une v.a. distribu√©e selon la loi de Rayleigh, et donc aussi \\(\\sqrt{-2\\log(U)}\\).\n\n\nEnfin, on prouve le bien fond√© de la m√©thode de Box-M√ºller en utilisant le lemme ci-dessus, et en notant que \\(~U\\sim\\mathcal{U}[0,1] \\implies 2 \\pi U\\sim\\mathcal{U}[0,2\\pi]\\)",
    "crumbs": [
      "Slides",
      "Loi normale: cas univari√©"
    ]
  },
  {
    "objectID": "Slides/slides_loi_normale1D.html#alternatives",
    "href": "Slides/slides_loi_normale1D.html#alternatives",
    "title": "Loi normale: cas univari√©",
    "section": "Alternatives",
    "text": "Alternatives\n\nl‚Äôalgorithme de Box-M√ºller n‚Äôest pas utilis√© si souvent en pratique (evaluation de fonctions co√ªteuses: logarithme, cosinus, sinus).\nPour s‚Äôaffranchir des fonctions trigonom√©triques, une version modifi√©e de l‚Äôalgorithme de Box-M√ºller a √©t√© propos√©e : la m√©thode de Marsaglia, qui s‚Äôappuie sur des variables al√©atoires uniformes sur le disque unit√© (voir l‚Äôexercice d√©di√© en TD).\nUne autre alternative est la m√©thode de Ziggurat impl√©ment√©e dans la librairie numpy, notamment.",
    "crumbs": [
      "Slides",
      "Loi normale: cas univari√©"
    ]
  },
  {
    "objectID": "Slides/slides_loi_normale1D.html#loi-du-chi2",
    "href": "Slides/slides_loi_normale1D.html#loi-du-chi2",
    "title": "Loi normale: cas univari√©",
    "section": "Loi du \\(\\chi^2\\)",
    "text": "Loi du \\(\\chi^2\\)\nConcernant la prononciation, on prononce ‚Äúkhi-deux‚Äù le nom de cette loi.\n\nD√©finition 1 (Loi du \\(\\chi^2\\)) Soit \\(X_1, \\dots, X_k\\) des variables al√©atoires i.I.d. de loi normale centr√©e r√©duite. La loi de la variable al√©atoire \\(X = X_1^2 + \\dots + X_k^2\\) est appel√©e loi du \\(\\chi^2\\) √† \\(k\\) degr√©s de libert√©. Sa densit√© est donn√©e par \\[\nf(x) = \\frac{1}{2^{\\frac{k}{2}}\\Gamma(\\frac{k}{2})} x^{\\frac{k}{2}-1} e^{-x/2}\\,, \\quad x \\geq 0\\,,\n\\] o√π \\(\\Gamma\\) d√©signe la fonction gamma d‚ÄôEuler : \\[\n\\Gamma(x) = \\int_0^{\\infty} t^{x-1} e^{-t}\\,  dt\\,.\n\\] On note alors \\(X \\sim \\chi^2(k)\\).",
    "crumbs": [
      "Slides",
      "Loi normale: cas univari√©"
    ]
  },
  {
    "objectID": "Slides/slides_loi_normale1D.html#loi-de-student",
    "href": "Slides/slides_loi_normale1D.html#loi-de-student",
    "title": "Loi normale: cas univari√©",
    "section": "Loi de Student",
    "text": "Loi de Student\n\n\n\nD√©finition 2 (Loi de Student) Soit \\(X \\sim \\mathcal{N}(0,1)\\) et \\(Y \\sim \\chi^2(k)\\) deux variables al√©atoires ind√©pendantes. La loi de la variable al√©atoire \\(V = \\frac{X}{\\sqrt{Y/k}}\\) est appel√©e loi de Student √† \\(k\\) degr√©s de libert√©. Elle admet pour densit√© \\[\n    f_V(t)\n    = \\dfrac{1}{\\sqrt{k \\pi}} \\dfrac{\\Gamma(\\frac{k+1}{2})}{\\Gamma(\\frac{k}{2})} \\Big(1+\\dfrac{t^2}{k}\\Big)^{-\\frac{k+1}{2}}\\,,\n    \\quad t \\in \\mathbb{R}\\,.\n\\]\n\n\nApplication: elle est utilis√©e en statistiques pour d√©terminer l‚Äôintervalle de confiance de l‚Äôesp√©rance d‚Äôune loi normale, quand la variance est inconnue (en lien avec le th√©or√®me de Cochran)\n\n\n\nCette loi a √©t√© d√©crite en 1908 par William Gosset: (1876-1937) statisticien et chimiste anglais. Il √©tait employ√© √† la brasserie Guinness √† Dublin. Son employeur lui refusant le droit de publier sous son propre nom, W. Gosset choisit un pseudonyme, Student (üá´üá∑: √©tudiant).",
    "crumbs": [
      "Slides",
      "Loi normale: cas univari√©"
    ]
  },
  {
    "objectID": "Slides/slides_loi_normale1D.html#loi-de-cauchy",
    "href": "Slides/slides_loi_normale1D.html#loi-de-cauchy",
    "title": "Loi normale: cas univari√©",
    "section": "Loi de Cauchy",
    "text": "Loi de Cauchy\n\n\n\nD√©finition 3 (Loi de Cauchy standard) Une v.a. \\(X\\) suit une loi de Cauchy standard si sa densit√© est donn√©e par \\[\n    f_X(x) = \\dfrac{1}{\\pi(1+x^2)}\\,, \\quad x \\in \\mathbb{R}\\,.\n\\] On note alors \\(X\\sim \\mathcal{C}(0,1)\\) dans ce cas.\n\n\nApplication: Loi souvent utile comme contre-exemple, n‚Äôayant ni esp√©rance (ni variance a fortiori), et ne satisfaisant pas la loi des grands nombres ou le TCL.\n\n\n\n\nLoi √©tudi√©e en particulier par Augustin-Louis Cauchy: (1789-1857) math√©maticien et physicien fran√ßais, connu pour ses travaux fondateurs en analyse complexe et dans l‚Äô√©tude du groupe des permutations. \ncf. (Stigler 1974) pour plus de d√©tails historiques sur cette loi",
    "crumbs": [
      "Slides",
      "Loi normale: cas univari√©"
    ]
  },
  {
    "objectID": "Slides/slides_loi_normale1D.html#loi-de-cauchy-fonctions-caract√©ristiques",
    "href": "Slides/slides_loi_normale1D.html#loi-de-cauchy-fonctions-caract√©ristiques",
    "title": "Loi normale: cas univari√©",
    "section": "Loi de Cauchy: fonctions caract√©ristiques",
    "text": "Loi de Cauchy: fonctions caract√©ristiques\n\nD√©finition 4 (Loi de Cauchy) On dit que \\(Y\\) suit une loi de Cauchy de param√®tres \\((\\mu,\\sigma)\\in \\mathbb{R} \\times ]0,+\\infty[\\) si \\(Y=\\mu + \\sigma X\\), o√π \\(X\\) suit une loi de Cauchy standard. On note alors \\(X\\sim \\mathcal{C}(0,1)\\) dans ce cas, et la densit√© de \\(Y\\) est donn√©e par \\[\n    f_Y(y) = \\dfrac{1}{\\sigma \\pi(1 + \\tfrac{1}{\\sigma^2}\\left(y-\\mu\\right)^2)}\\,, \\quad y \\in \\mathbb{R}\\,.\n\\]\n\n\nProposition 2 (Loi de Cauchy et fonction caract√©ristique) La fonction caract√©ristique de la loi de Cauchy standard est donn√©e par \\[\n\\begin{align*}\n\\varphi_X(t) & \\triangleq \\int_{\\mathbb{R}} e^{itx} f_X(x) \\, dx = e^{-|t|}\\,.\n\\end{align*}\n\\] et donc si \\(Y\\sim \\mathcal{C}(\\mu,\\sigma)\\), alors pour tout \\(t \\in \\mathbb{R}\\), \\(\\varphi_Y(t) = e^{i\\mu t - \\sigma |t|}\\).\n\nPour la preuve voir par exemple (Exemple III.5.5., Barbe et Ledoux 2006)",
    "crumbs": [
      "Slides",
      "Loi normale: cas univari√©"
    ]
  },
  {
    "objectID": "Slides/slides_loi_normale1D.html#loi-de-cauchy-stabilit√©-par-somme",
    "href": "Slides/slides_loi_normale1D.html#loi-de-cauchy-stabilit√©-par-somme",
    "title": "Loi normale: cas univari√©",
    "section": "Loi de Cauchy : stabilit√© par somme",
    "text": "Loi de Cauchy : stabilit√© par somme\nImplications:\n\nla somme de deux variables al√©atoires ind√©pendantes de loi de Cauchy est de Cauchy: Si \\(X_1 \\sim \\mathcal{C}(\\mu_1,\\sigma_2)\\) et \\(X_2 \\sim \\mathcal{C}(\\mu_2,\\sigma_2)\\) sont ind√©pendantes, alors \\(X_1+X_2 \\sim \\mathcal{C}(\\mu_1+\\mu_2,\\sigma_1+\\sigma_2)\\) (preuve: m√™me fonction caract√©ristique).\n\n\n\nla moyenne de variables de Cauchy standard i.i.d suit la loi de Cauchy standard: si \\(X_1, \\ldots, X_n\\) sont i.i.d de loi de Cauchy standard alors \\(\\bar{X}_n \\sim \\mathcal{C}(0,1)\\) (preuve: pour tout \\(t \\in \\mathbb{R}\\), \\(\\varphi_{\\bar{X}_n}(t) = e^{-|t|}\\))\n\nConclusion: la moyenne empirique de v.a. \\(\\mathcal{C}(0,1)\\) i.i.d ne converge pas en probabilit√© vers une constante!",
    "crumbs": [
      "Slides",
      "Loi normale: cas univari√©"
    ]
  },
  {
    "objectID": "Slides/slides_loi_normale1D.html#loi-de-cauchy-propri√©t√©",
    "href": "Slides/slides_loi_normale1D.html#loi-de-cauchy-propri√©t√©",
    "title": "Loi normale: cas univari√©",
    "section": "Loi de Cauchy: propri√©t√©",
    "text": "Loi de Cauchy: propri√©t√©\n\nProposition 3 (Loi de Cauchy et loi normale) Soient \\(X\\) et \\(Y\\) deux variables al√©atoires ind√©pendantes de loi normale centr√©e r√©duite. Alors, \\(Y/X\\) suit une loi de Cauchy standard.\n\nRemarque: la m√©thode d‚Äôinversion permet aussi de simuler une variable al√©atoire de loi de Cauchy (cf.¬†TD/TP).\n\n\nCons√©quence: \\(X\\sim \\mathcal{C}(0,1) \\implies 1/X \\sim \\mathcal{C}(0,1)\\)",
    "crumbs": [
      "Slides",
      "Loi normale: cas univari√©"
    ]
  },
  {
    "objectID": "Slides/slides_loi_normale1D.html#preuve-rapport-de-variables-gaussiennes",
    "href": "Slides/slides_loi_normale1D.html#preuve-rapport-de-variables-gaussiennes",
    "title": "Loi normale: cas univari√©",
    "section": "Preuve (rapport de variables gaussiennes):",
    "text": "Preuve (rapport de variables gaussiennes):\nPreuve: Comme pour la loi de Student, on d√©montre ce r√©sultat avec un changement de variables. On consid√®re l‚Äôapplication \\[\n    \\begin{array}{ccccc}\n        \\phi & : & \\mathbb{R}^* \\times \\mathbb{R} & \\to     & \\mathbb{R}^* \\times \\mathbb{R} \\\\\n                &   & (x,y) & \\mapsto & \\Big(x, \\dfrac{y}{x}\\Big)\\\\\n        \\phi^{-1} & :  & \\mathbb{R}^* \\times \\mathbb{R} & \\to     & \\mathbb{R}^* \\times \\mathbb{R}\\\\\n                &   & (u, v) & \\mapsto & \\Big(u, uv)\n\\end{array}\n\\]\n\\[\n    J_{\\phi^{-1}} (u,v)\n    =\n    \\begin{pmatrix}\n        1 & 0 \\\\\n        v & u\n    \\end{pmatrix}\\,,\n\\] et son d√©terminant vaut \\(u\\). Le reste est calculatoire et laiss√© en exercice.",
    "crumbs": [
      "Slides",
      "Loi normale: cas univari√©"
    ]
  },
  {
    "objectID": "Slides/slides_loi_normale1D.html#bibliographie",
    "href": "Slides/slides_loi_normale1D.html#bibliographie",
    "title": "Loi normale: cas univari√©",
    "section": "Bibliographie",
    "text": "Bibliographie\n\n\nBarbe, Philippe, et Michel Ledoux. 2006. Probabilit√©s.\n\n\nStigler, Stephen M. 1974. ¬´¬†Studies in the History of Probability and Statistics. XXXIII Cauchy and the witch of Agnesi: An historical note on the Cauchy distribution¬†¬ª. Biometrika, 375‚Äë80.\n\n\n\n\n\nLoi normale: cas univari√©",
    "crumbs": [
      "Slides",
      "Loi normale: cas univari√©"
    ]
  },
  {
    "objectID": "Slides/slides_intro.html#section",
    "href": "Slides/slides_intro.html#section",
    "title": "Introduction",
    "section": "",
    "text": "PS: n‚Äôoubliez pas de mettre [HAX603X] dans le titre de vos mails!",
    "crumbs": [
      "Slides",
      "Introduction"
    ]
  },
  {
    "objectID": "Slides/slides_intro.html#enseignants",
    "href": "Slides/slides_intro.html#enseignants",
    "title": "Introduction",
    "section": "Enseignants",
    "text": "Enseignants\n\n\nJoseph Salmon : CM et TP\n\nSituation actuelle : Professeur √† l‚ÄôUniversit√© de Montpellier\nPr√©c√©demment : Paris Diderot-Paris 7, Duke Univ., T√©l√©com ParisTech, Univ. Washington\nSp√©cialit√©s : statistiques, optimisation, traitement des images, sciences participatives\nBureau : 415, Bat. 9\n\n\n\n\n\n\n\n\nBenjamin Charlier : CM, TD et TP\n\nSituation actuelle : Ma√Ætre de conf√©rences √† l‚ÄôUniversit√© de Montpellier\nPr√©c√©demment : Universit√© Paul Sabatier, ENS Paris-Saclay\nSp√©cialit√©s : traitement des images, statistiques, diff√©rentiation automatique\nBureau : 423, Bat. 9",
    "crumbs": [
      "Slides",
      "Introduction"
    ]
  },
  {
    "objectID": "Slides/slides_intro.html#ressources-en-ligne",
    "href": "Slides/slides_intro.html#ressources-en-ligne",
    "title": "Introduction",
    "section": "Ressources en ligne",
    "text": "Ressources en ligne\n\nInformations principales : site du cours http://josephsalmon.github.io/HAX603X\n\n\n\nSyllabus\nCours (d√©taill√©: site web)\nSlides (r√©sum√©)\nFeuilles de TD\nFeuilles de TP\nRendu TP : Moodle de l‚Äôuniversit√© (https://moodle.umontpellier.fr/course/view.php?id=5558)",
    "crumbs": [
      "Slides",
      "Introduction"
    ]
  },
  {
    "objectID": "Slides/slides_intro.html#validation",
    "href": "Slides/slides_intro.html#validation",
    "title": "Introduction",
    "section": "Validation",
    "text": "Validation\n\nTP not√©s : Rendu = fichier Python .py unique\n\nTP not√© 1 : rendre en fin de session (en S11)\nTP not√© 2 : rendre en fin de session (en S17)\n\nCC : devoir sur table d‚Äôune heure (S18)\n\n\n\nCoefficients:\n\nNote Session 1 : (40% CC + 30% TP 1 + 30% TP 2)\nNote Session 2 : (30% CC + 35% TP 1 + 35% TP 2)\n\n\n\n\n\n\n\n\nImportant\n\n\nLe rendu est individuel pour le TP not√© !!!",
    "crumbs": [
      "Slides",
      "Introduction"
    ]
  },
  {
    "objectID": "Slides/slides_intro.html#notation-pour-les-tps",
    "href": "Slides/slides_intro.html#notation-pour-les-tps",
    "title": "Introduction",
    "section": "Notation pour les TPs",
    "text": "Notation pour les TPs\nRendu : sur Moodle, en d√©posant un fichier nom_prenom.py dans le dossier ad√©quat.\nD√©tails de la notation des TPs :\n\nQualit√© des r√©ponses aux questions\nQualit√© de r√©daction et d‚Äôorthographe\nQualit√© des graphiques (l√©gendes, couleurs)\nQualit√© du code (noms de variables, clairs, commentaires utiles, code synth√©tique, etc.)\nCode reproductible et absence de bug\n\n\n\n\n\n\n\n\nP√©nalit√©s\n\n\n\nEnvoi par mail : z√©ro\nRetard : z√©ro (uploader avant la fin, fermeture automatique de moodle)",
    "crumbs": [
      "Slides",
      "Introduction"
    ]
  },
  {
    "objectID": "Slides/slides_intro.html#pr√©requis---√†-revoir-seul",
    "href": "Slides/slides_intro.html#pr√©requis---√†-revoir-seul",
    "title": "Introduction",
    "section": "Pr√©requis - √† revoir seul",
    "text": "Pr√©requis - √† revoir seul\n\n \n\nBases de probabilit√©s (en particulier ‚ÄúHAX506X- Th√©orie des Probabilit√©s‚Äù): probabilit√©, densit√©, esp√©rance, fonction de r√©partition, mesure, int√©gration, analyse num√©rique √©l√©mentaire, etc. (Foata et Fuchs 1996; Barbe et Ledoux 2006; Ouvrard 2007, 2008)\n\n\n\nProgrammation √©l√©mentaire (en Python): if ‚Ä¶ then‚Ä¶ else ‚Ä¶, for, while, fonctions, etc. HLMA310 - Logiciels scientifiques, (Courant et al. 2013), Cours de Python: Univ. Paris Diderot\n\n\n\nPour aller plus loin: conditionnement, martingales (Williams 1991)",
    "crumbs": [
      "Slides",
      "Introduction"
    ]
  },
  {
    "objectID": "Slides/slides_intro.html#description-du-cours",
    "href": "Slides/slides_intro.html#description-du-cours",
    "title": "Introduction",
    "section": "Description du cours",
    "text": "Description du cours\n\n\nG√©n√©rer l‚Äôal√©a\n\ng√©n√©rateurs pseudo-al√©atoires, simulations de variables al√©atoires¬†(inverse, rejet, etc.)\nillustrations num√©riques et visualisation en Python (loi des grands nombres, TCL)\n\nM√©thode de Monte-Carlo\n\nm√©thode de Monte-Carlo pour le calcul approch√© d‚Äôune int√©grale\nr√©duction de la variance¬†: variables antith√©tiques, variables de contr√¥le, etc.\n\nCompl√©ments\n\nvecteurs gaussiens et lien avec les lois usuelles de la statistique inf√©rentielle (student, chi2)\nconstruction d‚Äôintervalles de confiance.\nmarche al√©atoire simple, etc.",
    "crumbs": [
      "Slides",
      "Introduction"
    ]
  },
  {
    "objectID": "Slides/slides_intro.html#buffon-et-les-pr√©misses-de-la-m√©thode-de-monte-carlo",
    "href": "Slides/slides_intro.html#buffon-et-les-pr√©misses-de-la-m√©thode-de-monte-carlo",
    "title": "Introduction",
    "section": "Buffon et les pr√©misses de la m√©thode de Monte-Carlo",
    "text": "Buffon et les pr√©misses de la m√©thode de Monte-Carlo\n\n\n\n\n1733: l‚Äôaiguille de Buffon, m√©thode d‚Äôestimation de la valeur de \\(\\pi\\).\n\n\n\n\nProbl√®me initial: une aiguille de taille 1 tombe sur un parquet compos√© de lattes de largeur \\(1\\): quelle est alors la probabilit√© \\(P\\) que l‚Äôaiguille croise une ligne de la trame du parquet ?\n\n\n\n\n\n\n                                                \n\n\n\n\n\n\nGeorges-Louis Leclerc, Comte de Buffon(1707-1788) : naturaliste, math√©maticien et industriel fran√ßais du si√®cle des Lumi√®res",
    "crumbs": [
      "Slides",
      "Introduction"
    ]
  },
  {
    "objectID": "Slides/slides_intro.html#laiguille-de-buffon-suite",
    "href": "Slides/slides_intro.html#laiguille-de-buffon-suite",
    "title": "Introduction",
    "section": "L‚Äôaiguille de Buffon (suite)",
    "text": "L‚Äôaiguille de Buffon (suite)\n\nProbl√®me initial: une aiguille de taille 1 tombe sur un parquet compos√© de lattes de largeur \\(1\\): quelle est alors la probabilit√© \\(P\\) que l‚Äôaiguille croise une ligne de la trame du parquet ?\n\n\n\nR√©ponse: \\[\nP = \\frac{2}{\\pi} \\approx 0.6366 \\enspace.\n\\] Une preuve de ce r√©sultat est donn√©e ici.",
    "crumbs": [
      "Slides",
      "Introduction"
    ]
  },
  {
    "objectID": "Slides/slides_intro.html#principe-de-monte-carlo-et-estimation",
    "href": "Slides/slides_intro.html#principe-de-monte-carlo-et-estimation",
    "title": "Introduction",
    "section": "Principe de Monte Carlo et estimation",
    "text": "Principe de Monte Carlo et estimation\nId√©e sous-jacente de Buffon :\nsi l‚Äôon r√©p√®te cette exp√©rience un grand nombre de fois, on peut approch√© la quantit√© \\(P\\) num√©riquement, par exemple en proposant un estimateur \\(\\hat{P}_n\\) qui compte la proportion de chevauchement apr√®s avoir fait \\(n\\) r√©p√©tition des lancers.\n Estimation de \\(\\pi\\):\n\\[\n\\pi \\approx \\frac{2}{\\hat{P}_n}\n\\]",
    "crumbs": [
      "Slides",
      "Introduction"
    ]
  },
  {
    "objectID": "Slides/slides_intro.html#principe-de-monte-carlo-pour-lestimation-suite",
    "href": "Slides/slides_intro.html#principe-de-monte-carlo-pour-lestimation-suite",
    "title": "Introduction",
    "section": "Principe de Monte Carlo pour l‚Äôestimation (suite)",
    "text": "Principe de Monte Carlo pour l‚Äôestimation (suite)",
    "crumbs": [
      "Slides",
      "Introduction"
    ]
  },
  {
    "objectID": "Slides/slides_intro.html#m√©thode-de-monte-carlo",
    "href": "Slides/slides_intro.html#m√©thode-de-monte-carlo",
    "title": "Introduction",
    "section": "M√©thode de Monte-Carlo",
    "text": "M√©thode de Monte-Carlo\nM√©thode de calcul num√©rique qui consiste √† utiliser des nombres al√©atoires pour r√©soudre des probl√®mes d√©terministes.\n\nDomaines d‚Äôapplications:\n\nla physique\nla chimie\nla biologie\nla finance\nl‚Äôapprentissage automatique",
    "crumbs": [
      "Slides",
      "Introduction"
    ]
  },
  {
    "objectID": "Slides/slides_intro.html#contexte-de-la-naissance-de-la-m√©thode-de-monte-carlo",
    "href": "Slides/slides_intro.html#contexte-de-la-naissance-de-la-m√©thode-de-monte-carlo",
    "title": "Introduction",
    "section": "Contexte de la naissance de la m√©thode de Monte Carlo",
    "text": "Contexte de la naissance de la m√©thode de Monte Carlo\n\n\n\n\nLieu: Los Alamos\n√âpoque: seconde guerre mondial\nContexte: Projet Manathan, produire une bombe atomique\nBesoins: mod√©liser les r√©actions nucl√©aires en cha√Æne (combinatoires)\n\n\n\n\n\n\n\n\nJohn von Neumann (1903-1957), math√©maticien et physicien am√©ricano-hongrois, un des p√®res de l‚Äôinformatique.\n\n\n\n\n\n\n\nNicholas Metropolis (1915-1999), physicien gr√©co-am√©ricain, un des initiateurs de la m√©thode de Monte Carlo et du recuit simul√©\n\n\n\n\n\n\n\nStanis≈Çaw Ulam (1909-1984), math√©maticien polono-am√©ricainm, un des initiateurs de la m√©thode de Monte Carlo et de la propulsion nucl√©aire puls√©e\n\n\n\n\n\n\n\n\n\n\nExplosion de Trinity (16 Juillet 1945)",
    "crumbs": [
      "Slides",
      "Introduction"
    ]
  },
  {
    "objectID": "Slides/slides_intro.html#lorigine-du-nom-monte-carlo",
    "href": "Slides/slides_intro.html#lorigine-du-nom-monte-carlo",
    "title": "Introduction",
    "section": "L‚Äôorigine du nom ‚ÄúMonte-Carlo‚Äù",
    "text": "L‚Äôorigine du nom ‚ÄúMonte-Carlo‚Äù\nInitialement: besoin de confidentialit√© du projet Manhattan\n\nMonte-Carlo: connue pour ses jeux de hasard, o√π l‚Äôoncle de Stanis≈Çaw Ulam aimait se rendre pour assouvir sa soif de jeu.\n Ce serait N. Metropolis qui aurait propos√© ce nom, cf. (Metropolis 1987):\nIt was at that time that I suggested an obvious name for the statistical method‚Äîa suggestion not unrelated to the fact that Stan had an uncle who would borrow money from relatives because he ‚Äújust had to go to Monte Carlo‚Äù.",
    "crumbs": [
      "Slides",
      "Introduction"
    ]
  },
  {
    "objectID": "Slides/slides_intro.html#essor-de-la-m√©thode-de-monte-carlo",
    "href": "Slides/slides_intro.html#essor-de-la-m√©thode-de-monte-carlo",
    "title": "Introduction",
    "section": "Essor de la m√©thode de Monte Carlo",
    "text": "Essor de la m√©thode de Monte Carlo\n\n\n\n\nPopularisation croissante:\n\nEssor de l‚Äôinformatique (depuis les ann√©es 80)\nEssor des m√©thodes de calcul parall√®le (GPUs, clusters, etc.)\n\n\n\n\n\nDomaine principaux impact√©s:\n\nfinance : √©valuation des prix de produits d√©riv√©s\napprentissage automatique: utilisation de l‚Äôal√©atoire pour g√©n√©r√© des sc√©narios\nExemples: Alphago (2016), AlphaGeometry (2024)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRecherche arborescente Monte-Carlo (üá¨üáß: Monte Carlo tree search): analyse des sc√©narios les plus prometteurs, en √©largissant l‚Äôarbre de recherche sur la base d‚Äôun √©chantillonnage al√©atoire de l‚Äôespace entier (ingr√©dient important d‚ÄôAlphaGo)",
    "crumbs": [
      "Slides",
      "Introduction"
    ]
  },
  {
    "objectID": "Slides/slides_intro.html#bibliographie",
    "href": "Slides/slides_intro.html#bibliographie",
    "title": "Introduction",
    "section": "Bibliographie",
    "text": "Bibliographie\n\n\nBarbe, Philippe, et Michel Ledoux. 2006. Probabilit√©s.\n\n\nCourant, J., M. de Falco, S. Gonnord, J.-C. Filli√¢tre, S. Conchon, G. Dowek, et B. Wack. 2013. Informatique pour tous en classes pr√©paratoires aux grandes √©coles: Manuel d‚Äôalgorithmique et programmation structur√©e avec Python. Eyrolles.\n\n\nFoata, D., et A. Fuchs. 1996. Calcul des probabilit√©s: cours et exercices corrig√©s. Masson.\n\n\nMetropolis, Nicholas. 1987. ¬´¬†The beginning of the Monte Carlo method¬†¬ª. Los Alamos Science, n·µí 15: 125‚Äë30.\n\n\nOuvrard, J.-Y. 2007. Probabilit√©s : Tome 2, Licence - CAPES. 2·µâ √©d. Enseignement des math√©matiques. Cassini.\n\n\n‚Äî‚Äî‚Äî. 2008. Probabilit√©s : Tome 1, Licence - CAPES. 2·µâ √©d. Enseignement des math√©matiques. Cassini.\n\n\nWilliams, D. 1991. Probability with martingales. Cambridge Mathematical Textbooks. Cambridge: Cambridge University Press.\n\n\n\n\n\nIntroduction",
    "crumbs": [
      "Slides",
      "Introduction"
    ]
  },
  {
    "objectID": "Courses/slides.html",
    "href": "Courses/slides.html",
    "title": "Slides: menu principal",
    "section": "",
    "text": "Vous trouverez ci-dessous la listes des slides associ√©s:\nCours introduction, plein √©cran\n\nCours: notations premiers pas, plein √©cran\n\nCours: th√©or√®me asymptotiques, plein √©cran\n\nCours: simulation, m√©thodes classiques\n\n\n\n\n Retour au sommet",
    "crumbs": [
      "Slides",
      "Slides: menu principal"
    ]
  },
  {
    "objectID": "Courses/perspective_historique.html",
    "href": "Courses/perspective_historique.html",
    "title": "Perspectives historiques",
    "section": "",
    "text": "Nous allons pr√©senter ici quelques √©l√©ments historiques sur les m√©thodes de Monte-Carlo, dont les pr√©misses remontent au XVIII√®me si√®cle.",
    "crumbs": [
      "Cours",
      "Perspectives historiques"
    ]
  },
  {
    "objectID": "Courses/perspective_historique.html#laiguille-de-buffon",
    "href": "Courses/perspective_historique.html#laiguille-de-buffon",
    "title": "Perspectives historiques",
    "section": "L‚Äôaiguille de Buffon",
    "text": "L‚Äôaiguille de Buffon\nGeorges-Louis Leclerc, Comte de Buffon1 proposa en 1733 une m√©thode qui s‚Äôav√©ra √™tre utile pour estimer la valeur de \\pi. On d√©signe de nos jours cette exp√©rience sous le nom de l‚Äôaiguille de Buffon. C‚Äôest l‚Äôune des premi√®res m√©thodes de Monte-Carlo r√©f√©renc√©e dans la litt√©rature (la source du texte est disponible ici sur le site de la BNF).\n1¬†Georges-Louis Leclerc, Comte de Buffon: (1707-1788) naturaliste, math√©maticien et industriel fran√ßais du si√®cle des Lumi√®res La question initiale (simplifi√©e ici) pos√©e par Buffon √©tait la suivante: une aiguille de taille 1 tombe sur un parquet compos√© de lattes de largeur 1: quelle est alors la probabilit√© P que l‚Äôaiguille croise une ligne de la trame du parquet ?\nLe contexte original √©tait dans celui d‚Äôun jeu √† deux joueurs: un joueur parie sur le fait que l‚Äôaiguille croise une ligne de la trame du parquet, l‚Äôautre sur le fait que l‚Äôaiguille ne croise pas une ligne de la trame du parquet. L‚Äôenjeu est alors de calculer la probabilit√© de succ√®s de chacun des joueurs, et de voir si le jeu est √©quilibr√© ou non.\nVoil√† bri√®vement la question que s‚Äôest pos√©e Buffon en 1733. La r√©ponse est donn√©e par la formule suivante, qui montre que le jeu qu‚Äôil propose n‚Äôest pas √©quilibr√©:\n\nP = \\frac{2}{\\pi} \\approx 0.6366 \\enspace.\n Une preuve de ce r√©sultat sera donn√©e ci-dessous.\nL‚Äôid√©e sous-jacente de Buffon est que si l‚Äôon r√©p√®te cette exp√©rience un grand nombre de fois, on peut approch√© la quantit√© P num√©riquement, par exemple en proposant un estimateur \\hat{P}_n qui compte la proportion de chevauchement apr√®s avoir fait n r√©p√©tition des lancers. Pour estimer \\pi, il ne restera donc plus qu‚Äô√† √©valuer \\frac{2}{\\hat{P}_n}.\nOn peut faire cette exp√©rience dans le monde r√©elle (c‚Äôest un peu long pour n grand!), mais on peut aussi utiliser une m√©thode num√©rique pour cela. Il s‚Äôagit alors de tirer al√©atoire la position du centre de l‚Äôaiguille, puis de tirer aussi de mani√®re al√©atoire son angle de chute. On teste √† la fin si l‚Äôaiguille croise une ligne de la trame du parquet ou non, et on recommence l‚Äôexp√©rience un grand nombre de fois.\nCette m√©thode est donn√©e ci-dessous, avec un exemple interactif g√©n√©r√© en Python.\n\nCode\nimport numpy as np\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\nrng = np.random.default_rng(44)\n\nn_samples = 200\nxmax = 14.499999\nxmin = -xmax\n\n\n# Create the needles\ncenters_x = rng.uniform(xmin, xmax, n_samples)\nangles = rng.uniform(0, 2 * np.pi, n_samples)\ncenters_y = rng.uniform(-2, 2, n_samples)\n\n# Compute the right borders of the needles\nborders_right = np.zeros((n_samples, 2))\nborders_right[:, 0] = centers_x + np.cos(angles) / 2\nborders_right[:, 1] = centers_y + np.sin(angles) / 2\n\n# Compute the left borders of the needles\nborders_left = np.zeros((n_samples, 2))\nborders_left[:, 0] = centers_x + np.cos(angles + np.pi) / 2\nborders_left[:, 1] = centers_y + np.sin(angles + np.pi) / 2\n\ncenters_x_round = np.round(centers_x)\noverlap = (borders_right[:, 0] - centers_x_round) * (\n    borders_left[:, 0] - centers_x_round\n) &lt; 0\noverlap = np.where(overlap, 1, 0)\nn_overlap = int(np.sum(overlap))\n\n\n# Check if the needles cross a line\nborders_red = np.empty((3 * n_overlap, 2), dtype=object)\nborders_red.fill(None)\nborders_red[::3, :] = borders_right[overlap == 1]\nborders_red[1::3, :] = borders_left[overlap == 1]\n\nborders_blue = np.empty((3 * (n_samples - n_overlap), 2), dtype=object)\nborders_blue.fill(None)\nborders_blue[::3, :] = borders_right[overlap == 0]\nborders_blue[1::3, :] = borders_left[overlap == 0]\n\noverlaps = np.empty((3 * n_samples), dtype=object)\noverlaps.fill(None)\noverlaps[::3] = overlap\noverlaps[1::3] = overlap\noverlaps[2::3] = overlap\n\nidx_red = np.cumsum(overlaps)\nidx_blue = np.cumsum(1 - overlaps)\n\n\n# Create subplots with 2 rows and 1 column with ratio x /  y  of 10\nfig = make_subplots(rows=2, cols=1, vertical_spacing=0.1, row_heights=[2, 1])\n\n# Use a loop to plot vertical lines equation \"y=c\" for integer values c in [-2, -1, 0, 1, 2]\nfor i in range(int(np.round(xmin)), int(np.round(xmax)) + 1):\n    fig.add_shape(\n        type=\"line\",\n        y0=-3,\n        x0=i,\n        y1=3,\n        x1=i,\n        line=dict(\n            color=\"black\",\n            width=2,\n        ),\n        row=1,\n        col=1,\n    )\n\ncolor = np.where(overlaps, 1.0, 0.0)\n\nn_samples_array = np.arange(1, n_samples + 1)\npi_estimate = 2 / (np.cumsum(overlap) / n_samples_array)\nt = n_samples\n\nfig.update_layout(\n    template=\"simple_white\",\n    xaxis=dict(range=[xmin, xmax], constrain=\"domain\", showgrid=False),\n    yaxis_scaleanchor=\"x\",\n    xaxis_visible=False,\n    yaxis_visible=False,\n)\n\nfor i in range(3, t):\n    fig.add_trace(\n        go.Scatter(\n            x=borders_red[: idx_red[3 * i] + 1, 0],\n            y=borders_red[: idx_red[3 * i] + 1, 1],\n            mode=\"lines\",\n            line=dict(width=2),\n            marker=dict(color=\"red\"),\n            name=\"Avec intersection\",\n            visible=False,\n        ),\n        row=1,\n        col=1,\n    )\n    fig.add_trace(\n        go.Scatter(\n            x=borders_blue[: idx_blue[3 * i] + 1, 0],\n            y=borders_blue[: idx_blue[3 * i] + 1, 1],\n            mode=\"lines\",\n            line=dict(width=2),\n            marker=dict(color=\"darkblue\"),\n            name=\"Sans intersection\",\n            visible=False,\n        ),\n        row=1,\n        col=1,\n    )\n\n    fig.add_trace(\n        go.Scatter(\n            x=n_samples_array[:i],\n            y=pi_estimate[:i],\n            mode=\"lines\",\n            line=dict(width=1),\n            marker=dict(color=\"red\"),\n            showlegend=False,\n            visible=False,\n        ),\n        row=2,\n        col=1,\n    )\n\nfig.add_annotation(\n    dict(\n        x=1.01,\n        y=0.14,\n        xref=\"paper\",\n        yref=\"paper\",\n        text=\"Estimation de pi\",\n        showarrow=False,\n        font=dict(color=\"red\"),\n    )\n)\n\nfig.add_annotation(\n    dict(x=-0.04, y=0.19, xref=\"paper\", yref=\"paper\", text=\"pi\", showarrow=False)\n)\n\nfig.update_xaxes(title_text=\"Nombre d'aiguilles tir√©es\", row=2, col=1)\n\nfig.update_layout(\n    template=\"none\",\n    xaxis2=dict(showgrid=True, zeroline=True, zerolinewidth=1, range=[0, n_samples]),\n    yaxis2=dict(showgrid=True, zeroline=True, zerolinewidth=1, range=[0, 6]),\n)\n# plot a dash line at y=pi\nfig.add_shape(\n    type=\"line\",\n    y0=np.pi,\n    x0=0,\n    y1=np.pi,\n    x1=n_samples,\n    line=dict(\n        color=\"black\",\n        width=1,\n        dash=\"dashdot\",\n    ),\n    row=2,\n    col=1,\n)\n\n\nfig.data[10 * 3].visible = True\nfig.data[10 * 3 + 1].visible = True\nfig.data[10 * 3 + 2].visible = True\n\n\nsteps = []\nfor i in range(len(fig.data) // 3):\n    step = dict(\n        label=str(i + 4),\n        method=\"update\",\n        args=[\n            {\"visible\": [False] * len(fig.data)},\n            {\n                \"title\": \"Estimation avec \"\n                + str(i + 4)\n                + f\" aiguilles: pi = {pi_estimate[i]:.4f}\"\n            },\n        ],\n    )\n    step[\"args\"][0][\"visible\"][3 * i] = True\n    step[\"args\"][0][\"visible\"][3 * i + 1] = True\n    step[\"args\"][0][\"visible\"][3 * i + 2] = True\n\n    steps.append(step)\n\nslider = dict(\n    active=0,\n    currentvalue={\"prefix\": \"Nombre d'aiguilles: \"},\n    pad={\"t\": 50},\n    y=-0.32,\n    steps=steps,\n)\n\nfig.update_layout(legend=dict(x=0.5, y=0.31, xanchor='center', yanchor='bottom'))\nfig.update_layout(sliders=[slider])\nfig.show()\n\n\n\n\n                                                \n\n\n\nOn va fournir ici le calcul de la probabilit√© P. Pour cela on aura besoin de quelques √©l√©ments d√©crits dans le dessin ci-dessous.\n\nx : distance entre le centre de l‚Äôaiguille et la ligne de la trame du parquet la plus proche\n\\theta : angle entre l‚Äôaiguille et la ligne de la trame du parquet la plus proche\n1 : longueur de l‚Äôaiguille (et donc la demi longueur est \\frac{1}{2})\n\\frac{1}{2}\\sin(\\theta) : distance entre l‚Äôextr√©mit√© de l‚Äôaiguille et la ligne de la trame du parquet la plus proche\n\n\n\n\n\n\n\n\n\nSans croisement\n\n\n\n\n\n\n\nAvec croisement\n\n\n\n\n\n\nFigure¬†1: Configuration sans croisement (√† gauche) et avec croisement (√† droite) de l‚Äôaiguille avec une ligne de la trame du parquet.\n\n\n\nAvec les √©l√©ments ci-dessus, on voit qu‚Äôil y a chevauchement si et seulement si: \\frac{1}{2}\\sin(\\theta) \\geq x.\nMaintenant par des arguments de sym√©trie on voit qu‚Äôon peut se restreindre √† \\theta \\in [0, \\frac{\\pi}{2}], et √† x \\in [0, \\frac{1}{2}]. Les lois de g√©n√©rations des variables al√©atoires X et \\Theta sont les suivantes:\n\nX \\sim \\mathcal{U}([0, \\frac{1}{2}]), de densit√© f_X(x) = 2 {1\\hspace{-3.8pt} 1}_{[0, \\frac{1}{2}]}(x)\n\\Theta \\sim \\mathcal{U}([0, \\frac{\\pi}{2}]) de densit√© f_\\Theta(\\theta) = \\frac{2}{\\pi} {1\\hspace{-3.8pt} 1}_{[0, \\frac{\\pi}{2}]}(\\theta)\n\nDe plus on suppose que X et \\Theta sont ind√©pendantes.\nMaintenant pour calculer la probabilit√© P on proc√®de comme suit: \n\\begin{align*}\nP\n& = \\mathbb{P}\\left(\\frac{1}{2}\\sin(\\Theta) \\geq X\\right) \\\\\n& = \\int_{\\mathbb{R}^2} {1\\hspace{-3.8pt} 1}_{\\{\\frac{1}{2}\\sin(\\theta) \\geq x\\}} f_{\\Theta}(\\theta) f_X(x) d\\theta dx  \\quad (\\text{par ind√©pendance})\\\\\n& = \\int_{\\mathbb{R}} \\int_{\\mathbb{R}}\n{1\\hspace{-3.8pt} 1}_{\\{\\frac{1}{2}\\sin(\\theta) \\geq x\\}} \\frac{2}{\\pi} {1\\hspace{-3.8pt} 1}_{[0, \\frac{\\pi}{2}]}(\\theta) \\cdot 2 {1\\hspace{-3.8pt} 1}_{[0, \\frac{1}{2}]}(x) d\\theta dx \\\\\n& = \\int_{0}^{\\frac{\\pi}{2}} \\int_{0}^{\\frac{1}{2}\\sin(\\theta)} \\frac{4}{\\pi} dx  d\\theta \\\\\n& = \\frac{4}{\\pi} \\int_{0}^{\\frac{\\pi}{2}} {\\frac{1}{2}\\sin(\\theta)}  d\\theta\\\\  \n& = \\frac{2}{\\pi} \\Big[ -\\cos(\\theta)\\Big]_{0}^{\\frac{\\pi}{2}} \\\\\n& = \\frac{2}{\\pi} \\enspace.\n\\end{align*}\n\n\n\n\n\n\n\nExercice: rendre le jeu √©quilibr√©?\n\n\n\nEn reprenant le m√™me type de raisonnement que ci-dessus, trouver la distance entre les lattes du parquet qui rend le jeu √©quilibrer entre les deux joueurs introduit par Buffon (l‚Äôun pariant sur le fait que l‚Äôaiguille croise une ligne de la trame du parquet, l‚Äôautre pariant sur le fait que l‚Äôaiguille ne croise pas une ligne de la trame du parquet).",
    "crumbs": [
      "Cours",
      "Perspectives historiques"
    ]
  },
  {
    "objectID": "Courses/perspective_historique.html#m√©thode-de-monte-carlo",
    "href": "Courses/perspective_historique.html#m√©thode-de-monte-carlo",
    "title": "Perspectives historiques",
    "section": "M√©thode de Monte-Carlo",
    "text": "M√©thode de Monte-Carlo\nLa m√©thode de Monte-Carlo, est une m√©thode de calcul num√©rique qui consiste √† utiliser des nombres al√©atoires pour r√©soudre des probl√®mes d√©terministes. Elle est utilis√©e dans de nombreux domaines, comme la physique, la chimie, la biologie, la finance, ou encore l‚Äôapprentissage automatique. Cette m√©thode bas√©e sur la loi des grands nombres a √©t√© mis au point √† Los Alamos, dans le cadre du projet Manhattan (dont l‚Äôobjectif √©tait le d√©veloppement du nucl√©aire civil et militaire) par un groupe de scientifiques dont les plus connus sont: John von Neumann2, Nicholas Metropolis3 ou encore Stanis≈Çaw Ulam4\n2¬†John von Neumann: (1903-1957) math√©maticien et physicien am√©ricano-hongrois, un des p√®res de l‚Äôinformatique. 3¬†Nicholas Metropolis: (1915-1999), physicien gr√©co-am√©ricain, est des initiateurs de la m√©thode de Monte Carlo et du recuit simul√© 4¬†Stanis≈Çaw Ulam: (1909-1984) Dans le cadre du projet Manhattan, il s‚Äôagissait de calculer des int√©grales de mani√®re num√©rique pour mod√©liser l‚Äô√©volution de particules, en utilisant des nombres al√©atoires.\nEckhardt (1987) donne un bref aper√ßu historique, et mentionne les premi√®res description de la m√©thode du rejet et de la m√©thode de l‚Äôinversion dans des lettres entre Von Neumann et Ulam datant de 1947. Ulam aurait une l‚Äôid√©e d‚Äôutiliser de telles m√©thodes pour r√©soudre le jeu du solitaire lors d‚Äôun s√©jour √† l‚Äôh√¥pital en 1946, et √©viter ainsi de faire des calculs combinatoires fastidieux. Rapidement, la possibilit√© d‚Äôappliquer cette approche pour des calculs en physique math√©matiques (diffusion des neutrons notamment) lui serait apparue prometteuse. Le d√©veloppement de l‚Äôinformatique naissante allait permettre une mise en oeuvre pratique de ces id√©es, et c‚Äôest ainsi que la m√©thode de Monte-Carlo est n√©e. Le nom Monte-Carlo est lui venu du besoin de confidentialit√© du projet, et provient du nom de la ville de Monte-Carlo, connue pour ses jeux de hasard, o√π l‚Äôoncle de Stanis≈Çaw Ulam aimait se rendre pour assouvir sa soif de jeu. Ce serait N. Metropolis qui aurait propos√© ce nom (cf. Metropolis 1987):\n\nEckhardt, R. 1987. ¬´¬†Stan Ulam, John Von Neumann, and the Monte Carlo Method¬†¬ª. Los Alamos Science, n·µí 15: 131‚Äë37.\n\nMetropolis, Nicholas. 1987. ¬´¬†The beginning of the Monte Carlo method¬†¬ª. Los Alamos Science, n·µí 15: 125‚Äë30.\nIt was at that time that I suggested an obvious name for the statistical method‚Äîa suggestion not unrelated to the fact that Stan had an uncle who would borrow money from relatives because he ‚Äújust had to go to Monte Carlo‚Äù.",
    "crumbs": [
      "Cours",
      "Perspectives historiques"
    ]
  },
  {
    "objectID": "Courses/perspective_historique.html#autres-m√©thodes-stochastiques-populaires",
    "href": "Courses/perspective_historique.html#autres-m√©thodes-stochastiques-populaires",
    "title": "Perspectives historiques",
    "section": "Autres m√©thodes stochastiques populaires",
    "text": "Autres m√©thodes stochastiques populaires\n\nM√©thode d‚ÄôHasting-Metropolis\nL‚Äôalgorithme de Hasting-Metropolis est une m√©thode MCMC (üá¨üáß: Monte Carlo Markov Chains) dont le but est d‚Äôobtenir un √©chantillonnage al√©atoire d‚Äôune distribution de probabilit√© quand l‚Äô√©chantillonnage direct en est difficile (en particulier en grande dimension)\nUn avantage est qu‚Äôil ne requiert la connaissance de loi de densit√© qu‚Äô√† constante multiplicative pr√®s.\n\n\nüá¨üáß: Recuit simul√©\nLe recuit simul√© est une m√©thode (empirique) d‚Äôoptimisation, inspir√©e d‚Äôun processus, le recuit, utilis√© en m√©tallurgie. On alterne dans cette derni√®re des cycles de refroidissement lent et de r√©chauffage (recuit) qui ont pour effet de minimiser l‚Äô√©nergie du mat√©riau. Cette m√©thode est transpos√©e en optimisation pour trouver les extrema d‚Äôune fonction.",
    "crumbs": [
      "Cours",
      "Perspectives historiques"
    ]
  },
  {
    "objectID": "Courses/monte_carlo.html",
    "href": "Courses/monte_carlo.html",
    "title": "M√©thode de Monte Carlo",
    "section": "",
    "text": "{{ % variance % covariance }}\nDe mani√®re g√©n√©rale, ce qu‚Äôon appelle une m√©thode de Monte-Carlo est une technique visant √† calculer une quantit√© d√©terministe par le biais d‚Äôun proc√©d√© al√©atoire. Une application tr√®s usuelle des m√©thodes Monte-Carlo est l‚Äôapproximation num√©rique d‚Äôint√©grales ou d‚Äôesp√©rance, c‚Äôest ce que nous allons beaucoup d√©velopper dans ce chapitre.\nIl existe √©galement de nombreuses m√©thodes pour l‚Äôint√©gration num√©rique d√©terministe (m√©thodes des rectangles, trap√®zes,‚Ä¶). Ces m√©thodes sont tr√®s rapides et efficaces en petite dimension, mais elles ne le sont plus du tout pour des dimensions plus grandes ou pour l‚Äôint√©gration de fonction peu r√©guli√®res. Ainsi une int√©gration par Monte Carlo pour le cas de la grande dimension sera bien plus profitable.",
    "crumbs": [
      "Cours",
      "M√©thode de Monte Carlo"
    ]
  },
  {
    "objectID": "Courses/monte_carlo.html#principe-de-la-m√©thode",
    "href": "Courses/monte_carlo.html#principe-de-la-m√©thode",
    "title": "M√©thode de Monte Carlo",
    "section": "Principe de la m√©thode",
    "text": "Principe de la m√©thode\n\nL‚Äôid√©e de la M√©thode de Monte Carlo est d‚Äôexprimer l‚Äôint√©grale qu‚Äôon cherche √† approcher comme l‚Äôesp√©rance d‚Äôune variable al√©atoire, puis d‚Äôapprocher cette esp√©rance par une moyenne empirique. Si on veut calculer une int√©grale de la forme \n    I=\\int g(x)f(x)dx,\n on la r√©√©crit comme \n    I=\\mathbb E[g(X)],\n o√π X est une variable al√©atoire de densit√© f. On suppose √©videmment que g(X) est bien int√©grable, puis on approche l‚Äôesp√©rance par la moyenne empirique \n    I_n=\\frac{g(X_1)+g(X_2)+\\cdots+g(X_n)}{n}.\n Il y a donc deux √©tapes : traduction sous forme d‚Äôesp√©rance puis calcul de la moyenne empirique. Si l‚Äôexpression √† calculer est d√©j√† sous forme d‚Äôesp√©rance, il y a juste le calcul de la moyenne empirique √† faire. Ce calcul se fait √† partir de simulation de la variable al√©atoire X sous-jacente.\n\nExemple 1 On peut approcher l‚Äôint√©grale\n\n    I=\\int_0^1 g(x)dx \\simeq  \\sum_{i=0}^nw_ig(x_i),\n o√π les w_i sont des nombres entre 0 et 1 dont la somme vaut 1, et les x_i sont des points de l‚Äôintervalle [0,1]. Par exemple, pour la m√©thode des trap√®zes, on prend w_0=w_n=\\frac{1}{2n} et w_i=\\frac 1 n sinon et les x_i=\\frac i  n sont r√©guli√®rement r√©partis. Pour une m√©thode de Monte Carlo, on interpr√®te l‚Äôint√©grale comme \\mathbb E[g(X)] o√π X est une variable al√©atoire de loi uniforme sur [0,1]. Alors les w_i valent \\frac{1}{n+1} et les x_i sont tir√©s selon la loi uniforme sur [0,1]. Contrairement aux m√©thodes d√©terministes, la valeur de l‚Äôapproximation change si on relance le calcul puisque les tirages sont al√©atoires.\n\nLa m√©thode de Monte Carlo est tr√®s facile √† programmer si on dispose d‚Äôun simulateur de la loi de X, et n‚Äôimpose aucune r√©gularit√© sur g √† part de la mesurabilit√©.",
    "crumbs": [
      "Cours",
      "M√©thode de Monte Carlo"
    ]
  },
  {
    "objectID": "Courses/monte_carlo.html#th√©or√®mes-de-convergence",
    "href": "Courses/monte_carlo.html#th√©or√®mes-de-convergence",
    "title": "M√©thode de Monte Carlo",
    "section": "Th√©or√®mes de convergence",
    "text": "Th√©or√®mes de convergence\nOn s‚Äôint√©resse maintenant √† la convergence de cet algorithme : sous quelles conditions est-ce que I_n converge vers I, en quel sens, et √† la vitesse de cette convergence : combien de tirages faut-il faire pour atteindre une pr√©cision souhait√©e.\n\nConvergence de la m√©thode\nCommen√ßons par √©noncer des conditions sous lesquelles la m√©thode converge.\n\nProposition 1 Soit (X_n)_{n\\in\\N} une suite de variables al√©atoires ind√©pendantes et de m√™me loi que X, et g une fonction mesurable telle que g(X) est int√©grable. Alors la variable al√©atoire \n    I_n=\\frac{g(X_1)+g(X_2)+\\cdots+g(X_n)}{n}\n converge presque presque s√ªrement vers I=\\mathbb E[g(X)] lorsque n tend vers l‚Äôinfini.\n\n\nPreuve. Comme g(X) est int√©grable, et que les variables sont ind√©pendantes et de m√™me loi, il s‚Äôagit d‚Äôune application directe de la loi forte des grands nombres.\n\nEn particulier, si on sait simuler X, on peut en faire des tirages X_1, , X_n et ainsi calculer I_n qui est une valeur approch√©e de I pour n assez grand.\n\nExemple 2 Par exemple, si l‚Äôon r√©p√®te un tr√®s grand nombre de fois n une exp√©rience al√©atoire et que l‚Äôon s‚Äôint√©resse √† la r√©alisation de l‚Äô√©v√©nement A alors la loi des grands nombres nous garantit que : \nf_n(A)=\\frac1n \\times \\hbox{ Nbre de fois o√π } A \\hbox{ s'est r√©alis√©} \\xrightarrow[n \\to \\infty]{ps}\\mathbb P(A).\n car f_n(A)=(1/n) \\sum_{i=1}^n X_i o√π X_i est une v.a. de Bernoulli de param√®tre p=\\mathbb P(A)=\\mathbb E[X]. Autrement dit, la fr√©quence observ√©e f_n(A) de la r√©alisation de l‚Äô√©v√©nement A au cours d‚Äôun grand nombre de r√©p√©titions de l‚Äôexp√©rience se rapproche de la probabilit√© P(A).\n\n\nExemple 3 (Le jeu des trois portes ou paradoxe de Monty Hall) Un jeu t√©l√©vis√© se d√©roule face √† trois portes identiques. On a plac√© une voiture derri√®re l‚Äôune des portes, et une ch√®vre derri√®re chacune des deux autres. Le candidat est plac√© devant les trois portes, il en choisit une au hasard et se place devant.\nLa pr√©sentatrice (qui sait o√π se trouve la voiture) ouvre alors l‚Äôune des deux autres portes, derri√®re laquelle il y a une ch√®vre, puis elle demande au candidat : ‚ÄúVoulez-vous modifier votre choix ?‚Äù\nLe candidat a alors deux possibilit√©s :\n\nsoit il ouvre la porte qu‚Äôil avait choisie en premier ; (il conserve son premier choix)\nsoit il ouvre une autre porte (celle qui n‚Äôa pas √©t√© ouverte par la pr√©sentatrice).\n\nQuelle est la meilleure strat√©gie pour le candidat ? Garder son premier choix ou changer de porte ?\nUn raisonnement probabiliste peut fournir la solution. Mais vous pouvez vous aider en faisant des exp√©riences pour faire une conjecture ! On peut par exemple lancer le programme ci-dessous en demandant de reproduire dix mille fois l‚Äôexp√©rience (n= 10 000).\n\nimport numpy as np\n\nportes = np.array([\"chevre\", \"chevre\", \"voiture\"])\nNbSimulations = 10000\n\ngagne_avec_change = 0\ngagne_avec_conserve = 0\n\nfor _ in range(NbSimulations):\n    porte_choisie = np.random.choice(portes)\n    if porte_choisie == \"voiture\":\n        gagne_avec_conserve += 1\n    else:\n        gagne_avec_change += 1\n\nprint(f\"Gagne en changeant : {gagne_avec_change / NbSimulations * 100:.3f} %\")\nprint(f\"Gagne sans changer : {gagne_avec_conserve / NbSimulations * 100:.3f} %\")\n\nGagne en changeant : 67.150 %\nGagne sans changer : 32.850 %\n\n\nOn peut alors conjecturer que la probabilit√© de gagner en changeant de porte est √©gale √† 2/3 alors que si le candidat reste sur son premier choix la probabilit√© de gagner ne sera que de 1/3.\n\n\n\nPr√©cision de la m√©thode\nOn souhaiterait maintenant savoir quand on peut consid√©rer que n est assez grand pour que l‚Äôapproximation soit satisfaisante.\n\nProposition 2 Si g(X) est de carr√© int√©grable, alors un intervalle de confiance asymptotique de niveau de confiance 1-\\alpha pour l‚Äôint√©grale I est \n    \\left(I_n-F^{-1}_{\\mathcal{N}(0,1)}\\big(\\frac{\\alpha}{2}\\big)\\sqrt{\\frac{S_n^2}{n}}\\leq I\\leq I_n+F^{-1}_{\\mathcal{N}(0,1)}\\big(\\frac{\\alpha}{2}\\big)\\sqrt{\\frac{S_n^2}{n}}\\right),\n o√π F^{-1}_{\\mathcal{N}(0,1)} est la fonction quantile de la loi normale centr√©e r√©duite et \n    S_n^2=\\frac{1}{n-1}\\sum_{i=1}^n\\big(g(X_i)-I_n\\big)^2\n est l‚Äôestimateur sans biais de la variance empirique.\n\n\nPreuve. Comme g(X) est de carr√© int√©grable, et que les X_i sont iid, on peut appliquer le th√©or√®me central limite √† la suite (g(X_i)), ce qui donne \n    \\frac{\\frac{1}{n}\\sum_{i=1}^n g(X_i)-\\mathbb E[g(X)]}{\\sqrt{\\frac{\\mathop{\\mathrm{Var}}(g(X))}{n}}}\\xrightarrow[n\\to\\infty]{\\mathcal L}\\mathcal N(0,1),\n ce qui se traduit dans notre contexte par \n    \\frac{I_n-I}{\\sqrt{\\frac{\\mathop{\\mathrm{Var}}(g(X))}{n}}}\\xrightarrow[n\\to\\infty]{\\mathcal L}\\mathcal N(0,1).\n En g√©n√©ral, on ne conna√Æt pas non plus \\mathop{\\mathrm{Var}}(g(X)), donc on la remplace par un estimateur de la variance S_n^2=\\frac{1}{n-1}\\sum_{i=1}^n(g(X_i)-I_n)^2. D‚Äôapr√®s la loi des grands nombres, cet estimateur converge ps, donc en probabilit√© (donc en loi) vers \\mathop{\\mathrm{Var}}(g(X)) qui est une constante. D‚Äôapr√®s le lemme de Slutsky, le couple (\\frac{I_n-I}{\\sqrt{n\\mathop{\\mathrm{Var}}(g(X))}},S^2_n) converge en loi vers le couple (\\mathcal N(0,1), \\delta_{\\mathop{\\mathrm{Var}}(g(X))}). En particulier, on a encore la convergence en loi du produit \n    \\frac{I_n-I}{\\sqrt{\\frac{S^2_n}{n}}}=\\frac{I_n-I}{\\sqrt{\\frac{\\mathop{\\mathrm{Var}}(g(X))}{n}}}\\times \\frac{\\sqrt{\\mathop{\\mathrm{Var}}(g(X))}}{\\sqrt{S^2_n}}\\xrightarrow[n\\to\\infty]{\\mathcal L}\\mathcal N(0,1) \\times 1.\n On peut en d√©duire facilement la construction de l‚Äôintervalle de confiance asymptotique de l‚Äô√©nonc√©.\n\nCe r√©sultat a deux cons√©quences importantes. D‚Äôune part, la vitesse de convergence est en n^{-1/2}, ce qui est relativement lent, mais ind√©pendant √† la fois de la dimension de X et de la r√©gularit√© de g. D‚Äôautre part, on peut calculer S_n^2 donc estimer l‚Äôerreur commise sans tirages al√©atoires suppl√©mentaires, avec le m√™me √©chantillon qui sert pour estimer I. Il est fortement recommand√© de toujours faire une estimation de la pr√©cision en m√™me temps que l‚Äôestimation Monte Carlo de l‚Äôesp√©rance.\n\nExemple 4 En finance, certains contrats permettent de vendre un actif √† une date future N fix√©e par le contrat s‚Äôil d√©passe √† ce moment l√† une certaine valeur cible K √©galement fix√©e dans la contrat. Consid√©rons un mod√®le simplifi√© pour le cours de l‚Äôactif (S_n)_{n\\geq 1} : pour n\\geq 1, on pose S_n=\\sum_{i=1}^nX_i o√π les X_i sont des variables al√©atoires ind√©pendantes de loi normale centr√©e r√©duite. Le gain moyen de la d√©tentrice ou du d√©tenteur du contrat d√©crit ci-dessus est donc \\mathbb E[\\max(S_N-K,0)]. Estimons cette quantit√© par la m√©thode de Monte Carlo.\n\nimport numpy as np\n\nn = 100000\nN = 60\nI = []\n\nfor i in range(n):\n    X = np.random.normal(0, 1, N)\n    S = max(np.sum(X) - 1, 0)\n    I.append(S)\n\nI = np.array(I)\nm = np.mean(I)\ns = np.var(I)\n\nprint(f\"[{m - 1.96 * np.sqrt(s / n):.3f}, {m:.3f}, {m + 1.96 * np.sqrt(s / n):.3f}]\")\n\n[2.623, 2.649, 2.675]",
    "crumbs": [
      "Cours",
      "M√©thode de Monte Carlo"
    ]
  },
  {
    "objectID": "Courses/monte_carlo.html#r√©duction-de-variance",
    "href": "Courses/monte_carlo.html#r√©duction-de-variance",
    "title": "M√©thode de Monte Carlo",
    "section": "R√©duction de variance",
    "text": "R√©duction de variance\nLa proposition Proposition¬†2 donne deux leviers pour augmenter la pr√©cision de l‚Äôestimation I_n de I :\n\naugmenter le nombre n de tirages,\ndiminuer la variance \\mathop{\\mathrm{Var}}(g(X)).\n\nSi le premier levier est facilement praticable pour des cas simples, il n‚Äôest pas envisageable lorsque une simulation de g(X) est co√ªteuse en temps de calcul ou lorsque la variance est tr√®s grande, ce qui arrive couramment. Nous allons examiner le deuxi√®me levier dans cette partie, c‚Äôest ce qu‚Äôon appelle les m√©thodes de r√©duction de variance.\nRegardons d‚Äôabord un exemple pour bien comprendre l‚Äôint√©r√™t de ces m√©thodes.\n\nExemple 5 Revenons sur l‚Äôexemple Exemple¬†2 o√π on cherche √† estimer p=\\mathbb P(A) √† partir de tirages de loi de Bernoulli. Avec n tirages, on a I_n\\simeq p + \\sqrt{\\frac{p(1-p)}{n}}Z avec Z de loi normale centr√©e r√©duite. La valeur de p est inconnue, mais on peut majorer p(1-p) par 1/4. Si on veut faire une erreur de l‚Äôordre 0.01 sur p, il convient de choisir n=\\frac{1}{4\\times 0.01^2}=0.25\\cdot 10^{4}=2500.\nSi p est proche de 0.5, une erreur de 10^{-2} peut √™tre acceptable. Mais si on cherche √† approcher une probabilit√© tr√®s petite, par exemple p=10^{-5}, il faudra une pr√©cision d‚Äôau moins 10^{-6}, et donc prendre n=0.25\\cdot 10^{12} tirages. On voit bien appara√Ætre les limites de la m√©thode directe, et l‚Äôint√©r√™t de toujours calculer l‚Äôordre de grandeur de l‚Äôerreur commise.\n\nIl existe de nombreuses techniques de r√©duction de variance pour am√©liorer la vitesse de convergence. Malheureusement, aucune ne marche de fa√ßon automatique, il faut les choisir et les adapter au cas par cas. Il s‚Äôagit essentiellement d‚Äôexploiter d‚Äôune fa√ßon ou d‚Äôune autre l‚Äôid√©e que l‚Äôint√©grale I peut s‚Äô√©crire de nombreuses fa√ßons diff√©rentes comme une esp√©rance, et de choisir l‚Äôexpression de variance minimale.\n\nVariable de contr√¥le\nUne premi√®re fa√ßon tr√®s simple de d√©composer I est la suivante \n    I=\\mathbb{E}[g(X)]=\\mathbb{E}[g(X)-h(X)]+\\mathbb{E}[h(X)].\n Cette m√©thode est int√©ressante si on sait calculer explicitement \\mathbb{E}[h(X)] et si la variance diminue : \\mathop{\\mathrm{Var}}\\big(g(X)-h(X)\\big)&lt;\\mathop{\\mathrm{Var}}\\big(g(X)\\big). Id√©alement, on voudrait que la variance de g(X)-h(X) soit tr√®s petite. Or rappelons qu‚Äôune variable al√©atoire est de variance nulle si et seulement si elle est constante. Pour mettre en oeuvre cette m√©thode, on va donc chercher une fonction h qui soit une bonne approximation de g sur l‚Äôintervalle d‚Äôint√©gration pour que la diff√©rence g-h soit presque constante.\n\nExemple 6 Supposons que l‚Äôon veuille calculer I=\\int_0^1 e^{x^2} dx √† l‚Äôaide de la loi uniforme sur [0,1]. Comme au voisinage de 0, on a e^{x^2}\\simeq 1+x^2, on propose de prendre h(x)=1+x^2. On a alors \n    \\mathbb E[h(X)]=\\int_0^11+x^2dx=\\big[x+\\frac{x^3}{3}\\big]_0^1=\\frac{4}{3}.\n On peut donc approcher I par \n    I_n^{c}=\\frac{1}{n}\\sum_{i=1}^n\\big(e^{X_i^2}-1-X_i^2\\big)+\\frac{4}{3},\n o√π les X_i sont iid de loi uniforme sur [0,1]. Regardons ce que √ßa donne num√©riquement.\n\nimport numpy as np\n\nn = 10000\nX = np.random.uniform(0, 1, n)\n\n# Direct method\ngX = np.exp(X**2)\nm1 = np.mean(gX)\ns1 = np.var(gX)\n\nprint(f\"Direct method confidence interval: [{m1 - 1.96 * np.sqrt(s1 / n):.3f}, {m1:.3f}, {m1 + 1.96 * np.sqrt(s1 / n):.3f}]\")\nprint(f\"Variance {s1:.3f}\")\n\nDirect method confidence interval: [1.458, 1.467, 1.476]\nVariance 0.228\n\n\n\n# Control variable\nghX = np.exp(X**2) - 1 - X**2\nm2 = np.mean(ghX) + 4/3\ns2 = np.var(ghX)\n\nprint(f\"Control variable confidence interval: [{m2 - 1.96 * np.sqrt(s2 / n):.3f}, {m2:.3f}, {m2 + 1.96 * np.sqrt(s2 / n):.3f}]\")\nprint(f\"Variance {s2:.3f}\")\n\nControl variable confidence interval: [1.461, 1.465, 1.468]\nVariance 0.034\n\n\nOn constate que l‚Äôintervalle de confiance est plus √©troit donc la pr√©cision meilleure avec la variable de contr√¥le. On a divis√© la variance (empirique) par 6.7. Pour obtenir le m√™me gain en augmentant le nombre de simulations, il aurait fallu 6.7^2\\simeq45 fois plus de simulations.\n\n\n\nVariables antith√©tiques\nS‚Äôil existe une fonction h telle que X et h(X) ont la m√™me loi, on peut √©crire I comme \nI=\\mathbb{E}\\left[\\frac{g(X)+g(h(X))}{2}\\right].\n On a en fait d√©compos√© l‚Äôint√©grale I comme somme de deux esp√©rance. Notons que ces deux int√©grales font appara√Ætre la m√™me variable al√©atoire, donc on peut estimer les deux esp√©rances avec les . En fait on a introduit une certaine forme de d√©pendance puisqu‚Äôon utilise g(X_i) et g(h(X_i)) dans l‚Äôestimateur.\nExaminons l‚Äôeffet de cette d√©composition sur la variance.\n\nSupposons qu‚Äôil existe une fonction h telle que X et h(X) ont la m√™me loi de carr√© int√©grable. Si \\mathop{\\mathrm{Cov}}(g(X),g(h(X))\\leq 0 alors \n    \\mathop{\\mathrm{Var}}\\left(\\frac{g(X)+g(h(X))}{2}\\right) \\leq \\frac{\\mathop{\\mathrm{Var}}(g(X))}{2},\n\nAutrement dit la m√©thode des variables antith√©tiques permet de r√©duire la variance au moins de moiti√© si g(X) et g(h(X)) sont n√©gativement corr√©l√©es.\n\n\nPreuve. Calculons la variance V=\\mathop{\\mathrm{Var}}\\big(g(X)+g(h(X))\\big): \n\\begin{align*}\n  V &= \\mathop{\\mathrm{Var}}\\big(g(X)+g(h(X))\\big)\\\\\n    &=\\mathbb E[(g(X)-\\mathbb E[g(X)]+g(h(X))-\\mathbb E[g(h(X))])^2]\\\\\n    &= \\mathbb E[(g(X)-\\mathbb E[g(X)])^2]+\\mathbb E[(g(h(X))-\\mathbb E[g(h(X))])^2]\\\\\n    &\\quad + 2\\mathbb E[(g(X)-\\mathbb E[g(X)])(g(h(X))-\\mathbb E[g(h(X))])]\\\\\n    &= \\mathop{\\mathrm{Var}}(g(X))+\\mathop{\\mathrm{Var}}(g(h(X)) + 2 \\mathop{\\mathrm{Cov}}(g(X),g(h(X)).\n\\end{align*}\n\nEn particulier, si \\mathop{\\mathrm{Cov}}(g(X),g(h(X))\\leq 0, on a \n\\begin{align*}\n    \\mathop{\\mathrm{Var}}\\left(\\frac{g(X)+g(h(X))}{2}\\right)\n    & \\leq \\frac{ \\mathop{\\mathrm{Var}}(g(X))+\\mathop{\\mathrm{Var}}(g(h(X))}{4}\\\\\n    & =\\frac{\\mathop{\\mathrm{Var}}(g(X))}{2},\n\\end{align*}\n\ncar g(X) et g(h(X)) ont la m√™me loi donc la m√™me variance. Dans le cas g√©n√©ral, on a quand toujours 2\\mathop{\\mathrm{Cov}}(g(X),g(h(X))\\leq \\mathop{\\mathrm{Var}}(g(X))+\\mathop{\\mathrm{Var}}(g(h(X)) par l‚Äôin√©galit√© de Cauchy-Schwartz, ce qui implique \n\\begin{align*}\n    \\mathop{\\mathrm{Var}}\\left(\\frac{g(X)+g(h(X))}{2}\\right)\n    & \\leq 2\\frac{\\mathop{\\mathrm{Var}}(g(X))+\\mathop{\\mathrm{Var}}(g(h(X)))}{4}\\\\\n    & = \\mathop{\\mathrm{Var}}(g(X)),\n\\end{align*}\n\ndonc m√™me si on ne r√©duit pas effectivement la variance, on ne peut pas l‚Äôaugmenter par cette m√©thode. On peut faire des calculs plus pr√©cis en faisant intervenir la covariance de g(X) et g(h(X)) dans certains cas particuliers si on sp√©cifie les lois.\n\nCette m√©thode est particuli√®rement simple √† mettre en oeuvre pour les lois qui ont des propri√©t√©s de sym√©trie\n\nExemple 7 Revenons sur l‚Äôexemple Exemple¬†6 o√π on cherche √† approcher I=\\int_0^1 e^{x^2} dx √† l‚Äôaide de la loi uniforme sur [0,1]. Cette loi a une propri√©t√© de sym√©trie : X et 1-X ont la m√™me loi. On a donc une approximation de I de qualit√© identique ou meilleure en prenant \n    I_n^{a}=\\frac{1}{2n}\\sum_{i=1}^{n}\\big(e^{X_i^2}+e^{(1-X_i)^2}\\big).\n\nRegardons ce que cela donne num√©riquement.\n\nimport numpy as np\n\nn = 10000\nX = np.random.uniform(0, 1, n)\n\n# Direct method\ngX = np.exp(X**2)\nm1 = np.mean(gX)\ns1 = np.var(gX)\n\nprint(f\"Direct method confidence interval: [{m1 - 1.96 * np.sqrt(s1 / n):.3f}, {m1:.3f}, {m1 + 1.96 * np.sqrt(s1 / n):.3f}]\")\nprint(f\"Variance {s1:.3f}\")\n\nDirect method confidence interval: [1.464, 1.473, 1.483]\nVariance 0.229\n\n\n\n# Antithetic variable method\ngaX = (np.exp(X**2) + np.exp((1 - X)**2)) / 2\nm3 = np.mean(gaX)\ns3 = np.var(gaX)\n\nprint(f\"Antithetic variable method confidence interval:, [{m3 - 1.96 * np.sqrt(s3 / n):.3f}, {m3:.3f}, {m3 + 1.96 * np.sqrt(s3 / n):.3f}]\")\nprint(f\"Variance {s3:.3f}\")\n\nAntithetic variable method confidence interval:, [1.461, 1.464, 1.468]\nVariance 0.028\n\n\nOn constate que l‚Äôintervalle de confiance est plus √©troit donc la pr√©cision meilleure avec la variable antith√©tique. On a divis√© la variance (empirique) par 8. Pour obtenir le m√™me gain en augmentant le nombre de simulations, il aurait fallu 8^2=64 fois plus de simulations.\n\n\n\n√âchantillonnage pr√©f√©rentiel\nL‚Äôid√©e derri√®re l‚Äô√©chantillonnage pr√©f√©rentiel est similaire √† celle des variables de contr√¥le. Cette fois-ci, au lieu de retrancher et d‚Äôajouter une quantit√©, on va multiplier et diviser. L‚Äôinterpr√©tation en terme d‚Äôesp√©rance est cependant compl√®tement diff√©rente.\nSupposons que l‚Äôon sache simuler une variable al√©atoire Y de densit√© h. On peut alors √©crire I comme \n\\begin{align*}\nI&=\\mathbb E[g(X)]=\\int g(x)f(x)dx\n\\\\ &= \\int \\frac{g(x)f(x)}{h(x)}h(x)dx=\\mathbb{E}\\left[ \\frac{g(Y)f(Y)}{h(Y)}\\right].\n\\end{align*}\n\nOn a gagn√© quelque chose si \\mathop{\\mathrm{Var}}\\Big(\\frac{gf}{h}(Y)\\Big)&lt;\\mathop{\\mathrm{Var}}\\big(g(X)\\big).\nComme pour la m√©thode de la variable de contr√¥le, on souhaite que la nouvelle variable \\frac{g(Y)f(Y)}{h(Y)} soit de variance aussi petite que possible, donc proche d‚Äôune constante. Si on prend h(x)=\\frac{g(x)f(x)}{\\mathbb E[g(X)]} (qui est bien une densit√© si g est positive), alors la variance de \\frac{g(Y)f(Y)}{h(Y)} est nulle. √âvidemment, ce r√©sultat n‚Äôest pas utilisable en pratique puisqu‚Äôon cherche justement √† calculer \\mathbb E[g(X)]. Cependant, il guide la recherche d‚Äôune bonne fonction h vers une approximation de |gf| normalis√©e pour obtenir une densit√©.\n\nExemple 8 Revenons une fois de plus sur l‚Äôexemple Exemple¬†6 o√π on cherche √† approcher I=\\int_0^1 e^{x^2} dx √† l‚Äôaide de la loi uniforme sur [0,1]. On a donc f(x)=1 sur l‚Äôintervalle d‚Äôint√©gration, on cherche donc √† nouveau une approximation de g(x)=e^{x^2}. Utilisons encore l‚Äôapproximation g(x)\\simeq 1+x^2, ce qui nous conne comme densit√© candidate h(x)=\\frac{3}{4}(1+x^2). Regardons si on peut facilement simuler cette densit√©. Calculons sa fonction de r√©partition (sur [0,1]) \n    F(t)=\\int_0^t \\frac{3}{4}(1+x^2)=\\frac{3}{4}\\big[x+\\frac{x^3}{3}\\big]_0^1=\\frac{3}{4}(t+\\frac{t^3}{3}).\n Essayons de l‚Äôinverser \n    F(t)=u \\Leftrightarrow u = \\frac{4}{3}(t+\\frac{t^3}{3}) \\Leftrightarrow t^3+3t-4u=0\n\nOn trouve alors par la m√©thode de Cardan1 \n    F(t)=u \\Leftrightarrow t=(\\sqrt{1+4u^2}+2u)^{1/3}-(\\sqrt{1+4u^2}-2u)^{1/3}.\n\n1¬†voir par exemple la page Equation du troisi√®me degr√© / M√©thode de CardanRegardons ce que √ßa donne num√©riquement.\n\nimport numpy as np\n\nn = 10000\nX = np.random.uniform(0, 1, n)\n\n# Direct method\ngX = np.exp(X**2)\nm1 = np.mean(gX)\ns1 = np.var(gX)\n\nprint(f\"Direct method confidence interval: [{m1 - 1.96 * np.sqrt(s1 / n):.3f}, {m1:.3f}, {m1 + 1.96 * np.sqrt(s1 / n):.3f}]\")\nprint(f\"Variance {s1:.3f}\")\n\nDirect method confidence interval: [1.454, 1.463, 1.472]\nVariance 0.226\n\n\n\n# Importance sampling\nY = (2*X + np.sqrt(1 + 4*X**2))**(1/3) - (np.sqrt(1 + 4*X**2) - 2*X)**(1/3)\ngpX = np.exp(Y**2) / (3 * (1 + Y**2) / 4)\nm4 = np.mean(gpX)\ns4 = np.var(gpX)\n\nprint(f\"Importance sampling confidence interval: [{m4 - 1.96 * np.sqrt(s4 / n):.3f}, {m4:.3f}, {m4 + 1.96 * np.sqrt(s4 / n):.3f}]\")\nprint(f\"Variance {s4:.3f}\")\n\nImportance sampling confidence interval: [1.460, 1.463, 1.466]\nVariance 0.020\n\n\nOn constate que l‚Äôintervalle de confiance est plus √©troit donc la pr√©cision meilleure avec l‚Äô√©chantillonnage pr√©f√©rentiel. On a divis√© la variance (empirique) par environ 12. Pour obtenir le m√™me gain en augmentant le nombre de simulations, il aurait fallu 12^2=144 fois plus de simulations.\n\nIl existe bien d‚Äôautres m√©thodes de r√©duction de variance, comme par exemple la m√©thode de stratification tr√®s utilis√©e en th√©orie des sondages.",
    "crumbs": [
      "Cours",
      "M√©thode de Monte Carlo"
    ]
  },
  {
    "objectID": "Courses/marche_aleatoire.html",
    "href": "Courses/marche_aleatoire.html",
    "title": "Marches al√©atoires",
    "section": "",
    "text": "{{ % variance % covariance\n}}\nL‚Äôobjet de ce chapitre est de faire une premi√®re introduction √† un exemple int√©ressant de processus stochastique, soit la marche al√©atoire simple sym√©trique, et ce avec un minimum de formalisme.\nDans la partie Section¬†1, on d√©finit la marche al√©atoire simple sym√©trique, et on √©tudie l‚Äôallure de ses trajectoires. La partie Section¬†2 est d√©di√©e √† son comportement en temps long. La partie Section¬†3 pr√©sente une premi√®re application vers les math√©matiques financi√®res avec l‚Äô√©tude d‚Äôun probl√®me de ruine.",
    "crumbs": [
      "Cours",
      "Marches al√©atoires"
    ]
  },
  {
    "objectID": "Courses/marche_aleatoire.html#sec-chap03-traj",
    "href": "Courses/marche_aleatoire.html#sec-chap03-traj",
    "title": "Marches al√©atoires",
    "section": "Trajectoires de la marche al√©atoire",
    "text": "Trajectoires de la marche al√©atoire\nUn processus stochastique est une suite de variables al√©atoires index√©e par le temps. Il permet de mod√©liser l‚Äô√©volution temporelle d‚Äôun ph√©nom√®ne comme la richesse d‚Äôune joueuse ou la valeur d‚Äôun portefeuille d‚Äôactions par exemple. La marche al√©atoire simple sym√©trique est le mod√®le le plus simple de processus √† temps discret.\nDans toute la suite, on se place sur un espace probabilis√© (\\Omega, \\mathcal{F}, \\mathbb{P}).\n\nD√©finition\nCommen√ßons par d√©finir la marche al√©atoire simple sym√©trique.\n\nD√©finition 1 Une marche al√©atoire simple sym√©trique sur \\Z est une suite (S_n)_{n\\in\\mathbb{N}} de variables al√©atoires telles que\n\nS_0=x\\in\\Z est d√©terministe,\nS_{n+1}=S_n+X_{n+1} pour tout n\\in\\mathbb{N},\n\no√π (X_n)_{n\\geq 1} est une suite de variables al√©atoires ind√©pendantes et de m√™me loi de Rademacher de param√®tre \\frac 1 2 : \n\\mathbb{P}(X_1=-1)=\\mathbb{P}(X_1=1)=\\frac{1}{2}.\n\n\nOn parle de marche car ce processus peut repr√©senter la position d‚Äôune personne qui se d√©place en ligne droite par pas d‚Äôune unit√© avec √©quiprobabilit√© d‚Äôaller vers l‚Äôavant (+1) ou vers l‚Äôarri√®re (-1). Cette marche est dite simple car on ne fait que des pas d‚Äôamplitude 1 et sym√©trique car il y a la m√™me probabilit√© de tirer 1 et -1 pour chaque incr√©ment.\nCe processus peut √©galement servir √† mod√©liser la richesse d‚Äôune joueuse qui joue √† pile ou face avec une pi√®ce √©quilibr√©e. Partant d‚Äôune richesse initiale S_0=x&gt;0, √† chaque pas de temps elle perd ou gagne 1 en fonction du tirage obtenu. Les variables X_n repr√©sentent alors le gain du n-√®me jeu, et S_n la richesse totale de la joueuse apr√®s le n-√®me tirage.\n\n\n\nPlotly = require('plotly.js-dist');\ndists = require( 'https://cdn.jsdelivr.net/gh/stdlib-js/stats-base-dists@umd/browser.js' );\njstat = require('jstat');\nseedrandom = require('seedrandom@3.0.5');\nmath = require(\"mathjs\");\n// see source here: https://github.com/stdlib-js/stats-base-dists/tree/umd\n// continuous case\n\nviewof inputs = Inputs.form([\n  Inputs.range([-10, 10], {label: tex`x`, step: 1}, {value: 3}),\n  Inputs.range([1, 100], {label: tex`n`, step: 1}),\n  Inputs.button(\"Re-Tirage\")\n])\n\nmu = inputs[0];\nn_samples = inputs[1];\nseed = 44 + inputs[2];\n\n{\n\nconst rng = seedrandom(seed)\n\nfunction random_walk(n=100){\n    var samples = Array.from({length: n}, () =&gt; {\n         if (rng.double() &gt; 0.5) {\n            return 1;\n        } else {\n            return -1;\n            }\n    });\n\n    return samples;\n}\nvar samples = random_walk(100);\n\n\n// create a cumsum function:\nfunction cumsum(array) {\n  var result = [];\n  array.reduce(function(a,b,i) { return result[i] = a+b; },0);\n  return result;\n}\n\n\nvar npoints=100, mini = 0, maxi=100;\nvar x = new Array(npoints), walk = new Array(npoints), zeros = new Array(npoints), z = new Array(npoints), i, j;\n\n//  Densit√©:s\nfor(var i = 0; i &lt; npoints; i++) {\n    x[i] = i ;\n    zeros[i] = 0;\n    }\n\n{\n    // points lower\nvar trace1 = {\n        x: x.slice(1, n_samples+1),\n        y: zeros.slice(1, n_samples+1),\n        z: samples.slice(0, n_samples),\n        // mode: 'markers',\n        type: 'heatmap',\n            xaxis: 'x2',\n\n        // marker: {\n        //     color: samples2.slice(0, n_samples2).map((x) =&gt; x &gt; 0 ? 'blue' : 'red'),\n        //     size: 5,\n        // },\n        showscale: false,\n        colorscale: [[0, \"rgb(66, 139, 202)\"], [1, \"rgb(255,0,0)\"]],\n\n  }\n\nwalk = cumsum(samples);\nfor(var i = 0; i &lt; npoints; i++) {\n    walk[i] = walk[i] + mu ;\n    }\n\n\nvar trace22 = {\n        x: x.slice(0, n_samples),\n        y: walk.slice(0, n_samples),\n        type: \"scatter\",\n        mode: \"lines\",\n        name: 'Marche al√©atoire',\n        line: {color: 'black'},\n        yaxis: 'y2',\n        xaxis: 'x2',\n        }\n\n\n\nvar data = [\n  trace1,\n  trace22,\n  ];\n\n\nvar layout = {\n    width: 600,\n  xaxis: {range: [0, n_samples+1],\n          showticklabels: false,\n          autorange: false},\n    yaxis: {domain: [0, 0.08],\n            showticklabels: false,\n      ticks:\"\",\n            range: [-0.5, 0.5],\n            autorange: false},\n    xaxis2: {matches: 'x',\n              range: [0, n_samples+1],\n          autorange: true},\n    yaxis2: {domain: [0.29, 0.99],\n              range: [-20, 20]},\n\n  showlegend: false,\n\n};\n\n    var config = {responsive: true}\n    const div = DOM.element('div');\n    Plotly.react(div, data, layout, config);\n    return div;\n    }\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†1: Marche al√©atoire simple sym√©trique et son chemin, initialis√©e en (0,x).\n\n\n\nA partir de la d√©finition, on obtient tr√®s facilement le comportement moyen de la marche.\n\nProposition 1 Pour tout n\\in\\mathbb{N}, on a \n\\mathbb{E}[S_n]=S_0,\\quad \\mathop{\\mathrm{Var}}(S_n)=n.\n\n\n\nPreuve. Remarquons d‚Äôabord que \n\\begin{align*}\n\\mathbb{E}[X_1]&=(-1)\\times \\frac{1}{2}+1\\times  \\frac{1}{2}=0,\\\\\n\\mathop{\\mathrm{Var}}(X_1)&=\\mathbb{E}[X_1^2]=(-1)^2\\times \\frac{1}{2}+1^2\\times  \\frac{1}{2}=1,\n\\end{align*}\n les incr√©ments sont donc centr√©s et r√©duits. Par lin√©arit√© de l‚Äôesp√©rance, on obtient \n\\mathbb{E}[S_n]=\\mathbb{E}[S_0+\\sum_{k=1}^n X_k]=S_0+\\sum_{k=1}^n \\mathbb{E}[X_k]=S_0,\n puisque S_0 est d√©terministe. Enfin, en utilisant l‚Äôind√©pendance des X_k, on obtient \n\\mathop{\\mathrm{Var}}(S_n)=\\mathop{\\mathrm{Var}}(S_0+\\sum_{k=1}^n X_k)=\\sum_{k=1}^n \\mathop{\\mathrm{Var}}(X_k)=n,\n d‚Äôo√π le r√©sultat.\n\nAinsi, en moyenne la marche reste constante, mais sa variance devient de plus en plus grande au cours du temps.\n\n\nRepr√©sentation graphique\nLes marches al√©atoires, comme tous les processus √† valeurs discr√®tes, ont une repr√©sentation graphique naturelle. On repr√©sente graphiquement un tirage de la marche (S_n)_{n\\in\\mathbb{N}} sur l‚Äôintervalle de temps \\llbracket 0,N \\rrbracket par une ligne bris√©e joignant les points (0,S_0), (1,S_1), ,(N,S_N). On a donc en abscisse le temps, et en ordonn√©e la valeur de la marche, cf. Figure¬†1.\n\nLa marche al√©atoire est un processus √† temps discret, donc rigoureusement sa repr√©sentation graphique devrait √™tre constitu√©e des points (n,S_n) non reli√©s. Par exemple, S_{1/2} n‚Äôest pas d√©fini. Cependant, on relie les points pour une meilleure lecture des graphiques.\nOn s‚Äôint√©resse maintenant aux repr√©sentations graphiques qui peuvent correspondre √† des tirages de la marche al√©atoire.\n\nD√©finition 2 Soit n&gt;m deux entiers naturels et (a,b)\\in\\Z^2 deux entiers relatifs. On appelle chemin de (m,a) √† (n,b) la repr√©sentation graphique d‚Äôun tirage de la marche al√©atoire telle que S_m=a et S_n=b.\n\n\nExemple 1 La figure pr√©sente deux chemins de (1,1) √† (5,3). \n\nOn peut maintenant se poser la question de savoir s‚Äôil existe toujours un chemin qui relie deux points quelconques, et s‚Äôil en existe, combien on peut construire de chemins diff√©rents reliant ces deux points. On peut facilement montrer que certains chemins sont impossibles en regardant la parit√© de la marche al√©atoire et son amplitude maximale.\n\nProposition 2 (Parit√©) Pour tout n\\in\\mathbb{N}, S_{2n} a la m√™me parit√© que S_0 et S_{2n+1} a la parit√© oppos√©e √† celle de S_0.\n\n\nPreuve. Pour tout n, X_{n+1} prend les valeurs -1 ou 1 donc est impair. Comme S_{n+1}=S_n+X_{n+1}, la marche change de parit√© √† chaque pas de temps.\n\nOn en d√©duit imm√©diatement que les chemins doivent respecter cette alternance de parit√©.\n\nCorollaire 1 Si n-m et b-a n‚Äôont pas la m√™me parit√©, alors il n‚Äôexiste pas de chemin de (m,a) √† (n,b).\n\n\nPreuve. Supposons sans perte de g√©n√©ralit√© que n-m est pair, et qu‚Äôil existe un chemin de (m,a) √† (n,b). Alors S_m=a et S_n=b ont n√©cessairement la m√™me parit√©, donc S_n-S_m=b-a est pair.\n\n\nExemple 2 Il n‚Äôexiste pas de chemin de (0,0) √† (2,1).\n\n\nProposition 3 (Amplitude) Pour tout n\\in\\mathbb{N}, on a S_0-n\\leq S_n\\leq S_0+n.\n\n\nPreuve. Pour tout n, X_{n} prend les valeurs -1 ou 1 donc en particulier \n-1\\leq X_n\\leq 1.\n Comme S_{n}=S_0+\\sum_{k=1}^n X_k, en sommant les in√©galit√©s on obtient bien \nS_0-n\\leq S_n\\leq S_0+n,\n d‚Äôo√π le r√©sultat.\n\n\nCorollaire 2 Si n-m&lt;|b-a|, alors il n‚Äôexiste pas de chemin de (m,a) √† (n,b).\n\n\nPreuve. Supposons qu‚Äôil existe un chemin de (m,a) √† (n,b). Alors S_m=a et S_n=b. En particulier, S_n=S_m+\\sum_{k=m+1}^nX_k, d‚Äôo√π \\begin{align*}\n&S_m-(n-m)\\leq S_n\\leq S_m+(n-m)\\\\\n\\Rightarrow & \\; a-(n-m)\\leq b\\leq a+(n-m)\\\\\n\\Rightarrow & \\; -(n-m)\\leq b-a\\leq n-m.\n\\end{align*} On a donc n√©cessairement |b-a|\\leq n-m.\n\n\nExemple 3 Il n‚Äôexiste pas de chemin de (0,0) √† (2,4).\n\n\n\n\nviewof inputs2 = Inputs.form([\n  Inputs.range([2, 100], {label: tex`n`, step: 2}),\n  Inputs.button(\"Re-Tirage\")\n])\n\nn_samples2 = inputs2[0];\nseed2 = 44 + inputs2[1];\n\n{\nvar npoints2=n_samples2\nconst rng = seedrandom(seed2)\nvar samples2 = new Array(npoints2)\n// create a vector of size 100 whose first half is 1 and second half is -1\nfor (var i = 0; i &lt; npoints2 ; i++) {\n  samples2[i] = i &gt;= npoints2/2  ? 1 : -1;\n}\n\n// https://www.codemzy.com/blog/shuffle-array-javascript\nfunction shuffleArray(array) {\n  let length = array.length;\n  let shuffle = array.slice(); // copy of array\n  // loop over the array\n  for (let i = length - 1; i &gt; 0; i -= 1) {\n    let random = Math.floor(rng.double() * (i + 1)); // random card position\n    let current = shuffle[i]; // current card\n    // swap the random card and the current card\n    shuffle[i] = shuffle[random]; // move the random card to the current position\n    shuffle[random] = current; // put the current card in the random position\n  }\n  return shuffle; // return shuffled array\n};\n\n\nsamples2=shuffleArray(samples2);\n\nvar x2 = new Array(npoints2), walk2 = new Array(npoints2), zeros2 = new Array(npoints2), z = new Array(npoints2), i, j;\n\n\n// create a cumsum function:\nfunction cumsum(array) {\n  var result = [];\n  array.reduce(function(a,b,i) { return result[i] = a+b; },0);\n  return result;\n}\n\nvar walkint = cumsum(samples2);\nwalk2 = new Array(npoints2+1),\nwalk2[0] = 0\nfor(i = 0; i &lt; npoints2; i++) {\n    walk2[i+1] = walkint[i];\n    }\n\n//  Densit√©:\nfor(var i = 0; i &lt; npoints2+1; i++) {\n    x2[i] = i ;\n    zeros2[i] = 0;\n    }\n\n{\n    // points lower\nvar trace1 = {\n        x: x2.slice(1, n_samples2+1),\n        y: zeros2.slice(1, n_samples2+1),\n        z: samples2.slice(0, n_samples2),\n        // mode: 'markers',\n        type: 'heatmap',\n            xaxis: 'x2',\n\n        // marker: {\n        //     color: samples2.slice(0, n_samples2).map((x) =&gt; x &gt; 0 ? 'blue' : 'red'),\n        //     size: 5,\n        // },\n        showscale: false,\n        colorscale: [[0, \"rgb(66, 139, 202)\"], [1, \"rgb(255,0,0)\"]],\n\n  }\n\n\n\nvar trace22 = {\n        x: x2.slice(0, n_samples2+1),\n        y: walk2.slice(0, n_samples2+1),\n        type: \"scatter\",\n        mode: \"lines\",\n        name: 'Marche al√©atoire',\n        line: {color: 'black'},\n        yaxis: 'y2',\n        xaxis: 'x2',\n        }\n\n\n\nvar data = [\n  trace1,\n  trace22,\n  ];\n\n\nvar layout = {\n    width: 600,\n  xaxis: {range: [0, n_samples2+1],\n          showticklabels: false,\n          autorange: false},\n    yaxis: {domain: [0, 0.08],\n            showticklabels: false,\n      ticks:\"\",\n            range: [-0.5, 0.5],\n            autorange: false},\n    xaxis2: {matches: 'x',\n              range: [0, n_samples2+1],\n          autorange: true},\n    yaxis2: {domain: [0.29, 0.99],\n              range: [-14, 14]},\n\n  showlegend: false,\n\n};\n\n    var config = {responsive: true}\n    const div = DOM.element('div');\n    Plotly.react(div, data, layout, config);\n    return div;\n    }\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†2: Marche al√©atoire simple sym√©trique passant en (0,0) et en (100,0).\n\n\n\nUne fois les conditions de parit√© et d‚Äôamplitude satisfaites, il existe toujours des chemins reliant deux points, et on peut les d√©nombrer explicitement.\n\nProposition 4 (Nombre de chemins) Si |b-a|\\leq n-m et n-m et b-a ont la m√™me parit√©, alors le nombre de chemins de (m,a) √† (n,b) est le nombre de combinaisons \\displaystyle \\binom{n-m}{\\frac{n-m}{2}+\\frac{b-a}{2}}.\n\n\nPreuve. Un chemin de (m,a) √† (n,b) correspond √† n-m pas de la marche al√©atoire en partant de a et en arrivant en b. Supposons sans perte de g√©n√©ralit√© que b&gt;a. Alors on peut d√©composer les pas de la marche entre x pas vers le haut (tirages de +1) et y pas vers le bas (tirages de -1). On doit avoir un nombre total de pas de x+y=n-m, et une diff√©rence des ordonn√©es de x-y=b-a, ce qui donne le syst√®me \n\\begin{align*}\n\\left\\{\\begin{array}{rcl}\nx+y&=&n-m\\\\\nx-y&=&b-a\n\\end{array}\n\\right.\n\\Leftrightarrow\n\\left\\{\\begin{array}{rcl}\nx&=&\\frac{n-m}{2}+\\frac{b-a}{2}\\\\\ny&=&\\frac{n-m}{2}-\\frac{b-a}{2}\n\\end{array}\n\\right..\n\\end{align*}\n Pour d√©nombrer le nombre total de chemins de longueur x+y avec x mont√©es et y descentes, il suffit de choisir les emplacements de x mont√©es parmi les x+y pas. On obtient donc \\binom{x+y}{x}=\\binom{n-m}{\\frac{n-m}{2}+\\frac{b-a}{2}} chemins.\n\nRemarquons que pour d√©nombrer le nombre de chemins d‚Äôun point √† un autre, nous n‚Äôavons pas utilis√© le fait que \\mathbb P(X=1)=\\mathbb P(X=-1), mais uniquement le fait que X ne peut prendre que les valeurs -1 et 1. Le r√©sultat ci-dessus est donc √©galement valable pour les marches asym√©triques o√π p=\\mathbb P(X=1)=1-\\mathbb P(X=-1)\\neq \\frac{1}{2}.\n\n\nPrincipe de r√©flexion\nOn souhaite maintenant d√©nombrer les chemins qui ont une propri√©t√© particuli√®re suppl√©mentaire, comme passer par un point fix√©. Pour cela, on utilise les propri√©t√©s de sym√©trie de la marche.\n\nProposition 5 (Principe de r√©flexion) Soit a et b deux entiers naturels non nuls et n,m deux entiers naturels tels que n&gt;m, |b-a|\\leq n-m et b-a et n-m ont la m√™me parit√©. Alors le nombre de chemins de (m,a) √† (n,b) passant par 0 (i.e. touchant l‚Äôaxe des abscisses) est √©gal au nombre de chemins de (m,a) √† (n,-b).\n\n\nPreuve. \nLe principe est de construire une bijection entre les deux ensembles de chemins pour prouver qu‚Äôils ont le m√™me cardinal. Pour cela, on exploite la sym√©trie de la marche, comme illustr√© figure . %=========================\n%========================= Consid√©rons un chemin (S_m,\\ldots,S_n) de (m,a) √† (n,b) passant par 0 pour la premi√®re fois en p. On a alors m&lt;p&lt;n. On construit un chemin (\\widetilde{S}_m,\\ldots,\\widetilde{S}_n)de (m,a) √† (n,-b) en gardant la premi√®re partie du chemin initial de (m,a) √† (p,0), puis en prenant ensuite le sym√©trique du chemin initial par rapport √† l‚Äôaxe des abscisses de (p,0) √† (n,-b) : \\begin{align*}\n\\widetilde{S}_k=\\left\\{\n\\begin{array}{lcl}\nS_k &\\text{ si }& m\\leq k\\leq p,\\\\\n-S_k&\\text{ si }& p\\leq k\\leq n.\n\\end{array}\n\\right.\n\\end{align*} Cette application d√©finit une bijection entre les deux ensembles de chemins. En effet, si on prend maintenant un chemin (\\widetilde{S}_m,\\ldots,\\widetilde{S}_n) de (m,a) √† (n,-b), comme il fait des pas de 1, que a&gt;0 et -b&lt;0, ce chemin passe n√©c√©ssairement par 0. Soit p le premier instant o√π le chemin est en 0. On construit alors un chemin (S_m,\\ldots,S_n) de (m,a) √† (n,b) passant par 0 en posant \\begin{align*}\nS_k=\\left\\{\n\\begin{array}{lcl}\n\\widetilde{S}_k &\\text{ si }& m\\leq k\\leq p,\\\\\n-\\widetilde{S}_k&\\text{ si }& p\\leq k\\leq n.\n\\end{array}\n\\right.\n\\end{align*} Il y a donc le m√™me nombre de chemins des deux types.\n\nA nouveau, nous n‚Äôavons pas utilis√© le fait que \\mathbb P(X=1)=\\mathbb P(X=-1), mais uniquement le fait que X ne peut prendre que les valeurs -1 et 1 qui sont sym√©triques l‚Äôune de l‚Äôautre. Le r√©sultat ci-dessus est donc √©galement valable pour les marches asym√©triques o√π p=\\mathbb P(X=1)=1-\\mathbb P(X=-1)\\neq \\frac{1}{2}. Une premi√®re application de cette propri√©t√© est le r√©sultat suivant connu sous le nom de th√©or√®me su scrutin.\n\nTh√©or√®me 1 Au cours d‚Äôune √©lection opposant deux candidates A et B, la candidate A (resp. B) a obtenu a (resp. b) voix, avec a&gt;b. Alors la probabilit√© que A ait √©t√© majoritaire (au sens large) tout au long du d√©pouillement est \np=1-\\frac{b}{a+1}.\n\n\n\nPreuve. Tous les d√©pouillement √©tant √©quiprobables, p s‚Äôobtient comme le rapport du nombre de d√©pouillements avec A en t√™te tout le long par le nombre total de d√©pouillements. On peut mod√©liser un d√©pouillement par une marche al√©atoire (S_n) (pas n√©cessairement sym√©trique) o√π S_n est le nombre de voix d‚Äôavance de A sur B apr√®s le d√©pouillement du n-√®me bulletin.\nIl y a en tout a+b bulletins et √† la fin a-b voix d‚Äôavance de A sur B, donc le nombre total de d√©pouillements est le nombre de chemins de (0,0) √† (a+b,a-b) et vaut \\binom{a+b}{a}.\nLe nombre de d√©pouillements avec A en t√™te tout le long correspond\nIl vaut donc \\binom{a+b}{a}-\\binom{a+b}{b-1}. Ainsi \\begin{align*}\np=\\frac{\\binom{a+b}{a}-\\binom{a+b}{b-1}}{\\binom{a+b}{a}}=1-\\frac{\\binom{a+b}{b-1}}{\\binom{a+b}{a}}=1-\\frac{(a+b)!a!b!}{(b-1)!(a+1)!(a+b)!}=1-\\frac{b}{a+1},\n\\end{align*} d‚Äôo√π le r√©sultat.",
    "crumbs": [
      "Cours",
      "Marches al√©atoires"
    ]
  },
  {
    "objectID": "Courses/marche_aleatoire.html#sec-chap03-infini",
    "href": "Courses/marche_aleatoire.html#sec-chap03-infini",
    "title": "Marches al√©atoires",
    "section": "Comportement asymptotique",
    "text": "Comportement asymptotique\nOn s‚Äôint√©resse maintenant au comportement de la marche sans limite de temps. Parmi les questions d‚Äôint√©r√™t, on peut se demander si elle revient √† son point de d√©part x, combien de fois et en combien de temps.\nComme les (X_n)_{n\\geq 1} sont ind√©pendantes et de m√™me loi int√©grable, on peut appliquer la loi des grands nombres pour obtenir que \n\\frac{S_n}{n}\\xrightarrow[n\\to\\infty]{ps}\\mathbb{E}[X_1]=0,\n ce qui ne donne aucune information sur le comportement en temps long de S_n.\n\nPropri√©t√© de Markov et stationnarit√©\nLes deux propri√©t√©s suivantes sont des propri√©t√©s du processus et non plus seulement des trajectoires. Elles √©noncent une certaine forme d‚Äôinvariance de la marche au cours du temps. La premi√®re est une propri√©t√© d‚Äôabsence de m√©moire : la position future de la marche ne d√©pend que de sa position actuelle et pas de la trajectoire qui l‚Äôa amen√©e √† cette position.\n\nProposition 6 (Markov) Pour tout n\\in\\mathbb{N} et pour tous (a_0,a_1,\\ldots,a_{n+1})\\in\\Z^{n+2}, on a \\begin{align*}\n\\mathbb{P}(S_{n+1}=a_{n+1}& | S_n=a_n, S_{n-1}=a_{n-1},\\ldots,S_1=a_1,S_0=a_0)\\\\\n=\\mathbb{P}(S_{n+1}=a_{n+1}& | S_n=a_n).\n\\end{align*}\n\n\nPreuve. Par d√©finition des probabilit√©s conditionnelles, on a \\begin{align*}\n\\mathbb{P}(S_{n+1}=a_{n+1} &| S_n=a_n, S_{n-1}=a_{n-1},\\ldots,S_1=a_1,S_0=a_0)\\\\\n&=\\frac{\\mathbb{P}(S_{n+1}=a_{n+1}, S_n=a_n, S_{n-1}=a_{n-1},\\ldots,S_1=a_1,S_0=a_0)}{\\mathbb{P}(S_n=a_n, S_{n-1}=a_{n-1},\\ldots,S_1=a_1,S_0=a_0)}\\\\\n&=\\frac{\\mathbb{P}(X_{n+1}=a_{n+1}-a_n, S_n=a_n, S_{n-1}=a_{n-1},\\ldots,S_1=a_1,S_0=a_0)}{\\mathbb{P}(S_n=a_n, S_{n-1}=a_{n-1},\\ldots,S_1=a_1,S_0=a_0)},\n\\end{align*} puisque S_{n+1}=S_n+X_{n+1}. Remarquons maintenant que X_{n+1} est ind√©pendante de X_1,\\ldots, X_n, donc de S_0,\\ldots,S_n. On obtient alors \\begin{align*}\n\\mathbb{P}(S_{n+1}=a_{n+1} &| S_n=a_n, S_{n-1}=a_{n-1},\\ldots,S_1=a_1,S_0=a_0)\\\\\n&=\\frac{\\mathbb{P}(X_{n+1}=a_{n+1}-a_n)\\mathbb{P}(S_n=a_n, S_{n-1}=a_{n-1},\\ldots,S_1=a_1,S_0=a_0)}{\\mathbb{P}(S_n=a_n, S_{n-1}=a_{n-1},\\ldots,S_1=a_1,S_0=a_0)}\\\\\n&=\\mathbb{P}(X_{n+1}=a_{n+1}-a_n).\n\\end{align*} Par ailleurs, par un raisonnement analogue on a \\begin{align*}\n\\mathbb{P}(S_{n+1}=a_{n+1} | S_n=a_n)\n&=\\frac{\\mathbb{P}(S_{n+1}=a_{n+1},S_n=a_n)}{\\mathbb{P}(S_n=a_n)}\\\\\n&=\\frac{\\mathbb{P}(X_{n+1}=a_{n+1}-a_n,S_n=a_n)}{\\mathbb{P}(S_n=a_n)}\\\\\n&=\\frac{\\mathbb{P}(X_{n+1}=a_{n+1}-a_n)\\mathbb{P}(S_n=a_n)}{\\mathbb{P}(S_n=a_n)}\\\\\n&=\\mathbb{P}(X_{n+1}=a_{n+1}-a_n),\n\\end{align*} d‚Äôo√π le r√©sultat.\n\n\nProposition 7 Pour tout n &gt; m entiers positifs S_n-S_m est ind√©pendant de (S_0,S_1,\\ldots, S_m).\n\n\nPreuve. Comme les (X_n) sont ind√©pendantes, on a directement que \nS_n-S_m=\\sum_{k=m+1}^nX_k,\n qui est ind√©pendant de (S_0=x,S_1=x+X_1,\\ldots, S_m=x+\\sum_{k=1}^mX_k).\n\nOn a un r√©sultat analogue √† la propri√©t√© de Markov lorsqu‚Äôon regarde deux pas de temps plus √©loign√©s.\n\nCorollaire 3 Pour tout n&gt;m entiers positifs et pour tous (a_0,a_1,\\ldots,a_m,a_{n})\\in\\Z^{m+2}, on a \\begin{align*}\n\\mathbb{P}(S_{n}=a_{n}& | S_m=a_m, S_{m-1}=a_{m-1},\\ldots,S_1=a_1,S_0=a_0)\\\\\n=\\mathbb{P}(S_{n}=a_{n}& | S_m=a_m).\n\\end{align*}\n\n\nPreuve. Par un raisonnement analogue √† la preuve de la propri√©t√© de Markov, on obtient \\begin{align*}\n\\mathbb{P}(S_{n}=a_{n}&| S_m=a_m, S_{m-1}=a_{m-1},\\ldots,S_1=a_1,S_0=a_0)\\\\\n&=\\frac{\\mathbb{P}(S_{n}=a_{n},S_m=a_m, S_{m-1}=a_{m-1},\\ldots,S_1=a_1,S_0=a_0)}{\\mathbb{P}(S_m=a_m, S_{m-1}=a_{m-1},\\ldots,S_1=a_1,S_0=a_0)}\\\\\n&=\\frac{\\mathbb{P}(S_{n}-S_m=a_{n}-a_m,S_m=a_m, S_{m-1}=a_{m-1},\\ldots,S_1=a_1,S_0=a_0)}{\\mathbb{P}(S_m=a_m, S_{m-1}=a_{m-1},\\ldots,S_1=a_1,S_0=a_0)}\\\\\n&=\\frac{\\mathbb{P}(S_{n}-S_m=a_{n}-a_m)\\mathbb{P}(S_m=a_m, S_{m-1}=a_{m-1},\\ldots,S_1=a_1,S_0=a_0)}{\\mathbb{P}(S_m=a_m, S_{m-1}=a_{m-1},\\ldots,S_1=a_1,S_0=a_0)}\\\\\n&=\\mathbb{P}(S_{n}-S_m=a_{n}-a_m)\\\\\n&=\\mathbb{P}(S_n=a_n|S_m=a_m),\n\\end{align*} en utilisant la propri√©t√© des accroissement ind√©pendants.\n\nLa seconde propri√©t√© dite de stationnarit√© signifie qu‚Äôon peut changer l‚Äôorigine du rep√®re sans influence sur le comportement futur de la marche.\n\nProposition 8 (Sationnarit√©) Pour tout n&gt;m entiers positifs et (a,b)\\in\\Z^2, on a \n\\mathbb{P}(S_n=b|S_m=a)=\\mathbb{P}(S_{n-m}= b-a|S_0= 0).\n\n\n\nPreuve. En utilisant l‚Äôind√©pendance des accroissements, on obtient \\begin{align*}\n\\mathbb{P}(S_n=b|S_m=a)\n&=\\frac{\\mathbb{P}(S_n=b,S_m=a)}{\\mathbb{P}(S_m=a)}\\\\\n&=\\frac{\\mathbb{P}(S_n-S_m=b-a,S_m=a)}{\\mathbb{P}(S_m=a)}\\\\\n&=\\frac{\\mathbb{P}(S_n-S_m=b-a)\\mathbb{P}(S_m=a)}{\\mathbb{P}(S_m=a)}\\\\\n&=\\mathbb{P}(S_n-S_m=b-a)\\\\\n&=\\mathbb{P}(\\sum_{k=m+1}^nX_k=b-a)\\\\\n&=\\mathbb{P}(\\sum_{k=1}^{n-m}X_k=b-a)\\\\\n&=\\mathbb{P}(S_{n-m}=b-a|S_0=0),\n\\end{align*} car les (X_n) sont ind√©pendantes et de m√™me loi.\n\n\n\nPremier retour en 0\nOn suppose maintenant que la marche part de 0 : S_0=0 et on cherche √† savoir si la marche reviendra en 0 presque s√ªrement. Commen√ßons par remarquer que si S_n=0 alors n est n√©cessairement pair.\n\nProposition 9 Pour tout n\\in\\mathbb{N}, on a \n\\mathbb{P}(S_{2n}=0)=\\frac{1}{4^n}\\binom{2n}{n}.\n\n\n\nPreuve. Cette probabilit√© correspond au nombre de chemins de (0,0) √† (2n,0) divis√© par le nombre total de chemins de longueur 2n, puisque tous les chemins sont √©quiprobables. On a donc directement \\mathbb{P}(S_{2n}=0)=\\frac{\\binom{2n}n}{4^n}.\n\nOn s‚Äôint√©resse maintenant au premier instant de retour en 0. On le note T_0. Il s‚Äôagit donc de la variable al√©atoire \nT_0=\\inf\\{n\\geq 1; S_n=0\\},\n avec la convention que \\inf\\emptyset= +\\infty. C‚Äôest donc une variable al√©atoire discr√®te √† valeurs dans \\mathbb{N}^*\\cup\\{+\\infty\\}. On peut calculer explicitement la loi de T_0.\n\nProposition 10 Pour tout n\\in\\mathbb{N}^*, on a \n\\mathbb{P}(T_0=2n)=\\frac{(2n-2)!}{2^{2n-1}n!(n-1)!}.\n\n\n\nPreuve. L‚Äô√©v√©nement (T_0= 2n) correspond √† (S_2\\neq 0, \\ldots,S_{2n-2}\\neq 0,S_{2n}=0) puisqu‚Äôalors 2n est le premier temps o√π la marche retourne en 0. En particulier, entre l‚Äôinstant 0 et l‚Äôinstant 2n la marche ne change pas de signe, et garde le signe de S_1. Ainsi, on a \\begin{align*}\n\\mathbb{P}(T_0=2n)&=\\mathbb{P}(S_1=1,S_2&gt;0,\\ldots,S_{2n-2}&gt;0,S_{2n}=0)\\\\\n&\\quad+\\mathbb{P}(S_1=-1,S_2&lt;0,\\ldots,S_{2n-2}&lt;0,S_{2n}=0).\n\\end{align*} Par sym√©trie, ces deux probabilit√©s sont √©gales. De plus, si S_{2n-2}&gt;0 et S_{2n}=0 alors n√©cessairement on a S_{2n-1}=1. Ainsi il vient \\begin{align*}\n{\\mathbb{P}(T_0=2n)}\\\\\n&=2\\mathbb{P}(S_1=1,S_2&gt;0,\\ldots,S_{2n-2}&gt;0,S_{2n-1}=1,S_{2n}=0)\\\\\n&=2\\mathbb{P}(S_{2n}=0|S_1=1,S_2&gt;0,\\ldots,S_{2n-2}&gt;0,S_{2n-1}=1)\\\\\n&\\quad \\times\\mathbb{P}(S_1=1,S_2&gt;0,\\ldots,S_{2n-2}&gt;0,S_{2n-1}=1)\\\\\n&=2\\mathbb{P}(S_{2n}=0|S_{2n-1}=1)\\mathbb{P}(S_1=1,S_2&gt;0,\\ldots,S_{2n-2}&gt;0,S_{2n-1}=1)\\\\\n&=2\\mathbb{P}(S_{1}=-1|S_0=0)\\mathbb{P}(S_1=1,S_2&gt;0,\\ldots,S_{2n-2}&gt;0,S_{2n-1}=1)\\\\\n&=\\mathbb{P}(S_1=1,S_2&gt;0,\\ldots,S_{2n-2}&gt;0,S_{2n-1}=1),\n\\end{align*} en utilisant la propri√©t√© de Markov, la propri√©t√© de stationnarit√© puis le fait que \n{\\mathbb P}(S_1=-1|S_0=0) = {\\mathbb P}(X_1=-1) = \\frac{1}{2}.\n Cette derni√®re probabilit√© correspond au nombre de chemins de (1,1) √† (2n-1,1) ne touchant pas 0 divis√©e par le nombre total de chemins de longueur 2n-1. En utilisant le principe de r√©flexion, on obtient \\begin{align*}\n\\mathbb{P}(T_0=2n)\n&=\\frac{\\binom{2n-2}{n-1}-\\binom{2n-2}{n}}{2^{2n-1}}\\\\\n&=\\frac{1}{2^{2n-1}}\\left(\\frac{(2n-2)!}{(n-1)!(n-1)!}-\\frac{(2n-2)!}{n!(n-2)!}\\right)\\\\\n&=\\frac{1}{2^{2n-1}}\\frac{(2n-2)!}{(n-1)!(n-2)!}\\left(\\frac{1}{n-1}-\\frac{1}{n}\\right)\\\\\n&=\\frac{(2n-2)!}{2^{2n-1}n!(n-1)!},\n\\end{align*} d‚Äôo√π le r√©sultat.\n\nNous pouvons maintenant √©noncer le r√©sultat principal de cette partie.\n\nTh√©or√®me 2 La marche al√©atoire partant de 0 revient en 0 en temps fini avec probabilit√© 1, i.e. \n\\mathbb{P}(T_0 &lt; +\\infty)=1.\n\n\n\nPreuve. On a \\mathbb{P}(T_0&lt;+\\infty)=\\sum_{n=1}^\\infty\\mathbb{P}(T_0=2n), il s‚Äôagit donc d‚Äôidentifier la somme de cette s√©rie. Pour cela, on exprime diff√©remment le probabilit√© de l‚Äô√©v√©nement (T_0=2n), en repartant de l‚Äôexpression du nombre de chemins de (1,1) √† (2n-1,1) ne touchant pas 0 divis√©e par le nombre total de chemins de longueur 2n-1. Ce nombre de chemins est √©gal √† la diff√©rence entre le nombre de chemins de (1,1) √† (2n-1,1) et le nombre de chemins de (1,1) √† (2n-1,-1) d‚Äôapr√®s le principe de r√©flexion. On obtient ainsi : \\begin{align*}\n{\\mathbb{P}(T_0=2n)}\\\\\n&=\\mathbb{P}(S_1=1, S_{2n-1}=1)-\\mathbb{P}(S_1=1, S_{2n-1}=-1)\\\\\n&=\\mathbb{P}(S_{2n-1}=1|S_1=1)\\mathbb{P}(S_1=1)-\\mathbb{P}(S_{2n-1}=-1|S_1=1)\\mathbb{P}(S_1=1)\\\\\n&=\\frac{1}{2}\\mathbb{P}(S_{2n-2}=0|S_0=0)-\\frac{1}{2}\\mathbb{P}(S_{2n-2}=-2|S_0=0)\\\\\n&=\\frac{1}{2}\\mathbb{P}(S_{2n-2}=0)-\\frac{1}{2}\\mathbb{P}(S_{2n-2}=-2),\n\\end{align*} par la propri√©t√© de stationnarit√© et puisque S_0=0. Par ailleurs, on a \\begin{align*}\n{\\mathbb{P}(S_{2n}=0)}  \\\\\n&=\\mathbb{P}(S_{2n}=0, S_{2n-2} =-2)+\\mathbb{P}(S_{2n}=0, S_{2n-2}=0) +\\mathbb{P}(S_{2n}=0, S_{2n-2}=2)\\\\\n&=2\\mathbb{P}(S_{2n}=0, S_{2n-2}=-2)+\\mathbb{P}(S_{2n}=0, S_{2n-2}=0),\n\\end{align*} par sym√©trie. Il vient \\begin{align*}\n{\\mathbb{P}(S_{2n}=0)}\\\\\n&=2\\mathbb{P}(S_{2n}=0|S_{2n-2}=-2)\\mathbb{P}(S_{2n-2}=-2)+\\mathbb{P}(S_{2n}=0|S_{2n-2}=0)\\mathbb{P}(S_{2n-2}=0)\\\\\n&=2\\mathbb{P}(S_2=2|S_0=0)\\mathbb{P}(S_{2n-2}=-2)+\\mathbb{P}(S_2=0|S_0=0)\\mathbb{P}(S_{2n-2}=0)\\\\\n&=2\\frac{1}{4}\\mathbb{P}(S_{2n-2}=-2)+\\frac{1}{2}\\mathbb{P}(S_{2n-2}=0)\\\\\n&=\\frac{1}{2}\\mathbb{P}(S_{2n-2}=-2)+\\frac{1}{2}\\mathbb{P}(S_{2n-2}=0),\n\\end{align*} o√π on a utilis√© le fait que \\begin{align*}\n\\mathbb{P}(S_2=2|S_0=0)& =   \\mathbb{P}(X_1 + X_2 = 0) \\\\\n&= \\mathbb{P}(X_1 = -1; X_2= 1)+ \\mathbb{P}(X_1 = 1; X_2= -1)\\\\\n&=\\mathbb{P}(X_1 = -1) \\mathbb{P}( X_2= 1)+ \\mathbb{P}(X_1 = 1)\\mathbb{P}( X_2= -1) \\\\\n&=\\frac{1}{2}.\n\\end{align*} Donc on a \\mathbb{P}(S_{2n-2}=-2)=2\\mathbb{P}(S_{2n}=0)-\\mathbb{P}(S_{2n-2}=0).\nRevenons maintenant √† T_0. On obtient \\begin{align*}\n\\mathbb{P}(T_0=2n)\n&=\\frac{1}{2}\\mathbb{P}(S_{2n-2}=0)-\\frac{1}{2}\\mathbb{P}(S_{2n-2}=-2)\\\\\n&=\\frac{1}{2}\\mathbb{P}(S_{2n-2}=0)-\\mathbb{P}(S_{2n}=0)+\\frac{1}{2}\\mathbb{P}(S_{2n-2}=0)\\\\\n&=\\mathbb{P}(S_{2n-2}=0)-\\mathbb{P}(S_{2n}=0).\n\\end{align*} Ainsi, la probabilit√© de l‚Äô√©v√©nement (T_0= +\\infty) peut s‚Äô√©crire √† l‚Äôaide d‚Äôune somme t√©lescopique : \\begin{align*}\n\\mathbb{P}(T_0=+\\infty)\n&=1-\\sum_{n=1}^\\infty\\mathbb{P}(T_0=2n)\\\\\n&=1-\\lim_{N\\to\\infty}\\sum_{n=1}^N\\mathbb{P}(T_0=2n)\\\\\n&=1-\\lim_{N\\to\\infty}\\sum_{n=1}^N\\left(\\mathbb{P}(S_{2n-2}=0)-\\mathbb{P}(S_{2n}=0)\\right)\\\\\n&=1-\\lim_{N\\to\\infty}\\left(\\mathbb{P}(S_0=0)-\\mathbb{P}(S_{2N}=0)\\right)\\\\\n&=\\lim_{N\\to\\infty}\\mathbb{P}(S_{2N}=0)\\\\\n&=\\lim_{N\\to\\infty}\\frac{1}{4^N}\\binom{2N}{N}.\n\\end{align*} En utilisant la formule de Stirling, on obtient l‚Äô√©quivalence \\begin{align*}\n\\frac{1}{4^N}\\binom{2N}{N} &= \\frac{1}{4^N}\\frac{(2N)!}{N!N!}\n\\sim  \\frac{1}{4^N}\\frac{(2N)^{2N}\\sqrt{4\\pi N}e^{-2N}}{(N^{N}\\sqrt{2\\pi N}e^{-N})^2}\n= \\frac{1}{\\sqrt{\\pi N}},\n\\end{align*} donc on a bien \\mathbb{P}(T_0=+\\infty)=0.\n\n\n\nPremier passage en a\\neq 0\nOn peut obtenir par une m√©thode similaire la loi du temps d‚Äôatteinte de n‚Äôimporte quel autre point. Soit a\\in\\Z^*. Le temps d‚Äôatteinte de a est la variable al√©atoire \nT_a=\\inf\\{n\\geq 1; S_n=a\\}.\n\n\nTh√©or√®me 3 La marche al√©atoire partant de 0 atteint tout √©tat a\\in\\Z en temps fini avec probabilit√© 1. Plus pr√©cis√©ment, si a\\neq 0, on a \\mathbb{P}(T_a=n)=0 si n et a n‚Äôont pas la m√™me parit√© ou si |a|&gt;n, et sinon \n\\mathbb{P}(T_a=n)=\\frac{|a|}{n2^n}\\binom{n}{\\frac{n+a}{2}}.\n\n\n\nPreuve. \nOn suit les m√™mes √©tapes que ce qu‚Äôon a fait pour T_0. Les contraintes de parit√© et de longueur de chemin ont d√©j√† √©t√© vues. Si n et a ont la m√™me parit√© et que |a|&gt;n, alors l‚Äô√©v√©nement (T_a= n) correspond √† (S_1\\neq a, \\ldots,S_{n-1}\\neq a,S_{n}=a) puisqu‚Äôalors n est le premier temps o√π la marche atteint a. Supposons sans perte de g√©n√©ralit√© que a&gt;0. Donc la probabilit√© \\mathbb{P}(T_a= n) est √©gale au nombre de chemins de (0,0) √† (n,a) ne touchant pas a avant n divis√©e par le nombre total de chemins de longueur n. 0r, ce nombre de chemin est √©gal\nAinsi, on a\n\\begin{align*}\n\\mathbb{P}(T_a=n)\n&=\\frac{\\binom{n-1}{\\frac{n-1}{2}+\\frac{a-1}{2}}-\\binom{n-1}{\\frac{n-1}{2}+\\frac{1+a}{2}}}{2^{n}}\\\\\n&=\\frac{1}{2^{n}}\\left(\\frac{(n-1)!}{(\\frac{n+a-2}{2})!(\\frac{n-a}{2})!}-\\frac{(n-1)!}{(\\frac{n+a}{2})!(\\frac{n-a-2}{2})!}\\right)\\\\\n&=\\frac{1}{2^n}\\frac{(n-1)!}{(\\frac{n+a-2}{2})!(\\frac{n-a-2}{2})!}\\left(\\frac{1}{\\frac{n-a}{2}}-\\frac{1}{\\frac{n+a}{2}}\\right)\\\\\n&=\\frac{a}{2^n}\\frac{(n-1)!}{(\\frac{n+a}{2})!(\\frac{n-a}{2})!}\\\\\n&=\\frac{a}{n2^n}\\binom{n}{\\frac{n+a}{2}}\n\\end{align*} d‚Äôo√π le r√©sultat.\nNous d√©montrerons dans la partie suivante que \\mathbb{P}(T_a&lt;+\\infty)=1.\n\nEn particulier, la marche partant de 0 va atteindre tout point de \\Z avec probabilit√© 1, donc elle est presque s√ªrement non born√©e.",
    "crumbs": [
      "Cours",
      "Marches al√©atoires"
    ]
  },
  {
    "objectID": "Courses/marche_aleatoire.html#sec-chap03-ruine",
    "href": "Courses/marche_aleatoire.html#sec-chap03-ruine",
    "title": "Marches al√©atoires",
    "section": "La ruine de la joueuse",
    "text": "La ruine de la joueuse\nOn s‚Äôint√©resse maintenant √† un probl√®me de ruine. Une joueuse dispose d‚Äôun capital initial x. Elle joue √† pile ou face avec une pi√®ce √©quilibr√©e, et gagne 1 si elle obtient pile, perd 1 si elle obtient face. La joueuse s‚Äôest fix√© un objectif de gain a\\geq x et un plancher de perte b\\leq x. Elle joue jusqu‚Äô√† ce que sa richesse atteigne a ou b.\nOn mod√©lise la fortune de la joueuse par une marche al√©atoire (S_n)_{n\\in\\mathbb{N}}, avec S_0=x et S_n=S_0+\\sum_{k=1}^n X_k, o√π X_k repr√©sente le gain du k-√®me tirage.\nOn sait qu‚Äôatteindre a ou b partant de x est √©quivalent √† atteindre a-x ou b-x partant de 0. Le jeu s‚Äôarr√™te donc presque s√ªrement au bout d‚Äôun temps fini.\nOn note p_x la probabilit√© de ruine partant d‚Äôun capital initial x, et R_x l‚Äô√©v√©nement √™tre ruin√©e en partant de x, c‚Äôest-√†-dire \np_x=\\mathbb{P}(T_b &lt; T_a)= \\mathbb{P}(R_x).\n On peut remarquer directement que p_a=0 et p_b=1 puisque dans ces deux situations, le jeu ne d√©marre pas, la ruine est impossible dans le premier cas, et certaine dans le second.\nNous allons obtenir une formule de r√©currence sur les p_x. Si b&lt;x&lt;a, on a par la formule des probabilit√©s totales \\begin{align*}\np_x&=\\mathbb{P}(R_x)\\\\\n&=\\mathbb{P}(R_x|S_1=x+1)\\mathbb{P}(S_1=x+1)+\\mathbb{P}(R_x|S_1=x-1)\\mathbb{P}(S_1=x-1)\\\\\n&=\\mathbb{P}(R_{x+1})\\frac{1}{2}+\\mathbb{P}(R_{x-1})\\frac{1}{2},\n\\end{align*} en utilisant la stationnarit√©. On obtient ainsi que (p_x)_{a&lt;x&lt; b} est une suite r√©currente lin√©aire1 d‚Äôordre 2, de polyn√¥me caract√©ristique X^2-2X+1=(X-1)^2. Ainsi p_x=\\alpha +\\beta x. On identifie \\alpha et \\beta avec les conditions extr√©males p_a=0 et p_b=1 \\begin{align*}\n\\left\\{\\begin{array}{rcl}\np_a=0&=&\\alpha+\\beta a\\\\\np_b=1&=&\\alpha+\\beta b\n\\end{array}\n\\right.\n\\Leftrightarrow\n\\left\\{\\begin{array}{rcl}\n\\alpha&=&\\frac{a}{a-b},\\\\\n\\beta&=&-\\frac{1}{a-b}.\n\\end{array}\n\\right.\n\\end{align*}\n1¬†Une suite (u_n)_{n \\in {\\mathbb N}} est dite r√©currente lin√©aire d‚Äôordre 2 s‚Äôil existe (a,b)\\in\\mathbb{R}\\times\\mathbb{R}^* tel que, pour entier n, on a \nu_{n+2} = a u_{n+1} + b u_n .\n Le polyn√¥me X^2 - a X + b est appel√© le polyn√¥me caract√©ristique de la suite. Si le polyn√¥me caract√©ristique admet deux racines simples r_1 et r_2, alors il existe deux r√©els \\alpha et \\beta tels que pour tout entier n, \nu_n = \\alpha r_1^n  + \\beta  r_2^n.\n Si le polyn√¥me caract√©ristique admet une racine double r, alors il existe deux r√©els \\alpha et \\beta tels que pour tout entier n, \nu_n = (\\alpha  + \\beta n) r^n.\nAinsi la probabilit√© de ruine partant de x vaut \np_x=\\frac{a-x}{a-b}.\n\nUn calcul analogue en cherchant la probabilit√© q_x de faire fortune partant d‚Äôune richesse x conduit √† q_x=\\frac{x-b}{a-b}=1-p_x, autrement dit il n‚Äôy a pas d‚Äôautre issue possible : le jeu va s‚Äôarr√™ter avec probabilit√© 1. Si on fait tendre b vers -\\infty, on trouve que \\mathbb{P}(T_a&lt;\\infty |x_0=a)=1. Donc partant de n‚Äôimporte quel point, la marche atteindra n‚Äôimporte quel autre point avec probabilit√© 1.\nOn peut montrer avec un raisonnement analogue que la dur√©e du jeu d_x partant de x est solution de l‚Äô√©quation r√©currente lin√©aire avec second membre2 \nd_x=\\frac{1}{2}(d_{x+1}+d_{x-1})+1,\n avec d_a=d_b=0.\n2¬†Une suite (u_n)_{n \\in {\\mathbb N}} est dite r√©currente lin√©aire d‚Äôordre 2 avec second membre s‚Äôil existe (a,b) \\in {\\mathbb R} \\times {\\mathbb R}^* et une fonction f √† valeurs dans {\\mathbb R} tels que, pour entier n, on a \nu_{n+2} = a u_{n+1} + b u_n  + f(n).\n L‚Äô√©quation \\displaystyle u_{n+2} = a u_{n+1} + b u_n s‚Äôappelle alors l‚Äô√©quation homog√®ne associ√©e. Les solutions de l‚Äô√©quation avec second membre sont la somme de la solution g√©n√©rique de l‚Äô√©quation homog√®ne et d‚Äôune solution particuli√®re de l‚Äô√©quation avec second membre.En effet, il est clair que d_a=d_b=0, et si a&lt;x&lt;b, alors on a d_x=\\mathbb{E}(T_a\\wedge T_b). Attention, la variable al√©atoire T_a\\wedge T_b est √† valeurs dans \\mathbb{N}\\cup\\{+\\infty\\}. Si \\mathbb{P}(T_a\\wedge T_b=+\\infty|S_0=x)&gt;0, alors \\mathbb{E}(T_a\\wedge T_b)=+\\infty. Par ailleurs, partant de x+1, on a \\begin{align*}\n\\mathbb{P}(T_a\\wedge T_b=+\\infty|S_0=x+1)&\\geq \\mathbb{P}(T_a\\wedge T_b=+\\infty, S_1=x|S_0=x+1)\\\\\n&= \\mathbb{P}(T_a\\wedge T_b=+\\infty|S_1=x,S_0=x+1)\\mathbb{P}(S_1=x|S_0=x+1)\\\\\n&=\\frac{1}{2}\\mathbb{P}(T_a\\wedge T_b=+\\infty|S_0=x)\n\\end{align*} par les propri√©t√©s de Markov et de stationnarit√©. Donc \\mathbb{P}(T_a\\wedge T_b=+\\infty|S_0=x+1)&gt;0. Ainsi, les membres de gauche et de droite de l‚Äô√©quation d_x=\\frac{1}{2}(d_{x+1}+d_{x-1})+1 valent tous les deux +\\infty, et l‚Äô√©galit√© est vraie. On montrerait de m√™me que \\mathbb{P}(T_a\\wedge T_b=+\\infty|S_0=x)&gt;0 implique \\mathbb{P}(T_a\\wedge T_b=+\\infty|S_0=x-1)&gt;0, et par contrapos√©e et d√©calage d‚Äôindices que \\mathbb{P}(T_a\\wedge T_b=+\\infty|S_0=x)=0 implique \\mathbb{P}(T_a\\wedge T_b=+\\infty|S_0=x+1)=\\mathbb{P}(T_a\\wedge T_b=+\\infty|S_0=x-1)=0. Pla√ßons nous dans ce dernier cas. On a donc \\begin{align*}\nd_x&=\\mathbb{E}(T_a\\wedge T_b)\\\\\n&=\\sum_{k=1}^\\infty k \\mathbb{P}(T_a\\wedge T_b=k|S_0=x)\\\\\n&=\\sum_{k=1}^\\infty k \\big(\\mathbb{P}(T_a\\wedge T_b=k|S_1=x+1)\\mathbb{P}(S_1=x+1)+\\mathbb{P}(T_a\\wedge T_b=k|S_1=x-1)\\mathbb{P}(S_1=x-1)\\big)\\\\\n&=\\frac{1}{2}\\sum_{k=1}^\\infty k \\mathbb{P}(T_a\\wedge T_b=k-1|S_0=x+1)+\\frac{1}{2}\\sum_{k=1}^\\infty k\\mathbb{P}(T_a\\wedge T_b=k-1|S_0=x-1)\\\\\n&=\\frac{1}{2}\\sum_{j=0}^\\infty (j+1) \\mathbb{P}(T_a\\wedge T_b=j|S_0=x+1)+\\frac{1}{2}\\sum_{j=0}^\\infty (j+1)\\mathbb{P}(T_a\\wedge T_b=j|S_0=x-1)\\\\\n&=\\frac{1}{2}\\sum_{j=0}^\\infty j \\mathbb{P}(T_a\\wedge T_b=j|S_0=x+1)+\\frac{1}{2}\\sum_{j=0}^\\infty j\\mathbb{P}(T_a\\wedge T_b=j|S_0=x-1)+1\\\\\n&=1+\\frac{1}{2}\\mathbb{E}(T_a\\wedge T_b|S_0=x+1)+\\frac{1}{2}\\mathbb{E}(T_a\\wedge T_b|S_0=x-1)\\\\\n&=1+\\frac{1}{2}(d_{x+1}+d_{x-1}).\n\\end{align*} % Cherchons une solution particuli√®re sous la forme cn^2 (puisque les constantes et les multiples de n sont solutions de l‚Äô√©quation homog√®ne) : \\begin{align*}\ncn^2=\\frac{1}{2}(c(n+1)^2+c(n-1)^2)+1\n&\\Leftrightarrow\ncn^2=\\frac{1}{2}(cn^2+c+2cn+cn^2+c-2cn)+1\\\\\n&\\Leftrightarrow\ncn^2=cn^2+c+1\\\\\n&\\Leftrightarrow\nc=-1.\n\\end{align*} % Ansi, d est la somme d‚Äôune solution homog√®ne et d‚Äôune solution particuli√®re, donc d_x=\\alpha +\\beta x-x^2. En utilisant d_a=d_b=0, on trouve \\alpha et \\beta \\begin{align*}\n\\left\\{\\begin{array}{rcl}\nd_a=0&=&\\alpha+\\beta a-a^2\\\\\nd_b=0&=&\\alpha+\\beta b-b^2\n\\end{array}\n\\right.\n\\Leftrightarrow\n\\left\\{\\begin{array}{rcl}\n\\alpha&=-ab,\\\\\n\\beta&=a+b.\n\\end{array}\n\\right.\n\\end{align*} Finalement, on obtient que d_x=-ab+x(a+b)-x^2.\nEn particulier, si x=0 et b tend vers -\\infty, on obtient que \\mathbb{E}[T_a]=+\\infty. On sait que la marche partant de 0 atteindra a presque s√ªrement en temps fini, cependant ce temps est en moyenne infini.",
    "crumbs": [
      "Cours",
      "Marches al√©atoires"
    ]
  },
  {
    "objectID": "Courses/marche_aleatoire.html#sec-chap03-MB",
    "href": "Courses/marche_aleatoire.html#sec-chap03-MB",
    "title": "Marches al√©atoires",
    "section": "De la marche al√©atoire vers le mouvement brownien",
    "text": "De la marche al√©atoire vers le mouvement brownien\nOn s‚Äôint√©resse maintenant √† une autre forme de comportement asymptotique de la marche al√©atoire. Nous allons la renormaliser pour la contraindre √† rester dans l‚Äôintervalle de temps [0,1]. Pour cela, pour tout N\\in\\mathbb{N}^*, on pose \nB^N_t  =\\frac{1}{\\sqrt{N}} S_{\\lfloor Nt \\rfloor} = \\frac{1}{\\sqrt{N}} \\sum_{k=1}^{\\lfloor Nt \\rfloor} X_k.\n\nAlors la suite (B^N_t) converge en loi lorsque N tend vers l‚Äôinfini, et c‚Äôest √©galement le cas lorsqu‚Äôon prend plusieurs temps t_i.\n\nProposition 11 Pour tous t_0=0&lt;t_1 &lt;\\dots&lt; t_p, le vecteur (B^N_{t_1},\\dots,B^N_{t_p}) converge en loi vers un vecteur (B_{t_1},\\dots,B_{t_p}) tel que pour tout i \\in\\{1,\\dots,p\\}, B_{t_{i}}-B_{t_{i-1}} est ind√©pendant de B_{t_{i-1}} et suit une loi normale \\mathcal{N}(0, t_i - t_{i-1} )\n\n\nPreuve. On a \nB^N_{t_i} - B^N_{t_{i-1}}  =\\frac{1}{\\sqrt{N}} S_{\\lfloor Nt \\rfloor} = \\frac{1}{\\sqrt{N}} \\sum_{k=\\lfloor N t_{i-1} \\rfloor + 1}^{\\lfloor N t_i \\rfloor} X_k\n donc les accroissements du vecteur (B^N_{t_1},\\dots,B^N_{t_p}) sont ind√©pendant. Il suffit maintenant de montrer qu‚Äôils convergent vers la loi normale. On a \nB^N_{t_i} - B^N_{t_{i-1}}  = \\sqrt{\\frac{\\lfloor N t_i \\rfloor - \\lfloor N t_{i-1} \\rfloor }{N}}  \\frac{1}{\\sqrt{\\lfloor N t_i \\rfloor - \\lfloor N t_{i-1} \\rfloor }} \\sum_{k=\\lfloor N t_{i-1} \\rfloor + 1}^{\\lfloor N t_i \\rfloor}   X_k \\to \\sqrt{t_i - t_{i-1}} \\mathcal{N}(0,1),\n par le th√©or√®me central limite appliqu√© √† la suite (X_k).\n\nCe r√©sultat permet de poser la d√©finition suivante.\n\nD√©finition 3 (Mouvement Brownien) On appelle mouvement Brownien standard, tout processus (B_t)_{t\\geq 0} satisfaisant les trois points suivants\n\nB_0=0,\n\\forall t\\geq s, B_{t} - B_s indepandant de (B_r)_{r \\leq s},\n\\forall t\\geq s, B_{t} - B_s \\sim \\mathcal{N}(0, t-s).\n\n\nLe th√©or√®me pr√©c√©dent ne donne pas l‚Äôexistence d‚Äôun tel processus, mais si un tel processus existe, il donne la convergence des lois marginales fini-dimensionnelles de la marche al√©atoire renormalis√©e vers les lois marginales fini-dimensionnelles du mouvement Brownien. On admettra qu‚Äôil existe bien un mouvement Brownien. On peut montrer que la marche al√©atoire converge pour la norme infinie vers le mouvement Brownien dans l‚Äôespace des variables al√©atoires √† valeurs dans les fonctions continues de [0,1] dans \\R. Ce r√©sultat est connu sous le nom de th√©or√®me de Donsker. Pour simuler un mouvement Brownien, on simule en fait une marche al√©atoire et on la renormalise comme ci-dessus avec N assez grand.",
    "crumbs": [
      "Cours",
      "Marches al√©atoires"
    ]
  },
  {
    "objectID": "Courses/loi_normale1D.html",
    "href": "Courses/loi_normale1D.html",
    "title": "Loi normale: cas univari√©",
    "section": "",
    "text": "On rappelle que la loi normale de param√®tres \\mu \\in \\mathbb{R} et \\nu &gt; 0 a une densit√© donn√©e pour tout x \\in \\mathbb{R} par \n    \\varphi_{\\mu, \\nu}(x)=\\frac{1}{\\sqrt{2 \\pi \\nu}}\\exp\\Big(-\\frac{(x-\\mu)^2}{2\\nu}\\Big)\\enspace.\n\nOn note X \\sim \\mathcal{N}(\\mu, \\nu), si X est une variable al√©atoire ayant pour densit√© \\varphi_{\\mu, \\nu}. Notons que si X \\sim \\mathcal{N}(\\mu,\\nu), alors X a pour esp√©rance \\mu et pour variance \\nu. Le cas particulier \\mu=0 et \\nu=1 correspond √† une variable al√©atoire dite centr√©e r√©duite.\nPlotly = require('plotly.js-dist');\ndists = require( 'https://cdn.jsdelivr.net/gh/stdlib-js/stats-base-dists@umd/browser.js' );\njstat = require('jstat');\nmath = require(\"mathjs\");\n// see source here: https://github.com/stdlib-js/stats-base-dists/tree/umd\n// continuous case\n\nviewof inputs = Inputs.form([\n  Inputs.range([-2, 2], {label: tex`\\mu`, step: 0.1}, {value: 0}),\n  Inputs.range([0.1, 10], {label: tex`\\nu`, step: 0.1, value: 1}),\n  Inputs.range([1, 1000], {label: tex`n`, step: 1}),\n  Inputs.button(\"Re-Tirage\")\n])\n\nmu = inputs[0];\nnu = inputs[1];\nn_samples = inputs[2];\n\n{\n\nfunction mvnpdf(x, mu, nu){\n    return 1.0 / (2*math.pi *nu)**(0.5)* math.exp(-0.5*(x-mu)**2 / nu);\n}\n\n\nfunction normal_rng(mu, nu, n=100){\n    var samples = Array.from({length: n}, () =&gt; {\n        var x = jstat.normal.sample(mu, nu**0.5);\n        return x;\n    });\n\n    return samples;\n}\n\nvar samples = normal_rng(mu, nu, 1000);\nvar samples_jitter = normal_rng(0, 0.03, 1000);\nvar npoints=500, mini = -10, maxi=10, x = new Array(npoints), z = new Array(npoints), i, j;\n\n//  Densit√©:s\nfor(var i = 0; i &lt; npoints; i++) {\n    x[i] = mini + i * (maxi - mini) / (npoints - 1);\n    z[i] = mvnpdf(x[i], mu, nu);\n    }\n\n{\nvar trace1 = {\n        x: samples.slice(0, n_samples),\n        y: samples_jitter.slice(0, n_samples),\n        mode: 'markers',\n        type: 'scatter',\n        marker: {\n            color: 'rgba(0,0,0,0.5)',\n            size: 5,\n        },\n\n  }\n\nvar trace22 = {\n        x: x,\n        y: z,\n        type: \"scatter\",\n        mode: \"lines\",\n        name: 'Pdf',\n        line: {color: 'black'},\n        yaxis: 'y2',\n        xaxis: 'x2',\n        }\n\n\nvar data = [\n  trace1,\n  trace22,\n  ];\n\n\nvar layout = {\n    width: 600,\n    yaxis: {domain: [0, 0.2],\n            showticklabels: false,\n            range: [-0.6, 0.6],\n            autorange: false},\n    xaxis2: {matches: 'x',\n              range: [-10, 10],\n          autorange: false},\n    yaxis2: {domain: [0.29, 0.99]},\n\n  showlegend: false,\n\n};\n\n    var config = {responsive: true}\n    const div = DOM.element('div');\n    Plotly.react(div, data, layout, config);\n    return div;\n    }\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOn parle aussi souvent de loi gaussienne, en hommage au math√©maticien Carl Friedrich Gauss, le prince des math√©maticiens1.\n1¬†Carl Friedrich Gauss: (1777-1855) math√©maticien, astronome et physicien n√© √† Brunswick, directeur de l‚Äôobservatoire de G√∂ttingen de 1807 jusqu‚Äô√† sa mort en 1855 La loi normale v√©rifie la propri√©t√© de stabilit√© par transformation affine : si X \\sim \\mathcal{N}(\\mu, \\nu) et si (a,b) \\in \\mathbb{R}^* \\times \\mathbb{R}, alors la variable al√©atoire a X + b suit une loi normale \\mathcal{N}(a\\mu + b, a^2 \\nu). On peut donc facilement passer d‚Äôune loi normale centr√©e r√©duite √† une loi normale quelconque via une transformation affine :\n\nsi X \\sim \\mathcal{N}(0,1), alors \\sqrt{\\nu} X + \\mu \\sim \\mathcal{N}(\\mu, \\nu),\nsi X \\sim \\mathcal{N}(\\mu, \\nu), alors (X-\\mu)/\\sqrt{\\nu} \\sim \\mathcal{N}(0,1).\n\nAinsi, savoir simuler une loi normale centr√©e r√©duite, permet de simuler n‚Äôimporte quelle loi normale.\n\nProposition 1 (Fonction caract√©ristique de la loi normale) La fonction caract√©ristique d‚Äôune variable al√©atoire X \\sim \\mathcal{N}(\\mu, \\nu) est donn√©e pour tout t \\in \\mathbb{R} par \n\\begin{align*}\n\\phi_{\\mu,\\nu}(t) & \\triangleq \\mathbb{E}(e^{i t X})  \\\\\n& = \\exp\\Big( i \\mu t - \\frac{\\nu t^2}{2}\\Big)\\enspace.\n\\end{align*}\n\n\n\nPreuve. on remarque d‚Äôabord que si X \\sim \\mathcal{N}(0,1) alors pour tout z \\in \\mathbb{R}, on a\n\n\\begin{align*}\n\\mathbb{E}[e^{zX}]&=\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^\\infty e^{-\\frac12x^2}e^{zx}\\,dx\\\\\n&= \\frac{e^{\\frac12z^2}}{\\sqrt{2\\pi}}\\int_{-\\infty}^\\infty e^{-\\frac12(x-z)^2}\\,dx\\\\\n&=\\frac{e^{\\frac12z^2}}{\\sqrt{2\\pi}}\\int_{-\\infty}^\\infty e^{-\\frac12y^2}\\,dy\\\\\n&=e^{\\frac12z^2}.\n\\end{align*}\n En utilisant le th√©or√®me de prolongement analytique (voir par exemple (Th√©or√®me I.10., Queff√©lec et Zuily 2013) on peut donc √©tendre cette formule √† tout z \\in \\mathbb{C}, et particulier au cas z=it pour t \\in \\mathbb{R}. On obtient alors \\phi_{\\mu,\\nu}(t)=e^{-\\frac{t^2}{2}}. Enfin, on utilise la lin√©arit√© de l‚Äôesp√©rance pour obtenir le r√©sultat pour X \\sim \\mathcal{N}(\\mu,\\nu). En effet, si X \\sim \\mathcal{N}(0,1), alors X=\\mu+\\sqrt{\\nu}Z avec Z \\sim \\mathcal{N}(0,1), et donc \\phi_{\\mu,\\nu}(t)=e^{i\\mu t}\\phi_{0,1}(\\sqrt{\\nu}t)=e^{i\\mu t-\\frac{\\nu t^2}{2}}.\n\nQueff√©lec, H., et C. Zuily. 2013. Analyse pour l‚Äôagr√©gation. Dunod.\n\n\n\n\n\n\n\n\n\nUne mauvaise piste pour simuler une loi normale\n\n\n\nOn peut simuler une loi normale √† partir de variables al√©atoires uniformes U_1, \\dots, U_n iid en appliquant le th√©or√®me central limite √† \n    \\frac{U_1 + \\cdots + U_n - n/2}{\\sqrt{n/12}}\\,.\n Cependant, cette m√©thode ne donne qu‚Äôune approximation d‚Äôune loi normale. Par ailleurs, la vitesse de convergence √©tant relativement lente (de l‚Äôordre de \\sqrt n), il faudra simuler beaucoup de variables al√©atoires uniformes pour avoir une approximation correcte, ce qui demande un temps de calcul assez √©lev√©.\n\n\n\n\nLe th√©or√®me suivant permet de passer de la loi d‚Äôun couple (X,Y) √† celle de (U,V) = \\phi(X,Y), o√π \\phi est un C^1-diff√©omorphisme, c‚Äôest-√†-dire une application bijective dont la r√©ciproque est √©galement de classe C^1.\nPour cela rappelons que la jacobienne de \\phi^{-1} correspond √† la matrice (application lin√©aire) des d√©riv√©es partielles. Ainsi, si \\phi(x,y) = (u,v) \\iff (x,y) = \\phi^{-1}(u,v), alors \n\\begin{align*}\n{\\rm{J}}_{\\phi^{-1}}: (u,v) & \\mapsto\n\\begin{pmatrix}\n  \\frac{\\partial x}{\\partial u} & \\frac{\\partial x}{\\partial v}    \\\\\n  \\frac{\\partial y}{\\partial u} & \\frac{\\partial y}{\\partial v}\n\\end{pmatrix} \\enspace.\n\\end{align*}\n\n\nTh√©or√®me 1 (Caract√©risation de la loi d‚Äôune variable al√©atoire r√©elle) Soit (X,Y) un vecteur al√©atoire de densit√© f_{(X,Y)} d√©finie sur l‚Äôouvert A \\subset \\mathbb{R}^2 et \\phi : A \\to B \\subset \\mathbb{R}^2 un C^1-diff√©omorphisme. Le vecteur al√©atoire (U,V)=\\phi(X,Y) admet alors pour densit√© f_{(U,V)} d√©finie sur B pour tout (u,v) \\in \\mathbb{R}^2 par \n\\begin{align*}\n    (u,v) & \\mapsto\n    f_{(X,Y)} (\\phi^{-1}(u,v)) |\\det ({\\rm{J}}_{\\phi^{-1}} (u,v))| {1\\hspace{-3.8pt} 1}_B(u,v)\\enspace.\n\\end{align*}\n\n\nOn a √©nonc√© le r√©sultat en dimension 2 par simplicit√©. Il s‚Äô√©tend bien √©videmment √† une dimension d quelconque. En particulier, pour d=1, on retrouve le changement de variable classique dans le cas de l‚Äôint√©gration d‚Äôune fonction √† valeurs r√©elles.\n\nPreuve. On rappelle que la loi de (U,V) est caract√©ris√©e par les quantit√©s \\mathbb{E}[h(U,V)] pour tout h : \\mathbb{R}^2 \\to \\mathbb{R} mesurable born√©e. On consid√®re donc une telle fonction h et on applique la formule de transfert : \n\\begin{align*}\n  \\mathbb{E}[h(U,V)] &\n  =\\mathbb{E}[h(\\phi(X,Y))]\\\\\n& = \\int_{\\mathbb{R}^2} h(\\phi(x,y)) f_{(X,Y)}(x,y) \\, dx dy \\\\\n& = \\int_{A} h(\\phi(x,y)) f_{(X,Y)}(x,y) \\, d x d y\\enspace.\n\\end{align*}\n On applique alors la formule du changement de variables vu en th√©orie de l‚Äôint√©gration avec (u,v) = \\phi(x,y) \\iff \\phi^{-1}(u,v) = (x,y) : \n\\begin{align*}\n  & \\mathbb{E}[h(U,V)]\\\\\n  & = \\!\\int_{B}  \\!\\!\\! h(u,v) f_{(X,Y)}(\\phi^{-1}(u,v)) |\\det ({\\rm{J}}_{\\phi^{-1}} (u,v))| \\, d u d v\\\\\n  & = \\!\\int_{\\mathbb{R}^2} \\!\\!\\!\\! h(u,v) f_{(X,Y)}(\\phi^{-1}(u,v)) |\\det ({\\rm{J}}_{\\phi^{-1}} (u,v))| {1\\hspace{-3.8pt} 1}_B(u,v)\\, d u d v .\n\\end{align*}\n  ce qui donne le r√©sultat voulu.\n\n\n\nExemple 1 (Exemple : loi de \\cos(X), avec X \\sim \\mathcal{U}(]0,\\pi[)) Donnons un exemple dans le cas r√©el. On consid√®re une variable al√©atoire X de loi uniforme sur ]0,\\pi[. Sa densit√© est donn√©e par f_X(x) = {1\\hspace{-3.8pt} 1}_{]0,\\pi[}(x)/\\pi. On pose U = \\cos(X) et on souhaite d√©terminer la loi de U.\nOn applique le th√©or√®me pr√©c√©dent avec la fonction \\phi^{-1}(u) = \\arccos(u) sur ]-1,1[. La densit√© de U est alors donn√©e pour tout u \\in \\mathbb{R} par \n\\begin{align*}\n  f_U(u)\n& = \\frac{{1\\hspace{-3.8pt} 1}_{]0,\\pi[}(\\arccos(u))}{\\pi} \\Big| \\frac{-1}{\\sqrt{1-u^2}} \\Big| {1\\hspace{-3.8pt} 1}_{]-1,1[}(u)\\\\\n& = \\frac{1}{\\pi \\sqrt{1-u^2}} {1\\hspace{-3.8pt} 1}_{]-1,1[}(u)\\enspace.\n\\end{align*}",
    "crumbs": [
      "Cours",
      "Loi normale: cas univari√©"
    ]
  },
  {
    "objectID": "Courses/loi_normale1D.html#d√©finitions-et-propri√©t√©s-de-la-loi-normale",
    "href": "Courses/loi_normale1D.html#d√©finitions-et-propri√©t√©s-de-la-loi-normale",
    "title": "Loi normale: cas univari√©",
    "section": "",
    "text": "On rappelle que la loi normale de param√®tres \\mu \\in \\mathbb{R} et \\nu &gt; 0 a une densit√© donn√©e pour tout x \\in \\mathbb{R} par \n    \\varphi_{\\mu, \\nu}(x)=\\frac{1}{\\sqrt{2 \\pi \\nu}}\\exp\\Big(-\\frac{(x-\\mu)^2}{2\\nu}\\Big)\\enspace.\n\nOn note X \\sim \\mathcal{N}(\\mu, \\nu), si X est une variable al√©atoire ayant pour densit√© \\varphi_{\\mu, \\nu}. Notons que si X \\sim \\mathcal{N}(\\mu,\\nu), alors X a pour esp√©rance \\mu et pour variance \\nu. Le cas particulier \\mu=0 et \\nu=1 correspond √† une variable al√©atoire dite centr√©e r√©duite.\nPlotly = require('plotly.js-dist');\ndists = require( 'https://cdn.jsdelivr.net/gh/stdlib-js/stats-base-dists@umd/browser.js' );\njstat = require('jstat');\nmath = require(\"mathjs\");\n// see source here: https://github.com/stdlib-js/stats-base-dists/tree/umd\n// continuous case\n\nviewof inputs = Inputs.form([\n  Inputs.range([-2, 2], {label: tex`\\mu`, step: 0.1}, {value: 0}),\n  Inputs.range([0.1, 10], {label: tex`\\nu`, step: 0.1, value: 1}),\n  Inputs.range([1, 1000], {label: tex`n`, step: 1}),\n  Inputs.button(\"Re-Tirage\")\n])\n\nmu = inputs[0];\nnu = inputs[1];\nn_samples = inputs[2];\n\n{\n\nfunction mvnpdf(x, mu, nu){\n    return 1.0 / (2*math.pi *nu)**(0.5)* math.exp(-0.5*(x-mu)**2 / nu);\n}\n\n\nfunction normal_rng(mu, nu, n=100){\n    var samples = Array.from({length: n}, () =&gt; {\n        var x = jstat.normal.sample(mu, nu**0.5);\n        return x;\n    });\n\n    return samples;\n}\n\nvar samples = normal_rng(mu, nu, 1000);\nvar samples_jitter = normal_rng(0, 0.03, 1000);\nvar npoints=500, mini = -10, maxi=10, x = new Array(npoints), z = new Array(npoints), i, j;\n\n//  Densit√©:s\nfor(var i = 0; i &lt; npoints; i++) {\n    x[i] = mini + i * (maxi - mini) / (npoints - 1);\n    z[i] = mvnpdf(x[i], mu, nu);\n    }\n\n{\nvar trace1 = {\n        x: samples.slice(0, n_samples),\n        y: samples_jitter.slice(0, n_samples),\n        mode: 'markers',\n        type: 'scatter',\n        marker: {\n            color: 'rgba(0,0,0,0.5)',\n            size: 5,\n        },\n\n  }\n\nvar trace22 = {\n        x: x,\n        y: z,\n        type: \"scatter\",\n        mode: \"lines\",\n        name: 'Pdf',\n        line: {color: 'black'},\n        yaxis: 'y2',\n        xaxis: 'x2',\n        }\n\n\nvar data = [\n  trace1,\n  trace22,\n  ];\n\n\nvar layout = {\n    width: 600,\n    yaxis: {domain: [0, 0.2],\n            showticklabels: false,\n            range: [-0.6, 0.6],\n            autorange: false},\n    xaxis2: {matches: 'x',\n              range: [-10, 10],\n          autorange: false},\n    yaxis2: {domain: [0.29, 0.99]},\n\n  showlegend: false,\n\n};\n\n    var config = {responsive: true}\n    const div = DOM.element('div');\n    Plotly.react(div, data, layout, config);\n    return div;\n    }\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOn parle aussi souvent de loi gaussienne, en hommage au math√©maticien Carl Friedrich Gauss, le prince des math√©maticiens1.\n1¬†Carl Friedrich Gauss: (1777-1855) math√©maticien, astronome et physicien n√© √† Brunswick, directeur de l‚Äôobservatoire de G√∂ttingen de 1807 jusqu‚Äô√† sa mort en 1855 La loi normale v√©rifie la propri√©t√© de stabilit√© par transformation affine : si X \\sim \\mathcal{N}(\\mu, \\nu) et si (a,b) \\in \\mathbb{R}^* \\times \\mathbb{R}, alors la variable al√©atoire a X + b suit une loi normale \\mathcal{N}(a\\mu + b, a^2 \\nu). On peut donc facilement passer d‚Äôune loi normale centr√©e r√©duite √† une loi normale quelconque via une transformation affine :\n\nsi X \\sim \\mathcal{N}(0,1), alors \\sqrt{\\nu} X + \\mu \\sim \\mathcal{N}(\\mu, \\nu),\nsi X \\sim \\mathcal{N}(\\mu, \\nu), alors (X-\\mu)/\\sqrt{\\nu} \\sim \\mathcal{N}(0,1).\n\nAinsi, savoir simuler une loi normale centr√©e r√©duite, permet de simuler n‚Äôimporte quelle loi normale.\n\nProposition 1 (Fonction caract√©ristique de la loi normale) La fonction caract√©ristique d‚Äôune variable al√©atoire X \\sim \\mathcal{N}(\\mu, \\nu) est donn√©e pour tout t \\in \\mathbb{R} par \n\\begin{align*}\n\\phi_{\\mu,\\nu}(t) & \\triangleq \\mathbb{E}(e^{i t X})  \\\\\n& = \\exp\\Big( i \\mu t - \\frac{\\nu t^2}{2}\\Big)\\enspace.\n\\end{align*}\n\n\n\nPreuve. on remarque d‚Äôabord que si X \\sim \\mathcal{N}(0,1) alors pour tout z \\in \\mathbb{R}, on a\n\n\\begin{align*}\n\\mathbb{E}[e^{zX}]&=\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^\\infty e^{-\\frac12x^2}e^{zx}\\,dx\\\\\n&= \\frac{e^{\\frac12z^2}}{\\sqrt{2\\pi}}\\int_{-\\infty}^\\infty e^{-\\frac12(x-z)^2}\\,dx\\\\\n&=\\frac{e^{\\frac12z^2}}{\\sqrt{2\\pi}}\\int_{-\\infty}^\\infty e^{-\\frac12y^2}\\,dy\\\\\n&=e^{\\frac12z^2}.\n\\end{align*}\n En utilisant le th√©or√®me de prolongement analytique (voir par exemple (Th√©or√®me I.10., Queff√©lec et Zuily 2013) on peut donc √©tendre cette formule √† tout z \\in \\mathbb{C}, et particulier au cas z=it pour t \\in \\mathbb{R}. On obtient alors \\phi_{\\mu,\\nu}(t)=e^{-\\frac{t^2}{2}}. Enfin, on utilise la lin√©arit√© de l‚Äôesp√©rance pour obtenir le r√©sultat pour X \\sim \\mathcal{N}(\\mu,\\nu). En effet, si X \\sim \\mathcal{N}(0,1), alors X=\\mu+\\sqrt{\\nu}Z avec Z \\sim \\mathcal{N}(0,1), et donc \\phi_{\\mu,\\nu}(t)=e^{i\\mu t}\\phi_{0,1}(\\sqrt{\\nu}t)=e^{i\\mu t-\\frac{\\nu t^2}{2}}.\n\nQueff√©lec, H., et C. Zuily. 2013. Analyse pour l‚Äôagr√©gation. Dunod.\n\n\n\n\n\n\n\n\n\nUne mauvaise piste pour simuler une loi normale\n\n\n\nOn peut simuler une loi normale √† partir de variables al√©atoires uniformes U_1, \\dots, U_n iid en appliquant le th√©or√®me central limite √† \n    \\frac{U_1 + \\cdots + U_n - n/2}{\\sqrt{n/12}}\\,.\n Cependant, cette m√©thode ne donne qu‚Äôune approximation d‚Äôune loi normale. Par ailleurs, la vitesse de convergence √©tant relativement lente (de l‚Äôordre de \\sqrt n), il faudra simuler beaucoup de variables al√©atoires uniformes pour avoir une approximation correcte, ce qui demande un temps de calcul assez √©lev√©.\n\n\n\n\nLe th√©or√®me suivant permet de passer de la loi d‚Äôun couple (X,Y) √† celle de (U,V) = \\phi(X,Y), o√π \\phi est un C^1-diff√©omorphisme, c‚Äôest-√†-dire une application bijective dont la r√©ciproque est √©galement de classe C^1.\nPour cela rappelons que la jacobienne de \\phi^{-1} correspond √† la matrice (application lin√©aire) des d√©riv√©es partielles. Ainsi, si \\phi(x,y) = (u,v) \\iff (x,y) = \\phi^{-1}(u,v), alors \n\\begin{align*}\n{\\rm{J}}_{\\phi^{-1}}: (u,v) & \\mapsto\n\\begin{pmatrix}\n  \\frac{\\partial x}{\\partial u} & \\frac{\\partial x}{\\partial v}    \\\\\n  \\frac{\\partial y}{\\partial u} & \\frac{\\partial y}{\\partial v}\n\\end{pmatrix} \\enspace.\n\\end{align*}\n\n\nTh√©or√®me 1 (Caract√©risation de la loi d‚Äôune variable al√©atoire r√©elle) Soit (X,Y) un vecteur al√©atoire de densit√© f_{(X,Y)} d√©finie sur l‚Äôouvert A \\subset \\mathbb{R}^2 et \\phi : A \\to B \\subset \\mathbb{R}^2 un C^1-diff√©omorphisme. Le vecteur al√©atoire (U,V)=\\phi(X,Y) admet alors pour densit√© f_{(U,V)} d√©finie sur B pour tout (u,v) \\in \\mathbb{R}^2 par \n\\begin{align*}\n    (u,v) & \\mapsto\n    f_{(X,Y)} (\\phi^{-1}(u,v)) |\\det ({\\rm{J}}_{\\phi^{-1}} (u,v))| {1\\hspace{-3.8pt} 1}_B(u,v)\\enspace.\n\\end{align*}\n\n\nOn a √©nonc√© le r√©sultat en dimension 2 par simplicit√©. Il s‚Äô√©tend bien √©videmment √† une dimension d quelconque. En particulier, pour d=1, on retrouve le changement de variable classique dans le cas de l‚Äôint√©gration d‚Äôune fonction √† valeurs r√©elles.\n\nPreuve. On rappelle que la loi de (U,V) est caract√©ris√©e par les quantit√©s \\mathbb{E}[h(U,V)] pour tout h : \\mathbb{R}^2 \\to \\mathbb{R} mesurable born√©e. On consid√®re donc une telle fonction h et on applique la formule de transfert : \n\\begin{align*}\n  \\mathbb{E}[h(U,V)] &\n  =\\mathbb{E}[h(\\phi(X,Y))]\\\\\n& = \\int_{\\mathbb{R}^2} h(\\phi(x,y)) f_{(X,Y)}(x,y) \\, dx dy \\\\\n& = \\int_{A} h(\\phi(x,y)) f_{(X,Y)}(x,y) \\, d x d y\\enspace.\n\\end{align*}\n On applique alors la formule du changement de variables vu en th√©orie de l‚Äôint√©gration avec (u,v) = \\phi(x,y) \\iff \\phi^{-1}(u,v) = (x,y) : \n\\begin{align*}\n  & \\mathbb{E}[h(U,V)]\\\\\n  & = \\!\\int_{B}  \\!\\!\\! h(u,v) f_{(X,Y)}(\\phi^{-1}(u,v)) |\\det ({\\rm{J}}_{\\phi^{-1}} (u,v))| \\, d u d v\\\\\n  & = \\!\\int_{\\mathbb{R}^2} \\!\\!\\!\\! h(u,v) f_{(X,Y)}(\\phi^{-1}(u,v)) |\\det ({\\rm{J}}_{\\phi^{-1}} (u,v))| {1\\hspace{-3.8pt} 1}_B(u,v)\\, d u d v .\n\\end{align*}\n  ce qui donne le r√©sultat voulu.\n\n\n\nExemple 1 (Exemple : loi de \\cos(X), avec X \\sim \\mathcal{U}(]0,\\pi[)) Donnons un exemple dans le cas r√©el. On consid√®re une variable al√©atoire X de loi uniforme sur ]0,\\pi[. Sa densit√© est donn√©e par f_X(x) = {1\\hspace{-3.8pt} 1}_{]0,\\pi[}(x)/\\pi. On pose U = \\cos(X) et on souhaite d√©terminer la loi de U.\nOn applique le th√©or√®me pr√©c√©dent avec la fonction \\phi^{-1}(u) = \\arccos(u) sur ]-1,1[. La densit√© de U est alors donn√©e pour tout u \\in \\mathbb{R} par \n\\begin{align*}\n  f_U(u)\n& = \\frac{{1\\hspace{-3.8pt} 1}_{]0,\\pi[}(\\arccos(u))}{\\pi} \\Big| \\frac{-1}{\\sqrt{1-u^2}} \\Big| {1\\hspace{-3.8pt} 1}_{]-1,1[}(u)\\\\\n& = \\frac{1}{\\pi \\sqrt{1-u^2}} {1\\hspace{-3.8pt} 1}_{]-1,1[}(u)\\enspace.\n\\end{align*}",
    "crumbs": [
      "Cours",
      "Loi normale: cas univari√©"
    ]
  },
  {
    "objectID": "Courses/loi_normale1D.html#m√©thode-de-box-m√ºller",
    "href": "Courses/loi_normale1D.html#m√©thode-de-box-m√ºller",
    "title": "Loi normale: cas univari√©",
    "section": "M√©thode de Box-M√ºller",
    "text": "M√©thode de Box-M√ºller\nUn cas particulier fondamental de la formule de changement de variables concerne le passage en coordonn√©es polaires. Cette transformation est d√©finie via l‚Äôapplication \n    \\begin{array}{ccccc}\n        \\phi^{-1} & : & ]0, \\infty[ \\times ]0, 2\\pi[ & \\to   & \\mathbb{R}^2 \\setminus ([0,\\infty[ \\times \\{0\\}) \\\\\n                  &   & \\begin{pmatrix} r \\\\ \\theta \\end{pmatrix}                   & \\mapsto & \\begin{pmatrix} r \\cos(\\theta) \\\\ r \\sin(\\theta) \\end{pmatrix}\\,.\n    \\end{array}\n L‚Äôexpression de \\phi ne nous sera pas utile. On peut tout de m√™me la donner au passage :\n\n    \\begin{array}{ccccc}\n        \\phi & : & \\mathbb{R}^2 \\setminus ([0,\\infty[ \\times \\{0\\}) & \\to     & ]0, \\infty[ \\times ]0, 2\\pi[                                                      \\\\\n             &   & \\begin{pmatrix} x \\\\ y \\end{pmatrix}                                            & \\mapsto & \\begin{pmatrix}\\sqrt{x^2+y^2} \\\\ 2 \\arctan \\Big( \\frac{y}{x+\\sqrt{x^2+y^2}} \\Big)\\end{pmatrix}\\,.\n    \\end{array}\n\nIci, le jacobien de \\phi^{-1} est la matrice \n    {\\rm{J}}_{\\phi^{-1}} (r,\\theta)\n    =\n    \\begin{pmatrix}\n        \\cos(\\theta) & -r \\sin(\\theta) \\\\\n        \\sin(\\theta) & r \\cos(\\theta)\n    \\end{pmatrix}\\,,\n qui v√©rifie |\\det({\\rm{J}}_{\\phi^{-1}} (r, \\theta))| = r. Ainsi, si (X,Y) a pour densit√© f_{(X,Y)}, alors (R, \\Theta) = \\phi(X,Y) a pour densit√© \n  f_{(R, \\Theta)} (r, \\theta)\n  = r\\cdot f_{(X,Y)}(r \\cos(\\theta), r \\sin(\\theta)) \\!\\cdot\\! {1\\hspace{-3.8pt} 1}_{]0, \\infty[}(r)  {1\\hspace{-3.8pt} 1}_{]0, 2 \\pi[}(\\theta).\n\nDans le cas o√π X et Y sont des variables al√©atoires gaussiennes ind√©pendantes, on obtient le r√©sultat suivant.\n\nTh√©or√®me 2 (M√©thode de Box-M√ºller) Soit X et Y deux variables al√©atoires ind√©pendantes de loi normales centr√©es r√©duites : X,Y \\sim \\mathcal{N}(0,1). Le couple de variables al√©atoires polaires (R, \\Theta) = \\phi(X,Y) a pour densit√© \n            f_{R, \\Theta}(r,\\theta)\n            = \\Big( r \\cdot e^{-\\tfrac{r^2}{2}} {1\\hspace{-3.8pt} 1}_{]0, \\infty[}(r) \\Big) \\bigg(\\frac{{1\\hspace{-3.8pt} 1}_{]0, 2 \\pi[}(\\theta)}{2 \\pi} \\bigg)\\,.\n Autrement dit, elles sont ind√©pendantes, l‚Äôangle \\Theta suit une loi uniforme sur ]0, 2\\pi[ et la distance √† l‚Äôorigine R suit une loi de Rayleigh donn√©e par la densit√© \n    f_R(r) =  r \\cdot e^{-r^2/2} {1\\hspace{-3.8pt} 1}_{]0, \\infty[}(r)\\,, \\quad r &gt; 0\\,.\n\n\n\nPreuve. La densit√© du couple (X,Y) est donn√©e par \n  f_{(X,Y)}(x,y) = \\frac{1}{2\\pi} e^{-\\frac{x^2+y^2}{2}}\\,, \\quad x,y \\in \\mathbb{R}\\,.\n Le th√©or√®me pr√©c√©dent donne alors la densit√© de (R, \\Theta) : \n\\begin{align*}\n  f_{(R, \\Theta)} (r, \\theta) &\n  = r\\cdot f_{(X,Y)}(r \\cos(\\theta), r \\sin(\\theta)) \\!\\cdot\\! {1\\hspace{-3.8pt} 1}_{]0, \\infty[}(r)  {1\\hspace{-3.8pt} 1}_{]0, 2 \\pi[}(\\theta)\\\\\n  &= r \\cdot\\frac{1}{2\\pi} e^{-\\frac{r^2}{2}} \\cdot  {1\\hspace{-3.8pt} 1}_{]0, \\infty[}(r) {1\\hspace{-3.8pt} 1}_{]0, 2 \\pi[}(\\theta)\\,,\n\\end{align*}\n ce qui conclut la preuve.\n\nSi R suit une loi de Rayleigh alors \\sqrt{-2 \\log(U)} a la m√™me loi que R, o√π U est une variable al√©atoire de loi uniforme sur ]0,1[: Pour cela il suffit de remarquer que pour tout x &gt; 0, F_R(x)=\\mathbb{P}(R\\leq x) = 1-\\exp(-\\tfrac{x^2}{2}), et donc que pour tout q \\in ]0,1[, F_R^{^\\leftarrow}(q)=\\sqrt{-2\\log(1-q)} et donc \\sqrt{-2\\log(1-U)}, puis \\sqrt{-2\\log(U)} on donc la m√™me loi que R.\nL‚Äôalgorithme de Box-M√ºller s‚Äôen suit: si U et V sont des v.a. ind√©pendantes de loi uniforme sur [0,1] et qu‚Äôon d√©finit X et Y par \n\\begin{cases}\n  X = \\sqrt{-2 \\log(U)} \\cos(2\\pi V)\\\\\n  Y = \\sqrt{-2 \\log(U)} \\sin(2\\pi V)\\,.\n\\end{cases}\n alors X et Y des variables al√©atoires gaussiennes centr√©es r√©duites ind√©pendantes.\n\n\n\n\n\n\nNote\n\n\n\nCet algorithme n‚Äôest en fait pas souvent utilis√© en pratique : il fait appel √† des fonctions dont l‚Äô√©valuation est co√ªteuse (logarithme, cosinus, sinus). Pour s‚Äôaffranchir des fonctions trigonom√©triques, une version modifi√©e de l‚Äôalgorithme de Box-M√ºller a √©t√© propos√©e : la m√©thode de Marsaglia, qui s‚Äôappuie sur des variables al√©atoires uniformes sur le disque unit√© (voir l‚Äôexercice d√©di√© en TD). Une autre alternative est la m√©thode de Ziggurat.",
    "crumbs": [
      "Cours",
      "Loi normale: cas univari√©"
    ]
  },
  {
    "objectID": "Courses/loi_normale1D.html#lois-autour-de-la-loi-normale",
    "href": "Courses/loi_normale1D.html#lois-autour-de-la-loi-normale",
    "title": "Loi normale: cas univari√©",
    "section": "Lois autour de la loi normale",
    "text": "Lois autour de la loi normale\n\nLoi du \\chi^2\nConcernant la prononciation, on prononce ‚Äúkhi-deux‚Äù le nom de cette loi.\n\nD√©finition 1 (Loi du \\chi^2) Soit X_1, \\dots, X_k des variables al√©atoires i.i.d. de loi normale centr√©e r√©duite. La loi de la variable al√©atoire X = X_1^2 + \\dots + X_k^2 est appel√©e loi du \\chi^2 √† k degr√©s de libert√©. Sa densit√© est donn√©e par \nf(x) = \\frac{1}{2^{\\frac{k}{2}}\\Gamma(\\frac{k}{2})} x^{\\frac{k}{2}-1} e^{-x/2}\\,, \\quad x \\geq 0\\,,\n o√π \\Gamma d√©signe la fonction gamma d‚ÄôEuler : \n\\Gamma(x) = \\int_0^{\\infty} t^{x-1} e^{-t}\\,  dt\\,.\n On note alors X \\sim \\chi^2(k).\n\nAu vu de sa d√©finition, la simulation d‚Äôune loi du \\chi^2 est claire : on simule k variables al√©atoires gaussiennes centr√©es r√©duites ind√©pendantes et on somme leur carr√©s.\n\nPreuve. Montrons pour k=1 que la densit√© est bien de la forme pr√©c√©dente, c‚Äôest-√†-dire \n    f(x) = \\frac{1}{\\sqrt{2\\pi}} \\frac{e^{-x/2}}{\\sqrt x}\\,, \\quad x \\geq 0\\,,\n o√π on a utilis√© la relation \\Gamma(1/2) = \\sqrt \\pi (int√©grale de Gauss).\nSoit h : \\mathbb{R} \\to \\mathbb{R} une fonction mesurable born√©e. On a \n\\begin{align*}\n    \\mathbb{E}[h(X_1^2)]\n    & = \\int_\\mathbb{R} h(x^2) \\frac{e^{-\\frac{x^2}{2}}}{\\sqrt{2 \\pi}} \\, dx\\\\\n    & = \\int_{-\\infty}^0 h(x^2) \\frac{e^{-\\frac{x^2}{2}}}{\\sqrt{2 \\pi}} \\, dx\n    + \\int_0^{\\infty} h(x^2) \\frac{e^{-\\frac{x^2}{2}}}{\\sqrt{2 \\pi}} \\, dx\\,.\n\\end{align*}\n En effectuant le changement de variable x=-\\sqrt u dans la premi√®re int√©grale et x=\\sqrt u dans la deuxi√®me, on obtient \n    \\mathbb{E}[h(X_1^2)]\n    = \\int_{\\infty}^0 h(u) \\frac{e^{-u/2}}{\\sqrt{2 \\pi}} \\frac{ du}{2 \\sqrt u}\n    + \\int_0^{\\infty} h(u) \\frac{e^{-u/2}}{\\sqrt{2 \\pi}} \\frac {du}{2 \\sqrt u}\\,.\n Les deux int√©grales √©tant √©gales, on conclut que \n    \\mathbb{E}[h(X_1^2)] = \\int_0^{\\infty} h(u) \\frac{e^{-u/2}}{\\sqrt{2 \\pi}\\sqrt u}\\, du\\,,\n ce qui prouve le r√©sultat pour k=1.\nLa g√©n√©ralisation √† k quelconque se fait par r√©currence: on utilise la formule de convolution des la loi pour obtenir la loi pour k+1:\n\n\\begin{align*}\n    X_1^2 + \\dots + X_k^2 + X_{k+1}^2\n    & = (X_1^2 + \\dots + X_k^2) + X_{k+1}^2\\\\\n    & = \\chi^2(k) + X_{k+1}^2\\,.\n\\end{align*}\n Ainsi, \n\\begin{align*}\n    f_{\\chi^2(k+1)}(x)\n    & = \\int_0^{\\infty} f_{\\chi^2(k)}(x-y) f_{X_{k+1}^2}(y) \\, dy\\\\\n    & = \\int_0^x \\frac{1}{2^{\\frac{k}{2}}\\Gamma(\\frac{k}{2})} (x-y)^{\\frac{k}{2}-1} e^{-\\frac{x-y}{2}} \\tfrac{e^{-\\frac{y}{2}}}{\\sqrt{2\\pi y}} \\, dy\\\\\n    & = \\frac{e^{-\\frac{x}{2}}}{2^{\\frac{k+1}{2}} \\Gamma(\\frac{k}{2})  \\Gamma(\\frac{1}{2})}\\int_0^x  (x-y)^{\\frac{k}{2}-1}  \\tfrac{1}{\\sqrt{ y}} \\, dy\\\\\n    & = \\frac{e^{-\\frac{x}{2}}}{2^{\\frac{k+1}{2}} \\Gamma(\\frac{k}{2})  \\Gamma(\\frac{1}{2})} x \\int_0^1  (x-ux)^{\\frac{k}{2}-1}  \\tfrac{1}{\\sqrt{xu}} \\, du\\\\\n\\end{align*}\n avec le changement de variable y=ux. Ensuite, \n\\begin{align*}\n    f_{\\chi^2(k+1)}(x)\n    & = \\frac{x^{\\frac{k+1}{2}} e^{-\\frac{x}{2}}}{2^{\\frac{k+1}{2}} \\Gamma(\\frac{k}{2})  \\Gamma(\\frac{1}{2})}  \\int_0^1  (1-u)^{\\frac{k}{2}-1}  u^{1/2-1} du\\enspace.\n\\end{align*}\n Or rappelons que si \\Beta(a,b) =  \\int_0^1  (1-u)^{a-1}  u^{b-1} \\, du, alors pour tout a,b \\in [0,+\\infty[, \\Beta(a,b)=\\frac{\\Gamma(a)\\Gamma(b)}{\\Gamma(a+b)}. En effet, en faisant le changement de variable dans l‚Äôint√©grale double qui suit: \n\\begin{array}{ccccc}\n    \\psi & : & \\mathbb{R}^+ \\times \\mathbb{R}^+ & \\to     & \\mathbb{R}^+\\times ]0,1[                 \\\\\n            &   & (s,t)                          & \\mapsto & \\Big(s+t, \\frac{t}{s+t}\\Big)\\,,\n\\end{array}\n c‚Äôest-√†-dire \\psi^{-1}(r,w) = (r(1-w), rw), et le jacobien est donn√© par J_{\\psi^{-1}}(r,w) = \\begin{pmatrix} 1-w & -r \\\\ w & r \\end{pmatrix}, et donc J_{\\psi^{-1}}(r,w)=r, on obtient: \n\\begin{align*}\n\\Gamma(a)\\Gamma(b) & = \\int_0^{\\infty} t^{a-1} e^{-t} \\, dt \\int_0^{\\infty} s^{b-1} e^{-s} \\, ds\\\\\n& = \\int_0^{\\infty} \\int_0^{\\infty} e^{-t-s} t^{a-1} s^{b-1} \\, dt \\, ds\\\\\n& = \\int_0^{1} \\int_0^{\\infty} e^{-r} (rw)^{a-1} (r(1-w))^{b-1} r \\, dr \\, dw\\\\\n& = \\int_0^1 w^{a-1} (1-w)^{b-1} \\int_0^{\\infty} e^{-r} r^{a+b-1} \\, dr \\, dw\\\\\n\\end{align*}\n et donc \\Beta(a,b)=\\frac{\\Gamma(a)\\Gamma(b)}{\\Gamma(a+b)}. En appliquant cette relation pour a=\\frac{k}{2} et b=1/2, on obtient\n\n\\begin{align*}\n    f_{\\chi^2(k+1)}(x)\n    & = \\frac{x^{\\frac{k+1}{2}} e^{-\\frac{x}{2}}}{2^{\\frac{k+1}{2}} \\Gamma(\\frac{k+1}{2})} \\enspace.\n\\end{align*}\n Le r√©sultat est donc prouv√© par r√©currence.\n\n\n\n\nLoi de Student\n\nD√©finition 2 (Loi de Student) Soit X \\sim \\mathcal{N}(0,1) et Y \\sim \\chi^2(k) deux variables al√©atoires ind√©pendantes. La loi de la variable al√©atoire V = \\frac{X}{\\sqrt{Y/k}} est appel√©e loi de Student √† k degr√©s de libert√©. Elle admet pour densit√© \n    f_V(t)\n    = \\dfrac{1}{\\sqrt{k \\pi}} \\dfrac{\\Gamma(\\frac{k+1}{2})}{\\Gamma(\\frac{k}{2})} \\Big(1+\\dfrac{t^2}{k}\\Big)^{-\\frac{k+1}{2}}\\,,\n    \\quad t \\in \\mathbb{R}\\,.\n\n\nLa loi de Student correspond donc au ratio d‚Äôune loi normale par la racine carr√©e d‚Äôune loi du \\chi^2(k) normalis√©e. Ce ratio appara√Æt souvent en statistique lors de la construction d‚Äôintervalles de confiance. Cette loi a √©t√© d√©crite en 1908 par William Gosset2.\n2¬†William Gosset: (1876-1937) statisticien et chimiste anglais. Il √©tait employ√© √† la brasserie Guinness √† Dublin, charg√© du contr√¥le qualit√©. Son employeur lui refusant le droit de publier sous son propre nom, W. Gosset choisit un pseudonyme, Student (üá´üá∑: √©tudiant). Au vu de la proposition pr√©c√©dente, simuler une loi de Student est assez simple : on simule k+1 loi normales ind√©pendantes X_1, \\ldots, X_{k+1} et on consid√®re \n    V = \\dfrac{\\sqrt{k} X_{k+1}}{\\sqrt{X_1^2+\\cdots + X_k^2}}\\,.\n\n\nPreuve (Formule de la densit√©). On applique pour cela la formule du changement de variables avec la transformation \n    \\begin{array}{ccccc}\n        \\phi & : & \\mathbb{R}^* \\times ]0, \\infty[ & \\to     & \\mathbb{R}^* \\times \\mathbb{R}^*      \\\\\n             &   & (x,y)                           & \\mapsto & \\Big(x, \\dfrac{x}{\\sqrt{y/k}}\\Big)\\,,\n    \\end{array}\n c‚Äôest-√†-dire \n    \\phi^{-1}(u,v) = \\Big(u, k\\dfrac{u^2}{v^2}\\Big)\\,.\n La fonction \\phi^{-1} a pour matrice jacobienne \n    J_{\\phi^{-1}} (u,v)\n    =\n    \\begin{pmatrix}\n        1              & 0                  \\\\\n        \\frac{2k}{v^2} & \\frac{-2ku^2}{v^3}\n    \\end{pmatrix}\\,,\n dont le d√©terminant vaut \\frac{-2ku^2}{v^3}. Par ailleurs, les variables al√©atoires X et Y √©tant ind√©pendantes, la densit√© du couple (X,Y) correspond au produit des densit√©s : \n    f_{(X,Y)}(x,y) = \\dfrac{e^{-\\frac{x^2}{2}}}{\\sqrt{2 \\pi}}  \\dfrac{1}{2^{\\frac{k}{2}2} \\Gamma(\\frac{k}{2})} y^{\\frac{k}{2}-1} e^{-\\frac{y}{2}} {1\\hspace{-3.8pt} 1}_{]0, \\infty[}(y)\\, \\quad x,y \\in \\mathbb{R}\\,.\n Tout est pr√™t pour appliquer la th√©or√®me du changement de variables qui assure que la densit√© du couple (U,V) est donn√©e par \n    f_{(U,V)}(u,v)\n    = \\dfrac{e^{-\\frac{u^2}{2}}}{\\sqrt{2 \\pi}} \\dfrac{1}{2^{\\frac{k}{2}} \\Gamma(\\frac{k}{2})} \\bigg(\\dfrac{k u^2}{v^2}\\bigg)^{\\frac{k}{2}-1} e^{-\\frac{1}{2} \\frac{k u^2}{v^2}} \\dfrac{2 k u^2}{(v^2)^{\\frac{3}{2}}} {1\\hspace{-3.8pt} 1}_{\\mathbb{R}^*}(v)\\,.\n Il suffit alors de marginaliser pour obtenir la densit√© de V, ce qui s‚Äôeffectue en calculant l‚Äôint√©grale \n    \\int_\\mathbb{R} f_{(U,V)} (u,v) \\, du\\,.\n Les termes en u de l‚Äôexpression pr√©c√©dente s‚Äôint√®gre en \n\\begin{align*}\n    \\int_{-\\infty}^\\infty e^{-\\frac{u^2}{2}(1+\\frac{k}{v^2})} (u^2)^{\\frac{k}{2}} \\, d u\n     & = \\int_0^\\infty e^{-s} \\bigg( \\dfrac{2 s}{1+\\frac{k}{v^2}} \\bigg)^{\\frac{k}{2}} \\sqrt{\\dfrac{2}{1+\\frac{k}{v^2}}} \\dfrac{ds}{2\\sqrt{s}} \\\\\n     & = \\frac{2^{\\frac{k+1}{2}}}{2} \\frac{1}{\\left(1+\\tfrac{k}{v^2}\\right)^{ \\frac{k+1}{2}}} \\!\\!\\! \\int_0^\\infty e^{-s} s^{\\frac{k}{2} - \\frac{1}{2}} \\, ds\\,,\n\\end{align*}\n o√π la premi√®re √©galit√© r√©sulte du changement de variable \n    s = \\dfrac{u^2}{2} \\left( 1 + \\dfrac{k}{v^2} \\right) \\iff \\sqrt{\\dfrac{2s }{1+\\frac{k}{v^2}}} = u\\,.\n On reconna√Æt dans l‚Äôint√©grale la valeur de \\Gamma(\\frac{k+1}{2}) ce qui conduit √† \n    f_V(v)\n    =\n    \\dfrac{1}{\\sqrt{2 \\pi}} \\dfrac{1}{2^{\\frac{k}{2}} \\Gamma(\\frac{k}{2})} \\bigg(\\dfrac{k}{v^2}\\bigg)^{\\frac{k}{2}-1} \\dfrac{2 k}{(v^2)^{\\frac{3}{2}}}\n    \\frac{2^{\\frac{k+1}{2}}}{2} \\frac{\\Gamma\\left(\\tfrac{k+1}{2}\\right)}{\\left(1+\\tfrac{k}{v^2}\\right)^{ \\frac{k+1}{2}}}\\,.\n On r√©√©crit alors les termes en k/v^2 via \n\\begin{align*}\n\\left(\\tfrac{k}{v^2}\\right)^{\\frac{k}{2}-1} \\tfrac{k}{(v^2)^{\\frac{3}{2}}}\n    \\frac{1}{\\left(1+\\tfrac{k}{v^2}\\right)^{ \\frac{k+1}{2}}}\n    & =\n    \\left(\\tfrac{k}{v^2}\\right)^{\\frac{k}{2}-1} \\tfrac{1}{\\sqrt{k}} \\left(\\tfrac{k}{v^2}\\right)^{\\frac{3}{2}}\n    \\frac{1}{\\left(1+\\tfrac{k}{v^2}\\right)^{ \\frac{k+1}{2}}}\\\\\n    & = \\tfrac{1}{\\sqrt{k}}\\left(1+\\tfrac{v^2}{k}\\right)^{- \\frac{k+1}{2}}\\,,\n\\end{align*}\n ce qui permet de conclure : \n    f_V(v)\n    =\n    \\dfrac{1}{\\sqrt{k \\pi}} \\dfrac{\\Gamma(\\frac{k+1}{2})}{\\Gamma(\\frac{k}{2})} \\Big(  1+\\dfrac{v^2}{k}\\Big)^{-\\frac{k+1}{2}}\\,.\n\n\n\n\n\nLoi de Cauchy\n\nD√©finition 3 (Loi de Cauchy) Une variable al√©atoire X suit une loi de Cauchy standard si sa densit√© est donn√©e par \n    f_X(x) = \\dfrac{1}{\\pi(1+x^2)}\\,, \\quad x \\in \\mathbb{R}\\,.\n\n\nOn note alors X\\sim \\mathcal{C}(0,1) dans ce cas. Plus g√©n√©ralement on dit que Y suit une loi de Cauchy de param√®tres (\\mu,\\sigma)\\in \\mathbb{R} \\times ]0,+\\infty[ si Y=\\mu+\\sigma X o√π X suit une loi de Cauchy standard. Sa densit√© est alors donn√©e par \n    f_Y(y) = \\dfrac{1}{\\sigma \\pi(1 + \\tfrac{(y-\\mu)^2}{\\sigma^2})}\\,, \\quad y \\in \\mathbb{R}\\,.\n\nPour rappel cette loi est importante comme exemple de loi qui n‚Äôadmet ni esp√©rance, ni variance a fortiori. En effet, si X suit une loi de Cauchy standard, \n\\int_{\\mathbb{R}} |x| f_X(x) \\, dx = \\int_{\\mathbb{R}} \\frac{|x|}{\\pi(1+x^2)} \\, dx.\n Or \\frac{|x|}{\\pi(1+x^2)}\\sim \\frac{1}{\\pi x} quand |x|\\to \\infty, et x\\mapsto \\frac{|x|}{\\pi(1+x^2)} n‚Äôest donc pas int√©grable sur \\mathbb{R} au sens de Lebesgue. C‚Äôest donc un exemple de loi pour laquelle la loi des grands nombres et le th√©or√®me central limite ne s‚Äôappliquent pas.\n\nFonction caract√©ristique\nLa fonction caract√©ristique de la loi de Cauchy standard est donn√©e par \n\\begin{align*}\n\\varphi_X(t) & \\triangleq \\int_{\\mathbb{R}} e^{itx} f_X(x) \\, dx = e^{-|t|}\\,.\n\\end{align*}\n et donc si X\\sim \\mathcal{C}(\\mu,\\sigma), alors pour tout t \\in \\mathbb{R}, \\varphi_X(t) = e^{i\\mu t - \\sigma |t|}. Pour la preuve voir par exemple (Exemple III.5.5., Barbe et Ledoux 2006) Une cons√©quence directe est que la somme de deux variables al√©atoires ind√©pendantes de loi de Cauchy reste une loi de Cauchy: Si X_1 \\sim \\mathcal{C}(\\mu_1,\\sigma_2) et X_2 \\sim \\mathcal{C}(\\mu_2,\\sigma_2) sont ind√©pendantes, alors X_1+X_2 \\sim \\mathcal{C}(\\mu_1+\\mu_2,\\sigma_1+\\sigma_2) (car elles ont la m√™me fonction caract√©ristique).\n\nBarbe, Philippe, et Michel Ledoux. 2006. Probabilit√©s.\nOn en d√©duit que la moyenne de variables de Cauchy standard i.i.d suit la loi de Cauchy standard. En effet, si X_1, \\ldots, X_n sont i.i.d de loi de Cauchy standard, alors pour tout t \\in \\mathbb{R}, \\varphi_{\\frac{1}{n}\\sum_{i=1}^n X_i}(t) = e^{-|t|}, et donc \\frac{1}{n}\\sum_{i=1}^n X_i \\sim \\mathcal{C}(0,1). Ainsi la moyenne empirique ne converge pas en probabilit√© vers une constante!\n\n\nFonction de r√©partition et simulation\nLa fonction de r√©partition de X correspond, √† une constante pr√®s, √† la fonction arctangente qui est bijective de \\mathbb{R} sur ]\\frac{-\\pi}{2},\\frac{\\pi}{2}[. La m√©thode d‚Äôinversion permet donc de simuler une variable al√©atoire de loi de Cauchy. La proposition suivante donne un autre moyen.\n\nProposition 2 (Loi de Cauchy et loi normale) Soit X et Y deux variables al√©atoires ind√©pendantes de loi normale centr√©e r√©duite. Alors la variable al√©atoire Y/X suit une loi de Cauchy.\n\nNotons que Y/X est bien d√©finie puisque X est diff√©rent de 0 presque s√ªrement.\n\nPreuve. Comme pour la loi de Student, on d√©montre ce r√©sultat avec un changement de variables. On consid√®re l‚Äôapplication \n    \\begin{array}{ccccc}\n        \\phi & : & \\mathbb{R}^* \\times \\mathbb{R} & \\to     & \\mathbb{R}^2                 \\\\\n                &   & (x,y)                          & \\mapsto & \\Big(x, \\dfrac{y}{x}\\Big)\\,,\n    \\end{array}\n c‚Äôest-√†-dire \n    \\phi^{-1}(u,v) = (u, uv)\\,.\n La matrice jacobienne de \\phi^{-1} est donn√©e par \n    J_{\\phi^{-1}} (u,v)\n    =\n    \\begin{pmatrix}\n        1 & 0 \\\\\n        v & u\n    \\end{pmatrix}\\,,\n dont le d√©terminant vaut u. Rappelons √©galement que la densit√© du couple (X,Y) vaut \n    f_{(X,Y)}(x,y) = \\dfrac{1}{2 \\pi} e^{- \\frac{x^2 + y^2}{2}}\\,, \\quad x,y \\in \\mathbb{R}\\,.\n La formule du changement de variables donne alors la densit√© de f_{(U,V)} : \n    f_{(U,V)}\n    = \\dfrac{1}{2 \\pi} e^{- \\frac{u^2 + u^2v^2}{2}} |u|\\,.\n On obtient alors la densit√© de V en int√©grant par rapport √† u : \n\\begin{align*}\n    f_V(v)\n    & = \\dfrac{1}{2 \\pi} \\int_\\mathbb{R} e^{- \\frac{u^2 + u^2v^2}{2}} |u| \\, \\mathrm du \\\\\n    & = \\dfrac{1}{\\pi} \\int_0^\\infty e^{- \\frac{u^2(1 + v^2)}{2}} u \\, \\mathrm du\\\\\n    & = \\dfrac{1}{\\pi} \\bigg[ -\\dfrac{e^{- \\frac{u^2(1 + v^2)}{2}}}{1+v^2} \\bigg]_0^\\infty\\\\\n    & = \\dfrac{1}{\\pi(1+v^2)}\\,.\n\\end{align*}\n\n\nOn obtient ainsi une autre mani√®re de simuler une loi de Cauchy, en prenant le ratio de deux gaussiennes ind√©pendantes. Cependant, la simulation via la m√©thode d‚Äôinversion peut-√™tre moins co√ªteuse puisqu‚Äôelle ne fait appel qu‚Äô√† une variable al√©atoire uniforme et √† la fonction tangente.",
    "crumbs": [
      "Cours",
      "Loi normale: cas univari√©"
    ]
  },
  {
    "objectID": "Courses/loi_normale_multi.html",
    "href": "Courses/loi_normale_multi.html",
    "title": "Loi normale: cas multivari√©",
    "section": "",
    "text": "On consid√®re ici \\mathbb{R}^d muni du produit scalaire euclidien \\langle \\cdot, \\cdot \\rangle et de la norme euclidienne \\|\\cdot\\| associ√©e.",
    "crumbs": [
      "Cours",
      "Loi normale: cas multivari√©"
    ]
  },
  {
    "objectID": "Courses/loi_normale_multi.html#rappel-sur-les-vecteurs-al√©atoires",
    "href": "Courses/loi_normale_multi.html#rappel-sur-les-vecteurs-al√©atoires",
    "title": "Loi normale: cas multivari√©",
    "section": "Rappel sur les vecteurs al√©atoires",
    "text": "Rappel sur les vecteurs al√©atoires\nSi \\mathbf{X} = (X_1, \\dots, X_d) \\in \\mathbb{R}^d est un vecteur al√©atoire, son esp√©rance est d√©finie coordonn√©e par coordonn√©e : \n    \\mathbb{E}[\\mathbf{X}] = (\\mathbb{E}[X_1], \\dots, \\mathbb{E}[X_d]) \\in \\mathbb{R}^d\\\n quantit√© qui existe d√®s que chaque esp√©rance est bien d√©finie. De plus, si \\mathbb{E}[X_j^2] &lt; \\infty pour tout j\\in \\llbracket 1, d\\rrbracket, on peut alors d√©finir les covariances pour tout (i,j) \\in \\llbracket 1, d\\rrbracket^2 : \n    \\textrm{cov}(X_i, X_j) = \\mathbb{E}[(X_i- \\mathbb{E}[X_i]) (X_j - \\mathbb{E}[X_j])] \\enspace,\n quantit√©s que l‚Äôon rassemble dans une matrice appel√©e matrice de variance-covariance : \n    \\Sigma = (\\textrm{cov}(X_i, X_j))_{1 \\leq i,j \\leq d} \\in \\mathbb{R}^{d \\times d} \\,.\n On peut montrer que cette matrice est sym√©trique et semi-d√©finie positive. En particulier, si les X_j sont ind√©pendants, alors \\Sigma est une matrice diagonale.\n\n\n\n\n\n\nNote\n\n\n\nUn point important. Si (X,Y) est un vecteur al√©atoire, il ne suffit pas de conna√Ætre les marginales X et Y pour caract√©riser enti√®rement le vecteur. Par exemple, si X et Y suivent toutes les deux une loi normale, alors on peut avoir par exemple X=Y, ou bien X ind√©pendant de Y, et ces deux cas mod√©lisent clairement deux vecteurs al√©atoires de lois distinctes.",
    "crumbs": [
      "Cours",
      "Loi normale: cas multivari√©"
    ]
  },
  {
    "objectID": "Courses/loi_normale_multi.html#vecteurs-gaussiens",
    "href": "Courses/loi_normale_multi.html#vecteurs-gaussiens",
    "title": "Loi normale: cas multivari√©",
    "section": "Vecteurs gaussiens",
    "text": "Vecteurs gaussiens\n\nD√©finition 1 (Vecteur gaussien) Un vecteur al√©atoire \\mathbf{X} = (X_1, \\dots, X_d)^\\top \\in \\mathbb{R}^d est un vecteur gaussien si pour tout \\bm{\\alpha} = (\\alpha_1, \\dots, \\alpha_d)^\\top, la variable al√©atoire r√©elle \n  \\langle \\bm{\\alpha}, \\mathbf{X} \\rangle = \\alpha_1 X_1 + \\cdots + \\alpha_d X_d \\enspace,\n suit une loi normale.\n\nEn particulier chaque (loi marginale) X_j suit alors une loi gaussienne (choisir ci-dessus \\alpha = e_j, les autres √©gaux √† 0).\nCependant, il ne suffit pas que les X_j soient des gaussiennes pour que le vecteur X soit un vecteur gaussien. Par exemple, si X suit une loi normale centr√©e r√©duite et \\varepsilon une loi uniforme (discr√®te) sur \\{-1,1\\}, alors on peut montrer que \\varepsilon X suit encore une loi normale centr√©e r√©duite. En effet, pour tout t \\in \\mathbb{R}, on a \n\\begin{align*}\n  \\mathbb{P}(\\varepsilon X \\leq t)\n  & =  \\mathbb{P}(X \\leq t) \\mathbb{P}(\\varepsilon = 1) + \\mathbb{P}(-X \\leq t) \\mathbb{P}(\\varepsilon = -1)\\\\\n  & = \\tfrac{1}{2} \\mathbb{P}(X \\leq t) + \\tfrac{1}{2} \\mathbb{P}(-X \\leq t) = \\mathbb{P}(X \\leq t) \\enspace.\n\\end{align*}\n Cependant, X + \\varepsilon X prend la valeur 0 avec probabilit√© 1/2 donc ne suit pas une loi normale. Ainsi le vecteur (X, \\varepsilon X)^{\\top} n‚Äôest pas un vecteur gaussien bi-dimensionnel.\nSoit \\mathbf{X} un vecteur gaussien. Notons \\bm{\\mu} son esp√©rance et \\Sigma sa matrice de variance-covariance. En reprenant les notations de la d√©finition, la variable al√©atoire \\langle \\bm{\\alpha}, \\mathbf{X} \\rangle v√©rifie \n\\begin{align*}\n  \\mathbb{E}[\\langle \\bm{\\alpha}, \\mathbf{X} \\rangle]\n  & = \\mathbb{E}[\\alpha_1 X_1 + \\cdots + \\alpha_d X_d] \\\\\n  & =  \\alpha_1 \\mathbb{E}[X_1] + \\cdots + \\alpha_d \\mathbb{E}[X_d]\n  & = \\langle \\bm{\\alpha}, \\bm{\\mu} \\rangle\\,,\n\\end{align*}\n et \n\\begin{align*}\n  \\mathrm{var}(\\langle \\bm{\\alpha}, \\mathbf{X} \\rangle)\n    & = \\mathrm{var}(\\alpha_1 X_1 + \\cdots + \\alpha_d X_d)\\\\\n    & = \\mathrm{cov}(\\alpha_1 X_1 + \\cdots + \\alpha_d X_d, \\alpha_1 X_1 + \\cdots + \\alpha_d X_d) \\\\\n    & = \\sum_{1 \\leq i,j \\leq d} \\alpha_i \\mathrm{cov}(X_i, X_j) \\alpha_j\n  = \\bm{\\alpha}^\\top \\Sigma \\bm{\\alpha}\n\\end{align*}\n Ainsi, \\langle \\bm{\\alpha}, \\mathbf{X} \\rangle \\sim \\mathcal{N}\\left(\\langle \\bm{\\alpha}, \\bm{\\mu} \\rangle, \\bm{\\alpha}^\\top \\Sigma \\bm{\\alpha}\\right).\n\nFonction caract√©ristique d‚Äôun vecteur gaussien\nLa fonction caract√©ristique d‚Äôun vecteur gaussien d‚Äôesp√©rance \\bm{\\mu} et de matrice de variance-covariance \\Sigma est donn√©e par \n    \\phi_\\mathbf{X}(\\bm{\\alpha})\n    = \\mathbb{E}[e^{i \\langle \\bm{\\alpha}, \\mathbf{X} \\rangle}]\n    = \\exp\\Big(i \\langle \\bm{\\alpha}, \\bm{\\mu} \\rangle - \\frac{\\bm{\\alpha}^\\top \\Sigma \\bm{\\alpha}}{2}\\Big)\\,,\n    \\quad \\bm{\\alpha} \\in \\mathbb{R}^d\\,.\n o√π on a utilis√© l‚Äôexpression de la fonction caract√©ristique d‚Äôune variable al√©atoire de loi normale \\mathcal{N}(\\langle \\bm{\\alpha}, \\bm{\\mu} \\rangle, \\bm{\\alpha}^\\top \\Sigma \\bm{\\alpha}). Ainsi, \\phi_\\mathbf{X} est enti√®rement d√©termin√©e par les quantit√©s \\bm{\\mu} et \\Sigma. Comme cette fonction caract√©rise la loi de \\mathbf{X}, on en d√©duit que la loi d‚Äôun vecteur gaussien est enti√®rement caract√©ris√©e par son esp√©rance et sa matrice de variance-covariance. On note alors \n    \\mathbf{X} \\sim \\mathcal{N}(\\bm{\\mu}, \\Sigma)\\,.\n En particulier, si les variables al√©atoires X_1, \\dots, X_d sont ind√©pendantes de loi \\mathcal{N}(0,1), alors \\bm{\\mu} = (0,\\ldots,0)^\\top et \\Sigma = \\mathrm{Id}_d.\n\n\nDensit√© de probabilit√©\nCommen√ßons par le cas \\bm{\\mu}=0 et \\Sigma = \\mathrm{Id}_d correspond √† la loi gaussienne centr√©e r√©duite \\mathcal{N}(0, \\mathrm{Id}_d). La loi de (X_1,\\dots,X_n)^\\top correspond alors √† la loi produit de n lois gaussiennes centr√©es r√©duites ind√©pendantes (pour les gaussiennes la d√©corr√©lation implique l‚Äôind√©pendance). Sa densit√© est donc donn√©e par \n\\varphi_{0,\\mathrm{Id}_d}(x) = \\frac{1}{ \\sqrt{(2\\pi)^d}} \\exp\\left( -\\tfrac{1}{2}x^\\top x   \\right) \\enspace.\n\n\nProposition 1 (Densit√© de la loi gaussienne multivari√©e) Soient \\bm{\\mu} \\in \\mathbb{R}^d et \\Sigma \\in \\mathbb{R}^{d \\times d} (sym√©trique et d√©finie positive) et supposons que X \\sim \\mathcal{N}(\\bm{\\mu},\\Sigma). Alors la densit√© de probabilit√© de X est donn√©e pour tout x \\in \\mathbb{R}^d par\n\n\\varphi_{\\bm{\\mu},\\Sigma}(x) = \\frac{1}{ \\sqrt{(2\\pi)^d |\\det(\\Sigma)|}}  \\exp\\Big( -\\tfrac{1}{2}(x-\\bm{\\mu})^\\top\\Sigma^{-1}(x - \\bm{\\mu})   \\Big) \\enspace.\n\n\n\nPreuve. Prenons une matrice L\\in \\mathbb{R}^{d \\times d} telle que LL^\\top = \\Sigma (par exemple, la d√©composition de Cholesky, que nous reverrons ci-dessous). Calculons alors la loi de \\mathbf{Y} = \\psi(X) \\triangleq L \\mathbf{X} + \\bm{\\mu}, pour X\\sim \\mathcal{N}(0,\\mathrm{Id_d}). Pour appliquer la formule du changement de variable nous donne, on calcule \\psi^{-1} et son jacobien: \\psi^{-1}(y) = L^{-1}(y-\\bm{\\mu}), et \\det(J_{\\psi^{-1}}) = \\det(L^{-1}) = \\det(L)^{-1} = |\\det(\\Sigma)|^{-1/2}.\nEn notant que LL^\\top \\left(L^{-1}\\right)^\\top L^{-1}=\\mathrm{Id}_d, et donc que \\left(L^{-1}\\right)^\\top L^{-1}=\\Sigma^{-1}, on en d√©duit que la densit√© de \\mathbf{Y} est donn√©e par \n\\begin{align*}\n\\varphi_{\\bm{\\mu},\\Sigma}(y)\n& = \\varphi_{0,\\mathrm{Id}_d}(\\psi^{-1}(y)) |\\det(J_{\\psi^{-1}})| \\\\\n& = \\frac{|\\det(\\Sigma)|^{-1/2}}{ \\sqrt{(2\\pi)^d }}  \\exp\\left( -\\tfrac{1}{2}(y-\\bm{\\mu})^\\top \\left(L^{-1}\\right)^\\top L^{-1}(y - \\bm{\\mu})\\right)\\\\\n& = \\frac{1}{ \\sqrt{(2\\pi)^d |\\det(\\Sigma)|}}  \\exp\\Big( -\\tfrac{1}{2}(y-\\bm{\\mu})^\\top\\Sigma^{-1}(y - \\bm{\\mu})   \\Big) \\enspace.\n\\end{align*}\n\n\n\nProposition 2 (Transformation affine de vecteurs gaussiens) Soit \\mathbf{X} \\sim \\mathcal{N}(\\bm{\\mu}, \\Sigma) un vecteur gaussien sur \\mathbb{R}^d, \\Omega \\in \\mathbb{R}^{d' \\times d} et \\bm{\\nu}\\in \\mathbb{R}^{d'}. Alors, le vecteur al√©atoire \\mathbf{Y} = \\Omega \\mathbf{X} + \\bm{\\nu} est un vecteur gaussien v√©rifiant \n  \\mathbf{Y} \\sim \\mathcal{N}(\\Omega \\bm{\\bm{\\mu}} + \\bm{\\nu}, \\Omega \\Sigma \\Omega^\\top)\\,.\n\n\nCette proposition se prouve sans peine en utilisant la fonction caract√©ristique On retrouve en particulier la stabilit√© par transformation affine √©tablie en dimension 1.",
    "crumbs": [
      "Cours",
      "Loi normale: cas multivari√©"
    ]
  },
  {
    "objectID": "Courses/loi_normale_multi.html#la-factorisation-de-cholesky",
    "href": "Courses/loi_normale_multi.html#la-factorisation-de-cholesky",
    "title": "Loi normale: cas multivari√©",
    "section": "La factorisation de Cholesky",
    "text": "La factorisation de Cholesky\nPour rappel, la factorisation de Cholesky1 d‚Äôune matrice sym√©trique d√©finie positive est donn√©e ci-dessous.\n1¬†Andr√©-Louis Cholesky, dit Ren√©: (1875-1918) ing√©nieur topographe et g√©od√©sien dans l‚Äôarm√©e fran√ßaise, mort des suites de blessures re√ßues au champs de bataille. \nTh√©or√®me 1 (Factorisation de Cholesky) Soit \\Sigma \\in \\mathbb{R}^{d \\times d} une matrice sym√©trique d√©finie positive. Alors il existe une matrice triangulaire inf√©rieure L \\in \\mathbb{R}^{d \\times d} telle que \\Sigma = LL^\\top. La d√©composition est unique si l‚Äôon impose que les √©l√©ments diagonaux de L soient strictement positifs.\n\n\nPreuve. La factorisation de Cholesky est une cons√©quence directe de la m√©thode du pivot de Gauss. Le d√©tail est donn√© par exemple dans (Th. 4.4.1, Ciarlet 2006).\n\nCiarlet, P. G. 2006. Introduction √† l‚Äôanalyse num√©rique matricielle et √† l‚Äôoptimisation. Cours et exercices corrig√©s. Dunod.",
    "crumbs": [
      "Cours",
      "Loi normale: cas multivari√©"
    ]
  },
  {
    "objectID": "Courses/loi_normale_multi.html#simulation-de-vecteurs-gaussiens",
    "href": "Courses/loi_normale_multi.html#simulation-de-vecteurs-gaussiens",
    "title": "Loi normale: cas multivari√©",
    "section": "Simulation de vecteurs gaussiens",
    "text": "Simulation de vecteurs gaussiens\nLa simulation d‚Äôun vecteur gaussien de param√®tres \\bm{\\mu} = (0,\\ldots,0)^\\top et \\Sigma = \\mathrm{Id}_d ne pose pas de probl√®me : il suffit de simuler X_1,\\dots, X_d, d variables al√©atoires ind√©pendantes de loi normale centr√©e r√©duite. En effet, le vecteur \\mathbf{X} = (X_1,\\dots, X_d)^\\top est alors un vecteur gaussien de loi \\mathcal{N}(0, \\mathrm{Id}_d).\nSupposons maintenant que l‚Äôon veuille simuler un vecteur gaussien de loi \\mathcal{N}(\\bm{\\mu}, \\Sigma) dans \\mathbb{R}^d, \\bm{\\mu} et \\Sigma sym√©trique d√©finie positive donn√©s.\n\nApproche par la factorisation de Cholesky\nLa matrice \\Sigma √©tant sym√©trique, elle peut s‚Äô√©crire comme \\Sigma = LL^\\top o√π L est une matrice triangulaire inf√©rieure de taille d \\times d. Gr√¢ce √† la d√©composition de Cholevsky et en reprenant les √©l√©ments de la preuve de la Proposition¬†1, on peut √©crire \\mathbf{Y} = L \\mathbf{X} + \\bm{\\mu} o√π \\mathbf{X} \\sim \\mathcal{N}(0, \\mathrm{Id}_d) et v√©rifier que \\mathbf{Y} \\sim \\mathcal{N}(\\bm{\\mu}, \\Sigma).\n\n\nApproche par la d√©composition spectrale de \\Sigma\nLa matrice \\Sigma √©tant sym√©trique, elle se diagonalise en base orthonorm√©e : il existe une matrice orthogonale P telle que \n    \\Sigma\n    = P \\mathrm{diag}(\\lambda_1 \\ldots, \\lambda_d) P^{-1}\n    = P \\mathrm{diag}(\\lambda_1 \\ldots, \\lambda_d) P^\\top\\,,\n o√π \\lambda_1, \\ldots, \\lambda_d \\geq 0 sont les valeurs propres de \\Sigma qui est semi-d√©finie positive. On pose alors R = P \\mathrm{diag}(\\sqrt \\lambda_1 \\ldots, \\sqrt \\lambda_d) qui est une racine carr√©e matricielle de \\Sigma au sens o√π \\Sigma = R R ^\\top. On part alors d‚Äôun vecteur gaussien centr√©e r√©duit \\mathbf{X}_0 \\sim \\mathcal{N}(0, \\mathrm{Id}_d) que l‚Äôon sait simuler (par exemple avec la m√©thode de Box-M√ºller). La proposition Proposition¬†2 assure alors que le vecteur \\mathbf{X} = R \\mathbf{X}_0 + \\bm{\\mu} est un vecteur gaussien de loi \\mathcal{N}(\\bm{\\mu}, \\Sigma).",
    "crumbs": [
      "Cours",
      "Loi normale: cas multivari√©"
    ]
  },
  {
    "objectID": "Courses/loi_normale_multi.html#vecteurs-gaussiens-cas-bidimensionnel",
    "href": "Courses/loi_normale_multi.html#vecteurs-gaussiens-cas-bidimensionnel",
    "title": "Loi normale: cas multivari√©",
    "section": "Vecteurs gaussiens : cas bidimensionnel",
    "text": "Vecteurs gaussiens : cas bidimensionnel\nEn dimension p=2, la matrice de covariance \\Sigma peut toujours s‚Äô√©crire comme suit, et la visualisation suivante montre l‚Äôimpact des diff√©rents param√®tres sur la densit√© de probabilit√©. \n\\Sigma =\n\\begin{pmatrix}\\cos(\\theta) & - \\sin(\\theta)\\\\  \\sin(\\theta)& \\cos(\\theta)\n\\end{pmatrix} \\cdot\n\\begin{pmatrix}\\sigma_1 & 0\\\\ 0 & \\sigma_2\n\\end{pmatrix}\\cdot\n\\begin{pmatrix}\n\\cos(\\theta) &\\sin(\\theta)\\\\  -\\sin(\\theta)& \\cos(\\theta)\\end{pmatrix}\n o√π \\theta est l‚Äôangle de rotation et \\sigma_1 et \\sigma_2 les √©carts-types des marginales (dans le rep√®re orthonormal apr√®s rotation).\nPlotly = require('plotly.js-dist');\ndists = require( 'https://cdn.jsdelivr.net/gh/stdlib-js/stats-base-dists@umd/browser.js' );\njstat = require('jstat');\nmath = require(\"mathjs\");\n// see source here: https://github.com/stdlib-js/stats-base-dists/tree/umd\n// continuous case\n\nviewof inputs = Inputs.form([\n  Inputs.range([-1, 1], {label: tex`\\mu_1`, step: 0.1}, {value: 0}),\n  Inputs.range([-1, 1], {label: tex`\\mu_2`, step: 0.1}, {value: 0}),\n  Inputs.range([0.1, 2], {label: tex`\\sigma_1`, step: 0.1, value: 1}),\n  Inputs.range([0.1, 2], {label: tex`\\sigma_2`, step: 0.1, value: 1}),\n  Inputs.range([0, 6.29], {label: tex`\\theta`, step: 0.01, value: 0}),\n  Inputs.range([0, 1000], {label: tex`n`, step: 1, value: 10}),\n  Inputs.button(\"Re-Tirage\")\n])\n\n\nmu1 = inputs[0];\nmu2 = inputs[1];\nsigma1 = inputs[2];\nsigma2 = inputs[3];\ntheta = inputs[4];\nn_samples = inputs[5];\n\n\nfunction create_sigma(theta, sigma1, sigma2){\n  const mat_rot = math.matrix([[math.cos(theta), -math.sin(theta)], [math.sin(theta), math.cos(theta)]]);\n  const mat_sigma = math.matrix([[sigma1**2, 0], [0, sigma2**2]]);\n  return math.multiply(mat_rot, math.multiply(mat_sigma, math.transpose(mat_rot)));\n}\n\n\nfunction mvnpdf(x, mu, Sigma){\n  const p = 2;\n    return (2*math.pi)**(-p/2)*math.det(Sigma)**(-0.5)*\n      math.exp(-0.5*math.multiply(math.multiply( math.transpose(math.subtract(x,mu)), math.inv(Sigma)), math.subtract(x, mu)));\n}\n\n{\n\n\nfunction normal_rng(mu, Sigma, n=100) {\n    // Compute the Cholesky decomposition of Sigma\n    var cholesky = jstat.cholesky(Sigma);\n\n    // Generate the samples\n    var samples = Array.from({length: n}, () =&gt; {\n        var x = jstat.normal.sample(0, 1);\n        var y = jstat.normal.sample(0, 1);\n        // Transform the standard normal random variables using the Cholesky decomposition\n        var transformedX = mu[0] + cholesky[0][0] * x + cholesky[0][1] * y;\n        var transformedY = mu[1] + cholesky[1][0] * x + cholesky[1][1] * y;\n        return [transformedX, transformedY];\n    });   \n\n    return samples;\n}\nvar Sigma = create_sigma(theta, sigma1, sigma2).toArray();\nvar mu = [mu1, mu2];\n\nvar samples = normal_rng(mu, Sigma, 1000);\nvar npoints=100, mini = -5, maxi=5, x = new Array(npoints), y = new Array(npoints), z = new Array(npoints), i, j;\n\n//  Densit√©:\nfor(var i = 0; i &lt; npoints; i++) {\n    x[i] = mini + i * (maxi - mini) / (npoints - 1);\n    y[i] = mini + i * (maxi - mini) / (npoints - 1);\n    z[i] = new Array(npoints);\n    }\n\nfor(var i = 0; i &lt; npoints; i++) {\n    for(j = 0; j &lt; npoints; j++) {\n\n        z[j][i] = mvnpdf([x[i], x[j]], mu, Sigma);\n    }\n\n}\n  \n\n{\n\nvar trace1 = {\n        x: samples.slice(0, n_samples).map(sample =&gt; sample[0]),\n        y: samples.slice(0, n_samples).map(sample =&gt; sample[1]),\n        mode: 'markers',\n        type: 'scatter',\n        marker: {\n            color: 'rgba(0,0,0,0.5)',\n            size: 5,\n        },\n\n        xaxis: 'x2',\n}\n\nvar trace22 = {\n        x: x,\n        y: y,\n        z: z,\n        type: 'surface',\n        colorscale: 'Oranges',\n        showscale: false,\n        color: {\n            legend: false,\n            label: \"pdf\",\n        },\n\n}\n\nvar trace21 = {\n        x: x,\n        y: y,\n        z: z,\n        type: 'contour',\n        colorscale: 'Oranges',\n        color: {\n            legend: true,\n            label: \"pdf\",\n        },\n        blur: 4,\n        xlim: [-5, 5],\n        ylim: [-5, 5],\n        xaxis: 'x2',\n}\n\n\nvar data = [\n  trace21,\n  trace22,\n  trace1,\n  ];\n\n\n  var layout = {\n       yaxis2: {\n          range: [-5, 5],\n          autorange: false,\n        },\n       xaxis2: {\n          range: [-5, 5],\n          autorange: false,\n        },\n\n      scene: {\n          camera: {\n              eye: {\n                  x: 0.5,\n                  y: 1.2,\n                  z: 1.5,\n              }\n          }\n      },\n    grid: {\n      rows: 1,\n      columns: 2,\n      subplots: [['xy','x2y']],\n    },\n    showlegend: false,\n\n  };\n\n    var config = {responsive: true}\n    const div = DOM.element('div');\n    Plotly.newPlot(div, data, layout, config);\n    return div;\n  }\n\n}",
    "crumbs": [
      "Cours",
      "Loi normale: cas multivari√©"
    ]
  },
  {
    "objectID": "Courses/matterjs-inverse-vizu.html",
    "href": "Courses/matterjs-inverse-vizu.html",
    "title": "inverse-vizu",
    "section": "",
    "text": "viewof dist = Inputs.select(['normal','cauchy','laplace','bimodal'], {value: \"bimodal\", label: \"Distribution type\"})\nviewof replay = html`&lt;button&gt;replay`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ninvcdfboard = require(await FileAttachment(\"../inverse-vizu/dist/invcdfboard.umd.cjs\").url())\n\n{\n  replay\n  const canvas = DOM.canvas(500, 500);\n  const galton = invcdfboard.galton(canvas,dist);\n  \n  return html`${galton.canvas}`\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Retour au sommet"
  },
  {
    "objectID": "Courses/notations.html",
    "href": "Courses/notations.html",
    "title": "Notations et rappels",
    "section": "",
    "text": "On consid√®re un espace probabilis√© (\\Omega, {\\mathcal{F}}, \\mathbb{P}), compos√© d‚Äôun ensemble \\Omega, d‚Äôune tribu \\mathcal{F}, et d‚Äôune mesure de probabilit√© \\mathbb{P}.\nCette d√©finition permet de transposer l‚Äôal√©a qui provient de \\Omega dans l‚Äôespace E. L‚Äôhypoth√®se \\{X \\in B\\} \\in \\mathcal{F} assure que cet ensemble est bien un √©v√®nement et donc que l‚Äôon peut calculer sa probabilit√©.\nUne fois que l‚Äôal√©a a √©t√© transpos√© de \\Omega vers E, on souhaite √©galement transposer la probabilit√© \\mathbb{P} sur E. Ceci motive l‚Äôintroduction de la notion de loi.\nLes propri√©t√©s de \\mathbb{P} assurent que \\mathbb{P}_X est bien une loi de probabilit√© sur l‚Äôespace mesurable (E, \\mathcal{E}).",
    "crumbs": [
      "Cours",
      "Notations et rappels"
    ]
  },
  {
    "objectID": "Courses/notations.html#loi-discr√®tes",
    "href": "Courses/notations.html#loi-discr√®tes",
    "title": "Notations et rappels",
    "section": "Loi discr√®tes",
    "text": "Loi discr√®tes\nLes variables al√©atoires discr√®tes sont celles √† valeurs dans un ensemble E discret, le plus souvent \\mathbb{N}, muni de la tribu pleine \\mathcal{F} = \\mathcal{P}(E).\n\nExemple 1 (Loi de Bernoulli) La loi la plus simple est la loi de Bernoulli de param√®tre p \\in [0,1], d√©finie sur \\{0,1\\} par \\mathbb{P}(X=1) = 1-\\mathbb{P}(X=0) = p qui mod√©lise une exp√©rience al√©atoire √† deux issues (succ√®s = 1 et √©chec = 0).\n\n\nExemple 2 (Loi binomiale) En sommant des variables al√©atoires ind√©pendantes de loi de Bernoulli on obtient une loi binomiale : \\mathbb{P}(X=k) = \\binom{n}{k} p^k (1-p)^{n-k}, pour k \\in \\{0,\\ldots,n\\}, qui mod√©lise le nombre de succ√®s parmi n lancers.\n\n\nExemple 3 (Loi g√©om√©trique) En observant le nombre d‚Äôexp√©riences n√©cessaires avant d‚Äôobtenir un succ√®s, on obtient une loi g√©om√©trique : \\mathbb{P}(X=k) = p (1-p)^{k-1}, pour k \\geq 1. C‚Äôest une loi de probabilit√© discr√®te qui d√©crit le comportement du nombre d‚Äô√©v√©nements se produisant dans un intervalle de temps fix√©, si ces √©v√©nements se produisent avec une fr√©quence moyenne ou esp√©rance connue, et ind√©pendamment du temps √©coul√© depuis l‚Äô√©v√©nement pr√©c√©dent (e.g., nombre de clients dans une file d‚Äôattente, nombre de mutations dans un g√®ne, etc.).\n\n\nExemple 4 (Loi de Poisson) La loi de Poisson de param√®tre \\lambda &gt; 0 est d√©finie par \\mathbb{P}(X=k) = e^{-\\lambda} \\lambda^k / k!, pour k \\in \\mathbb{N}",
    "crumbs": [
      "Cours",
      "Notations et rappels"
    ]
  },
  {
    "objectID": "Courses/notations.html#lois-continues",
    "href": "Courses/notations.html#lois-continues",
    "title": "Notations et rappels",
    "section": "Lois continues",
    "text": "Lois continues\nParmi les variables al√©atoires r√©elles non discr√®tes, beaucoup peuvent se repr√©senter avec une densit√©, c‚Äôest-√†-dire qu‚Äôil existe une fonction mesurable f : \\mathbb{R} \\to [0, \\infty[ d‚Äôint√©grale 1. La loi d‚Äôune telle variable al√©atoire X est alors donn√©e pour tout A \\in \\mathcal{B}(\\mathbb{R}) par \n    \\mathbb{P}(X \\in A) = \\int_A f(x) \\, \\mathrm d x \\enspace.\n Les propri√©t√©s de l‚Äôint√©grale de Lebesgue assure que cette formule d√©finit bien une loi de probabilit√©.\n\nExemple 5 (Loi uniforme) La loi uniforme sur un ensemble B \\in \\mathcal{B}(\\mathbb{R}), s‚Äôobtient avec la densit√© d√©finie par \nf(x) = {1\\hspace{-3.8pt} 1}_B(x) / \\lambda (B) \\enspace,\n o√π \\lambda (B) repr√©sente la mesure de Lebesgue de l‚Äôensemble B. En particulier pour la loi uniforme sur le segment [0,1] on obtient la fonction suivante: \nf(x) = {1\\hspace{-3.8pt} 1}_{[0,1]}(x)\\enspace.\n Si une variable al√©atoire U suit une telle loi on note U \\sim \\mathcal{U}([0,1]).\n\n\nExemple 6 (Loi exponentielle) La loi exponentielle de param√®tre \\gamma &gt; 0 est obtenue avec la densit√© donn√©e par \nf(x) = \\gamma e^{-\\gamma x} {1\\hspace{-3.8pt} 1}_{\\mathbb{R}_+}(x)\\enspace.\n Si une variable al√©atoire X suit cette loi on note X \\sim \\mathcal{Exp}(\\gamma).\n\n\nExemple 7 (Loi normale/gaussienne univari√©e) On obtient la loi normale de param√®tre \\mu \\in \\mathbb{R} et \\sigma^2 &gt; 0 correspond √† loi dont la densit√© est donn√©e par la fonction r√©elle: \nf(x) = \\frac{1}{\\sqrt{2 \\pi} \\sigma}e^{-\\frac{1}{2 \\sigma^2}(x-\\mu)^2} \\enspace.\n Si une variable al√©atoire X suit une telle loi on note X \\sim \\mathcal{N}(\\mu,\\sigma^2), \\mu correspondant √† l‚Äôesp√©rance de la loi, et \\sigma^2 √† sa variance. On nomme loi normale centr√©e r√©duite le cas correspondant √† \\mu = 0 et \\sigma^2 = 1.\n\n\nExemple 8 (Loi normale multivari√©e) On peut √©tendre les lois normales au cas multi-dimensionnel. Fixons d\\in\\mathbb{N}^* un entier non nul. Pour un vecteur \\mu \\in \\mathbb{R}^d et une matrice sym√©trique-d√©finie positive \\Sigma\\in \\mathbb{R^{d\\times d}}, la densit√© normale mutlivari√©e associ√©e est donn√©e par la fonction: \nf(x) = \\frac{1}{{(2 \\pi)}^{\\frac{d}{2}} {\\rm det}(\\Sigma)} e^{-\\frac{1}{2}(x-\\mu)^\\top \\Sigma ^{-1}(x-\\mu)}\n Notons que \\mu est l‚Äôesp√©rance de la loi et \\Sigma la matrice de variance-covariance.",
    "crumbs": [
      "Cours",
      "Notations et rappels"
    ]
  },
  {
    "objectID": "Courses/notations.html#fonction-de-r√©partition",
    "href": "Courses/notations.html#fonction-de-r√©partition",
    "title": "Notations et rappels",
    "section": "Fonction de r√©partition",
    "text": "Fonction de r√©partition\nLa notion de variable al√©atoire n‚Äôest pas facile √† manipuler puisqu‚Äôelle part d‚Äôun espace \\Omega dont on ne sait rien. On souhaite donc caract√©riser la loi d‚Äôune variable al√©atoire en ne consid√©rant que l‚Äôespace d‚Äôarriv√©e (E, \\mathcal{E}) .\nPlusieurs outils existent : la fonction de r√©partition (pour des variables al√©atoires r√©elles), la fonction caract√©ristique (pour des variables al√©atoires dans \\mathbb{R}^d), la fonction g√©n√©ratrice des moments (pour des variables al√©atoires discr√®tes), etc. On se contente ici de la fonction de r√©partition qui nous sera utile pour simuler des variables al√©atoires, ainsi que son inverse au sens de Levy.\n\nD√©finition 3 (Fonction de r√©partition üá¨üáß: cumulative distribution function) \nSoit X une variable al√©atoire sur (\\mathbb{R}, \\mathcal{B}(\\mathbb{R})). La fonction de r√©partition de X est la fonction F_X d√©finie sur \\mathbb{R} par \n\\begin{align*}\n    F_X(x) & = \\mathbb{P}(X \\leq x)\\\\\n           & = \\mathbb{P}(X \\in ]-\\infty, x]) \\enspace.\n\\end{align*}\n\n\n\nExemple 9 (Cas discret) Soit (x_i)_{i \\in I} une suite ordonn√©e de r√©els, avec I \\subset \\mathbb{N}. Si X est une variable al√©atoire discr√®te prenant les valeurs (x_i)_{i \\in I} et de loi (p_i = \\mathbb{P}(X=x_i))_{i \\in I}, alors \n    F_X(x) = \\sum_{i \\in I} p_i {1\\hspace{-3.8pt} 1}_{[x_i, \\infty[}(x) \\enspace.\n\n\n\nExemple 10 (Cas continu) Si X est une variable al√©atoire de densit√© f, alors \n    F_X(x) = \\int_{-\\infty}^x f(t) \\, \\mathrm dt \\enspace.\n\n\nLe graphe des fonctions de r√©partition des loi de Bernoulli, uniforme et normale sont repr√©sent√©es dans le widget ci-dessous. Notons que la fonction de r√©partition de la loi normale \\mathcal{N}(0,1), souvent not√©e \\Phi, n‚Äôadmet pas d‚Äôexpression explicite autre que \n\\Phi(x) = \\dfrac{1}{\\sqrt{2\\pi}} \\int_{-\\infty}^x e^{-\\frac{t^2}{2}}\\, \\mathrm d t\\enspace,\n Les valeurs num√©riques de \\Phi(x) √©taient autrefois report√©es dans des tables1. Par transformation affine, si X \\sim \\mathcal{N}(\\mu, \\sigma^2) ‚Äî ce que l‚Äôon peut aussi √©crire : X=\\mu + \\sigma Y, avec Y\\sim \\mathcal{N}(0,1) ‚Äî alors sa fonction de r√©partition est donn√©e par F_X(x)=\\Phi((x-\\mu)/\\sigma).\n1¬†Wikipedia: loi normale\nProposition 1 (Propri√©t√©s de la fonction de r√©partition) Soit X une variable al√©atoire de fonction de r√©partition F_X.\n\nF_X est une fonction croissante, de limite 0 en -\\infty et de limite 1 en +\\infty.\nF_X est continue √† droite en tout point.\nPour tout x \\in \\mathbb{R}, on a \\mathbb{P}(X=x) = F_X(x) - F_X(x-), o√π F_X(x-) = \\lim_{\\epsilon \\to 0+} F_X(x- \\epsilon).\nSi X a pour densit√© f, alors F_X est d√©rivable \\lambda-presque partout de d√©riv√©e f.\n\n\nPour les d√©monstrations, voir par exemple [@Barbe_Ledoux06].\nLa propri√©t√© 3. est utile dans le cas discret : les valeurs prises par X correspondent aux points de discontinuit√© de F_X et les probabilit√©s associ√©es correspondent √† la hauteur du saut.\nLa propri√©t√© 4. donne le lien entre la fonction de r√©partition d‚Äôune variable al√©atoire √† densit√© et sa densit√©. On peut donc retrouver la loi de X √† partir de sa fonction de r√©partition. Le th√©or√®me suivant g√©n√©ralise ce r√©sultat √† toute variable al√©atoire r√©elle (pas n√©cessairement discr√®te ou √† densit√©).\n\nTh√©or√®me 1 (Caract√©risation de la loi d‚Äôune variable al√©atoire r√©elle) La fonction de r√©partition d‚Äôune variable al√©atoire caract√©rise sa loi : deux variables al√©atoires ont m√™me loi si et seulement si elles ont m√™me fonction de r√©partition.\n\nD√©monstration: voir Wikipedia\nOn rappelle que la tribu des bor√©liens est engendr√©e par la famille d‚Äôensembles \\{]-\\infty,x], x \\in \\mathbb{R}\\}. Le th√©or√®me pr√©c√©dent assure que si on conna√Æt la mesure \\mathbb{P}_X sur cette famille d‚Äôensembles alors on la conna√Æt partout.\n\nExemple 11 (Loi exponentielle depuis une loi uniforme) On consid√®re une variable al√©atoire U de loi uniforme sur [0,1] et on pose X = -\\ln(1-U). D√©terminons la loi de X en calculant sa fonction de r√©partition. Pour tout x \\in \\mathbb{R}, \n\\begin{align*}\nF_X(x) = & \\mathbb{P}(X \\leq x) \\\\\n       = & \\mathbb{P}(-\\ln(1-U) \\leq x) \\\\\n       = & \\mathbb{P}(U \\leq 1-e^{-x}) \\\\\n       = &\n    \\begin{cases}\n        0           & \\text{ si }x &lt; 0\\,,    \\\\\n        1 - e^{-x} & \\text{ si }x \\geq 0\\,,\n    \\end{cases}\n\\end{align*}\n\no√π on a utilis√© l‚Äô√©galit√© \\mathbb{P}(U \\leq t) = t pour tout t \\in [0,1]. Ainsi la variable al√©atoire X a la m√™me fonction de r√©partition qu‚Äôune loi exponentielle de param√®tre 1. On en conclut que X \\sim \\mathcal{Exp}(1). Notons que l‚Äôon peut aussi montrer que -\\ln(X)\\sim\\mathcal{E}(1), sachant que U et 1-U ont la m√™me loi.",
    "crumbs": [
      "Cours",
      "Notations et rappels"
    ]
  },
  {
    "objectID": "Courses/notations.html#fonction-quantile-inverse-g√©n√©ralis√©e-√†-gauche",
    "href": "Courses/notations.html#fonction-quantile-inverse-g√©n√©ralis√©e-√†-gauche",
    "title": "Notations et rappels",
    "section": "Fonction quantile, inverse g√©n√©ralis√©e √† gauche",
    "text": "Fonction quantile, inverse g√©n√©ralis√©e √† gauche\nLa fonction de r√©partition √©tant une fonction croissante on peut donner un sens √† son inverse g√©n√©ralis√©e de la mani√®re suivante.\n\nD√©finition 4 (Fonction quantile/ inverse g√©n√©ralis√©e üá¨üáß: quantile distribution function) \nSoit X une variable al√©atoire sur (\\mathbb{R}, \\mathcal{B}(\\mathbb{R})) et F_X sa fonction de r√©partition. La fonction quantile associ√©e F_X^\\leftarrow:  ]0,1[ \\rightarrow \\mathbb{R} est d√©finie par \n  F_X^\\leftarrow(p)=  \\inf\\{ x \\in \\mathbb{R} \\colon F_X(x)\\geq p\\} \\enspace.\n\n\nOn parle parfois aussi d‚Äôinverse au sens de Levy pour cette inverse g√©n√©ralis√©e.\nDans le cas o√π la fonction de r√©partition F_X est bijective, alors l‚Äôinverse de la fonction de r√©partition coincide avec la fonction quantile.\nLa m√©diane est √©gale √† F_X^\\leftarrow(1/2), les premiers et troisi√®mes quartiles sont √©gaux √† F_X^\\leftarrow(1/4) et F_X^\\leftarrow(3/4). Enfin, les d√©ciles sont les quantiles F_X^\\leftarrow(k/10) pour k=1,\\dots, 9.",
    "crumbs": [
      "Cours",
      "Notations et rappels"
    ]
  },
  {
    "objectID": "Courses/notations.html#visualisation-densit√©-fonction-de-r√©partition-quantiles-etc.",
    "href": "Courses/notations.html#visualisation-densit√©-fonction-de-r√©partition-quantiles-etc.",
    "title": "Notations et rappels",
    "section": "Visualisation: densit√©, fonction de r√©partition, quantiles, etc.",
    "text": "Visualisation: densit√©, fonction de r√©partition, quantiles, etc.\n\nCas des variables continues\n\nObservablePython / Shiny\n\n\n\nPlotly = require('plotly.js-dist');\ndists = require( 'https://cdn.jsdelivr.net/gh/stdlib-js/stats-base-dists@umd/browser.js' );\n// see source here: https://github.com/stdlib-js/stats-base-dists/tree/umd\n// continuous case\njstatPDFs = () =&gt; {\n  const distributions = Object.keys(dists);\n  // Get in continuousDistributions the distributions whose pdf, cdf and quantile are defined\n    const continuousDistributions = distributions.filter(name =&gt; dists[name].pdf && dists[name].cdf && dists[name].quantile);\n  return continuousDistributions\n};\noutput = jstatPDFs();\n\nexcludedPDFs = [];\npdfNames_unsorted = output.filter(name =&gt; !excludedPDFs.includes(name));\npdfNames=pdfNames_unsorted.toSorted();\nviewof inputs = Inputs.form([\n      Inputs.range([-10, 10], {value: 0.1, step: 0.001, label: tex`\\mu`}),\n      Inputs.range([0.01, 5], {value: 1.01, step: 0.001, label: tex`\\sigma`}),\n      Inputs.range([0.001, 0.999], {value: 0.75, step: 0.001, label:tex`\\alpha`}),\n    ]);\n\nviewof distrib_name = Inputs.select(pdfNames, {value: \"normal\", label: \"Distribution\"});\n\n\n{\n  const x = d3.range(-5, 5, 0.01);\n  const pdf = x.map(x =&gt; dists[distrib_name].pdf(x, mu, sigma));\n  const cdf = x.map(x =&gt; dists[distrib_name].cdf(x, mu, sigma));\n  const inv = x.map(x =&gt; dists[distrib_name].quantile(x, mu, sigma));\n  const quantile = dists[distrib_name].quantile(alpha, mu, sigma);\n  const filteredX = x.filter(coord =&gt; coord &lt;= quantile);\n  const filteredPdf = pdf.filter((_, i) =&gt; x[i] &lt;= quantile);\n  const filteredCdf = cdf.filter(coord =&gt; coord &lt;= quantile);\n\n\n{\nvar trace1 = {\n      type: \"scatter\",\n      name: 'Quantile',\n      x : cdf,\n      y : x,\n      line: {color: 'black'},\n      xaxis: 'x1',\n      yaxis: 'y3',\n\n\n};\n\nvar trace12 = {\n        x : [alpha, alpha],\n        y : [x[0], quantile],\n        mode: 'lines',\n        line: {\n            dash: 'dash',\n            width: 1,\n            color: ' #428BCA',\n            marks: {\n                size: 0,\n                color: ' #428BCA',\n            }\n\n          },\n      xaxis: 'x1',\n      yaxis: 'y3',\n\n\n};\n\n\n\nvar trace13 = {\n    x: [0, alpha],\n    y: [quantile, quantile],\n    mode: 'scatter',\n    line: {\n        dash: 'dash',\n        width: 1,\n        color: ' #428BCA',\n        marks: {\n            size: 0,\n        }\n    },\n    xaxis: 'x1',\n    yaxis: 'y3'\n}\n\n\nvar trace2 = {\nx: cdf,\ny: cdf,\ntype: 'scatter',\nname: 'identity',\nline: {color: 'black'},\nxaxis: 'x1',\nyaxis: 'y2'\n\n};\n\n\nvar trace21 = {\n        x : [alpha, 1],\n        y : [alpha, alpha],\n        mode: 'lines',\n        line: {\n            dash: 'dash',\n            width: 1,\n            color: ' #428BCA',\n            marks: {\n                size: 0,\n                color: ' #428BCA',\n            }\n\n          },\n      xaxis: 'x1',\n      yaxis: 'y2'\n}\n\nvar trace22 = {\n        x : [alpha, alpha],\n        y : [alpha, 1],\n        mode: 'lines',\n        line: {\n            dash: 'dash',\n            width: 1,\n            color: ' #428BCA',\n            marks: {\n                size: 0,\n                color: ' #428BCA',\n            }\n\n          },\n      xaxis: 'x1',\n      yaxis: 'y2'\n}\n\n\n\nvar trace23 = {\n    x: [alpha],\n    y: [alpha],\n    mode: 'scatter',\n    line: {\n        dash: 'dash',\n        width: 1,\n        color: ' #428BCA',\n        marks: {\n            size: 0,\n        }\n    },\n    xaxis: 'x1',\n    yaxis: 'y2'\n}\n\n\nvar trace31 = {\n      type: \"scatter\",\n      mode: \"lines\",\n      name: 'PDF2',\n      x: x,\n      y: cdf,\n      line: {color: 'black'},\n      xaxis: 'x2',\n      yaxis: 'y2',\n};\n\nvar trace32 = {\n        x : filteredX,\n        y : filteredX.map(x =&gt; alpha),\n        mode: 'lines',\n        line: {\n            dash: 'dash',\n            width: 1,\n            color: ' #428BCA',\n            marks: {\n                size: 0,\n                color: ' #428BCA',\n            }\n          },\n      xaxis: 'x2',\n      yaxis: 'y2'\n}\n\nvar trace33 = {\n        x : [quantile, quantile],\n        y : [0, alpha],\n        mode: 'lines',\n        line: {\n            dash: 'dash',\n            width: 1,\n            color: ' #428BCA',\n            marks: {\n                size: 0,\n                color: ' #428BCA',\n            }\n          },\n\n      xaxis: 'x2',\n      yaxis: 'y2'\n}\n\nvar trace34 = {\n    x: [quantile],\n    y: [alpha],\n    mode: 'scatter',\n    line: {\n        dash: 'dash',\n        width: 1,\n        color: ' #428BCA',\n        marks: {\n            size: 0,\n        }\n    },\n    xaxis: 'x2',\n    yaxis: 'y2'\n}\n\n\nvar trace41 = {\n\n  type: \"scatter\",\n  name: 'Quantile2',\n  fill: 'tozeroy',\n  x : filteredX,\n  y : filteredPdf,\n  opacity: 0.9,\n  line: {color: ' #428BCA'},\n  xaxis: 'x2',\n  yaxis: 'y1'\n\n};\n\n\nvar trace42 = {\n\n  type: \"scatter\",\n  mode: \"lines\",\n  name: 'PDF2',\n  x: x,\n  y: pdf,\n  line: {color: 'black'},\n  xaxis: 'x2',\n  yaxis: 'y1'\n\n};\n\nvar data = [\n  trace1,\n  trace12, trace13,\n  trace2, trace21, trace22, trace23,\n  trace31, trace32, trace33, trace34,\n  trace41, trace42];\n\n\nvar layout = {\n\n  title: 'Distribution et quantile',\n  xaxis: {\n    domain: [0, 0.32],\n    anchor: 'y1'\n  },\n  yaxis: {\n    domain: [0, 0.24],\n    anchor: 'x1'\n\n  },\n  xaxis2: {\n    domain: [0.35, 1],\n    anchor: 'y'\n  },\n\n  yaxis2: {\n    domain: [0.26, 0.49],\n    anchor: 'x1'\n  },\n\n  yaxis3: {\n    domain: [0.5, 1],\n    anchor: 'x1'\n  },\n\n// legend offset\n  showlegend: false,\n  height: 680,\n  annotations: [\n\n    {\n      x: 1/4,\n      y: quantile - (x[0]-x.slice(-1))/20,\n      xref: 'x1',\n      yref: 'y3',\n      text: 'q= ' + quantile.toFixed(2),\n      font: {\n        size: 12,\n        color: '#428BCA',\n\n      },\n      showarrow: false,\n      arrowhead: 0,\n      ax: 25,\n      ay: -10,\n\n    },\n    {\n      x: 1/4,\n      y: alpha,\n      xref: 'x1',\n      yref: 'y2',\n      text: 'alpha= ' + alpha.toFixed(2),\n      font: {\n        size: 12,\n        color: '#428BCA',\n\n      },\n      showarrow: false,\n      arrowhead: 0,\n      ax: 25,\n      ay: -10,\n\n    },\n    {\n        text: \"Fonction quantile\",\n      font: {\n      size: 15,\n      color: 'black',\n    },\n    showarrow: false,\n    align: 'center',\n    x: -0.01,\n    y: 1.05,\n    xref: 'paper',\n    yref: 'paper',\n    },\n    {\n      text: \"Fonction de r√©partition\",\n      font: {\n      size: 15,\n      color: 'black',\n            },\n    showarrow: false,\n    align: 'center',\n    x: 0.65,\n    y: 0.52,\n    xref: 'paper',\n    yref: 'paper',\n    },\n    {\n      text: \"Fonction de densit√©\",\n      font: {\n      size: 15,\n      color: 'black',\n            },\n    showarrow: false,\n    align: 'center',\n    x: 0.65,\n    y: 0.225,\n    xref: 'paper',\n    yref: 'paper',\n    },\n  ]\n\n};\n// XXX: TODO: put the xticks labels on the middle plot for x and on the right plot for y\n    var config = {responsive: true}\n    const div = DOM.element('div');\n    Plotly.newPlot(div, data, layout, config);\n    return div;\n  }\n\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmu = inputs[0];\nsigma = inputs[1];\nalpha = inputs[2]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#| standalone: true\n#| viewerHeight: 830\nimport numpy as np\nfrom scipy import stats\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nfrom shiny import ui, render, App\nfrom shinywidgets import output_widget, render_widget\n\n\ndef keep_no_param_distribution():\n    distributions = stats._continuous_distns._distn_names\n    distributions_0 = []\n    for _, name in enumerate(distributions):\n        dist = getattr(stats, name)\n        if not dist.shapes or len(dist.shapes) == 0:\n            distributions_0.append(name)\n    distributions_0_val = [\n        getattr(stats.distributions, string) for string in distributions_0\n    ]\n    distributions_0_dict = dict(zip(distributions_0, distributions_0_val))\n    return distributions_0_dict\n\n\ndistributions_0_dict = keep_no_param_distribution()\n\nmu = 0\nsigma = 1\n\napp_ui = ui.page_fluid(\n    ui.div(\n        ui.input_slider(\"alpha\", \"Quantile\", 0.01, 0.99, value=0.5, step=0.01),\n        ui.input_slider(\"xrange\", \"x-range\", -10, 10, value=(-5, 5), step=0.2),\n        ui.input_select(\n            \"distrib\",\n            \"Distribution\",\n            list(distributions_0_dict.keys()),\n            selected='norm'\n        ),\n        class_=\"d-flex gap-3\",\n    ),\n    output_widget(\"my_widget\"),\n)\n\n\ndef server(input, output, session):\n    @output\n    @render_widget\n    def my_widget():\n        fig = make_subplots(\n            rows=3,\n            cols=2,\n            vertical_spacing=0.1,\n            horizontal_spacing=0.15,\n            subplot_titles=(\n                \"Fonction quantile\",\n                \"\",\n                \"\",\n                \"Fonction de r√©partition\",\n                \"\",\n                \"Densit√© et quantile\",\n            ),\n            column_widths=[0.2, 0.5],\n            row_heights=[0.35, 0.17, 0.17],\n        )\n\n        alpha = input.alpha()\n        distribution = distributions_0_dict[input.distrib()]\n        x = np.linspace(input.xrange()[0], input.xrange()[1], num=400)\n        cdf_data = distribution.cdf(x, loc=mu, scale=sigma)\n        pdf_data = distribution.pdf(x, loc=mu, scale=sigma)\n        q_alpha = distribution.ppf(alpha, loc=mu, scale=sigma)\n\n        fig.update_layout(autosize=True, height=700)\n\n        # Quantile plot\n        fig.add_trace(\n            go.Scatter(\n                x=cdf_data, y=x, mode=\"lines\", marker={\"color\": \"black\"}\n            ),\n            row=1,\n            col=1,\n        )\n        # Diagonal\n        fig.add_trace(\n            go.Scatter(\n                x=cdf_data, y=cdf_data, mode=\"lines\", marker={\"color\": \"black\"}\n            ),\n            row=2,\n            col=1,\n        )\n        # Cdf part\n        fig.add_trace(\n            go.Scatter(\n                x=x, y=cdf_data, mode=\"lines\", marker={\"color\": \"black\"}\n            ),\n            row=2,\n            col=2,\n        )\n        # pdf part\n        fig.add_scatter(\n            x=x[x &lt; q_alpha],\n            y=pdf_data[x &lt; q_alpha],\n            fill=\"tozeroy\",\n            mode=\"none\",\n            fillcolor=\"rgb(66, 139, 202)\",\n            row=3,\n            col=2,\n        )\n\n        fig.add_trace(\n            go.Scatter(\n                x=x, y=pdf_data, mode=\"lines\", marker={\"color\": \"black\"}\n            ),\n            row=3,\n            col=2,\n        )\n\n        # Dots\n        fig.add_trace(\n            go.Scatter(\n                x=[alpha],\n                y=[q_alpha],\n                mode=\"markers\",\n                marker={\"color\": \"rgb(66, 139, 202)\"},\n                marker_symbol=\"x\",\n                marker_size=8,\n            ),\n            row=1,\n            col=1,\n        )\n\n        fig.add_trace(\n            go.Scatter(\n                x=[alpha],\n                y=[alpha],\n                mode=\"markers\",\n                marker={\"color\": \"rgb(66, 139, 202)\"},\n                marker_symbol=\"x\",\n                marker_size=8,\n            ),\n            row=2,\n            col=1,\n        )\n\n        fig.add_trace(\n            go.Scatter(\n                x=[q_alpha],\n                y=[alpha],\n                mode=\"markers\",\n                marker={\"color\": \"rgb(66, 139, 202)\"},\n                marker_symbol=\"x\",\n                marker_size=8,\n            ),\n            row=2,\n            col=2,\n        )\n\n        # Lines\n        fig.add_trace(\n            go.Scatter(\n                x=[alpha, alpha],\n                y=[x[0], q_alpha],\n                mode=\"lines\",\n                line=dict(dash=\"dash\", color=\"rgb(66, 139, 202)\")\n            ),\n            row=1,\n            col=1\n        )\n\n        fig.add_trace(\n            go.Scatter(\n                x=[alpha, alpha],\n                y=[alpha, 1.],\n                mode=\"lines\",\n                line=dict(dash=\"dash\", color=\"rgb(66, 139, 202)\")\n            ),\n            row=2,\n            col=1\n        )\n        fig.add_trace(\n            go.Scatter(\n                x=[alpha, 1],\n                y=[alpha, alpha],\n                mode=\"lines\",\n                line=dict(dash=\"dash\", color=\"rgb(66, 139, 202)\")\n            ),\n            row=2,\n            col=1\n        )\n        fig.add_trace(\n            go.Scatter(\n                x=[x[0], q_alpha],\n                y=[alpha, alpha],\n                mode=\"lines\",\n                line=dict(dash=\"dash\", color=\"rgb(66, 139, 202)\")\n            ),\n            row=2,\n            col=2\n        )\n        fig.add_trace(\n            go.Scatter(\n                x=[x[0], q_alpha],\n                y=[x[0], 0],\n                mode=\"lines\",\n                line=dict(dash=\"dash\", color=\"rgb(66, 139, 202)\")\n            ),\n            row=2,\n            col=2\n        )\n        # Axes ranges\n        fig.update_xaxes(range=[0, 1.], row=1, col=1)\n        fig.update_yaxes(matches=\"x6\", row=1, col=1)\n\n        fig.update_yaxes(range=[0, 1.], row=2, col=1)\n        fig.update_xaxes(matches=\"x1\", row=2, col=1)\n\n        fig.update_yaxes(rangemode=\"tozero\", row=3, col=2)\n        fig.update_xaxes(range=[x[0], x[-1]], row=3, col=2)\n\n        fig.update_xaxes(matches=\"x6\", row=2, col=2)\n        fig.update_yaxes(matches=\"y3\", row=2, col=2)\n\n        # Add dropdown\n        fig.update_layout(\n            showlegend=False,\n            template=\"simple_white\",\n        )\n        return fig\n\n\napp = App(app_ui, server)\n\n\n\n\n\n\nCas des variables discr√®tes\n\nObservablePython / Shiny\n\n\n\ndiscretePDFs = () =&gt; {\n  const distributions = Object.keys(dists);\n  // Get in continuousDistributions the distributions whose pdf, cdf and quantile are defined\n    const continuousDistributions = distributions.filter(name =&gt; dists[name].pmf && dists[name].cdf && dists[name].quantile);\n  return continuousDistributions\n};\noutput_discr = discretePDFs();\n\npmfNames_unsorted = output_discr.filter(name =&gt; !excludedPDFs.includes(name));\npmfNames=pmfNames_unsorted.toSorted();\nviewof inputs_disc = Inputs.form([\n      Inputs.range([-10, 10], {value: 0.1, step: 0.001, label: tex`\\mu `}),\n      Inputs.range([0.01, 5], {value: 1.01, step: 0.001, label: tex`\\sigma `}),\n      Inputs.range([0.001, 0.999], {value: 0.75, step: 0.001, label:tex`\\alpha`}),\n    ]);\n\nviewof distrib_name_discr = Inputs.select(pmfNames, {value: \"normal\", label: \"Distribution\"});\n\n\n{\n  const z = d3.range(-5, 5, 0.01);\n  const pmf = z.map(z =&gt; dists[distrib_name_discr].pmf(z, mu_disc, sigma_disc));\n  const cdf = z.map(z =&gt; dists[distrib_name_discr].cdf(z, mu_disc, sigma_disc));\n  const inv = z.map(z =&gt; dists[distrib_name_discr].quantile(z, mu_disc, sigma_disc));\n  const quantile = dists[distrib_name_discr].quantile(alpha_disc, mu_disc, sigma_disc);\n  const filteredX = z.filter(coord =&gt; coord &lt;= quantile);\n  const filteredPmf = pmf.filter((_, i) =&gt; z[i] &lt;= quantile);\n  const filteredCdf = cdf.filter(coord =&gt; coord &lt;= quantile);\n\n\n{\nvar trace1 = {\n      type: \"scatter\",\n      name: 'Quantile',\n      x : cdf,\n      y : z,\n      line: {color: 'black'},\n      xaxis: 'x1',\n      yaxis: 'y3',\n\n\n};\n\nvar trace12 = {\n        x : [alpha_disc, alpha_disc],\n        y : [z[0], quantile],\n        mode: 'lines',\n        line: {\n            dash: 'dash',\n            width: 1,\n            color: ' #428BCA',\n            marks: {\n                size: 0,\n                color: ' #428BCA',\n            }\n\n          },\n      xaxis: 'x1',\n      yaxis: 'y3',\n\n\n};\n\n\n\nvar trace13 = {\n    x: [0, alpha_disc],\n    y: [quantile, quantile],\n    mode: 'scatter',\n    line: {\n        dash: 'dash',\n        width: 1,\n        color: ' #428BCA',\n        marks: {\n            size: 0,\n        }\n    },\n    xaxis: 'x1',\n    yaxis: 'y3'\n}\n\n\nvar trace2 = {\nx: cdf,\ny: cdf,\ntype: 'scatter',\nname: 'identity',\nline: {color: 'black'},\nxaxis: 'x1',\nyaxis: 'y2'\n\n};\n\n\nvar trace21 = {\n        x : [alpha_disc, 1],\n        y : [alpha_disc, alpha_disc],\n        mode: 'lines',\n        line: {\n            dash: 'dash',\n            width: 1,\n            color: ' #428BCA',\n            marks: {\n                size: 0,\n                color: ' #428BCA',\n            }\n\n          },\n      xaxis: 'x1',\n      yaxis: 'y2'\n}\n\nvar trace22 = {\n        x : [alpha_disc, alpha_disc],\n        y : [alpha_disc, 1],\n        mode: 'lines',\n        line: {\n            dash: 'dash',\n            width: 1,\n            color: ' #428BCA',\n            marks: {\n                size: 0,\n                color: ' #428BCA',\n            }\n\n          },\n      xaxis: 'x1',\n      yaxis: 'y2'\n}\n\nvar trace23 = {\n    x: [alpha_disc],\n    y: [alpha_disc],\n    mode: 'scatter',\n    line: {\n        dash: 'dash',\n        width: 1,\n        color: ' #428BCA',\n        marks: {\n            size: 0,\n        }\n    },\n    xaxis: 'x1',\n    yaxis: 'y2'\n}\n\n\nvar trace31 = {\n      type: \"scatter\",\n      mode: \"lines\",\n      name: 'PDF2',\n      x: z,\n      y: cdf,\n      line: {color: 'black'},\n      xaxis: 'x2',\n      yaxis: 'y2',\n};\n\nvar trace32 = {\n        x : filteredX,\n        y : filteredX.map(z =&gt; alpha_disc),\n        mode: 'lines',\n        line: {\n            dash: 'dash',\n            width: 1,\n            color: ' #428BCA',\n            marks: {\n                size: 0,\n                color: ' #428BCA',\n            }\n          },\n      xaxis: 'x2',\n      yaxis: 'y2'\n}\n\nvar trace33 = {\n        x : [quantile, quantile],\n        y : [0, alpha_disc],\n        mode: 'lines',\n        line: {\n            dash: 'dash',\n            width: 1,\n            color: ' #428BCA',\n            marks: {\n                size: 0,\n                color: ' #428BCA',\n            }\n          },\n\n      xaxis: 'x2',\n      yaxis: 'y2'\n}\n\nvar trace34 = {\n    x: [quantile],\n    y: [alpha_disc],\n    mode: 'scatter',\n    line: {\n        dash: 'dash',\n        width: 1,\n        color: ' #428BCA',\n        marks: {\n            size: 0,\n        }\n    },\n    xaxis: 'x2',\n    yaxis: 'y2'\n}\n\n\nvar trace41 = {\n\n  type: \"scatter\",\n  name: 'Quantile2',\n  fill: 'tozeroy',\n  x : filteredX,\n  y : filteredPmf,\n  opacity: 0.9,\n  line: {color: ' #428BCA'},\n  xaxis: 'x2',\n  yaxis: 'y1'\n\n};\n\n\nvar trace42 = {\n\n  type: \"scatter\",\n  mode: \"lines\",\n  name: 'PDF2',\n  x: z,\n  y: pmf,\n  line: {color: 'black'},\n  xaxis: 'x2',\n  yaxis: 'y1'\n\n};\n\nvar data = [\n  trace1,\n  trace12, trace13,\n  trace2, trace21, trace22, trace23,\n  trace31, trace32, trace33, trace34,\n  trace41, trace42];\n\n\nvar layout = {\n\n  title: 'Distribution et quantile',\n  xaxis: {\n    domain: [0, 0.32],\n    anchor: 'y1'\n  },\n  yaxis: {\n    domain: [0, 0.24],\n    anchor: 'x1'\n\n  },\n  xaxis2: {\n    domain: [0.35, 1],\n    anchor: 'y'\n  },\n\n  yaxis2: {\n    domain: [0.26, 0.49],\n    anchor: 'x1'\n  },\n\n\n\n  yaxis3: {\n    domain: [0.5, 1],\n    anchor: 'x1'\n\n  },\n\n  showlegend: false,\n  height: 680,\n\n  annotations: [\n\n    {\n      x: 1/4,\n      y: quantile - (z[0]-z.slice(-1))/20,\n      xref: 'x1',\n      yref: 'y3',\n      text: 'q=' + quantile.toFixed(2),\n      font: {\n        size: 12,\n        color: '#428BCA',\n\n      },\n      showarrow: false,\n      arrowhead: 0,\n      ax: 25,\n      ay: -10,\n\n    },\n    {\n      x: 1/4,\n      y: alpha_disc,\n      xref: 'x1',\n      yref: 'y2',\n      text: 'alpha=' + alpha_disc.toFixed(2),\n      font: {\n        size: 12,\n        color: '#428BCA',\n\n      },\n      showarrow: false,\n      arrowhead: 0,\n      ax: 25,\n      ay: -10,\n\n    },\n    {\n        text: \"Fonction quantile\",\n      font: {\n      size: 15,\n      color: 'black',\n    },\n    showarrow: false,\n    align: 'center',\n    x: -0.01,\n    y: 1.05,\n    xref: 'paper',\n    yref: 'paper',\n    },\n    {\n      text: \"Fonction de r√©partition\",\n      font: {\n      size: 15,\n      color: 'black',\n            },\n    showarrow: false,\n    align: 'center',\n    x: 0.65,\n    y: 0.52,\n    xref: 'paper',\n    yref: 'paper',\n    },\n    {\n      text: \"Fonction de densit√©\",\n      font: {\n      size: 15,\n      color: 'black',\n            },\n    showarrow: false,\n    align: 'center',\n    x: 0.65,\n    y: 0.225,\n    xref: 'paper',\n    yref: 'paper',\n    },\n  ]\n\n};\n// XXX: TODO: put the xticks labels on the middle plot for x and on the right plot for y\n\n    var config = {responsive: true}\n    const div = DOM.element('div');\n    Plotly.newPlot(div, data, layout, config);\n    return div;\n  }\n\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmu_disc = inputs_disc[0];\nsigma_disc = inputs_disc[1];\nalpha_disc = inputs_disc[2];\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#| standalone: true\n#| viewerHeight: 830\nimport numpy as np\nfrom scipy import stats\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nfrom shiny import ui, render, App\nfrom shinywidgets import output_widget, render_widget\n\n\ndef keep_no_param_distribution_disc():\n    distributions = stats._discrete_distns._distn_names\n    distributions_0 = [name for name in distributions if not getattr(stats, name).shapes or len(getattr(stats, name).shapes) in [1, 2]]\n    distributions_0_val = [getattr(stats.distributions, string) for string in distributions_0]\n    distributions_0_dict = dict(zip(distributions_0, distributions_0_val))\n    return distributions_0_dict\n\ndef cdf_tool(x, dtype='int64'):\n    y = np.zeros(2*(len(x)), dtype=dtype)\n    y[::2]=x\n    y[1::2]=x\n    return y[1::], y[:-1], y\n\ndef pmf_tool(x, dtype='int64'):\n    y = np.zeros(2*(len(x)), dtype=dtype)\n    y[::2]=x\n    return y[1::], y[:-1], y\n\ndef insert_nones(my_list):\n    for i, val in enumerate(my_list):\n        if i % 3 == 2:\n            my_list.insert(i, None)\n    return my_list\n\ndistributions_0_dict = keep_no_param_distribution_disc()\n\napp_ui = ui.page_fluid(\n    ui.div(\n        ui.input_slider(\"alpha\", \"Quantile\", 0.01, 0.99, value=0.5, step=0.01),\n        ui.input_slider(\"xrange\", \"x-range\", -10, 10, value=(-5.5, 5.5), step=0.2),\n        ui.input_select(\n            \"distrib\",\n            \"Distribution\",\n            list(distributions_0_dict.keys()),\n            selected='poisson'\n        ),\n        class_=\"d-flex gap-3\",\n    ),\n    output_widget(\"my_widget\"),\n)\n\n\ndef server(input, output, session):\n    @output\n    @render_widget\n    def my_widget():\n        fig = make_subplots(\n            rows=3,\n            cols=2,\n            vertical_spacing=0.1,\n            horizontal_spacing=0.15,\n            subplot_titles=(\n                \"Fonction quantile\",\n                \"\",\n                \"\",\n                \"Fonction de r√©partition\",\n                \"\",\n                \"Fonction de masse et quantile\",\n            ),\n            column_widths=[0.2, 0.5],\n            row_heights=[0.35, 0.17, 0.17],\n        )\n\n\n        alpha = input.alpha()\n        # alpha=0.5\n\n        mu = 0.5  # Param needed for some distribution\n        if input.distrib()=='zipf':\n            mu = 2\n        distribution = distributions_0_dict[input.distrib()]\n        # distribution=distributions_0_dict['poisson']\n        x = np.arange(np.floor(input.xrange()[0]), np.ceil(input.xrange()[1]))\n        # x = np.arange(np.floor(-5.5), np.ceil(5.5))\n\n        cdf_data = distribution.cdf(x, mu)\n        pmf_data = distribution.pmf(x, mu)\n        q_alpha = distribution.ppf(alpha, mu)\n        support = pmf_data.nonzero()[0]\n        fig.update_layout(autosize=True, height=700)\n\n        # Quantile plot\n        new_x, new_y, new_z = cdf_tool(support)\n        _, _, new_pmf = pmf_tool(support)\n\n        fig.add_trace(\n            go.Scatter(\n                x=insert_nones(list(np.append(cdf_data[new_y[::-1]], distribution.cdf(x[0], mu)))),\n                y=insert_nones(list(np.append(x[new_x[::-1]], x[new_x[0]]))),\n                mode=\"lines\",\n                line=dict(color=\"black\")\n            ),\n            row=1,\n            col=1,\n        )\n        fig.add_trace(\n             go.Scatter(\n                x=cdf_data[support], y=x[support],\n                mode=\"markers\", marker={\"color\": \"black\"}\n            ),\n            row=1,\n            col=1,\n        )\n        # Diagonal\n        fig.add_trace(\n            go.Scatter(\n                x=cdf_data, y=cdf_data, mode=\"lines\", marker={\"color\": \"black\"}\n            ),\n            row=2,\n            col=1,\n        )\n        # Cdf part\n        fig.add_trace(\n            go.Scatter(\n                x=x[support], y=cdf_data[support],\n                mode=\"markers\", marker={\"color\": \"black\"}\n            ),\n            row=2,\n            col=2,\n        )\n        fig.add_trace(\n            go.Scatter(\n                x=insert_nones(list(np.append(np.insert(x[new_x], 0, [x[0], x[new_x[0]]]),x[-1]))),\n                y=insert_nones(list(np.append(np.insert(cdf_data[new_y], 0, [0,0]), cdf_data[-1]))),\n                mode=\"lines\",\n                line=dict(color=\"black\")\n            ),\n            row=2,\n            col=2\n        )\n\n        fig.add_trace(\n            go.Scatter(\n                x=x, y=pmf_data, mode=\"markers\", marker={\"color\": \"black\"}\n            ),\n            row=3,\n            col=2,\n        )\n        fig.add_trace(\n            go.Scatter(\n                x=x, y=pmf_data, mode=\"markers\", marker={\"color\": \"black\"}\n            ),\n            row=3,\n            col=2,\n        )\n        x_bar = insert_nones(list(x[new_z]))\n        y_bar = insert_nones(list(pmf_data[new_pmf]))\n        fig.add_trace(\n            go.Scatter(\n                x=x_bar,\n                y=y_bar,\n                mode=\"lines\",\n                line=dict(color=\"black\")\n            ),\n            row=3,\n            col=2\n        )\n        _,_, devil_x = cdf_tool(x[x&lt;=q_alpha])\n        _,_, devil_y = cdf_tool(pmf_data[x&lt;q_alpha], dtype='float64')\n\n        x_bar_blue = insert_nones(list(devil_x))\n        y_bar_blue = np.array(insert_nones(list(devil_y)))\n        y_bar_blue[::-3]=0.\n        y_bar_blue = list(y_bar_blue)\n        fig.add_trace(\n            go.Scatter(\n                x=x_bar_blue,\n                y=y_bar_blue,\n                mode=\"lines\",\n                line=dict(color=\"rgb(66, 139, 202)\")\n            ),\n            row=3,\n            col=2\n        )\n        # pdf part\n        fig.add_scatter(\n            x=x[x &lt;= q_alpha],\n            y=pmf_data[x &lt;= q_alpha],\n            mode=\"markers\",\n            marker={\"color\":\"rgb(66, 139, 202)\"},\n            row=3,\n            col=2,\n        )\n\n        # Dots\n        fig.add_trace(\n            go.Scatter(\n                x=[alpha],\n                y=[q_alpha],\n                mode=\"markers\",\n                marker={\"color\": \"rgb(66, 139, 202)\"},\n                marker_symbol=\"x\",\n                marker_size=8,\n            ),\n            row=1,\n            col=1,\n        )\n\n        fig.add_trace(\n            go.Scatter(\n                x=[alpha],\n                y=[alpha],\n                mode=\"markers\",\n                marker={\"color\": \"rgb(66, 139, 202)\"},\n                marker_symbol=\"x\",\n                marker_size=8,\n            ),\n            row=2,\n            col=1,\n        )\n\n        fig.add_trace(\n            go.Scatter(\n                x=[q_alpha],\n                y=[alpha],\n                mode=\"markers\",\n                marker={\"color\": \"rgb(66, 139, 202)\"},\n                marker_symbol=\"x\",\n                marker_size=8,\n            ),\n            row=2,\n            col=2,\n        )\n\n        # Lines\n        fig.add_trace(\n            go.Scatter(\n                x=[alpha, alpha],\n                y=[x[0], q_alpha],\n                mode=\"lines\",\n                line=dict(dash=\"dash\", color=\"rgb(66, 139, 202)\")\n            ),\n            row=1,\n            col=1\n        )\n\n        fig.add_trace(\n            go.Scatter(\n                x=[alpha, alpha],\n                y=[alpha, 1.],\n                mode=\"lines\",\n                line=dict(dash=\"dash\", color=\"rgb(66, 139, 202)\")\n            ),\n            row=2,\n            col=1\n        )\n        fig.add_trace(\n            go.Scatter(\n                x=[alpha, 1],\n                y=[alpha, alpha],\n                mode=\"lines\",\n                line=dict(dash=\"dash\", color=\"rgb(66, 139, 202)\")\n            ),\n            row=2,\n            col=1\n        )\n        fig.add_trace(\n            go.Scatter(\n                x=[x[0], q_alpha],\n                y=[alpha, alpha],\n                mode=\"lines\",\n                line=dict(dash=\"dash\", color=\"rgb(66, 139, 202)\")\n            ),\n            row=2,\n            col=2\n        )\n        fig.add_trace(\n            go.Scatter(\n                x=[x[0], q_alpha],\n                y=[x[0], 0],\n                mode=\"lines\",\n                line=dict(dash=\"dash\", color=\"rgb(66, 139, 202)\")\n            ),\n            row=2,\n            col=2\n        )\n        # Axes ranges\n        fig.update_xaxes(range=[0, 1.05], row=1, col=1)\n        fig.update_yaxes(matches=\"x6\", row=1, col=1)\n\n        fig.update_yaxes(range=[0, 1.05], row=2, col=1)\n        fig.update_xaxes(matches=\"x1\", row=2, col=1)\n\n        fig.update_yaxes(rangemode=\"tozero\", row=3, col=2)\n        fig.update_xaxes(range=[x[0], x[-1]], row=3, col=2)\n\n        fig.update_xaxes(matches=\"x6\", row=2, col=2)\n        fig.update_yaxes(matches=\"y3\", row=2, col=2)\n\n        # Add dropdown\n        fig.update_layout(\n            showlegend=False,\n            template=\"simple_white\",\n        )\n        return fig\n\n\napp = App(app_ui, server)",
    "crumbs": [
      "Cours",
      "Notations et rappels"
    ]
  },
  {
    "objectID": "Courses/simulation.html",
    "href": "Courses/simulation.html",
    "title": "Simulation",
    "section": "",
    "text": "Dans ce chapitre on se demande comment simuler en pratique des variables al√©atoires i.i.d. L‚Äôid√©e est de commencer par le cas de variables al√©atoires de loi uniforme et d‚Äôen d√©duire les autres lois.",
    "crumbs": [
      "Cours",
      "Simulation"
    ]
  },
  {
    "objectID": "Courses/simulation.html#variables-al√©atoires-uniformes",
    "href": "Courses/simulation.html#variables-al√©atoires-uniformes",
    "title": "Simulation",
    "section": "Variables al√©atoires uniformes",
    "text": "Variables al√©atoires uniformes\nOn rappelle qu‚Äôune variable al√©atoire U suit une loi uniforme sur [0,1], not√© \\mathcal{U}([0,1]) si sa fonction de r√©partition F_U est donn√©e par \nF_U(x)\n=\n\\begin{cases}\n    0, & \\text{si }x &lt; 0\\,,        \\\\\n    x, & \\text{si }x \\in [0,1]\\,,  \\\\\n    1, & \\text{si }x &gt; 1\\,.        \\\\\n\\end{cases}\n\n\n\n\n\n                                                \n\n\nFigure¬†1: Fonction de r√©partition de la loi uniforme\n\n\n\n\nL‚Äôobjectif est de simuler sur machine une suite U_1, \\ldots, U_n de variables al√©atoires i.i.d. de loi \\mathcal{U}([0,1]). Plusieurs probl√®mes apparaissent alors :\n\nUne machine est d√©terministe.\nLes nombres entre 0 et 1 donn√©s par la machine sont de la forme k/2^p, pour k \\in \\{0, \\ldots, 2^{p-1}\\}. On ne pourra donc jamais g√©n√©rer des nombres qui ne sont pas de cette forme.\nV√©rifier qu‚Äôune suite est bien i.i.d. est un probl√®me difficile.\n\n\nD√©finition 1 (G√©n√©rateur de nombres pseudo-al√©atoires) \nUn g√©n√©rateur de nombres pseudo-al√©atoires (üá¨üáß: Pseudo Random Number Generator, PRNG), est un algorithme d√©terministe r√©cursif qui renvoie une suite U_1, \\ldots, U_n dans [0,1] qui a un ‚Äúcomportement similaire‚Äù √† une suite i.i.d. de loi \\mathcal{U}([0,1]). Pour √™tre plus rigoureux, ces nombres sont en fait des nombres entiers g√©n√©r√©s uniform√©ment sur un certain interval. Dans un second temps, une transformation simple (normalisation) permet d‚Äôobtenir des nombres flottants (üá¨üáß: floats) entre 0 et 1.\n\n\n\n\n\n\n\nPour aller plus loin\n\n\n\nParfois il est utile d‚Äôaller chercher dans le code source certaines information pour savoir comment les fonctions sont cod√©es dans les packages que l‚Äôon utiliser. Par exemple, pour numpy que l‚Äôon utilise fr√©quement, on peut voir l‚Äôop√©ration choisie ici: Random: int -&gt; float en numpy.\n\n\nUn tel algorithme se construit de la mani√®re suivante :\n\nOn part d‚Äôune graine (üá¨üáß: seed) U_0 qui d√©termine la premi√®re valeur de mani√®re la plus arbitraire possible.\nLa proc√©dure r√©cursive s‚Äô√©crit U_{n+1} = f(U_n), o√π f est une transformation d√©terministe, de sorte que U_{n+1} est le plus ind√©pendant possible de U_1, \\dots, U_n.\n\n\nLa fonction f est d√©terministe et prend ses valeurs dans un ensemble fini, donc l‚Äôalgorithme est p√©riodique. Le but est donc d‚Äôavoir la plus grande p√©riode possible.\nNotons qu‚Äôune fois que la graine est fix√©e, alors l‚Äôalgorithme donne toujours les m√™mes valeurs. Fixer la graine peut donc √™tre tr√®s utile pour r√©p√©ter des simulations dans des conditions identiques et ainsi rep√©rer des erreurs.\n\n\n\n\n\n\n\nExercice: bug ou feature?\n\n\n\nReprendre les widgets du chapitre Th√©or√®mes asymptotiques et faites varier doucement le param√®tre p (de Bernoulli). Que constatez-vous? Proposer une explication potentielle.\n\n\n\nG√©n√©rateur congruentiel lin√©aire\nLa plupart des PRNG s‚Äôappuient sur des r√©sultats arithm√©tiques. Un des plus connus est celui appel√© G√©n√©rateur congruentiel lin√©aire (üá¨üáß Linear congruential generator, LCG). Il est d√©fini comme suit: on construit r√©cursivement une suite d‚Äôentiers X_i via la congruence \n  X_{n+1} = a X_n + b \\quad \\text{mod } m \\enspace,\n o√π a,b,m sont des entiers bien choisis pour que la suite obtenue ait de bonnes propri√©t√©s. Il suffit alors de consid√©rer X_n/m. Par exemple, la fonction rand sur scilab utilise cette congruence avec m=2^{31}, a=843\\; 314\\; 861, et b=453\\; 816\\; 693.\n\n\nG√©n√©rateurs alternatifs\nLes langages Python et R utilisent par d√©faut le g√©n√©rateur Mersenne-Twister qui s‚Äôappuie sur la multiplication vectorielle, mais d‚Äôautres g√©n√©rateurs sont aussi disponibles. Ce g√©n√©rateur a pour p√©riode m =2^{19937}-1, nombre qu‚Äôon peut raisonnablement consid√©rer comme grand.\nPour numpy la m√©thode par d√©faut est PCG64 (cf.¬†documentation de numpy), qui dispose de meilleures garanties statistiques (Voir le site https://www.pcg-random.org pour cela).\n\n\nUsage en numpy\nOn suppose d√©sormais disposer d‚Äôun g√©n√©rateur pseudo-al√©atoire sur [0,1]. En numpy depuis la version 1.17, une bonne mani√®re d‚Äôutiliser des √©l√©ments al√©atoires est d‚Äôutiliser un g√©n√©rateur que l‚Äôon d√©finit soi-m√™me:\n\nseed = 12345  # Toujours √™tre conscient qu'une graine existe\nrng = np.random.default_rng(seed)  #\nprint(rng.random())  ##  un tirage uniforme sur [0,1]\nprint(rng.random(size=5))  ## cinq tirages uniformes sur [0,1]\nprint(rng.random(size=(3, 2)))  ## matrice 3x2, √† entr√©es unif. sur [0,1]\n\n0.22733602246716966\n[0.31675834 0.79736546 0.67625467 0.39110955 0.33281393]\n[[0.59830875 0.18673419]\n [0.67275604 0.94180287]\n [0.24824571 0.94888115]]\n\n\nDans la suite on va voir comment g√©n√©rer d‚Äôautres lois √† partir de la loi uniforme, mais il est clair que les logiciels modernes proposent un large √©ventail de distribution classique (gaussienne, exponentielle, etc.). Une liste exhaustive est donn√©e ici pour numpy.\n\n\n\n\n\n\nPour aller plus loin\n\n\n\nUne excellent discussion sur les bonnes pratiques al√©atoires en numpy, et l‚Äôusage de np.random.default_rng est donn√©e dans ce blog post d‚ÄôAlbert Thomas.\n\n\n\n\nPropri√©t√© de la loi uniforme\nOn verra souvent appara√Ætre la variable al√©atoire 1-U o√π U \\sim \\mathcal{U}([0,1]). Il se trouve que 1-U suit aussi une loi uniforme sur [0,1] comme le montre le calcul de sa fonction de r√©partition. Ainsi pour tout x \\in [0,1] on obtient \n\\begin{align*}\n\\mathbb{P}(1-U \\leq x) & = \\mathbb{P}(U \\geq 1-x),\\\\\n                       & = 1-(1-x), \\\\\n                       & = x\\,.\n\\end{align*}\n On peut d√©montrer facilement la m√™me relation pour x&lt;0 et x&gt;1, d‚Äôo√π le r√©sultat.",
    "crumbs": [
      "Cours",
      "Simulation"
    ]
  },
  {
    "objectID": "Courses/simulation.html#m√©thode-dinversion",
    "href": "Courses/simulation.html#m√©thode-dinversion",
    "title": "Simulation",
    "section": "M√©thode d‚Äôinversion",
    "text": "M√©thode d‚Äôinversion\nL‚Äôid√©e de la m√©thode d‚Äôinversion repose sur le r√©sultat suivant :",
    "crumbs": [
      "Cours",
      "Simulation"
    ]
  },
  {
    "objectID": "Courses/simulation.html#rappel-sur-la-fonction-quantile",
    "href": "Courses/simulation.html#rappel-sur-la-fonction-quantile",
    "title": "Simulation",
    "section": "Rappel sur la fonction quantile",
    "text": "Rappel sur la fonction quantile\nRappel : Pour F une fonction d√©finie sur \\mathbb{R} √† valeurs dans [0, 1], croissante, on note\n\n\\forall q \\in ]0,1[, \\quad F^\\leftarrow(q) = \\inf\\{ x \\in \\mathbb{R} : F(x)\\geq q\\}\n\\tag{1}\n\nTh√©or√®me 1 (Carat√©risation des quantiles) Soit F une fonction d√©finie sur \\mathbb{R} √† valeurs dans [0, 1], croissante et continue √† droite, alors pour tout q \\in ]0, 1[, on a \n\\begin{align}\n   \\{x \\in \\mathbb{R} :  F(x) \\geq q) \\} & =\n   \\{x \\in \\mathbb{R} : x \\geq F^\\leftarrow(q)  \\}\n\\end{align}\n\n\n\nPreuve. \n\nCas \\subset: Soit x \\in \\mathbb{R} t.q. F(x) \\geq q, alors par d√©finition de l‚Äôinf dans √âquation¬†1, x \\geq F^\\leftarrow(q).\nCas \\supset: Soit x \\in \\mathbb{R} t.q. x \\geq F_X^\\leftarrow(q) alors pour tout \\epsilon &gt; 0, x + \\epsilon &gt; F^\\leftarrow(q), donc F(x + \\epsilon) \\geq q. Puis, par continuit√© √† droite de F, F(x) \\geq q.",
    "crumbs": [
      "Cours",
      "Simulation"
    ]
  },
  {
    "objectID": "Courses/simulation.html#m√©thode-dinversion-1",
    "href": "Courses/simulation.html#m√©thode-dinversion-1",
    "title": "Simulation",
    "section": "M√©thode d‚Äôinversion",
    "text": "M√©thode d‚Äôinversion\n\nTh√©or√®me 2 (M√©thode d‚Äôinversion) Soit X une v.a r√©elle, et U \\sim\\mathcal{U}([0,1]), alors la variable al√©atoire F_X^{\\leftarrow}(U) a m√™me loi que X.\n\n\nPreuve. En utilisant le th√©or√®me pr√©c√©dent, on a \\mathbb{P}(F_X^{\\leftarrow}(U) \\leq x) = \\mathbb{P}(U \\leq F_X(x)) pour tout x\\in\\mathbb{R}. Puis, comme U est une loi uniforme sur [0,1],  \\mathbb{P}(U\\leq F_X(x))=F_X(x).\nOn en d√©duit donc que la loi de F_X^{-1}(U) est la m√™me que celle de X, car les deux v.a. ont la m√™me fonction de r√©partition.\n\n\n\nExemple 1 (Simulation d‚Äôune loi exponentielle) On rappelle que la loi exponentielle de param√®tre \\lambda &gt; 0 a pour densit√© \nf_{\\lambda}(x) = \\lambda e^{-\\lambda x} {1\\hspace{-3.8pt} 1}_{\\mathbb{R}_+}(x)\\enspace.\n et donc pour fonction de r√©partition \nF_{\\lambda}(x) = (1 - e^{-\\lambda x}) {1\\hspace{-3.8pt} 1}_{\\mathbb{R}_+}(x)\\enspace.\n On v√©rifie que F_{\\lambda} est bijective de \\mathbb{R}_+ dans ]0,1[ et que son inverse est donn√©e pour tout u \\in ]0,1[ par \nF_{\\lambda}^{-1}(u) = -\\frac{1}{\\lambda} \\log(1-u)\\enspace.\n\n\n\n\nExemple 2 (Simulation d‚Äôune loi de Weibull) La loi de Weibull de param√®tre \\lambda &gt; 0 et k &gt;0 est caract√©ris√©e par la fonction de r√©partition \nF(x) = (1 - e^{-(x/\\lambda)^k}){1\\hspace{-3.8pt} 1}_{\\mathbb{R}_+}(x)\n C‚Äôest une loi utilis√©e dans diff√©rents domaines, notamment en gestion des risques (hydrologie, finance, assurance, etc.). Le calcul de l‚Äôinverse g√©n√©ralis√©e F^\\leftarrow est imm√©diat car F est bijective sur ]0, \\infty[ : \n    F^\\leftarrow (u) = \\lambda (-\\ln(1-u))^{\\frac{1}{k}}\\,, \\quad u \\in ]0,1[\\,.\n La m√©thode d‚Äôinversion s‚Äôapplique : si U \\sim \\mathcal{U}([0,1]), alors la variable al√©atoire X = \\lambda (-\\ln(U))^{\\frac{1}{k}} suit une loi de Weibull de param√®tres \\lambda et k.\n\nMalheureusement, la fonction F n‚Äôest pas toujours inversible (penser aux lois discr√®tes) c‚Äôest donc pourquoi on utilise l‚Äôinverse l‚Äôinverse g√©n√©ralis√©e ou fonction quantile introduite dans la section Notations: \n\n  F^\\leftarrow(p)=  \\inf\\{ x \\in \\mathbb{R} \\colon F(x)\\geq p\\} \\enspace.\n\nInterpr√©tation: D√©finir l‚Äôinverse d‚Äôune fonction de r√©partition F revient √† r√©soudre l‚Äô√©quation F(x) = \\alpha d‚Äôinconnue x pour un \\alpha fix√©. Si F n‚Äôest pas bijective, deux probl√®mes apparaissent :\n\nl‚Äô√©quation n‚Äôa aucune solution ce qui revient √† dire que F n‚Äôest pas surjectif (graphiquement, F pr√©sente des sauts) ;\nl‚Äô√©quation a plusieurs solutions ce qui revient √† dire que F n‚Äôest pas injective (graphiquement cela se mat√©rialise par un plateau √† la hauteur \\alpha). Un exemple classique est celui o√π F est la fonction de r√©partition d‚Äôune variable al√©atoire discr√®te.\n\nLe passage √† l‚Äôin√©quation F(x) \\geq u permet de contourner la non-surjectivit√© : on ne regarde non plus les droites horizontales y=u mais la r√©gion \\{y \\geq \\alpha\\}. Le choix de l‚Äô\\inf dans la d√©finition de F^{\\leftarrow} permet de contourner la non-injectivit√© : vu qu‚Äôil y a possiblement plusieurs x tels que F(x) \\geq u, on choisit le ‚Äúpremier‚Äù. Ces consid√©rations sont illustr√©es en Figure Figure¬†2.\n\n\n\n\n                                                \n\n\nFigure¬†2\n\n\n\n\nRemarques additionnelles:\n\nLa fonction F √©tant croissante, la quantit√© F^\\leftarrow(u) correspond au premier instant o√π F d√©passe \\alpha. Si F est bijective (ce qui √©quivaut dans ce cas √† strictement croissante et injective), alors F^\\leftarrow = F^{-1}.\nLa fonction F^\\leftarrow n‚Äôest rien d‚Äôautre que la fonction quantile : si 0 &lt; \\alpha &lt; 1, q_{1-\\alpha} = F^\\leftarrow(1-\\alpha) est le quantile d‚Äôordre (1-\\alpha) de F. Par exemple, F^\\leftarrow(1/2) correspond √† la m√©diane.\nNotons que si u=0, on peut alors naturellement poser F^{\\leftarrow}(0) = -\\infty. De m√™me, avec la convention la convention \\inf \\emptyset = +\\infty, on peut alors √©tendre la d√©finition de F^\\leftarrow √† u=1 (mais F^\\leftarrow(1) n‚Äôest pas toujours √©gal √† \\infty, voir les exemples ci-dessous).\n\n\nExemple 3 (Simulation d‚Äôune loi de Bernoulli) La fonction de r√©partition F d‚Äôune loi de Bernoulli de param√®tre p \\in ]0,1[ est donn√©e par \n    F(x) =\n    \\Bigg\\{ \\begin{array}{ll}\n        0   & \\text{ si } x &lt; 0\\,,        \\\\\n        1-p & \\text{ si } 0 \\leq x &lt; 1\\,, \\\\\n        1   & \\text{ si } x \\leq 1\\,.\n    \\end{array}\n L‚Äôinverse g√©n√©ralis√©e de F peut ainsi √™tre calcul√©e via la formule de l‚Äôexemple : \n    F^\\leftarrow(u) =\n    \\bigg\\{ \\begin{array}{ll}\n        0 & \\text{ si } 0 &lt; u \\leq 1-p\\,, \\\\\n        1 & \\text{ si } 1-p &lt; u \\leq 1\\,.\n    \\end{array}\n ce qui se r√©√©crit plus simplement F^\\leftarrow(u) = {1\\hspace{-3.8pt} 1}_{\\{1-p &lt; u\\}}.\nAinsi, si U suit une loi uniforme sur [0,1] alors {1\\hspace{-3.8pt} 1}_{\\{1-p &lt; U\\}} suit une loi de Bernoulli de param√®tre p. Comme 1-U suit aussi une loi uniforme sur [0,1], on en d√©duit que {1\\hspace{-3.8pt} 1}_{\\{U &lt; p\\}} suit une loi de Bernoulli de param√®tre p. Notons que l‚Äôon peut remplacer l‚Äôin√©galit√© stricte par une in√©galit√© large.\n\n\n\nProposition 1 (Loi √† support fini) \nSoit X une variable al√©atoire discr√®te prenant uniquement les valeurs x_1 &lt; \\dots &lt; x_r (r modalit√© possibles) avec probabilit√© p_1, \\dots, p_r (donc p_1 + \\dots + p_r=1). On v√©rifie que pour tout u \\in ]0,1[, \n        F^\\leftarrow(u) =\n        \\begin{cases}\n            x_1 & \\text{si } 0 &lt; u \\leq p_1\\,,                  \\\\\n            x_2 & \\text{si } p_1 &lt; u \\leq p_1+p_2\\,,            \\\\\n                & \\vdots                                        \\\\\n            x_r & \\text{si }  \\sum_{i=1}^{r-1} p_i &lt; u &lt; 1\\,.\n        \\end{cases}\n\nSur cet exemple, on peut prolonger la d√©finition de F^\\leftarrow √† u=1 en posant F^\\leftarrow(1) = x_r. L‚Äôinverse g√©n√©ralis√©e se r√©√©crit alors sous la forme \n        F^\\leftarrow(u) = \\sum_{k=1}^r x_k {1\\hspace{-3.8pt} 1}_{ \\{  \\sum_{i=1}^{k-1}p_i &lt; u \\leq \\sum_{i=1}^{k}p_i \\} }\\enspace,\n o√π on a pos√© p_0=0.\n\nL‚Äôexpression pr√©c√©dente s‚Äô√©tend directement au cas o√π X prend un nombre (infini) d√©nombrable de valeurs, la somme devenant alors une s√©rie.\nLa m√©thode est illustr√© ci-dessous pour quelques lois int√©ressantes:\n\nviewof dist = Inputs.select(['normal','cauchy','laplace','bimodal'], {value: \"bimodal\", label: \"Loi\"})\nviewof replay = html`&lt;button&gt;Relancer`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ninvcdfboard = require(await FileAttachment(\"../inverse-vizu/dist/invcdfboard.umd.cjs\").url())\n\n{\n  replay\n  const canvas = DOM.canvas(500, 500);\n  const galton = invcdfboard.galton(canvas,dist);\n  \n  return html`${galton.canvas}`\n}",
    "crumbs": [
      "Cours",
      "Simulation"
    ]
  },
  {
    "objectID": "Courses/simulation.html#m√©thode-de-rejet",
    "href": "Courses/simulation.html#m√©thode-de-rejet",
    "title": "Simulation",
    "section": "M√©thode de rejet",
    "text": "M√©thode de rejet\nL‚Äôid√©e de la m√©thode de rejet est la suivante. On souhaite simuler une variable al√©atoire X de densit√© f, appel√©e loi cible, mais f est trop compliqu√©e pour que la simulation puisse se faire directement. On dispose cependant d‚Äôune autre densit√© g poss√©dant les propri√©t√©s suivantes :\n\non sait simuler Y de loi g,\nil existe m &gt; 0 tel que f(x) \\leq m \\cdot g(x),\non sait √©valuer le rapport d‚Äôacceptation r(x) = \\frac{f(x)}{mg(x)}.\n\nRemarquons d‚Äôores et d√©j√† que la constante m est n√©cessairement plus grande que 1 car \n    1 = \\int_\\mathbb{R} f(x) \\, dx \\leq m \\int_\\mathbb{R} g(x)\\, dx = m\\,.\n\nL‚Äôid√©e est alors de consid√©rer deux suites i.i.d. de variables al√©atoires ind√©pendantes entre elles:\n\n(Y_n)_{n \\geq 1} de loi g,\n(U_n)_{n \\geq 1} de loi uniforme sur [0,1].\n\nEn pratique, Y_n correspond √† une proposition et U_n permettra de d√©cider si on accepte la proposition ou non. Si oui, alors on conserve Y_n, sinon on simule Y_{n+1}. Le rapport d‚Äôacceptation, c‚Äôest-√†-dire la proportion de Y_n accept√©es, correspond √† r(x).\nAutrement dit, pour simuler X de densit√© f, il suffit de simuler Y de densit√© g et U uniforme jusqu‚Äô√† ce que U \\leq r(Y). La proposition suivante assure que cette m√©thode donne bien le r√©sultat voulu.\n\nProposition 2 (M√©thode de rejet) \nSoit T = \\inf \\{n \\geq 1 : U_n \\leq r(Y_n)\\} le premier instant o√π le tirage est accept√©. Alors :\n\nT suit une loi g√©om√©trique de param√®tre 1/m,\nla variable al√©atoire X = Y_T a pour densit√© f et est ind√©pendante de T.\n\n\n\nPreuve. Il s‚Äôagit d‚Äô√©tudier la loi du couple (X,T). Pour x \\in \\mathbb{R} et n \\in \\mathbb{N}^{*}, on √©crit \\mathbb{P}(X \\leq x, T=n)= \\mathbb{P}(U_1 &gt; r(Y_1), \\dots, U_{n-1} &gt; r(Y_{n-1}), U_n \\leq r(Y_n), Y_n \\leq x). Les n tirages √©tant iid, on obtient \n    \\mathbb{P}(X \\leq x, T=n) = \\mathbb{P}(U_1 &gt; r(Y_1))^{n-1} \\mathbb{P}(U_n \\leq r(Y_n), Y_n \\leq x)\\,.\n\nConcernant le premier terme, les variables al√©atoires Y_1 et U_1 sont ind√©pendantes donc leur loi jointe correspond au produit des densit√©s : \\begin{align*}\n        \\mathbb{P}(U_1 &gt; r(Y_1))\n         & = \\mathbb{P}((U_1, Y_1) \\in \\{(u,y) \\in \\mathbb{R}^2 : u &gt; r(y)\\})                             \\\\\n         & = \\int_{\\mathbb{R}^2} {1\\hspace{-3.8pt} 1}_{\\{u &gt; r(y)\\}} ({1\\hspace{-3.8pt} 1}_{[0,1]}(u) g(y)) \\, du dy  \\\\\n         & = \\int_\\mathbb{R} \\bigg( \\int_0^1 {1\\hspace{-3.8pt} 1}_{\\{u &gt; r(y)\\}} \\, du\\bigg) g(y)\\, d y \\\\\n         & =  \\int_\\mathbb{R} (1-r(y)) \\, g(y)\\, d y\\,,\n    \\end{align*}\n ce qui se r√©√©crit, comme f et g sont des densit√©s et que r(y) = f(y)/(m \\cdot g(y)): \\begin{align*}\n        \\mathbb{P}(U_1 &gt; r(Y_1))\n        & = \\int_\\mathbb{R} g(y)\\, d y - \\int_\\mathbb{R} \\dfrac{f(y)}{m}\\, dy \\\\\n        & = 1 - \\dfrac{1}{m}\\,.\n    \\end{align*}\n Le deuxi√®me terme se calcule de mani√®re analogue : \n\\begin{align*}\n    \\mathbb{P}(U_n \\leq r(Y_n), Y_n \\leq x)\n        & = \\int_{\\mathbb{R}^2} {1\\hspace{-3.8pt} 1}_{\\{u \\leq r(y)\\}} {1\\hspace{-3.8pt} 1}_{\\{y \\leq x\\}} ({1\\hspace{-3.8pt} 1}_{[0,1]}(u) g(y)) \\, du dy       \\\\\n        & = \\int_\\mathbb{R} \\bigg( \\int_0^1  {1\\hspace{-3.8pt} 1}_{\\{u \\leq r(y)\\}} \\, du\\bigg) {1\\hspace{-3.8pt} 1}_{\\{y \\leq x\\}}  g(y)\\, d y\\,,\n\\end{align*}\n c‚Äôest-√†-dire \n\\begin{align*}\n        \\mathbb{P}(U_n \\leq r(Y_n), Y_n \\leq x)\n        & = \\int_\\mathbb{R} r(y) {1\\hspace{-3.8pt} 1}_{\\{y \\leq x\\}}  g(y)\\, d y \\\\\n        & = \\int_{-\\infty}^x \\dfrac{f(y)}{m}\\, d y \\\\\n        & = \\dfrac{F(x)}{m}\\,,\n\\end{align*}\n o√π F est la fonction de r√©partition de la loi de densit√© f. On peut ainsi conclure que \n    \\mathbb{P}(X \\leq x, T=n)\n    =\n    \\bigg(1 - \\dfrac{1}{m}\\bigg)^{n-1} \\dfrac{F(x)}{m}\\,.\n Il ne reste plus qu‚Äô√† √©tudier les lois marginales. D‚Äôune part, par continuit√© monotone croissante, \n    \\mathbb{P}(T=n)\n    = \\lim_{q \\to \\infty} \\mathbb{P}(X \\in ]-\\infty, q], T=n)\\,,\n ce qui donne \n\\begin{align*}\n    \\mathbb{P}(T=n)\n    & = \\lim_{q \\to \\infty} \\bigg(1 - \\dfrac{1}{m}\\bigg)^{n-1} \\dfrac{F(q)}{m}\\\\\n    & = \\bigg(1 - \\dfrac{1}{m}\\bigg)^{n-1} \\dfrac{1}{m}\\,.\n\\end{align*}\n On en d√©duit que T suit une loi g√©om√©trique de param√®tre 1/m. D‚Äôautre part, par \\sigma-additivit√©, \n\\begin{align*}\n    \\mathbb{P}(X \\leq x)\n    & = \\mathbb{P}(X \\leq x, T \\in \\mathbb{N}^*)\\\\\n    & = \\sum_{n=1}^\\infty \\mathbb{P}(X \\leq x, T=n)\\,,\n\\end{align*}\n ce qui donne \n\\begin{align*}\n    \\mathbb{P}(X \\leq x)\n    & = \\sum_{n=1}^\\infty \\bigg(1 - \\dfrac{1}{m}\\bigg)^{n-1} \\dfrac{F(x)}{m}\\\\\n    & = \\dfrac{1}{1-(1-1/m)} \\dfrac{F(x)}{m}\\\\\n    & = F(x)\\,,\n\\end{align*}\n ce qui prouve que X a pour loi F.\nEnfin, la loi du couple (X,T) est √©gale au produit des lois \n\\begin{align*}\n    \\mathbb{P}(X \\leq x, T=n)\n    & = \\bigg(1 - \\dfrac{1}{m}\\bigg)^{n-1} \\dfrac{F(x)}{m}\\\\\n    & = \\mathbb{P}(T=n) \\mathbb{P}(X \\leq x)\\,,\n\\end{align*}\n\n\n\ndef accept_reject(n, f, g, g_sampler, m):\n    \"\"\"\n    n: nombre de simulations\n    f: densit√© cible\n    g: densit√© des propositions, g_sampler: simulateur selon g\n    m: constante pour la majoration\n    \"\"\"\n    x_samples = np.zeros(n)\n    u_samples = np.zeros(n)\n    accepted = np.zeros(n)\n    n_accepted = 0\n    while n_accepted &lt; n:\n        x = g_sampler()\n        u = np.random.uniform()\n        alpha = u * m * g(x)\n        u_samples [n_accepted] = alpha\n        x_samples[n_accepted] = x\n        if  alpha &lt;= f(x):\n            accepted[n_accepted] = 1\n        n_accepted += 1\n    return x_samples, u_samples, accepted\n\n\n\n\n\n\n\nEn pratique‚Ä¶\n\n\n\nOn simule U_1 et Y_1. Si U_1 \\leq r(Y_1) c‚Äôest gagn√©, on pose X=Y_1. Sinon, on simule U_2 et Y_2 et on teste √† nouveau l‚Äôin√©galit√© U_2 \\leq r(Y_2). Et ainsi de suite. Comme T suit une loi g√©om√©trique de param√®tre 1/m, son esp√©rance vaut m : il faut en moyenne m tentatives pour obtenir une simulation de la loi de densit√© f. L‚Äôobjectif est alors de choisir un couple (g, m) de sorte que m soit le plus proche possible de 1.\n\n\n\nExemple 4 (Rejet d‚Äôune loi polynomiale) Donnons un exemple jouet (on √©tudiera des exemples plus pertinents en TD). On consid√®re la densit√© f(x) = 4x^3 {1\\hspace{-3.8pt} 1}_{[0,1]}(x). Comme f est major√©e par 4, on peut choisir pour g la densit√© de la loi uniforme sur [0,1] et m=4. Alors, r(x) =f(x) / (mg(x)) = x^3, pour x \\in [0,1]. On simule donc (Y_1, U_1) et on teste si U_1 \\leq Y_1^3, etc.\nBien √©videmment, on privil√©giera ici une simulation via F^\\leftarrow qui permet de g√©n√©rer des variables al√©atoires de loi f plus rapidement.\n\n\n\n\n                                                \n\n\nFigure¬†3: Visualisation des zones d‚Äôacceptations/rejet (g uniforme)\n\n\n\n\nNous pouvons facilement am√©liorer la proportion de point accept√©s en proposant par exemple g d√©finie par g(x) = 2x {1\\hspace{-3.8pt} 1}_{[0, 1]}(x), et m=2.\n\n\n\n\n                                                \n\n\nFigure¬†4: Visualisation des zones d‚Äôacceptations/rejet (g triangulaire)\n\n\n\n\n\n\n\n\n\n\n\n\n\nmd`Le taux d'acceptation est pass√© de **${ratio1}** √†\n**${ratio2}** en utilisant une loi triangulaire au lieu d'une loi uniforme.`\n\n\n\n\n\n\n\n\n\nExemple 5 (Rejet d‚Äôune loi de densit√© d‚ÄôAndrews) Consid√©rons la densit√© d‚ÄôAndrews d√©finie par f(x) = \\frac{1}{S} \\frac{\\sin(\\pi\\cdot x)}{\\pi \\cdot x}  {1\\hspace{-3.8pt} 1}_{[-1,1]}(x), avec S = \\int_{-1}^{1}\\frac{\\sin(\\pi\\cdot x)}{\\pi \\cdot x}dx. Dans ce contexte, on ne connait pas la valeur exacte de S, et on va donc utiliser la m√©thode de rejet pour simuler des variables al√©atoires de loi f sans cette information. On peut l‚Äôadapter le test de la mani√®re suivante: si l‚Äôon prend m=2/S et g(x) = \\frac{1}{2} {1\\hspace{-3.8pt} 1}_{[-1,1]}(x), on observe que tester u\\leq \\frac{f(x)}{m \\cdot g(x)} est √©quivalent √† tester u \\leq r(x)=\\frac{1}{S} \\frac{\\sin(\\pi\\cdot x)}{\\pi \\cdot x}  \\cdot \\frac{1}{\\frac{2}{S} g(x)} = \\frac{\\sin(\\pi\\cdot x)}{\\pi \\cdot x}  \\cdot \\frac{1}{2 \\cdot g(x)}, ce qui peut se faire sans connaissance de S. De plus on peut v√©rifier que g(x) = \\frac{1}{2} {1\\hspace{-3.8pt} 1}_{[-1,1]}(x) d√©finit une densit√© et que f(x) \\leq m \\cdot g(x) pour tout x\\in \\mathbb{R}.\n\nn = 10000\ng = lambda x: np.ones_like(x) / 2\ng_sampler = lambda: 2 * np.random.uniform() - 1\nm = 2\n\nx_samples, u_samples, accepted = accept_reject(n, np.sinc, g, g_sampler, m)\nratio = np.sum(accepted) / n\n# Note: https://stackoverflow.com/questions/70804891/how-to-vectorize-a-function-in-python-that-includes-a-limit-e-g-sinx-x\n\n\n\n\n\n\nOn peut approcher num√©riquement la valeur exacte de S en utilisant une m√©thode de calcul approch√©e, ce qui permet de comparer ici notre m√©thode de rejet avec la densit√© sous-jacente:\n\nfrom scipy import integrate\nS = integrate.quad(np.sinc, -1, 1)[0]\nprint(f\"En utilisant la m√©thode de rejet, on trouve que S = {S:.3f}\")\n\nEn utilisant la m√©thode de rejet, on trouve que S = 1.179\n\n\nEnfin, on peut visualiser la qualit√© l‚Äôapproximation de la densit√© par la m√©thode de rejet en comparant la densit√© approch√©e (avec un histogramme) avec la densit√© exacte:\n\n\n\n\n                                                \n\n\nFigure¬†5: M√©thode de rejet pour simuler une loi de densit√© de type Andrews, sans connaissance de la valeur exacte de la constante de normalisation.\n\n\n\n\n\nmd`Dans cet example, le taux d'acceptation est ici de **${ratio}**.`\n\n\n\n\n\n\n\n\nCas mutlidimensionnel\nCommen√ßons par un cas de dimension deux.\nPour cela on va utiliser la m√©thode de rejet pour simuler une loi de densit√© f sur \\mathbb{R}^2. En particulier, un exemple classique est de tirer des points dans le disque unit√©, c‚Äôest-√†-dire de simuler une loi uniforme sur le disque unit√©. Pour cela, on va utiliser la m√©thode de rejet avec g la densit√© de la loi uniforme sur le carr√© [-1,1]^2.\nMais prenons un autre exemple, √† savoir tirer des points uniform√©ment dans la surface d√©limit√© par une cardio√Øde. Pour cela, on va utiliser la m√©thode de rejet avec g la densit√© de la loi uniforme sur le carr√© [-2,2]^2.\n\n\n\n\n                                                \n\n\nFigure¬†6: M√©thode de rejet pour simuler une loi uniforme sur un disque unit√©.\n\n\n\n\nAire estim√©e: 3.164\n\n\n\n\n\n\n                                                \n\n\nFigure¬†7: M√©thode de rejet pour simuler une loi uniforme sur une surface d√©limit√©e par une cardio√Øde.\n\n\n\n\nAire estim√©e: 4.795\n\n\n\n\n\n\n\n\nEXERCICE loi uniforme sur un cylindre\n\n\n\nProposer une m√©thode pour simuler une loi uniforme sur un cylindre de rayon 1 et de hauteur 10.",
    "crumbs": [
      "Cours",
      "Simulation"
    ]
  },
  {
    "objectID": "Courses/simulation.html#autres-m√©thodes",
    "href": "Courses/simulation.html#autres-m√©thodes",
    "title": "Simulation",
    "section": "Autres m√©thodes",
    "text": "Autres m√©thodes\n\nSommation de variables al√©atoires\nPour simuler une variable al√©atoire de loi binomiale \\mathcal{B}(n,p), on peut utiliser la m√©thode d‚Äôinversion. Cependant, cela n√©cessite le calcul de l‚Äôinverse g√©n√©ralis√©e de F, donc de coefficients binomiaux et de puissances de p et 1-p. √Ä la place, on utilisera plut√¥t la relation bien connue suivante : si X_1, \\ldots, X_n est une suite iid de variables al√©atoires de loi de Bernoulli de param√®tre p, alors \n    X = X_1 + \\cdots + X_n \\sim \\mathcal{B}(n,p)\\,.\n\nPour simuler des variables al√©atoires de Bernoulli, on utilise la m√©thode d‚Äôinversion (voir Exemple ). Ainsi, si U_1, \\ldots, U_n sont des variables al√©atoires iid de loi uniforme sur [0,1], alors \n    \\sum_{i=1}^n {1\\hspace{-3.8pt} 1}_{\\{U_i \\leq p\\}} \\sim \\mathcal{B}(n,p)\\,.\n\n\n\nLoi de Poisson\nRappelons qu‚Äôune variable al√©atoire X suit une loi de Poisson de param√®tre \\lambda &gt; 0, not√©e X \\sim \\mathcal{P}(\\lambda) si \n    \\mathbb{P}(X = k) = e^{-\\lambda} \\dfrac{\\lambda^k}{k!}\\,, \\quad k \\in \\mathbb{N}\\,.\n Une m√©thode pour simuler une variable al√©atoire de loi de Poisson est donn√©e par la proposition suivante.\n\nProposition 3 (G√©n√©ration de v.a. de loi de Poisson) \nSoit (E_n)_{n \\geq 1} des variables al√©atoires i.i.d. de loi exponentielle de param√®tre \\lambda &gt; 0. On pose S_k = E_1 + \\cdots + E_k. Alors, pour tout n \\in \\mathbb{N} \n    \\mathbb{P}(S_n \\leq 1 &lt; S_{n+1}) =  e^{-\\lambda} \\dfrac{\\lambda^n}{n!}\\enspace .\n Ainsi, la variable al√©atoire T d√©finie par \n    T \\triangleq \\sup \\{n \\in \\mathbb{N} : S_n \\leq 1\\}\n suit une loi de Poisson de param√®tre \\lambda : T \\sim \\mathcal{P}(\\lambda).\n\nLa preuve repose sur le lemme suivant.\n\nLemme 1 (Loi de Erlang) \nSoit n variables al√©atoires E_1, \\dots, E_n i.i.d. de loi exponentielle de param√®tre \\lambda &gt;0. La somme E_1+\\dots+E_n suit une loi d‚ÄôErlang de param√®tres (n,\\lambda), donn√©e par la fonction de r√©partition \n    F_{n,\\lambda}(t) = 1 - \\sum_{k=0}^{n-1} e^{-\\lambda t} \\frac{(\\lambda t)^k}{k!}\\,.\n\n\n\nPreuve. On montre le r√©sultat pour n=2. La g√©n√©ralisation √† k quelconque se fait par r√©currence. Soit t &gt; 0, et f_{\\lambda}(x)={1\\hspace{-3.8pt} 1}_{\\{x \\geq 0 \\}} \\lambda e^{-\\lambda x} la densit√© d‚Äôune loi exponentielle de param√®tre \\lambda. Les variables al√©atoires E_1 et E_2 √©tant ind√©pendantes et suivant des lois exponentielles de param√®tre \\lambda_1 et \\lambda_2, on a \n\\begin{align*}\n    \\mathbb{P}(E_1+E_2 \\leq t)\n        & = \\int_{\\mathbb{R}^2} {1\\hspace{-3.8pt} 1}_{\\{x_1 + x_2 \\leq t\\}} f_{\\lambda}(x_1) f_{\\lambda}(x_2)\\, d x_1 d x_2 \\\\\n        & = \\int_{\\mathbb{R}^2} {1\\hspace{-3.8pt} 1}_{\\{x_1 + x_2 \\leq t\\}} \\lambda^2 e^{-\\lambda (x_1+x_2)} {1\\hspace{-3.8pt} 1}_{\\{x_1 \\geq 0\\}} {1\\hspace{-3.8pt} 1}_{\\{x_2 \\geq 0\\}}\\, d x_1 d x_2 \\\\\n        & = \\int_{\\mathbb{R}^2} {1\\hspace{-3.8pt} 1}_{\\{0 \\leq x_1 \\leq t\\}} {1\\hspace{-3.8pt} 1}_{\\{0 \\leq x_2 \\leq t-x_1\\}} \\lambda^2 e^{-\\lambda x_1} e^{-\\lambda x_2}\\, d x_1 d x_2             \\\\\n        & = \\int_0^t \\lambda e^{-\\lambda x_1} \\bigg(\\int _0^{t-x_1} \\lambda e^{-\\lambda x_2}\\, d x_2\\bigg)  d x_1\\,.\n\\end{align*}\n La premi√®re int√©grale se calcule alors facilement : \n    \\int _0^{t-x_1} \\lambda e^{-\\lambda x_2}\\, d x_2 = 1 - e^{-\\lambda(t-x_1)}\\,.\n On obtient alors \n\\begin{align*}\n    \\mathbb{P}(E_1+E_2 \\leq t)\n   & = \\int_0^t \\lambda e^{-\\lambda x_1}dx_1 -  \\int_0^t e^{-\\lambda t} d x_1\\\\\n   & = 1 - e^{-\\lambda t} - \\lambda t e^{-\\lambda t}\\,.\n\\end{align*}\n Si t&lt;0, alors comme les E_i ne prennent que des valeurs positives on trouve \\mathbb{P}(E_1 + E_2 \\leq t) = 0. Ceci prouve le r√©sultat pour n=2.\n\nOn peut d√©sormais prouver le r√©sultat de la Proposition¬†3.\n\nPreuve. Pour n \\in \\mathbb{N}, on d√©compose la probabilit√© \\mathbb{P}(S_n \\leq 1 &lt; S_{n+1}) via \n\\begin{align*}\n    \\mathbb{P}(S_n \\leq 1 &lt; S_{n+1})\n    & = \\mathbb{P}(\\{S_n \\leq 1\\} \\setminus \\{S_{n+1} \\leq 1\\})\\\\\n    & = \\mathbb{P}(S_n \\leq 1) - \\mathbb{P}(S_{n+1} \\leq 1)\\,.\n\\end{align*}\n Le lemme pr√©c√©dent donne \n    \\mathbb{P}(S_n \\leq 1) = 1 - \\sum_{k=0}^{n-1} e^{-\\lambda} \\dfrac{\\lambda^k}{k!}\n et \n    \\mathbb{P}(S_{n+1} \\leq 1) = 1 - \\sum_{k=0}^{n} e^{-\\lambda} \\dfrac{\\lambda^k}{k!}\\,.\n On obtient alors le r√©sultat souhait√© : \n    \\mathbb{P}(S_n \\leq 1 &lt; S_{n+1})\n    = e^{-\\lambda} \\dfrac{\\lambda^n}{n!}\\,.\n\nOn conclut la preuve de la proposition en remarquant que \n    \\mathbb{P}(T=n) = \\mathbb{P}(S_n \\leq 1 &lt; S_{n+1})\\,.\n\n\nLa simulation d‚Äôune variable al√©atoire de Poisson repose donc sur la simulation de lois exponentielles qui se fait via la m√©thode d‚Äôinversion, comme vu dans Exemple¬†1. En pratique, on simule E_1 et on teste si E_1 &gt; 1. Si oui, on pose alors T=0. Si non, on simule E_2 et on teste si E_1 + E_2 &gt; 1. Si oui, on pose T=1. Sinon on continue la proc√©dure.\n\n\nBibliographie et pour aller plus loin\n\nGenerating Random Floating-Point Numbers by Dividing Integers: a Case Study par Fr√©d√©ric Goualard\nGenerating Pseudo-random Floating-Point Values par Allen Downey.",
    "crumbs": [
      "Cours",
      "Simulation"
    ]
  },
  {
    "objectID": "Courses/th_asymptotique.html",
    "href": "Courses/th_asymptotique.html",
    "title": "Th√©or√®mes asymptotiques",
    "section": "",
    "text": "Le premier r√©sultat fondamental en probabilit√©s concerne le comportement asymptotique de la moyenne empirique: \n\\bar X_n = \\dfrac{X_1 + \\cdots + X_n}{n} \\enspace.\n quand on observe n variables al√©atoires i.i.d X_1,\\dots,X_n, ayant une esp√©rance finie.\n\nTh√©or√®me 1 (Loi forte des grands nombres) \nSoit (X_n)_{n \\geq 1} une suite de variables al√©atoires ind√©pendantes et identiquement distribu√©es (i.i.d.) dans L^1(\\Omega, \\mathcal{F}, \\mathbb{P}). Notons \\mu = \\mathbb{E}[X_1]. Alors \\bar X_n converge vers \\mu presque s√ªrement : \n\\mathbb{P}\\bigg( \\dfrac{X_1 + \\cdots + X_n}{n} \\underset{n \\to \\infty}{\\longrightarrow} \\mu \\bigg) = 1\\,.\n\n\nInterpr√©tation: Intuitivement, la probabilit√© d‚Äôun √©v√©nement A correspond √† la fr√©quence d‚Äôapparition de A quand on r√©p√®te une exp√©rience qui fait intervenir cet √©v√©nement. Par exemple, si on dispose une pi√®ce truqu√©e, on estimera la probabilit√© d‚Äôapparition du c√¥t√© pile en lan√ßant la pi√®ce un grand nombre de fois et en comptant le nombre de pile obtenu. La loi des grands nombres justifie a posteriori cette intuition : si X_1, \\ldots, X_n sont i.i.d. de loi de Bernoulli de param√®tre p, alors \n    \\dfrac{X_1 + \\cdots + X_n}{n} \\xrightarrow[n \\to \\infty]{p.s.} p \\enspace.\n Le membre de gauche correspond au nombre empirique de pile obtenu, celui de droite √† la valeur th√©orique.\nRemarque: Bien qu‚Äôassez intuitif, ce th√©or√®me est difficile √† d√©montrer, cf.[@Ouvrard08;@Barbe_Ledoux06] ou encore [@Williams91] pour une version de preuve avec des martingales.\n#| standalone: true\n#| viewerHeight: 550\nimport numpy as np\nfrom shiny import ui, render, App, reactive\nfrom shinywidgets import output_widget, register_widget\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objs as go\n\n\nn_init = 42\n\n\napp_ui = ui.page_fluid(\n    ui.tags.head(\n        ui.tags.style(\"\"\"\n            .bslib-sidebar-layout &gt; .sidebar &gt; .sidebar-content {\n            display: flex;\n            flex-direction: column;\n            padding: 0.5rem;\n            }\n            .irs--shiny .irs-line {\n            top: 27px;\n            height: 4px;\n            }\n            .irs.irs--shiny .irs-bar {\n            top: 27px;\n            height: 4px;\n            }\n            .irs--shiny .irs-handle {\n            top: 23px;\n            width: 22px;\n            height: 10px;\n        \"\"\")\n    ),\n    ui.panel_title(\"Loi des grands nombres\"),\n    ui.layout_sidebar(\n        ui.panel_sidebar(\n            ui.input_action_button(\"seed\", \"Nouveau tirage\", class_=\"btn-primary\"),\n            ui.input_slider( \"p\", \"Esp√©rance: p\", 0.01, 0.99, value=0.35, step=0.01, ticks=False,\n            ),\n            ui.input_slider( \"n_samples\", \"√âchantillons: n\", 2, 1000, value=15, step=1, ticks=False),\n        width=3),\n    ui.panel_main(output_widget(\"my_widget\"), width = 9)\n    )\n)\n\n\n\n\ndef server(input, output, session):\n    seed = reactive.Value(42)\n\n    @reactive.Effect\n    @reactive.event(input.seed)\n    def _():\n        seed.set(np.random.randint(0, 1000))\n\n    subplots = make_subplots(\n        rows=2,\n        cols=1,\n        vertical_spacing=0.45,\n        horizontal_spacing=0.04,\n        row_heights=[5, 1],\n        subplot_titles=(\n            f\"Moyenne empirique: loi de Bernoulli\",\n            \"Tirages al√©atoires &lt;span style='color:rgb(66, 139, 202)'&gt;bleu: 0&lt;/span&gt;, &lt;span style='color:rgb(255, 0, 0)'&gt;rouge: 1&lt;/span&gt; (seed=\"\n            + f\"{n_init:03}\"\n            + \")\",\n        ),\n    )\n    fig = go.FigureWidget(subplots) \n    fig.add_trace(\n        go.Scatter(\n            mode=\"lines\",\n            line=dict(color=\"black\", width=3),\n            x=[],\n            y=[],\n            name=r\"Moyenne &lt;br&gt; empirique\",\n        ),\n        row=1,\n        col=1,\n    )\n    fig.add_trace(\n        go.Scatter(\n            mode=\"lines\",\n            line=dict(dash=\"dash\", color=\"black\", width=1),\n            marker={},\n            x=[],\n            y=[],\n            name=r\"p\",\n        ),\n            row=1,\n            col=1,\n    )\n    fig.add_trace(\n        go.Heatmap(\n            x=[],\n            z=[],\n            colorscale=[[0, \"rgb(66, 139, 202)\"], [1, \"rgb(255,0,0)\"]],\n            showscale=False,\n        ),\n        row=2,\n        col=1,\n    )\n\n    fig.update_yaxes(range=[0, 1.1], row=1, col=1)\n    fig.update_xaxes(matches=\"x1\", row=2, col=1)\n    fig.update_yaxes(visible=False, row=2, col=1)\n    fig.update_xaxes(visible=False, row=2, col=1)\n\n    fig.update_layout(\n        template=\"simple_white\",\n        showlegend=True,\n        xaxis_title=\"√âchantillons: n\",\n    )\n    fig.update_layout(autosize=True)\n\n    fig.update_layout(\n        legend=dict(\n            yanchor=\"top\",\n            y=0.99,\n            xanchor=\"left\",\n            x=0.65,\n            bgcolor=\"rgba(0,0,0,0)\",\n        )\n    )\n    fig.update_layout(\n        margin=dict(l=0, r=0, b=10, t=70),\n    )\n\n    register_widget(\"my_widget\", fig)\n\n    @reactive.Effect\n    def _():\n        p = input.p()\n        n_samples = input.n_samples()\n\n        rng = np.random.default_rng(seed())\n        iterations = np.arange(1, n_samples + 1)\n        samples = rng.binomial(1, p, size=n_samples)\n        means_samples = np.cumsum(samples) / np.arange(1, n_samples + 1)\n\n        # Update data in fig:\n        fig.data[0].x = iterations\n        fig.data[0].y = means_samples\n\n        fig.data[1].x = iterations\n        fig.data[1].y = np.full((n_samples), p)\n\n        fig.data[2].x = iterations\n        fig.data[2].z = [samples]\n\n        fig.update_xaxes(range=[1, n_samples + 1])\n\n        # Update the subplot titles:\n        fig.layout.annotations[1].update(\n            text=f\"Tirages al√©atoires (seed=\"\n            + f\"{seed():03}\"\n            + \") &lt;br&gt; &lt;span style='color:rgb(66, 139, 202)'&gt;bleu: 0&lt;/span&gt;, &lt;span style='color:rgb(255, 0, 0)'&gt;rouge: 1&lt;/span&gt; \"\n        )\n\n\napp = App(app_ui, server)\n\nPour aller plus loin:\nQuant p varie, √† n fix√©‚Ä¶les signaux g√©n√©r√©s sont tr√®s tr√®s proches, ce qui ne devrait pas √™tre le cas sans structuration particuli√®re de la g√©n√©ration. L‚Äôal√©a est imparfait!",
    "crumbs": [
      "Cours",
      "Th√©or√®mes asymptotiques"
    ]
  },
  {
    "objectID": "Courses/th_asymptotique.html#loi-des-grands-nombres",
    "href": "Courses/th_asymptotique.html#loi-des-grands-nombres",
    "title": "Th√©or√®mes asymptotiques",
    "section": "",
    "text": "Le premier r√©sultat fondamental en probabilit√©s concerne le comportement asymptotique de la moyenne empirique: \n\\bar X_n = \\dfrac{X_1 + \\cdots + X_n}{n} \\enspace.\n quand on observe n variables al√©atoires i.i.d X_1,\\dots,X_n, ayant une esp√©rance finie.\n\nTh√©or√®me 1 (Loi forte des grands nombres) \nSoit (X_n)_{n \\geq 1} une suite de variables al√©atoires ind√©pendantes et identiquement distribu√©es (i.i.d.) dans L^1(\\Omega, \\mathcal{F}, \\mathbb{P}). Notons \\mu = \\mathbb{E}[X_1]. Alors \\bar X_n converge vers \\mu presque s√ªrement : \n\\mathbb{P}\\bigg( \\dfrac{X_1 + \\cdots + X_n}{n} \\underset{n \\to \\infty}{\\longrightarrow} \\mu \\bigg) = 1\\,.\n\n\nInterpr√©tation: Intuitivement, la probabilit√© d‚Äôun √©v√©nement A correspond √† la fr√©quence d‚Äôapparition de A quand on r√©p√®te une exp√©rience qui fait intervenir cet √©v√©nement. Par exemple, si on dispose une pi√®ce truqu√©e, on estimera la probabilit√© d‚Äôapparition du c√¥t√© pile en lan√ßant la pi√®ce un grand nombre de fois et en comptant le nombre de pile obtenu. La loi des grands nombres justifie a posteriori cette intuition : si X_1, \\ldots, X_n sont i.i.d. de loi de Bernoulli de param√®tre p, alors \n    \\dfrac{X_1 + \\cdots + X_n}{n} \\xrightarrow[n \\to \\infty]{p.s.} p \\enspace.\n Le membre de gauche correspond au nombre empirique de pile obtenu, celui de droite √† la valeur th√©orique.\nRemarque: Bien qu‚Äôassez intuitif, ce th√©or√®me est difficile √† d√©montrer, cf.[@Ouvrard08;@Barbe_Ledoux06] ou encore [@Williams91] pour une version de preuve avec des martingales.\n#| standalone: true\n#| viewerHeight: 550\nimport numpy as np\nfrom shiny import ui, render, App, reactive\nfrom shinywidgets import output_widget, register_widget\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objs as go\n\n\nn_init = 42\n\n\napp_ui = ui.page_fluid(\n    ui.tags.head(\n        ui.tags.style(\"\"\"\n            .bslib-sidebar-layout &gt; .sidebar &gt; .sidebar-content {\n            display: flex;\n            flex-direction: column;\n            padding: 0.5rem;\n            }\n            .irs--shiny .irs-line {\n            top: 27px;\n            height: 4px;\n            }\n            .irs.irs--shiny .irs-bar {\n            top: 27px;\n            height: 4px;\n            }\n            .irs--shiny .irs-handle {\n            top: 23px;\n            width: 22px;\n            height: 10px;\n        \"\"\")\n    ),\n    ui.panel_title(\"Loi des grands nombres\"),\n    ui.layout_sidebar(\n        ui.panel_sidebar(\n            ui.input_action_button(\"seed\", \"Nouveau tirage\", class_=\"btn-primary\"),\n            ui.input_slider( \"p\", \"Esp√©rance: p\", 0.01, 0.99, value=0.35, step=0.01, ticks=False,\n            ),\n            ui.input_slider( \"n_samples\", \"√âchantillons: n\", 2, 1000, value=15, step=1, ticks=False),\n        width=3),\n    ui.panel_main(output_widget(\"my_widget\"), width = 9)\n    )\n)\n\n\n\n\ndef server(input, output, session):\n    seed = reactive.Value(42)\n\n    @reactive.Effect\n    @reactive.event(input.seed)\n    def _():\n        seed.set(np.random.randint(0, 1000))\n\n    subplots = make_subplots(\n        rows=2,\n        cols=1,\n        vertical_spacing=0.45,\n        horizontal_spacing=0.04,\n        row_heights=[5, 1],\n        subplot_titles=(\n            f\"Moyenne empirique: loi de Bernoulli\",\n            \"Tirages al√©atoires &lt;span style='color:rgb(66, 139, 202)'&gt;bleu: 0&lt;/span&gt;, &lt;span style='color:rgb(255, 0, 0)'&gt;rouge: 1&lt;/span&gt; (seed=\"\n            + f\"{n_init:03}\"\n            + \")\",\n        ),\n    )\n    fig = go.FigureWidget(subplots) \n    fig.add_trace(\n        go.Scatter(\n            mode=\"lines\",\n            line=dict(color=\"black\", width=3),\n            x=[],\n            y=[],\n            name=r\"Moyenne &lt;br&gt; empirique\",\n        ),\n        row=1,\n        col=1,\n    )\n    fig.add_trace(\n        go.Scatter(\n            mode=\"lines\",\n            line=dict(dash=\"dash\", color=\"black\", width=1),\n            marker={},\n            x=[],\n            y=[],\n            name=r\"p\",\n        ),\n            row=1,\n            col=1,\n    )\n    fig.add_trace(\n        go.Heatmap(\n            x=[],\n            z=[],\n            colorscale=[[0, \"rgb(66, 139, 202)\"], [1, \"rgb(255,0,0)\"]],\n            showscale=False,\n        ),\n        row=2,\n        col=1,\n    )\n\n    fig.update_yaxes(range=[0, 1.1], row=1, col=1)\n    fig.update_xaxes(matches=\"x1\", row=2, col=1)\n    fig.update_yaxes(visible=False, row=2, col=1)\n    fig.update_xaxes(visible=False, row=2, col=1)\n\n    fig.update_layout(\n        template=\"simple_white\",\n        showlegend=True,\n        xaxis_title=\"√âchantillons: n\",\n    )\n    fig.update_layout(autosize=True)\n\n    fig.update_layout(\n        legend=dict(\n            yanchor=\"top\",\n            y=0.99,\n            xanchor=\"left\",\n            x=0.65,\n            bgcolor=\"rgba(0,0,0,0)\",\n        )\n    )\n    fig.update_layout(\n        margin=dict(l=0, r=0, b=10, t=70),\n    )\n\n    register_widget(\"my_widget\", fig)\n\n    @reactive.Effect\n    def _():\n        p = input.p()\n        n_samples = input.n_samples()\n\n        rng = np.random.default_rng(seed())\n        iterations = np.arange(1, n_samples + 1)\n        samples = rng.binomial(1, p, size=n_samples)\n        means_samples = np.cumsum(samples) / np.arange(1, n_samples + 1)\n\n        # Update data in fig:\n        fig.data[0].x = iterations\n        fig.data[0].y = means_samples\n\n        fig.data[1].x = iterations\n        fig.data[1].y = np.full((n_samples), p)\n\n        fig.data[2].x = iterations\n        fig.data[2].z = [samples]\n\n        fig.update_xaxes(range=[1, n_samples + 1])\n\n        # Update the subplot titles:\n        fig.layout.annotations[1].update(\n            text=f\"Tirages al√©atoires (seed=\"\n            + f\"{seed():03}\"\n            + \") &lt;br&gt; &lt;span style='color:rgb(66, 139, 202)'&gt;bleu: 0&lt;/span&gt;, &lt;span style='color:rgb(255, 0, 0)'&gt;rouge: 1&lt;/span&gt; \"\n        )\n\n\napp = App(app_ui, server)\n\nPour aller plus loin:\nQuant p varie, √† n fix√©‚Ä¶les signaux g√©n√©r√©s sont tr√®s tr√®s proches, ce qui ne devrait pas √™tre le cas sans structuration particuli√®re de la g√©n√©ration. L‚Äôal√©a est imparfait!",
    "crumbs": [
      "Cours",
      "Th√©or√®mes asymptotiques"
    ]
  },
  {
    "objectID": "Courses/th_asymptotique.html#th√©or√®me-central-limite-tcl",
    "href": "Courses/th_asymptotique.html#th√©or√®me-central-limite-tcl",
    "title": "Th√©or√®mes asymptotiques",
    "section": "Th√©or√®me central limite (TCL)",
    "text": "Th√©or√®me central limite (TCL)\nUne fois la loi des grands nombres √©tablie, on peut se demander quel est l‚Äôordre suivant dans le d√©veloppement asymptotique de \\bar X_n - \\mu, ou de mani√®re √©quivalente de S_n - n \\mu, o√π S_n = X_1 + \\cdots + X_n. Le th√©or√®me suivant r√©pond √† cette question, en donnant une convergence en loi d‚Äôune transformation affine de la moyenne empirique:\n\nTh√©or√®me 2 (Th√©or√®me central limite) Soit X_1, \\ldots, X_n une suite de variables al√©atoires i.i.d de variance \\sigma^2 = {\\rm var}(X_1) \\in ]0, \\infty[. On note \\mu = \\mathbb{E}[X_1] leur esp√©rance. Alors \n\\sqrt n \\left(\\tfrac{\\bar X_n - \\mu}{\\sigma} \\right) \\xrightarrow[n \\to +\\infty]{\\mathcal{L}} N\\enspace,\n o√π N suit une loi normale centr√©e r√©duite : N \\sim\\mathcal{N}(0,1).\n\nPreuve: cf.[@Ouvrard08;@Barbe_Ledoux06].\nOn peut interpr√©ter ce th√©or√®me grossi√®rement de la fa√ßon suivante: la moyenne empirique de variables al√©atoires i.i.d de variance \\sigma^2 se comporte asymptotiquement comme une loi normale \\mathcal{N}(\\mu, \\tfrac{\\sigma^2}{n}), ce que l‚Äôon √©crit avec un abus de notation:\n\n\\bar X_n \\approx \\mathcal{N}(\\mu, \\frac{\\sigma^2}{n}) \\enspace.\n\nEn termes de somme cumul√©e empirique, la convergence se r√©√©crit\n\n    \\tfrac{S_n - n \\mu}{\\sqrt n \\sigma} \\xrightarrow[n \\to +\\infty]{\\mathcal{L}} N \\enspace.\n\nLes hypoth√®ses de ce th√©or√®me sont plut√¥t faibles (il suffit de supposer une variance finie). Pourtant, le r√©sultat est universel : la loi de d√©part peut √™tre aussi farfelue que l‚Äôon veut, elle se rapprochera toujours asymptotiquement d‚Äôune loi normale.\nOn rappelle que la convergence en loi est √©quivalente √† la convergence des fonctions de r√©partition en tout point de continuit√© de la limite. Ainsi, le th√©or√®me central limite se r√©√©crit de la mani√®re suivante : pour tout a &lt; b, notons \\alpha_n=\\mathbb{P} \\left(\\bar X_n \\notin [ \\mu + \\tfrac{a \\sigma}{\\sqrt{n}}, \\mu + \\tfrac{ b \\sigma}{\\sqrt{n}}] \\right). Ainsi\n\n\\mathbb{P} \\left(\\tfrac{\\bar X_n - \\mu}{\\sigma}\\in [ \\tfrac{a}{\\sqrt{n}},\\tfrac{b}{\\sqrt{n}}] \\right)\\\\\n\\begin{align}\n    1-\\alpha_n& = \\mathbb{P} \\left(\\bar X_n \\in [ \\mu + \\tfrac{a \\sigma}{\\sqrt{n}}, \\mu + \\tfrac{ b \\sigma}{\\sqrt{n}}] \\right)\\nonumber\\\\\n    & =\n    \\mathbb{P} \\left(\\tfrac{\\bar X_n - \\mu}{\\sigma} \\in [ \\tfrac{a}{\\sqrt{n}},\\tfrac{b}{\\sqrt{n}}]\\right) \\nonumber\\\\\n    & =\n    \\mathbb{P} \\bigg( a \\leq \\sqrt n \\left(\\tfrac{\\bar X_n - \\mu}{\\sigma} \\right) \\leq b\\bigg) \\nonumber\\\\\n    & \\underset{n \\to \\infty}{\\longrightarrow}  \\int_a^b \\varphi(x) \\,  dx\\,. \\nonumber\\\\\n\\end{align}\n o√π l‚Äôon note \\varphi (resp. \\Phi) la densit√© (resp. la fonction de r√©partition) d‚Äôune loi normale centr√©e r√©duite, d√©finie pour tout x\\in\\mathbb{R} par \\varphi(x)=\\tfrac{e^{-\\frac{x^2}{2}}}{\\sqrt{2\\pi}} (resp. \\Phi(x)= \\int_{-\\infty}^{x}\\varphi(u) du).\nDans le cas classique d‚Äôun intervalle de confiance √† 95%, c‚Äôest-√†-dire quand \\alpha_n=0.05, et en prenant un intervalle de confiance sym√©trique (alors a=-t et b=q) on obtient 1-\\alpha_n= \\int_{-q}^q \\varphi(x) \\,  dx=\\Phi(q)-\\Phi(-q)=2 \\Phi(q)-1 \\implies \\boxed{q=\\Phi^{-1}(1-\\tfrac{\\alpha_n}{2})} et q est donc le quantile de niveau 1-\\tfrac{\\alpha_n}{2} de la loi normale centr√©e r√©duite. Num√©riquement on peut facilement √©valuer q et v√©rifier que q\\approx 1.96 avec scipy:\n\nfrom scipy.stats import norm\nq = norm.ppf(1-0.05/2)\nprint(f\"Gaussienne centr√©e r√©duite,\\nQuantile de niveau (1-Œ±/2):\\nq = {q:.2f}\")\n\nGaussienne centr√©e r√©duite,\nQuantile de niveau (1-Œ±/2):\nq = 1.96\n\n\n\nExemple 1 (Loi de Bernoulli) On consid√®re des variables al√©atoires X_1, \\ldots, X_n i.i.d. suivant une loi de Bernoulli de param√®tre p \\in ]0,1[, dont l‚Äôesp√©rance et la variance sont respectivemenbt p et p(1-p). Le th√©or√®me central limite donne alors \n    \\sqrt n \\left(\\frac{\\bar X_n - p}{p (1-p)} \\right) \\xrightarrow[n \\to +\\infty]{\\mathcal{L}} N\\,,\n avec N \\sim \\mathcal{N}(0,1). Cette convergence est illustr√©e dans le widget ci-dessous. Le contexte est le suivant. On r√©p√®te t fois le processus, qui consiste √† afficher (\\bar{X}_k)_{k \\in [n]}, o√π les n variables al√©atoires sont i.i.d. et suivent une loi de Bernoulli de param√®tre p.\n\n#| standalone: true\n#| viewerHeight: 600\nimport numpy as np\nfrom scipy.stats import norm\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nfrom shiny import ui, render, App, reactive\nfrom shinywidgets import output_widget, render_widget\n\n\napp_ui = ui.page_fluid(\n    ui.tags.head(\n        ui.tags.style(\"\"\"\n            .bslib-sidebar-layout &gt; .sidebar &gt; .sidebar-content {\n            display: flex;\n            flex-direction: column;\n            padding: 0.1rem;\n            }\n            .irs--shiny .irs-line {\n            top: 27px;\n            height: 4px;\n            }\n            .irs.irs.irs--shiny .irs-bar {\n            top: 27px;\n            height: 4px;\n            }\n            .irs--shiny .irs-handle {\n            top: 23px;\n            width: 22px;\n            height: 10px;\n        \"\"\")\n    ),\n    ui.panel_title(\"TCL\"),\n    ui.layout_sidebar(\n        ui.panel_sidebar(\n            ui.input_action_button(\"seed\", \"Nouveau tirage\",class_=\"btn-primary\"),\n            ui.input_slider(\"p\", \"Esp√©rance: p\", 0.01, 0.99, value=0.5, step=0.01, ticks=False),\n            ui.input_slider(\"n_samples\", \"√âchantillons: n\", 1, 200, value=100, step=1, ticks=False),\n            ui.input_slider(\"n_repetitions\", \"R√©p√©titions: t\", 1, 300, value=200, step=1, ticks=False),\n        width=3\n        ),\n    ui.panel_main(output_widget(\"my_widget\"), width = 9)\n    )\n)\n\n\n\ndef server(input, output, session):\n    seed = reactive.Value(42)\n\n\n    @reactive.Effect\n    @reactive.event(input.seed)\n    def _():\n        seed.set(np.random.randint(0, 1000))\n\n\n    @output\n    @render_widget\n    def my_widget():\n\n        rng = np.random.default_rng(seed())\n        p = input.p()\n        n_repetitions = input.n_repetitions()\n        n_samples = input.n_samples()\n        iterations = np.arange(1, n_samples + 1)\n        samples = rng.binomial(1, p, size=(n_repetitions, n_samples))\n        means_samples = np.cumsum(samples, axis=1) / np.arange(1, n_samples + 1)\n        x_hist = np.linspace(0, 1, num=300)\n\n        # Create figure\n        fig = make_subplots(\n                    rows=1,\n                    cols=3,\n                    # vertical_spacing=0.5,\n                    horizontal_spacing=0.02,\n                    column_widths=[20, 2, 3],\n                    subplot_titles=(\"t = \" + str(n_repetitions) + \" r√©p√©titions\",\"\",\"\")\n                )\n\n\n        for i in range(n_repetitions):\n            fig.add_trace(\n                    go.Scatter(\n                        mode='lines',\n                        line=dict(color=\"rgba(0,0,0,0.05)\", width=1),\n                        x=iterations,\n                        y=means_samples[i,:],\n                        ),\n                        row=1, col=1,\n            )\n        fig.add_trace(\n                go.Scatter(\n                    mode='lines',\n                    line=dict(dash=\"dash\", color=\"blue\", width=1),\n                    marker={},\n                    x=iterations,\n                    y=np.full((n_samples), p),\n                    name='p'),\n                    row=1, col=1,\n        )\n        fig.update_layout(\n            template=\"simple_white\",\n            showlegend=False,\n        )\n        fig.add_trace(\n                go.Scatter(\n                    mode='markers',\n                    marker=dict(color=\"rgba(0,0,0,0.05)\", size=4),\n                    x=np.zeros(n_samples),\n                    y=means_samples[:,-1],\n                ),\n                row=1, col=2,\n\n        )\n        y_hist, bins = np.histogram(means_samples[:,-1], bins=int(np.sqrt(n_repetitions)), density=True)\n        fig.add_trace(\n            go.Bar(x=y_hist, y=bins[:-1] + np.diff(bins)/2,\n                    opacity=0.75,\n                    marker_color = 'black',\n                    orientation='h',\n                    width=np.diff(bins),\n                    name=\"Tirages de moyennes empiriques\",\n                    ),\n                row=1, col=3,\n        )\n        fig.add_trace(\n            go.Scatter(x=norm.pdf(x_hist, p, np.sqrt(p*(1-p) / n_samples)),\n                       y=x_hist,\n                       mode='lines',\n                       line=dict(color=\"red\"),\n                       legendgroup='1',\n                       name=\"TCL\"\n                       ),\n                row=1, col=3,\n        )\n\n        fig.update_xaxes(range=[1, n_samples + 1])\n        fig.update_yaxes(range=[-.05, 1.05], row=1, col=1)\n        fig.update_yaxes(matches=\"y1\",visible = False,  row=1, col=2)\n        fig.update_xaxes(range=[-0.2, 0.2], visible = False, row=1, col=2)\n\n        fig.update_yaxes(matches=\"y1\", row=1, col=3, visible=False)\n        fig.update_xaxes(range=[0, 1.1 / np.sqrt(2*np.pi* p*(1-p) / n_samples)], row=1, col=3)\n        fig.update_xaxes(visible=False, row=1, col=3)\n\n\n\n        for trace in fig['data']:\n            print(trace)\n            if (trace['name'] != 'TCL') and (trace['name'] != 'p'):\n                trace['showlegend'] = False\n        fig.update_layout(\n        margin=dict(l=0, r=0, b=10, t=100),\n        )\n        fig.update_layout(\n            title=dict(text=\"Distribution de la moyenne empirique&lt;br&gt; (cas loi de Bernoulli)\", yanchor=\"top\", y=0.95),\n            title_x=0.5,\n            showlegend=True,\n\n        )\n        fig.update_layout(\n            legend=dict(\n                yanchor=\"top\",\n                y=0.99,\n                xanchor=\"left\",\n                x=0.89,\n                bgcolor=\"rgba(0,0,0,0)\",\n            )\n        )\n        fig['layout']['xaxis']['title']='√âchantillons: n'\n\n        fig.update_layout(autosize=True)\n\n        return fig\n\n\napp = App(app_ui, server)\n\nUne autre illustration possible de la convergence donn√©e par le TCL est celle qui correspond au point de vue donn√©e par l‚Äôanalyse. Pour cela supposons que l‚Äôon ait une suite de variables al√©atoires r√©elles X_1, \\dots, X_n, i.i.d. dont la fonction de densit√© commune est not√©e par f.\nOn rappelle quelques √©l√©ments de probabilit√©s concernant les densit√©s. Pour cela on rappelle la d√©finition de la convolution deux fonctions. Pour cela prenons deux fonctions f et g d√©finies sur \\mathbb{R} et qui sont int√©grables au sens de Lebesgue. La convolution de f par g est alors la fonction f*g suivante:\n\n\\begin{align}\n\\mathbb{R} &\\mapsto \\mathbb{R} \\nonumber\\\\\nx &\\to \\int_{-\\infty}^{+\\infty} f(x-y)g(y) dy \\enspace.\\nonumber\n\\end{align}\n\n\n\n\n\n\n\nNote\n\n\n\nOn peut aussi obtenir f*g(x) en calculant \\int_{\\mathbb{R}^2} f(u)g(v) {1\\hspace{-3.8pt} 1}_{u+v=x} du dv.\n\n\n\nTh√©or√®me 3 (Loi de la somme et convolutions) Soient X et Y des v.a. ind√©pendantes de densit√©s f et g respectivement, la loi de X+Y est donn√©e par la convolution f*g.\n\nRappel: pour un scalaire \\alpha\\neq 0, la densit√© de \\alpha X est donn√©e par la fonction x \\mapsto \\frac{1}{|\\alpha|} \\cdot f(\\frac{x}{\\alpha}).\n\nCorollaire 1 (Loi de la moyenne) Soient X_1,\\dots,X_n des v.a. i.i.d. de densit√© f, la densit√© de \\bar{X}_n est donn√©e par la fonction x \\mapsto n \\cdot [f*\\dots*f](n \\cdot x).\n\nDessous, pour X_1, \\dots, X_n, i.i.d., de densit√© f, on affiche la densit√© de la loi de \\bar{X}_n.\n#| standalone: true\n#| viewerHeight: 550\nimport numpy as np\nfrom shiny import ui, render, App, reactive\nfrom shinywidgets import output_widget, register_widget\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objs as go\nfrom scipy import signal\n\n\napp_ui = ui.page_fluid(\n    ui.tags.head(\n        ui.tags.style(\"\"\"\n            .bslib-sidebar-layout &gt; .sidebar &gt; .sidebar-content {\n            display: flex;\n            flex-direction: column;\n            padding: 0.5rem;\n            }\n            .irs--shiny .irs-line {\n            top: 27px;\n            height: 2px;\n            }\n            .irs.irs--shiny .irs-bar {\n            top: 27px;\n            height: 2px;\n            }\n            .irs--shiny .irs-handle {\n            top: 23px;\n            width: 22px;\n            height: 10px;\n        \"\"\")\n    ),\n    ui.panel_title(\"TCL et convolutions\"),\n    ui.layout_sidebar(\n        ui.panel_sidebar(\n            ui.input_select(\n                \"loi\",\n                \"Loi sous-jacente\",\n                {'uniforme': 'Uniforme', 'laplace' : 'Laplace'},\n            ),\n            ui.input_slider(\n                \"n_iter\",\n                \"√âchantillons: n\",\n                1,\n                10,\n                value=1,\n                step=1,\n                ticks=False,\n            ), width = 3\n    ),\n    ui.panel_main(\n        output_widget(\"my_widget\"), width = 9\n    )\n)\n)\n\n\nnnzeros = 10001\nx_min = -20\nx_max = 20\n\nx = np.linspace(x_min, x_max, nnzeros)\ny = np.zeros(nnzeros)\nmask = np.where(np.abs(x) &lt;= 0.5, 1, 0)\ny[mask == 1] = 1\ndelta = (x_max - x_min) / nnzeros\nvar = np.sum(y * x**2 *(delta)) - (np.sum(y * x * delta))**2\n\ndef convolve(signal_in, n_convolutions, delta):\n    output = np.zeros(len(signal_in))\n    if n_convolutions == 0:\n        return output\n    elif n_convolutions == 1:\n        return signal_in\n    else:\n        output = signal_in.copy()\n        for i in range(n_convolutions - 1):\n            output = signal.fftconvolve(\n                output * delta, signal_in, mode=\"same\"\n            )\n        return output\n\n\ndef server(input, output, session):\n\n    fig = go.FigureWidget()\n    fig.add_trace(\n        go.Scatter(\n            mode=\"lines\",\n            line=dict(color=\"black\", width=3),\n            x=[],\n            y=[],\n            name=\"loi de de la moyenne empirique&lt;br&gt;(variance ad√©quate)\",\n        )\n    )\n\n    fig.add_trace(\n        go.Scatter(\n            x=x,\n            y=np.exp(-(x**2) / (2 * var)) / np.sqrt(2 * var * np.pi),\n            mode=\"lines\",\n            line=dict(dash=\"dash\", color=\"red\", width=2),\n            name=f\"Loi normale&lt;br&gt;(variance ad√©quate)\",\n        )\n    )\n    fig.update_xaxes(range=[-3, 3], position=0.)\n    fig.update_yaxes(range=[0, 2 * np.max(y)], position=0.5, showticklabels=False)\n\n\n    fig.update_layout(\n        template=\"simple_white\",\n        showlegend=True,\n        autosize=True,\n        title=dict(text=\"Densit√© : &lt;br&gt; moyenne de n variables al√©atoires\", yanchor=\"top\", y=0.95),\n        title_x=0.5,\n    )\n    fig.update_layout(legend=dict(\n        yanchor=\"top\",\n        y=0.95,\n        xanchor=\"left\",\n        x=0.8,\n        font=dict(size= 18)\n    ))\n    fig.update_layout(\n    height=250,\n    margin=dict(l=0, r=0, b=10, t=100),\n    )\n\n    register_widget(\"my_widget\", fig)\n\n    @reactive.Effect\n    def _():\n        if str(input.loi()) == 'uniforme':\n            y = np.zeros(nnzeros)\n            mask = np.where(np.abs(x) &lt;= 0.5, 1, 0)\n            y[mask == 1] = 1\n\n        else:\n            y=np.exp(-np.abs(x)) / 2\n            y = y / (np.sum(y) * delta)\n\n        var = np.sum(y * x**2 * delta) - (np.sum(y * x * delta))**2\n\n        y_display = convolve(y, input.n_iter(), delta)\n        # Update data in fig:\n        fig.data[0].x = x / np.sqrt(input.n_iter())\n        fig.data[0].y = y_display * np.sqrt(input.n_iter())\n        fig.data[1].y = np.exp(-(x**2) / (2 * var)) / np.sqrt(2 * var * np.pi)\n        fig.update_yaxes(range=[0, 2 * np.max(y)], position=0.5, showticklabels=False)\n\n\napp = App(app_ui, server)\nPour aller plus loin sur les convolutions, voir la vid√©o de 3Blue1Brown √† ce sujet: Convolutions | Why X+Y in probability is a beautiful mess",
    "crumbs": [
      "Cours",
      "Th√©or√®mes asymptotiques"
    ]
  },
  {
    "objectID": "Slides/slides-index.html#test",
    "href": "Slides/slides-index.html#test",
    "title": "Visualisation: inversion",
    "section": "test",
    "text": "test\n\nviewof dist = Inputs.select(['normal','cauchy','laplace','bimodal'], {value: \"bimodal\", label: \"Loi\"})\nviewof replay = html`&lt;button&gt;Relancer`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ninvcdfboard = require(await FileAttachment(\"../inverse-vizu/dist/invcdfboard.umd.cjs\").url())\n\n{\n  replay\n  const canvas = DOM.canvas(500, 500);\n  const galton = invcdfboard.galton(canvas,dist);\n  \n  return html`${galton.canvas}`\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVisualisation: inversion"
  },
  {
    "objectID": "Slides/slides_loi_normale_multi.html#rappel-sur-les-vecteurs-al√©atoires",
    "href": "Slides/slides_loi_normale_multi.html#rappel-sur-les-vecteurs-al√©atoires",
    "title": "Loi normale: cas multivari√©",
    "section": "Rappel sur les vecteurs al√©atoires",
    "text": "Rappel sur les vecteurs al√©atoires\n\nVecteur al√©atoire: \\(\\mathbf{X} = (X_1, \\dots, X_d) \\in \\mathbb{R}^d\\)\nEsp√©rance: \\(\\mathbb{E}[\\mathbf{X}] = (\\mathbb{E}[X_1], \\dots, \\mathbb{E}[X_d]) \\in \\mathbb{R}^d\\) (\\(\\mathbb{E}[|X_j|] &lt; \\infty\\))\nCovariances: \\(\\textrm{cov}(X_i, X_j) = \\mathbb{E}[(X_i- \\mathbb{E}[X_i]) (X_j - \\mathbb{E}[X_j])] \\enspace,\\)\nMatrice de variance-covariance : \\(\\Sigma = (\\textrm{cov}(X_i, X_j))_{1 \\leq i,j \\leq d} \\in \\mathbb{R}^{d \\times d}\\)",
    "crumbs": [
      "Slides",
      "Loi normale: cas multivari√©"
    ]
  },
  {
    "objectID": "Slides/slides_loi_normale_multi.html#vecteurs-gaussiens",
    "href": "Slides/slides_loi_normale_multi.html#vecteurs-gaussiens",
    "title": "Loi normale: cas multivari√©",
    "section": "Vecteurs gaussiens",
    "text": "Vecteurs gaussiens\n\nD√©finition 1 (Vecteur gaussien) Un vecteur al√©atoire \\(\\mathbf{X} = (X_1, \\dots, X_d)^\\top \\in \\mathbb{R}^d\\) est un vecteur gaussien si pour tout \\({\\alpha} = (\\alpha_1, \\dots, \\alpha_d)^\\top\\), la variable al√©atoire r√©elle \\[\n  \\langle {\\alpha}, \\mathbf{X} \\rangle = \\alpha_1 X_1 + \\cdots + \\alpha_d X_d \\enspace,\n\\] suit une loi normale.\n\n\nCons√©quence chaque (loi marginale) \\(X_j\\) suit une loi gaussienne (choisir ci-dessus \\(\\alpha = e_j\\), les autres √©gaux √† \\(0\\))\n\n\nContre-exemple: \\(X\\sim \\mathcal{N}(0,1)\\) et \\(\\varepsilon\\) une loi uniforme (discr√®te) sur \\(\\{-1,1\\}\\), alors \\((X, \\varepsilon X)^{\\top}\\) n‚Äôest pas un vecteur gaussien bi-dimensionnel, mais \\(X\\) et \\(\\varepsilon X\\) sont gaussiennes.\n\\[\n\\begin{align*}\n  \\mathbb{P}(\\varepsilon X \\leq t)\n  & =  \\mathbb{P}(X \\leq t) \\mathbb{P}(\\varepsilon = 1) + \\mathbb{P}(-X \\leq t) \\mathbb{P}(\\varepsilon = -1)\\\\\n  & = \\tfrac{1}{2} \\mathbb{P}(X \\leq t) + \\tfrac{1}{2} \\mathbb{P}(-X \\leq t) = \\mathbb{P}(X \\leq t) \\enspace.\n\\end{align*}\n\\] mais \\(X + \\varepsilon X\\) prend la valeur \\(0\\) avec probabilit√© \\(1/2\\) donc ne suit pas une loi normale.",
    "crumbs": [
      "Slides",
      "Loi normale: cas multivari√©"
    ]
  },
  {
    "objectID": "Slides/slides_loi_normale_multi.html#calculs-√©l√©mentaires",
    "href": "Slides/slides_loi_normale_multi.html#calculs-√©l√©mentaires",
    "title": "Loi normale: cas multivari√©",
    "section": "Calculs √©l√©mentaires",
    "text": "Calculs √©l√©mentaires\nNotation: on note \\(X\\sim\\mathcal{N}(\\mu,\\Sigma)\\) un vecteur gaussien d‚Äôesp√©rance \\({\\mu}\\) et de matrice de variance-covariance \\(\\Sigma\\)\nPour un tel \\(\\mathbf{X}\\), on a \\(\\langle {\\alpha}, \\mathbf{X} \\rangle \\sim \\mathcal{N}\\left(\\langle {\\alpha}, {\\mu} \\rangle, {\\alpha}^\\top \\Sigma {\\alpha}\\right)\\) pour tout \\(\\alpha\\in \\mathbb{R}^{d}\\)\nPreuve \\[\n\\begin{align*}\n  \\mathbb{E}[\\langle {\\alpha}, \\mathbf{X} \\rangle] = \\mathbb{E}[\\alpha_1 X_1 + \\dots + \\alpha_d X_d] =  \\alpha_1 \\mathbb{E}[X_1] + \\cdots + \\alpha_d \\mathbb{E}[X_d] = \\langle {\\alpha}, {\\mu} \\rangle\\,,\n\\end{align*}\n\\]\n\net\n\\[\n\\begin{align*}\n  \\mathrm{var}(\\langle {\\alpha}, \\mathbf{X} \\rangle)\n    & = \\mathrm{var}(\\alpha_1 X_1 + \\cdots + \\alpha_d X_d)\\\\\n    & = \\mathrm{cov}(\\alpha_1 X_1 + \\cdots + \\alpha_d X_d, \\alpha_1 X_1 + \\cdots + \\alpha_d X_d) \\\\\n    & = \\sum_{1 \\leq i,j \\leq d} \\alpha_i \\mathrm{cov}(X_i, X_j) \\alpha_j\n  = {\\alpha}^\\top \\Sigma {\\alpha}\n\\end{align*}\n\\]",
    "crumbs": [
      "Slides",
      "Loi normale: cas multivari√©"
    ]
  },
  {
    "objectID": "Slides/slides_loi_normale_multi.html#fonction-caract√©ristique-dun-vecteur-gaussien",
    "href": "Slides/slides_loi_normale_multi.html#fonction-caract√©ristique-dun-vecteur-gaussien",
    "title": "Loi normale: cas multivari√©",
    "section": "Fonction caract√©ristique d‚Äôun vecteur gaussien",
    "text": "Fonction caract√©ristique d‚Äôun vecteur gaussien\n\\[\n    \\phi_\\mathbf{X}({\\alpha})\n    \\triangleq \\mathbb{E}[e^{i \\langle {\\alpha}, \\mathbf{X} \\rangle}]\n    = \\exp\\Big(i \\langle {\\alpha}, {\\mu} \\rangle - \\frac{{\\alpha}^\\top \\Sigma {\\alpha}}{2}\\Big)\\,,\n    \\quad {\\alpha} \\in \\mathbb{R}^d\\,.\n\\] Preuve: utiliser l‚Äôexpression de la fonction caract√©ristique d‚Äôune variable al√©atoire de loi normale \\(\\mathcal{N}(\\langle {\\alpha}, {\\mu} \\rangle, {\\alpha}^\\top \\Sigma {\\alpha})\\).\nCons√©quence: \\(\\phi_\\mathbf{X}\\) est enti√®rement d√©termin√©e par les quantit√©s \\({\\mu}\\) et \\(\\Sigma\\).\nEn particulier, si les variables al√©atoires \\(X_1, \\dots, X_d\\) sont ind√©pendantes de loi \\(\\mathcal{N}(0,1)\\), alors \\({\\mu} = (0,\\ldots,0)^\\top\\) et \\(\\Sigma = \\mathrm{Id}_d\\).",
    "crumbs": [
      "Slides",
      "Loi normale: cas multivari√©"
    ]
  },
  {
    "objectID": "Slides/slides_loi_normale_multi.html#densit√©-de-probabilit√©",
    "href": "Slides/slides_loi_normale_multi.html#densit√©-de-probabilit√©",
    "title": "Loi normale: cas multivari√©",
    "section": "Densit√© de probabilit√©",
    "text": "Densit√© de probabilit√©\nCas \\({\\mu}=0\\) et \\(\\Sigma = \\mathrm{Id}_d\\) : la loi gaussienne centr√©e r√©duite \\(\\mathcal{N}(0, \\mathrm{Id}_d)\\)\nLa loi de \\((X_1,\\dots,X_n)^\\top\\) correspond alors √† la loi produit de \\(n\\) lois gaussiennes centr√©es r√©duites ind√©pendantes (pour les gaussiennes la d√©corr√©lation implique l‚Äôind√©pendance), de densit√© \\[\n\\varphi_{0,\\mathrm{Id}_d}(x) = \\frac{1}{ \\sqrt{(2\\pi)^d}} \\exp\\left( -\\tfrac{1}{2}x^\\top x   \\right) \\enspace.\n\\]\n\nProposition 1 (Densit√© de la loi gaussienne multivari√©e) Soient \\({\\mu} \\in \\mathbb{R}^d\\) et \\(\\Sigma \\in \\mathbb{R}^{d \\times d}\\) (sym√©trique et d√©finie positive) et supposons que \\(X \\sim \\mathcal{N}({\\mu},\\Sigma)\\). Alors la densit√© de probabilit√© de \\(X\\) est donn√©e pour tout \\(x \\in \\mathbb{R}^d\\) par\n\\[\n\\varphi_{{\\mu},\\Sigma}(x) = \\frac{1}{ \\sqrt{(2\\pi)^d |\\det(\\Sigma)|}}  \\exp\\Big( -\\tfrac{1}{2}(x-{\\mu})^\\top\\Sigma^{-1}(x - {\\mu})   \\Big) \\enspace.\n\\]",
    "crumbs": [
      "Slides",
      "Loi normale: cas multivari√©"
    ]
  },
  {
    "objectID": "Slides/slides_loi_normale_multi.html#d√©monstration",
    "href": "Slides/slides_loi_normale_multi.html#d√©monstration",
    "title": "Loi normale: cas multivari√©",
    "section": "D√©monstration",
    "text": "D√©monstration\nPreuve: Soit \\(L\\in \\mathbb{R}^{d \\times d}\\) t.q. \\(LL^\\top = \\Sigma\\) (d√©composition spectrale, de Cholevsky, etc.). Loi de \\(\\mathbf{Y} = \\psi(X) \\triangleq L \\mathbf{X} + {\\mu}\\), pour \\(X\\sim \\mathcal{N}(0,\\mathrm{Id_d})\\).\n\nApplication de la formule du changement de variable avec \\(\\psi^{-1}\\) et son jacobien: \\(\\psi^{-1}(y) = L^{-1}(y-{\\mu})\\), et \\(|\\det(J_{\\psi^{-1}})| = |\\det(L^{-1})| = |\\det(L)|^{-1} = |\\det(\\Sigma)|^{-1/2}\\).\n\n\nDe plus \\(LL^\\top \\left(L^{-1}\\right)^\\top L^{-1}=\\mathrm{Id}_d\\), et donc que \\(\\left(L^{-1}\\right)^\\top L^{-1}=\\Sigma^{-1}\\).\n\n\nOn en d√©duit la densit√© de \\(\\mathbf{Y}\\) : \\[\n\\begin{align*}\n\\varphi_{{\\mu},\\Sigma}(y)\n& = \\varphi_{0,\\mathrm{Id}_d}(\\psi^{-1}(y)) |\\det(J_{\\psi^{-1}})| \\\\\n& = \\frac{|\\det(\\Sigma)|^{-1/2}}{ \\sqrt{(2\\pi)^d }}  \\exp\\left( -\\tfrac{1}{2}(y-{\\mu})^\\top \\left(L^{-1}\\right)^\\top L^{-1}(y - {\\mu})\\right)\\\\\n& = \\frac{1}{ \\sqrt{(2\\pi)^d |\\det(\\Sigma)|}}  \\exp\\Big( -\\tfrac{1}{2}(y-{\\mu})^\\top\\Sigma^{-1}(y - {\\mu})   \\Big) \\enspace.\n\\end{align*}\n\\]",
    "crumbs": [
      "Slides",
      "Loi normale: cas multivari√©"
    ]
  },
  {
    "objectID": "Slides/slides_loi_normale_multi.html#transformation-de-vecteurs-gaussiens",
    "href": "Slides/slides_loi_normale_multi.html#transformation-de-vecteurs-gaussiens",
    "title": "Loi normale: cas multivari√©",
    "section": "Transformation de vecteurs gaussiens",
    "text": "Transformation de vecteurs gaussiens\n\nProposition 2 (Transformation affine de vecteurs gaussiens) Soit \\(\\mathbf{X} \\sim \\mathcal{N}({\\mu}, \\Sigma)\\) un vecteur gaussien sur \\(\\mathbb{R}^d\\), \\(\\Omega \\in \\mathbb{R}^{d' \\times d}\\) et \\({\\nu}\\in \\mathbb{R}^{d'}\\). Alors, le vecteur al√©atoire \\(\\mathbf{Y} = \\Omega \\mathbf{X} + {\\nu}\\) est un vecteur gaussien v√©rifiant \\[\n  \\mathbf{Y} \\sim \\mathcal{N}(\\Omega {{\\mu}} + {\\nu}, \\Omega \\Sigma \\Omega^\\top)\\,.\n\\]\n\nCette proposition se prouve sans peine en utilisant la fonction caract√©ristique On retrouve en particulier la stabilit√© par transformation affine √©tablie en dimension \\(1\\).",
    "crumbs": [
      "Slides",
      "Loi normale: cas multivari√©"
    ]
  },
  {
    "objectID": "Slides/slides_loi_normale_multi.html#la-factorisation-de-cholesky",
    "href": "Slides/slides_loi_normale_multi.html#la-factorisation-de-cholesky",
    "title": "Loi normale: cas multivari√©",
    "section": "La factorisation de Cholesky",
    "text": "La factorisation de Cholesky\n\n\n\nTh√©or√®me 1 (Factorisation de Cholesky) Soit \\(\\Sigma \\in \\mathbb{R}^{d \\times d}\\) une matrice sym√©trique d√©finie positive. Alors il existe une matrice triangulaire inf√©rieure \\(L \\in \\mathbb{R}^{d \\times d}\\) telle que \\(\\Sigma = LL^\\top\\). La d√©composition est unique si l‚Äôon impose que les √©l√©ments diagonaux de \\(L\\) soient strictement positifs.\n\n\nPreuve: la factorisation de Cholesky est une cons√©quence directe de la m√©thode du pivot de Gauss; d√©tails (Th. 4.4.1, Ciarlet 2006).\n\n\nUtilit√©: R√©solution de syst√®mes lin√©aires ‚Äú\\(Ax=b\\)‚Äù avec \\(A\\) sym√©trique d√©finie positive (notamment pour r√©soudre plusieurs syst√®mes avec la m√™me matrice A)\n\n\n\n\nAlgorithme propos√© par Andr√©-Louis Cholesky: (1875-1918) ing√©nieur topographe et g√©od√©sien dans l‚Äôarm√©e fran√ßaise, mort des suites de blessures re√ßues au champs de bataille.",
    "crumbs": [
      "Slides",
      "Loi normale: cas multivari√©"
    ]
  },
  {
    "objectID": "Slides/slides_loi_normale_multi.html#num√©rique",
    "href": "Slides/slides_loi_normale_multi.html#num√©rique",
    "title": "Loi normale: cas multivari√©",
    "section": "Num√©rique",
    "text": "Num√©rique\nEn numpy, la factorisation de Cholesky est disponible via la fonction linalg.cholesky.\n\nimport numpy as np\nSigma = np.array([[1, 0.5], [0.5, 2]])\nL = np.linalg.cholesky(Sigma)\n\nprint(f\"Sigma:\\n{Sigma}\\n\")\nprint(f\"L:\\n{L}\\n\")\nprint(f\"LL^T:\\n{L@L.T}\\n\")\nprint(f\"L^TL:\\n{L.T@L}\\n\")\n\nSigma:\n[[1.  0.5]\n [0.5 2. ]]\n\nL:\n[[1.         0.        ]\n [0.5        1.32287566]]\n\nLL^T:\n[[1.  0.5]\n [0.5 2. ]]\n\nL^TL:\n[[1.25       0.66143783]\n [0.66143783 1.75      ]]",
    "crumbs": [
      "Slides",
      "Loi normale: cas multivari√©"
    ]
  },
  {
    "objectID": "Slides/slides_loi_normale_multi.html#simulation-de-vecteurs-gaussiens",
    "href": "Slides/slides_loi_normale_multi.html#simulation-de-vecteurs-gaussiens",
    "title": "Loi normale: cas multivari√©",
    "section": "Simulation de vecteurs gaussiens",
    "text": "Simulation de vecteurs gaussiens\n\nCas centr√© r√©duit \\(\\mathcal{N}(0, \\mathrm{Id}_d)\\): vecteur gaussien avec \\({\\mu} = (0,\\ldots,0)^\\top\\) et \\(\\Sigma = \\mathrm{Id}_d\\).\n\nsimuler \\(X_1,\\dots, X_d\\), \\(d\\) variables al√©atoires ind√©pendantes de loi normale centr√©e r√©duite (par Box-Muller ou autre)\nles concat√©ner en un vecteur \\(\\mathbf{X} = (X_1,\\dots, X_d)^\\top\\).\n\\(\\mathbf{X}\\) est alors un vecteur gaussien de loi \\(\\mathcal{N}(0, \\mathrm{Id}_d)\\).\n\n\n\n\nCas g√©n√©ral \\(\\mathcal{N}({\\mu}, \\Sigma)\\): la m√©thodologie est la suivante\n\nsimuler un vecteur gaussien \\(\\mathbf{X} \\sim \\mathcal{N}(0, \\mathrm{Id}_d)\\)\ntrouver une ‚Äúracine carr√©e‚Äù \\(L\\) de la matrice de covariance \\(\\Sigma\\)\nappliquer une transformation affine: \\(\\mathbf{Y} = L \\mathbf{X} + {\\mu}\\)",
    "crumbs": [
      "Slides",
      "Loi normale: cas multivari√©"
    ]
  },
  {
    "objectID": "Slides/slides_loi_normale_multi.html#approche-par-la-factorisation-de-cholesky",
    "href": "Slides/slides_loi_normale_multi.html#approche-par-la-factorisation-de-cholesky",
    "title": "Loi normale: cas multivari√©",
    "section": "Approche par la factorisation de Cholesky",
    "text": "Approche par la factorisation de Cholesky\nLa matrice \\(\\Sigma\\) √©tant sym√©trique, elle peut s‚Äô√©crire comme \\(\\Sigma = LL^\\top\\) o√π \\(L\\) est une matrice triangulaire inf√©rieure de taille \\(d \\times d\\). Gr√¢ce √† la d√©composition de Cholevsky et en reprenant les √©l√©ments de la preuve de la Proposition¬†1, on peut √©crire \\(\\mathbf{Y} = L \\mathbf{X} + {\\mu}\\) o√π \\(\\mathbf{X} \\sim \\mathcal{N}(0, \\mathrm{Id}_d)\\) et v√©rifier que \\(\\mathbf{Y} \\sim \\mathcal{N}({\\mu}, \\Sigma)\\).",
    "crumbs": [
      "Slides",
      "Loi normale: cas multivari√©"
    ]
  },
  {
    "objectID": "Slides/slides_loi_normale_multi.html#approche-par-la-d√©composition-spectrale-de-sigma",
    "href": "Slides/slides_loi_normale_multi.html#approche-par-la-d√©composition-spectrale-de-sigma",
    "title": "Loi normale: cas multivari√©",
    "section": "Approche par la d√©composition spectrale de \\(\\Sigma\\)",
    "text": "Approche par la d√©composition spectrale de \\(\\Sigma\\)\nLa matrice \\(\\Sigma\\) √©tant sym√©trique, elle se diagonalise en base orthonorm√©e : il existe une matrice orthogonale \\(P\\) telle que \\[\n    \\Sigma\n    = P \\mathrm{diag}(\\lambda_1 \\ldots, \\lambda_d) P^{-1}\n    = P \\mathrm{diag}(\\lambda_1 \\ldots, \\lambda_d) P^\\top\\\n\\] o√π \\(\\lambda_1, \\ldots, \\lambda_d \\geq 0\\) sont les valeurs propres de \\(\\Sigma\\) qui est semi-d√©finie positive. On pose alors \\(L = P \\mathrm{diag}(\\sqrt \\lambda_1 \\ldots, \\sqrt \\lambda_d)\\) qui est une racine carr√©e matricielle de \\(\\Sigma\\) au sens o√π \\(\\Sigma = L L ^\\top\\). On part alors d‚Äôun vecteur gaussien centr√©e r√©duit \\(\\mathbf{X} \\sim \\mathcal{N}(0, \\mathrm{Id}_d)\\) que l‚Äôon sait simuler\nLa proposition Proposition¬†2 assure alors que le vecteur \\(\\mathbf{X} = L \\mathbf{X} + {\\mu}\\) est un vecteur gaussien de loi \\(\\mathcal{N}({\\mu}, \\Sigma)\\).",
    "crumbs": [
      "Slides",
      "Loi normale: cas multivari√©"
    ]
  },
  {
    "objectID": "Slides/slides_loi_normale_multi.html#vecteurs-gaussiens-cas-bidimensionnel",
    "href": "Slides/slides_loi_normale_multi.html#vecteurs-gaussiens-cas-bidimensionnel",
    "title": "Loi normale: cas multivari√©",
    "section": "Vecteurs gaussiens : cas bidimensionnel",
    "text": "Vecteurs gaussiens : cas bidimensionnel\nEn dimension \\(p=2\\), la matrice de covariance \\(\\Sigma\\) peut toujours s‚Äô√©crire comme suit: \\[\n\\Sigma =\n\\begin{pmatrix}\\cos(\\theta) & - \\sin(\\theta)\\\\  \\sin(\\theta)& \\cos(\\theta)\n\\end{pmatrix} \\cdot\n\\begin{pmatrix}\\sigma_1 & 0\\\\ 0 & \\sigma_2\n\\end{pmatrix}\\cdot\n\\begin{pmatrix}\n\\cos(\\theta) &\\sin(\\theta)\\\\  -\\sin(\\theta)& \\cos(\\theta)\\end{pmatrix}\n\\]\n\n\\(\\theta\\) : l‚Äôangle de rotation des axes\n\\(\\sigma_1\\) et \\(\\sigma_2\\) √©carts-types des marginales (dans le rep√®re orthonormal apr√®s rotation)",
    "crumbs": [
      "Slides",
      "Loi normale: cas multivari√©"
    ]
  },
  {
    "objectID": "Slides/slides_loi_normale_multi.html#visualisation-de-la-densit√©-de-probabilit√©",
    "href": "Slides/slides_loi_normale_multi.html#visualisation-de-la-densit√©-de-probabilit√©",
    "title": "Loi normale: cas multivari√©",
    "section": "Visualisation de la densit√© de probabilit√©",
    "text": "Visualisation de la densit√© de probabilit√©\ncf.¬†cours",
    "crumbs": [
      "Slides",
      "Loi normale: cas multivari√©"
    ]
  },
  {
    "objectID": "Slides/slides_loi_normale_multi.html#bibliographie",
    "href": "Slides/slides_loi_normale_multi.html#bibliographie",
    "title": "Loi normale: cas multivari√©",
    "section": "Bibliographie",
    "text": "Bibliographie\n\n\nCiarlet, P. G. 2006. Introduction √† l‚Äôanalyse num√©rique matricielle et √† l‚Äôoptimisation. Cours et exercices corrig√©s. Dunod.\n\n\n\n\n\nLoi normale: cas multivari√©",
    "crumbs": [
      "Slides",
      "Loi normale: cas multivari√©"
    ]
  },
  {
    "objectID": "Slides/slides_simulation.html#enjeu",
    "href": "Slides/slides_simulation.html#enjeu",
    "title": "Simulation",
    "section": "Enjeu",
    "text": "Enjeu\n \nQuestion: Comment simuler en pratique des variables al√©atoires i.i.d?\n \n\nApproche: Commencer par les v.a. uniformes et en d√©duire les autres lois",
    "crumbs": [
      "Slides",
      "Simulation"
    ]
  },
  {
    "objectID": "Slides/slides_simulation.html#rappel-sur-les-variables-uniformes",
    "href": "Slides/slides_simulation.html#rappel-sur-les-variables-uniformes",
    "title": "Simulation",
    "section": "Rappel sur les variables uniformes",
    "text": "Rappel sur les variables uniformes\nRappel : \\(U\\) suit une loi uniforme sur \\([0,1]\\): \\(U\\sim\\mathcal{U}([0,1])\\) ssi sa fonction de r√©partition \\(F_U\\) vaut \\[\nF_U(x)\n=\n\\begin{cases}\n    0, & \\text{si }x &lt; 0\\,,        \\\\\n    x, & \\text{si }x \\in [0,1]\\,,  \\\\\n    1, & \\text{si }x &gt; 1\\,.\n\\end{cases}\n\\]",
    "crumbs": [
      "Slides",
      "Simulation"
    ]
  },
  {
    "objectID": "Slides/slides_simulation.html#challenges",
    "href": "Slides/slides_simulation.html#challenges",
    "title": "Simulation",
    "section": "Challenges",
    "text": "Challenges\n\nObjectif: simuler sur machine une suite \\(U_1, \\dots, U_n\\) de v.a., i.i.d., de loi \\(\\mathcal{U}([0,1])\\).\n Difficult√©s:\n\n\nUne machine est d√©terministe.\nLes nombres flottants entre \\(0\\) et \\(1\\) donn√©s par la machine sont de la forme \\(k/2^p\\), pour \\(k \\in \\{0, \\ldots, 2^{p-1}\\} \\implies\\) impossibilit√© de g√©n√©rer certains nombres.\nV√©rifier qu‚Äôune suite est bien i.i.d. est un probl√®me difficile.",
    "crumbs": [
      "Slides",
      "Simulation"
    ]
  },
  {
    "objectID": "Slides/slides_simulation.html#g√©n√©rateurs-de-nombres-pseudo-al√©atoires-1",
    "href": "Slides/slides_simulation.html#g√©n√©rateurs-de-nombres-pseudo-al√©atoires-1",
    "title": "Simulation",
    "section": "G√©n√©rateurs de nombres pseudo-al√©atoires",
    "text": "G√©n√©rateurs de nombres pseudo-al√©atoires\n\nD√©finition 1 (G√©n√©rateur de nombres pseudo-al√©atoires) \nUn g√©n√©rateur de nombres pseudo-al√©atoires (üá¨üáß: Pseudo Random Number Generator, PRNG), est un algorithme d√©terministe r√©cursif qui renvoie une suite \\(U_1, \\ldots, U_n\\) dans \\([0,1]\\) qui a un ‚Äúcomportement similaire‚Äù √† une suite i.i.d. de loi \\(\\mathcal{U}([0,1])\\).\n\n\nRemarque: ces nombres sont obtenus depuis des nombres entiers g√©n√©r√©s al√©atoirement et uniform√©ment sur grand interval, puis une transformation simple (normalisation) permet d‚Äôobtenir des nombres flottants (üá¨üáß: floats) entre 0 et 1.",
    "crumbs": [
      "Slides",
      "Simulation"
    ]
  },
  {
    "objectID": "Slides/slides_simulation.html#d√©tails-techniques",
    "href": "Slides/slides_simulation.html#d√©tails-techniques",
    "title": "Simulation",
    "section": "D√©tails techniques",
    "text": "D√©tails techniques\nUn PRNG se construit ainsi :\n\nInitialisation: une graine (üá¨üáß: seed) \\(U_0\\), d√©termine la premi√®re valeur (choix arbitraire)\nOn calcule \\(U_{n+1} = f(U_n)\\), o√π \\(f\\) est une transformation d√©terministe, telle que \\(U_{n+1}\\) est ‚Äúle plus ind√©pendant possible‚Äù de \\(U_1, \\dots, U_n\\).\n\n\n\n\\(f\\) : √† valeur dans un ensemble fini \\(\\implies\\) p√©riodicit√© (contrainte: utiliser la plus grande p√©riode possible)\nL‚Äôalgorithme est d√©terministe (une fois la graine fix√©e). Utilit√© de fixer la graine: r√©p√©ter des simulations dans des conditions identiques et ainsi rep√©rer des erreurs",
    "crumbs": [
      "Slides",
      "Simulation"
    ]
  },
  {
    "objectID": "Slides/slides_simulation.html#le-g√©n√©rateur-congruentiel-lin√©aire",
    "href": "Slides/slides_simulation.html#le-g√©n√©rateur-congruentiel-lin√©aire",
    "title": "Simulation",
    "section": "Le g√©n√©rateur congruentiel lin√©aire",
    "text": "Le g√©n√©rateur congruentiel lin√©aire\nLa plupart des PRNG s‚Äôappuient sur des r√©sultats arithm√©tiques.\n\n\nLe plus c√©l√®bre: G√©n√©rateur Congruentiel Lin√©aire (üá¨üáß Linear Congruential Generator, LCG).\nR√©currence: \\[\nX_{n+1} = a X_n + b \\quad \\text{mod } m \\enspace,\n\\] \\(a,b,m\\), entiers bien choisis pour que la suite obtenue ait de bonnes propri√©t√©s\nNormalisation: \\(X_n/m\\).\nExemple: la fonction rand de scilab utilisait cette congruence avec \\(m=2^{31}\\), \\(a=843\\; 314\\; 861\\), et \\(b=453\\; 816\\; 693\\).",
    "crumbs": [
      "Slides",
      "Simulation"
    ]
  },
  {
    "objectID": "Slides/slides_simulation.html#exemples-de-g√©n√©rateurs-alternatifs",
    "href": "Slides/slides_simulation.html#exemples-de-g√©n√©rateurs-alternatifs",
    "title": "Simulation",
    "section": "Exemples de g√©n√©rateurs alternatifs",
    "text": "Exemples de g√©n√©rateurs alternatifs\n \n\nm√©thode par d√©faut pour Python et R: Mersenne-Twister, s‚Äôappuie sur la multiplication vectorielle (p√©riode du g√©n√©rateur \\(m =2^{19937}-1\\))\n\n\n\nm√©thode par d√©faut pour numpy: PCG64 (cf.¬†documentation de numpy), dispose de meilleures garanties statistiques; voir https://www.pcg-random.org\n\n\n\nOn suppose d√©sormais disposer d‚Äôun g√©n√©rateur pseudo-al√©atoire sur \\([0,1]\\).",
    "crumbs": [
      "Slides",
      "Simulation"
    ]
  },
  {
    "objectID": "Slides/slides_simulation.html#usage-en-numpy",
    "href": "Slides/slides_simulation.html#usage-en-numpy",
    "title": "Simulation",
    "section": "Usage en numpy",
    "text": "Usage en numpy\nEn numpy (version&gt;1.17): utiliser des √©l√©ments al√©atoires est d‚Äôutiliser un g√©n√©rateur\n\n\nseed = 12345                       #  choix de la graine\nrng = np.random.default_rng(seed)  #  g√©n√©rateur\n\n\n\n\nprint(rng.random())                #  un tirage uniforme sur [0,1]\n\n0.22733602246716966\n\n\n\n\n\n\nprint(rng.random(size=5))          #  5 tirages uniformes sur [0,1]\n\n[0.31675834 0.79736546 0.67625467 0.39110955 0.33281393]\n\n\n\n\n\n\nprint(rng.random(size=(3, 2)))     #  matrice 3x2, √† entr√©es unif. sur [0,1]\n\n[[0.59830875 0.18673419]\n [0.67275604 0.94180287]\n [0.24824571 0.94888115]]",
    "crumbs": [
      "Slides",
      "Simulation"
    ]
  },
  {
    "objectID": "Slides/slides_simulation.html#plus-sur-les-lois-al√©atoires-et-python",
    "href": "Slides/slides_simulation.html#plus-sur-les-lois-al√©atoires-et-python",
    "title": "Simulation",
    "section": "Plus sur les lois al√©atoires et Python",
    "text": "Plus sur les lois al√©atoires et Python\n\n\n\nSuite du cours: apprendre √† g√©n√©rer de nombreuses lois √† partir de la loi uniforme\nEn pratique: les logiciels proposent les distributions classiques (gaussiennes, exponentielles, etc.), utiliser plut√¥t ces fonctions que de les impl√©menter soi-m√™me.\nListe exhaustive pour numpy:\nhttps://numpy.org/doc/stable/reference/random/generator.html#distributions\n\n\n\n\n\n\n\n\n\n\nPour aller plus loin\n\n\nUne excellent discussion sur les bonnes pratiques al√©atoires en numpy, et l‚Äôusage de np.random.default_rng est donn√©e dans ce blog post d‚ÄôAlbert Thomas.",
    "crumbs": [
      "Slides",
      "Simulation"
    ]
  },
  {
    "objectID": "Slides/slides_simulation.html#rappel-sur-la-fonction-quantile",
    "href": "Slides/slides_simulation.html#rappel-sur-la-fonction-quantile",
    "title": "Simulation",
    "section": "Rappel sur la fonction quantile",
    "text": "Rappel sur la fonction quantile\nRappel: Pour \\(F\\) une fonction d√©finie sur \\(\\mathbb{R}\\) √† valeurs dans \\([0, 1]\\), croissante, on note\n\\[\n\\forall q \\in ]0,1[, \\quad F^\\leftarrow(q) = \\inf\\{ x \\in \\mathbb{R} : F(x)\\geq q\\}\n\\tag{1}\\]\nNote: Si \\(x_0 \\in \\{ x \\in \\mathbb{R} : F(x)\\geq q\\}\\) alors \\([x_0,+\\infty[ \\subset \\{ x \\in \\mathbb{R} : F(x)\\geq q\\}\\)\n\n\nTh√©or√®me 1 (Carat√©risation des quantiles) Soit \\(F\\) une fonction d√©finie sur \\(\\mathbb{R}\\) √† valeurs dans \\([0, 1]\\), croissante et continue √† droite, alors pour tout \\(q \\in ]0, 1[\\), on a \\[\n\\begin{align}\n   \\{x \\in \\mathbb{R} :  F(x) \\geq q) \\} & =\n   \\{x \\in \\mathbb{R} : x \\geq F^\\leftarrow(q)  \\}\n\\end{align}\n\\]\n\n\n\nCas \\(\\subset\\): Soit \\(x \\in \\mathbb{R}\\) t.q. \\(F(x) \\geq q\\), alors par d√©finition de l‚Äôinf dans √âquation¬†1, \\(x \\geq F^\\leftarrow(q)\\)\n\n\n\nCas \\(\\supset\\): Soient \\(\\epsilon&gt;0\\) et \\(x \\in \\mathbb{R}\\) t.q. \\(x \\geq F^\\leftarrow(q)\\) alors (def. de l‚Äôinf) \\(\\exists x_0 \\in \\{ x \\in \\mathbb{R} : F(x)\\geq q\\}\\), t.q. \\(x + \\epsilon &gt; x_0\\). Ainsi, \\(F(x + \\epsilon) \\geq F(x_0) \\geq q\\); par continuit√© √† droite de \\(F\\), \\(F(x) \\geq q\\)",
    "crumbs": [
      "Slides",
      "Simulation"
    ]
  },
  {
    "objectID": "Slides/slides_simulation.html#m√©thode-dinversion",
    "href": "Slides/slides_simulation.html#m√©thode-dinversion",
    "title": "Simulation",
    "section": "M√©thode d‚Äôinversion",
    "text": "M√©thode d‚Äôinversion\n\nTh√©or√®me 2 (M√©thode d‚Äôinversion) Soit \\(X\\) une v.a r√©elle, et \\(U \\sim\\mathcal{U}([0,1])\\), alors la variable al√©atoire \\(F_X^\\leftarrow(U)\\) a m√™me loi que \\(X\\).\n\n\n\nPreuve: en utilisant le th√©or√®me pr√©c√©dent, on a \\[\n\\forall x\\in\\mathbb{R}, \\quad \\mathbb{P}(x \\geq F_X^\\leftarrow(U)) = \\mathbb{P}(F_X(x) \\geq U)\n\\]\n\n\nPuis, comme \\(U\\) est une loi uniforme sur \\([0,1]\\):\n\\[\n\\mathbb{P}(F_X(x) \\geq U) = F_X(x)\n\\]\n\n\nAinsi, \\(F_X^\\leftarrow(U)\\) et \\(X\\) ont m√™me loi: les deux v.a. ont la m√™me fonction de r√©partition",
    "crumbs": [
      "Slides",
      "Simulation"
    ]
  },
  {
    "objectID": "Slides/slides_simulation.html#sym√©trie-de-la-loi-uniforme",
    "href": "Slides/slides_simulation.html#sym√©trie-de-la-loi-uniforme",
    "title": "Simulation",
    "section": "Sym√©trie de la loi uniforme",
    "text": "Sym√©trie de la loi uniforme\n\nProposition 1 (Sym√©trie de la loi uniforme) Soit \\(U \\sim \\mathcal{U}([0,1])\\) une variable uniforme sur \\([0,1]\\). Alors, \\(1-U\\) suit aussi une loi uniforme sur \\([0,1]\\).\n\n\nPreuve:\nOn va d√©crire la fonction de r√©partition de \\(1-U\\) et montrer qu‚Äôelle est √©gale √† celle d‚Äôune loi uniforme sur \\([0,1]\\).\n\n\nLe r√©sultat est facile pour \\(x \\notin [0,1]\\), on suppose donc \\(x \\in [0,1]\\).\n\n\\[\n\\begin{align*}\n\\mathbb{P}(1-U \\leq x)} & \\class{fragment}{{}= \\mathbb{P}(U \\geq 1-x) }\\\\\n                       & \\class{fragment}{{} = 1-(1-x)} \\\\\n                       & \\class{fragment}{{} = x}\n\\end{align*}\n\\]",
    "crumbs": [
      "Slides",
      "Simulation"
    ]
  },
  {
    "objectID": "Slides/slides_simulation.html#exemple-loi-exponentielle",
    "href": "Slides/slides_simulation.html#exemple-loi-exponentielle",
    "title": "Simulation",
    "section": "Exemple : loi exponentielle",
    "text": "Exemple : loi exponentielle\n\n\n\nDensit√© d‚Äôune loi \\(\\mathcal{E}(\\lambda)\\) pour \\(\\lambda &gt; 0\\) : \\(f_{\\lambda}(x) = \\lambda e^{-\\lambda x}{1\\hspace{-3.8pt} 1}_{\\mathbb{R}_+}(x)\\)\nFonction de r√©partition: \\(F_{\\lambda}(x) = (1 - e^{-\\lambda x}) {1\\hspace{-3.8pt} 1}_{\\mathbb{R}_+}(x)\\)\n\\(F_{\\lambda}\\) est bijective (de \\(\\mathbb{R}_+\\) dans \\(]0,1[\\)) et pour tout \\(u \\in ]0,1[\\), \\(F_{\\lambda}^{-1}(u) = -\\frac{1}{\\lambda} \\log(1-u)\\)\n\n\n\n\nAvec le r√©sultat: \\[\nU \\sim \\mathcal{U}([0,1]) \\iff 1-U \\sim \\mathcal{U}([0,1])\\enspace,\n\\]\n\nPour simuler une loi exponentielle: simuler \\(U\\) uniforme et appliquer \\(-\\tfrac{1}{\\lambda} \\log(\\cdot)\\)\n\\[\n\\boxed{-\\tfrac{1}{\\lambda} \\log(U) \\sim \\mathcal{E}(\\lambda)}\n\\]",
    "crumbs": [
      "Slides",
      "Simulation"
    ]
  },
  {
    "objectID": "Slides/slides_simulation.html#visualisation",
    "href": "Slides/slides_simulation.html#visualisation",
    "title": "Simulation",
    "section": "Visualisation",
    "text": "Visualisation\nVoir animation dans la section Cours, section ‚ÄúM√©thode d‚Äôinversion‚Äù.",
    "crumbs": [
      "Slides",
      "Simulation"
    ]
  },
  {
    "objectID": "Slides/slides_simulation.html#m√©thode-de-rejet-contraintes",
    "href": "Slides/slides_simulation.html#m√©thode-de-rejet-contraintes",
    "title": "Simulation",
    "section": "M√©thode de rejet: contraintes",
    "text": "M√©thode de rejet: contraintes\nMotivation: simuler une variable al√©atoire \\(X\\) de densit√© \\(f\\) (loi cible), mais \\(f\\) est trop compliqu√©e pour la m√©thode de l‚Äôinverse.\n\nId√©e: tirer suivant une autre loi \\(g\\) (loi des propositions) et rejeter certains tirages.\n\n\non sait simuler \\(Y\\) de loi \\(g\\),\nil existe \\(m &gt; 0\\) tel que \\(f(x) \\leq m \\cdot g(x)\\) (constante de majoration)\non sait √©valuer le rapport d‚Äôacceptation \\(r(x) = \\frac{f(x)}{m\\cdot g(x)}\\)\n\n\n\n\nRemarque 1: \\(g(x)=0 \\implies f(x)=0\\), ainsi le support de \\(g\\) doit englober celui de \\(f\\)\n\n\nRemarque 2: \\(m \\geq 1\\) car \\(m = m \\displaystyle\\int_\\mathbb{R} g(x)\\, dx \\class{fragment}{{} \\geq \\displaystyle\\int_\\mathbb{R} f(x) dx} \\class{fragment}{{} = 1}\\)",
    "crumbs": [
      "Slides",
      "Simulation"
    ]
  },
  {
    "objectID": "Slides/slides_simulation.html#m√©thode-de-rejet-principe",
    "href": "Slides/slides_simulation.html#m√©thode-de-rejet-principe",
    "title": "Simulation",
    "section": "M√©thode de rejet: principe",
    "text": "M√©thode de rejet: principe\nConsid√©rer deux suites i.i.d. de v.a. ind√©pendantes entre elles:\n\n\\((Y_n)_{n \\geq 1}\\) de loi \\(g\\),\n\\((U_n)_{n \\geq 1}\\) de loi uniforme sur \\([0,1]\\).\n\nEn pratique, \\(Y_n\\) correspond √† une proposition et \\(U_n\\) permettra de d√©cider l‚Äôacceptation/rejet de la proposition:\n\n\nSi oui, alors on conserve \\(Y_n\\)\nSi non, on simule \\(Y_{n+1}\\)\n\nPour simuler \\(X\\) de densit√© \\(f\\), simuler \\(Y_n\\) (suivant \\(g\\)), \\(U_n\\) (suivant \\(\\mathcal{U}[0,1]\\)) et accepter si \\[\nU_n \\leq r(Y_n) = \\frac{f(Y_n)}{m\\cdot g(Y_n)}\n\\]",
    "crumbs": [
      "Slides",
      "Simulation"
    ]
  },
  {
    "objectID": "Slides/slides_simulation.html#exemple-simple",
    "href": "Slides/slides_simulation.html#exemple-simple",
    "title": "Simulation",
    "section": "Exemple simple",
    "text": "Exemple simple\n \n\n\n\\(f(x) = 4x^3 {1\\hspace{-3.8pt} 1}_{[0,1]}(x)\\)\n\\(f\\) est major√©e par \\(4\\) \\(\\implies\\) \\(g = {1\\hspace{-3.8pt} 1}_{[0,1]}\\) et \\(m=4\\) conviennent\n\\(r(x) =f(x) / (m\\cdot g(x)) = x^3\\), pour \\(x \\in [0,1]\\). On simule donc \\((Y_n, U_n)\\) et on teste si \\(4 \\cdot U_1 \\leq 4 Y_1^3\\), etc.\n\n\n\n\n\n\n\n\n\nNote\n\n\nDans la suite on verra qu‚Äôon tire des points \\((Y_n, 4U_n)\\) et qu‚Äôon teste si ils sont dans l‚Äôensemble \\(\\{(x,y) \\in \\mathbb{R}^2: y \\leq f(x) \\}\\)",
    "crumbs": [
      "Slides",
      "Simulation"
    ]
  },
  {
    "objectID": "Slides/slides_simulation.html#code-python",
    "href": "Slides/slides_simulation.html#code-python",
    "title": "Simulation",
    "section": "Code Python",
    "text": "Code Python\n\n\ndef accept_reject(n, f, g, g_sampler, m, rng):\n    \"\"\"\n    n: nombre de simulations\n    f: loi cible\n    g, g_sampler: loi et g√©n√©rateur des propositions                            \n    m: constante pour la majoration\n    rng: g√©n√©rateur pseudo-al√©atoire\n    \"\"\"\n    x_samples = np.zeros(n)\n    u_samples = np.zeros(n)\n    accepted = np.zeros(n)\n    for i in range(n):\n        x = g_sampler()\n        u = rng.uniform()\n        alpha = u * m * g(x)  # note: pour le test on peut √©viter les divisions\n        u_samples [i] = alpha\n        x_samples[i] = x\n        if  alpha &lt;= f(x):\n            accepted[i] = 1\n    return x_samples, u_samples, accepted",
    "crumbs": [
      "Slides",
      "Simulation"
    ]
  },
  {
    "objectID": "Slides/slides_simulation.html#visualisation-de-lexemple",
    "href": "Slides/slides_simulation.html#visualisation-de-lexemple",
    "title": "Simulation",
    "section": "Visualisation de l‚Äôexemple",
    "text": "Visualisation de l‚Äôexemple",
    "crumbs": [
      "Slides",
      "Simulation"
    ]
  },
  {
    "objectID": "Slides/slides_simulation.html#variante-avec-une-loi-triangulaire",
    "href": "Slides/slides_simulation.html#variante-avec-une-loi-triangulaire",
    "title": "Simulation",
    "section": "Variante avec une loi triangulaire",
    "text": "Variante avec une loi triangulaire\n \nSupposons disposer d‚Äôun g√©n√©rateur de loi triangulaire sur \\([0,1]\\) (cf.¬†np.random.triangular(0, 1, 1))\n\n\n\\(f(x) = 4x^3 {1\\hspace{-3.8pt} 1}_{[0,1]}(x)\\)\n\\(f(x)\\) est major√©e par \\(4 x \\cdot {1\\hspace{-3.8pt} 1}_{[0,1]}(x)\\) \\(\\implies\\) \\(g = 2x \\cdot {1\\hspace{-3.8pt} 1}_{[0,1]}\\) et \\(m=2\\) conviennent\n\\(r(x)=f(x) / (m\\cdot g(x)) = x^3\\), pour \\(x \\in [0,1]\\).",
    "crumbs": [
      "Slides",
      "Simulation"
    ]
  },
  {
    "objectID": "Slides/slides_simulation.html#variante-continu√©e",
    "href": "Slides/slides_simulation.html#variante-continu√©e",
    "title": "Simulation",
    "section": "Variante (continu√©e)",
    "text": "Variante (continu√©e)",
    "crumbs": [
      "Slides",
      "Simulation"
    ]
  },
  {
    "objectID": "Slides/slides_simulation.html#comparaison-des-deux-majorants",
    "href": "Slides/slides_simulation.html#comparaison-des-deux-majorants",
    "title": "Simulation",
    "section": "Comparaison des deux majorants",
    "text": "Comparaison des deux majorants\n\n\n\n\n\n\n\n\n\n\n\nmd`Taux d'acceptation: avec la loi uniforme **${ratio1.toPrecision(5)}**`\n\n\n\n\n\n\n\nmd`Taux d'acceptation: avec la loi triangulaire **${ratio2.toPrecision(5)}**`\n\n\n\n\n\n\n\nConclusion: plus le majorant est proche de la loi cible, plus le taux d‚Äôacceptation est √©lev√©, et moins de simulations sont n√©cessaires\n\n\n\n\n\n\n\n\nNote\n\n\nL‚Äôexemple est pour l‚Äôillustration de la m√©thode, dans le cas pr√©sent m√©thode de l‚Äôinverse fonctionnerait aussi (on peut calculer la fonction quantile explicitement).",
    "crumbs": [
      "Slides",
      "Simulation"
    ]
  },
  {
    "objectID": "Slides/slides_simulation.html#m√©thode-de-rejet-validation-th√©orique",
    "href": "Slides/slides_simulation.html#m√©thode-de-rejet-validation-th√©orique",
    "title": "Simulation",
    "section": "M√©thode de rejet: validation th√©orique",
    "text": "M√©thode de rejet: validation th√©orique\nRappel:\n\nil existe \\(m &gt; 0\\) tel que \\(f(x) \\leq m \\cdot g(x)\\) et \\(r(x) = \\frac{f(x)}{m\\cdot g(x)}\\)\n\\((Y_n)_{n \\geq 1}\\) i.i.d. de loi \\(g\\)\n\\((U_n)_{n \\geq 1}\\) i.i.d. de loi uniforme sur \\([0,1]\\) (ind√©pendamment des \\(Y_n\\))\n\n\n\nTh√©or√®me 3 (M√©thode de rejet) \nSoit \\(T = \\inf \\{n \\geq 1 : U_n \\leq r(Y_n)\\}\\) le premier instant o√π le tirage est accept√©. Alors :\n\n\\(T \\sim \\mathcal{G}(\\frac{1}{m})\\) : loi g√©om√©trique de param√®tre \\(\\frac{1}{m}\\)\n\\(Y_T\\) a pour densit√© \\(f\\) et est ind√©pendante de \\(T\\)",
    "crumbs": [
      "Slides",
      "Simulation"
    ]
  },
  {
    "objectID": "Slides/slides_simulation.html#d√©monstration",
    "href": "Slides/slides_simulation.html#d√©monstration",
    "title": "Simulation",
    "section": "D√©monstration",
    "text": "D√©monstration\nPour \\(x \\in \\mathbb{R}\\) et \\(n \\in \\mathbb{N}^{*}\\), et \\(X = Y_T\\), on √©crit \\(\\mathbb{P}(X \\leq x, T=n)= \\mathbb{P}(U_1 &gt; r(Y_1), \\dots, U_{n-1} &gt; r(Y_{n-1}), U_n \\leq r(Y_n), Y_n \\leq x)\\)\n\n\\[\n    \\mathbb{P}(X \\leq x, T=n) = {\\color{blue}\\mathbb{P}(U_1 &gt; r(Y_1))}^{n-1} \\cdot {\\color{brown}\\mathbb{P}(U_n \\leq r(Y_n), Y_n \\leq x)}, \\quad \\textsf{( tirages i.i.d.)}\n\\]\n\n\nPremier terme: \\(Y_1\\) et \\(U_1\\) sont ind√©pendantes, leur loi jointe correspond au produit des densit√©s :\n\n\n\\[\n{\\color{blue}\n\\begin{align*}\n        \\mathbb{P}(U_1 &gt; r(Y_1))\n         & \\class{fragment}{{} = \\mathbb{P}((U_1, Y_1) \\in \\{(u,y) \\in \\mathbb{R}^2 : u &gt; r(y)\\})}                             \\\\\n         & \\class{fragment}{{} = \\int_{\\mathbb{R}^2} \\left( {1\\hspace{-3.8pt} 1}_{\\{u &gt; r(y)\\}} \\right) \\cdot \\left({1\\hspace{-3.8pt} 1}_{[0,1]}(u) g(y)\\right) \\,  du  dy}  \\\\\n         & \\class{fragment}{{} = \\int_\\mathbb{R} \\left( \\int_0^1 {1\\hspace{-3.8pt} 1}_{\\{u &gt; r(y)\\}} \\,  du\\right) g(y)\\,  d y}\n         \\class{fragment}{{} =  \\int_\\mathbb{R} (1-r(y)) \\, g(y)\\,  d y}\\\\\n         & \\class{fragment}{{} =  \\int_\\mathbb{R} g(y) -  \\int_\\mathbb{R}\\frac{f(y)}{m} d y, \\quad\\quad \\text{car }  r(y) = \\frac{f(y)}{m \\cdot g(y)}}\\\\\n     & \\class{fragment}{{} = 1-\\tfrac{1}{m}}\n\\end{align*}\n}\n\\]",
    "crumbs": [
      "Slides",
      "Simulation"
    ]
  },
  {
    "objectID": "Slides/slides_simulation.html#d√©monstration-suite",
    "href": "Slides/slides_simulation.html#d√©monstration-suite",
    "title": "Simulation",
    "section": "D√©monstration (suite)",
    "text": "D√©monstration (suite)\nSecond terme: \\[\n{\\color{brown}\n\\begin{align*}\n    \\mathbb{P}(U_n \\leq r(Y_n), Y_n \\leq x)\n    & \\class{fragment}{{ = \\int_{\\mathbb{R}^2} {1\\hspace{-3.8pt} 1}_{\\{u \\leq r(y)\\}} {1\\hspace{-3.8pt} 1}_{\\{y \\leq x\\}} ({1\\hspace{-3.8pt} 1}_{[0,1]}(u) g(y)) \\,  du  dy  }} \\\\\n    & \\class{fragment}{{} = \\int_\\mathbb{R} \\left( \\int_0^1  {1\\hspace{-3.8pt} 1}_{\\{u \\leq r(y)\\}} \\,  du\\right) {1\\hspace{-3.8pt} 1}_{\\{y \\leq x\\}}  g(y)\\,  d y} \\\\\n        & \\class{fragment}{{} = \\int_\\mathbb{R} r(y) {1\\hspace{-3.8pt} 1}_{\\{y \\leq x\\}}  g(y)\\,  d y } \\\\\n        & \\class{fragment}{{} = \\int_{-\\infty}^x \\dfrac{f(y)}{m}\\,  d y } \\\\\n        & \\class{fragment}{{} = \\dfrac{F(x)}{m} , \\quad F \\textsf{ fonction de r√©partition associ√©e √†} f}\n\\end{align*}\n}\n\\]",
    "crumbs": [
      "Slides",
      "Simulation"
    ]
  },
  {
    "objectID": "Slides/slides_simulation.html#d√©monstration-suite-1",
    "href": "Slides/slides_simulation.html#d√©monstration-suite-1",
    "title": "Simulation",
    "section": "D√©monstration (suite)",
    "text": "D√©monstration (suite)\n\\[\n    \\mathbb{P}(X \\leq x, T=n)\n    =\n    {\\color{blue}\\left(1 - \\tfrac{1}{m}\\right)^{n-1}} \\cdot {\\color{brown}\\tfrac{F(x)}{m}}\n\\]\n\nOn peut alors obtenir les lois marginales: \\[\n\\begin{align*}\n    \\mathbb{P}(T=n)\n    & = \\lim_{q \\to \\infty} \\mathbb{P}(X \\in ]-\\infty, q], T=n) = \\lim_{q \\to \\infty} \\left(1 - \\tfrac{1}{m}\\right)^{n-1} \\tfrac{F(q)}{m}\\\\\n    & = \\left(1 - \\tfrac{1}{m}\\right)^{n-1} \\tfrac{1}{m}\n\\end{align*}\n\\] Ainsi, \\(T\\) suit une loi g√©om√©trique de param√®tre \\(1/m\\), puis \\(X\\) a pour loi \\(F\\):\n\n\n\\[\n\\begin{align*}\n    \\mathbb{P}(X \\leq x)\n    & = \\mathbb{P}(X \\leq x, T \\in \\mathbb{N}^*)\n      = \\sum_{n=1}^\\infty \\mathbb{P}(X \\leq x, T=n) \\\\\n    & = \\sum_{n=1}^\\infty \\left(1 - \\tfrac{1}{m}\\right)^{n-1} \\tfrac{F(x)}{m}\n      = \\tfrac{1}{1-(1-1/m)} \\tfrac{F(x)}{m} = F(x) \\enspace.\n\\end{align*}\n\\]",
    "crumbs": [
      "Slides",
      "Simulation"
    ]
  },
  {
    "objectID": "Slides/slides_simulation.html#fin-de-la-d√©monstration",
    "href": "Slides/slides_simulation.html#fin-de-la-d√©monstration",
    "title": "Simulation",
    "section": "Fin de la d√©monstration",
    "text": "Fin de la d√©monstration\nOn obtient l‚Äôind√©pendance de \\(T\\) et \\(X\\) car on peut alors √©crire: \\[\n    \\forall x \\in \\mathbb{R}, \\forall n \\in \\mathbb{N}^*, \\quad\n    \\mathbb{P}(X \\leq x, T=n)\n    =\n    \\mathbb{P}(X \\leq x) \\cdot \\mathbb{P}(T=n)\n\\]",
    "crumbs": [
      "Slides",
      "Simulation"
    ]
  },
  {
    "objectID": "Slides/slides_simulation.html#cas-de-densit√©-connue-√†-une-constante-pr√®s-exemple",
    "href": "Slides/slides_simulation.html#cas-de-densit√©-connue-√†-une-constante-pr√®s-exemple",
    "title": "Simulation",
    "section": "Cas de densit√© connue √† une constante pr√®s : exemple",
    "text": "Cas de densit√© connue √† une constante pr√®s : exemple\nLoi de Andrews (densit√© proportionnelle √† \\(\\mathrm{sinc}\\), sinus cardinal): \\[\n\\forall x \\in \\mathbb{R},\\quad f(x) = \\frac{1}{S} \\frac{\\sin(\\pi\\cdot x)}{\\pi \\cdot x}  {1\\hspace{-3.8pt} 1}_{[-1,1]}(x)\n\\] avec \\(S = \\int_{-1}^{1}\\frac{\\sin(\\pi\\cdot x)}{\\pi \\cdot x}dx\\) non explicite. On note parfois: \\(f(x) \\propto \\frac{\\sin(\\pi\\cdot x)}{\\pi \\cdot x}  {1\\hspace{-3.8pt} 1}_{[-1,1]}(x)\\)\n\n\nM√©thode du rejet: prendre \\(m=2/S\\) et \\(g(x) = \\frac{1}{2} {1\\hspace{-3.8pt} 1}_{[-1,1]}(x)\\):\n\\[\nu \\leq r(x) = \\frac{f(x)}{m \\cdot g(x)} \\iff u \\leq \\frac{1}{S} \\frac{\\sin(\\pi\\cdot x)}{\\pi \\cdot x}  \\cdot \\frac{1}{\\frac{2}{S} g(x)} = \\frac{\\sin(\\pi\\cdot x)}{\\pi \\cdot x}  \\cdot \\frac{1}{2 \\cdot g(x)}\n\\]\n\n\nAinsi l‚Äô√©valuation de \\(r(x)\\) est possible sans conna√Ætre \\(S\\)!",
    "crumbs": [
      "Slides",
      "Simulation"
    ]
  },
  {
    "objectID": "Slides/slides_simulation.html#loi-de-andrews-visualisation",
    "href": "Slides/slides_simulation.html#loi-de-andrews-visualisation",
    "title": "Simulation",
    "section": "Loi de Andrews: visualisation",
    "text": "Loi de Andrews: visualisation\n\nn = 300\nm = 2\nrng = np.random.default_rng(seed)\ng = lambda x: np.ones_like(x) / 2\ng_sampler = lambda: 2 * rng.uniform() - 1\nx_samples, u_samples, accepted = accept_reject(n, np.sinc, g, g_sampler, m, rng)\n\n\n\n\n\n\n\n\n\n\n\n\nmd`Le taux d'acceptation est ici de **${ratio_andrews.toPrecision(3)}**.`\n\n\n\n\n\n\n\n\n\n\n                                                \n\n\n\n\n\n\nmd`Pour la visualization, on approxime S=**${s_int.toPrecision(3)}** en utilisant une m√©thode de calcul num√©rique.`",
    "crumbs": [
      "Slides",
      "Simulation"
    ]
  },
  {
    "objectID": "Slides/slides_simulation.html#cas-de-densit√©-connue-√†-une-constante-pr√®s-cas-g√©n√©ral",
    "href": "Slides/slides_simulation.html#cas-de-densit√©-connue-√†-une-constante-pr√®s-cas-g√©n√©ral",
    "title": "Simulation",
    "section": "Cas de densit√© connue √† une constante pr√®s (cas g√©n√©ral)",
    "text": "Cas de densit√© connue √† une constante pr√®s (cas g√©n√©ral)\nSoit \\(\\tilde{f}: \\mathbb{R} \\to [0,+\\infty[\\) connue et \\(S \\triangleq \\int_{\\mathbb{R}} \\tilde{f}(x) \\,  d x  &lt; + \\infty\\) inconnue (ou dure √† √©valuer)\nDensit√© cible: \\(\\quad f(x) = \\frac{\\tilde{f}(x)}{S}\\)\n\nM√©thode du rejet pour \\(f\\), en utilisant seulement \\(\\tilde{f}\\): soit \\(\\tilde{m}&gt;0\\) un majorant de \\(\\tilde{f}\\) t.q. \\[\n\\begin{align*}\n\\tilde{f}(x) \\leq  \\tilde{m} \\cdot g(x)\n\\end{align*}\n\\]\nApplication avec \\(m=\\tilde{m}/S\\) (sans conna√Ætre \\(S\\)), le test d‚Äôacceptation donne:\n\n\\[\n\\begin{align*}\nU_n & \\leq \\frac{f(Y_n)}{m \\cdot g(Y_n)}\\\\\n  & \\class{fragment}{{}\\leq \\frac{\\frac{\\tilde{f}(Y_n)}{S}}{\\frac{\\tilde{m}}{S} \\cdot g(Y_n)}} \\class{fragment}{{}= \\frac{\\tilde{f}(Y_n)}{\\tilde{m} \\cdot g(Y_n)}}\n\\end{align*}\n\\]",
    "crumbs": [
      "Slides",
      "Simulation"
    ]
  },
  {
    "objectID": "Slides/slides_simulation.html#cas-multidimensionnel",
    "href": "Slides/slides_simulation.html#cas-multidimensionnel",
    "title": "Simulation",
    "section": "Cas multidimensionnel",
    "text": "Cas multidimensionnel\n\n\n\nimpossibilit√© de la m√©thode de l‚Äôinverse: fonction de r√©partition non disponible (en g√©n√©ral)\nla m√©thode de rejet : g√©n√©ralisable au cas multidimensionnel\n\n‚Äúfl√©au de la dimension‚Äù: plus la dimension est grande, plus la m√©thode est inefficace (penser au nombre de points n√©cessaires pour quadriller un hypercube‚Ä¶)\ndifficult√© d‚Äô√©crire une fonction de majoration en toute g√©n√©ralit√©",
    "crumbs": [
      "Slides",
      "Simulation"
    ]
  },
  {
    "objectID": "Slides/slides_simulation.html#exemple-loi-uniforme-sur-le-disque-unit√©-2d",
    "href": "Slides/slides_simulation.html#exemple-loi-uniforme-sur-le-disque-unit√©-2d",
    "title": "Simulation",
    "section": "Exemple: loi uniforme sur le disque unit√© (2D)",
    "text": "Exemple: loi uniforme sur le disque unit√© (2D)\n\nLoi cible: loi uniforme sur le disque unit√©, \\(f(x)\\propto {1\\hspace{-3.8pt} 1}_{x_1^2+x_2^2 \\leq 1}(x)\\) pour \\(x=(x_1,x_2)\\in\\mathbb{R}^2\\)\n\nLoi majorante: loi uniforme sur le carr√© \\([-1,1]^2\\), \\(g(x)\\triangleq \\tfrac{1}{2} {1\\hspace{-3.8pt} 1}_{[-1, 1]^2}(x)\\) et \\(m=2\\)\n\n\n\n\n\n\n\nNote\n\n\nLa loi uniforme sur le carr√© est une loi produit: il suffit de savoir g√©n√©rer une loi uniforme sur un segment 1D pour l‚Äôobtenir",
    "crumbs": [
      "Slides",
      "Simulation"
    ]
  },
  {
    "objectID": "Slides/slides_simulation.html#exemple-loi-uniforme-sur-le-disque-visualisation",
    "href": "Slides/slides_simulation.html#exemple-loi-uniforme-sur-le-disque-visualisation",
    "title": "Simulation",
    "section": "Exemple: loi uniforme sur le disque (visualisation)",
    "text": "Exemple: loi uniforme sur le disque (visualisation)\n\n\n\n\n\n                                                \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmd`Ratio estim√©e: **${ratio.toPrecision(5)}**`\nmd`Aire (bleue) estim√©: **${aire.toPrecision(5)}**`",
    "crumbs": [
      "Slides",
      "Simulation"
    ]
  },
  {
    "objectID": "Slides/slides_simulation.html#exemple-loi-uniforme-sur-une-cardio√Øde-2d",
    "href": "Slides/slides_simulation.html#exemple-loi-uniforme-sur-une-cardio√Øde-2d",
    "title": "Simulation",
    "section": "Exemple: loi uniforme sur une cardio√Øde (2D)",
    "text": "Exemple: loi uniforme sur une cardio√Øde (2D)\n\nLoi cible: loi uniforme sur le disque unit√©, \\(f(x)\\triangleq{1\\hspace{-3.8pt} 1}_{(x_1^2+x_2^2 - x_2)^2 \\leq x_1^2+ x_2^2}(x)\\) pour \\(x=(x_1,x_2)\\in\\mathbb{R}^2\\) \nLoi majorante: loi uniforme sur le rectangle \\([-2,3]\\times [-1.5,1.5]\\), \\(g(x)\\triangleq \\tfrac{1}{15}{1\\hspace{-3.8pt} 1}_{[-2,3]\\times [-1.5,1.5]}(x)\\) et \\(m=15\\)\n\n\n\n\n\n\n\nNote\n\n\nLa loi uniforme sur un rectangle est une loi produit: il suffit de savoir g√©n√©rer une loi uniforme sur un segment 1D pour l‚Äôobtenir",
    "crumbs": [
      "Slides",
      "Simulation"
    ]
  },
  {
    "objectID": "Slides/slides_simulation.html#exemple-loi-uniforme-sur-une-cardio√Øde-2d-1",
    "href": "Slides/slides_simulation.html#exemple-loi-uniforme-sur-une-cardio√Øde-2d-1",
    "title": "Simulation",
    "section": "Exemple: loi uniforme sur une cardio√Øde (2D)",
    "text": "Exemple: loi uniforme sur une cardio√Øde (2D)\n\n\n\n\n\n                                                \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmd`Ratio estim√©e: **${ratio_cardioid.toPrecision(5)}**`\nmd`Aire (bleue) estim√©: **${aire_cardioid.toPrecision(5)}**`",
    "crumbs": [
      "Slides",
      "Simulation"
    ]
  },
  {
    "objectID": "Slides/slides_simulation.html#th√©orie",
    "href": "Slides/slides_simulation.html#th√©orie",
    "title": "Simulation",
    "section": "Th√©orie",
    "text": "Th√©orie\n\nTh√©or√®me 4 (G√©neration uniforme sur un ensemble) Soient \\(A\\subset B \\subset \\mathbb{R}^d\\), deux ensembles mesurables pour la mesure de Lebesgue. Pour g√©n√©rer selon \\(\\mathcal{U}(A)\\), connaissant un g√©n√©rateur selon \\(\\mathcal{U}(B)\\), la m√©thode du rejet consiste ici √† tirer \\(Y_i \\sim \\mathcal{U}(B)\\) (i.i.d) et √† garder \\(Y_i\\) si \\(Y_i \\in A\\).\n\n\nPreuve: On note \\(f\\triangleq \\frac{1}{|A|} {1\\hspace{-3.8pt} 1}_{A}\\), \\(g\\triangleq \\frac{1}{|B|}{1\\hspace{-3.8pt} 1}_{B}\\) et \\(m\\triangleq\\frac{|B|}{|A|}\\). Comme \\(A \\subset B\\), pour tout \\(x\\in \\mathbb{R}^{d}\\): \\[\n\\begin{align*}\n\\class{fragment}{{}f(x)} \\class{fragment}{{}= \\frac{1}{|A|} {1\\hspace{-3.8pt} 1}_{A}(x)} \\class{fragment}{{}\\leq \\frac{1}{|B|} {1\\hspace{-3.8pt} 1}_{B}(x) \\cdot \\frac{|B|}{|A|}} \\class{fragment}{{}\\leq g(x) \\cdot m}\n\\end{align*}\n\\]\n\n\\[\n\\begin{align*}\n&\\class{fragment}{{}Y \\sim \\mathcal{U}(B),} \\quad \\class{fragment}{{} U \\sim \\mathcal{U}([0,1])} \\\\\n&\\class{fragment}{{}r(Y)} \\class{fragment}{{}= \\frac{f(Y)}{m\\cdot g(Y)}}\n\\class{fragment}{{}=\n\\frac{ \\frac{1}{|A|} {1\\hspace{-3.8pt} 1}_{A}(Y)}\n{\\frac{|B|}{|A|}\\cdot \\frac{1}{|B|} {1\\hspace{-3.8pt} 1}_{B}(Y)}}\n\\class{fragment}{{}=\n\\frac{  {1\\hspace{-3.8pt} 1}_{A}(Y)}\n{ {1\\hspace{-3.8pt} 1}_{B}(Y)}}\n\\class{fragment}{{}= {1\\hspace{-3.8pt} 1}_{A}(Y)}\n\\end{align*}\n\\]\n\\[\n\\begin{align*}\n\\class{fragment}{{}\\text{Enfin}, U \\leq {1\\hspace{-3.8pt} 1}_{A}(Y) \\iff {1\\hspace{-3.8pt} 1}_{A}(Y)=1}\\class{fragment}{{}\\iff Y \\in A}\n\\end{align*}\n\\]",
    "crumbs": [
      "Slides",
      "Simulation"
    ]
  },
  {
    "objectID": "Slides/slides_simulation.html#remarque-sur-les-constantes",
    "href": "Slides/slides_simulation.html#remarque-sur-les-constantes",
    "title": "Simulation",
    "section": "Remarque sur les constantes",
    "text": "Remarque sur les constantes\n\nDans l‚Äôexemple pr√©c√©dent, on a pu appliquer la m√©thode de rejet sans la connaissance de \\(m\\)\n\nPoint important: parfois la connaissance de \\(m\\) est difficile √† obtenir, et il est pr√©f√©rable de ne pas l‚Äôutiliser (notamment quand les constantes de normalisation des densit√©s sont difficiles √† calculer).\n Exemples: statistiques bayesiennes, mod√®les graphiques, etc.",
    "crumbs": [
      "Slides",
      "Simulation"
    ]
  },
  {
    "objectID": "Slides/slides_simulation.html#sommes-de-variables-al√©atoires",
    "href": "Slides/slides_simulation.html#sommes-de-variables-al√©atoires",
    "title": "Simulation",
    "section": "Sommes de variables al√©atoires",
    "text": "Sommes de variables al√©atoires\nLoi de Bernoulli: avec \\(U_1, \\ldots, U_n\\) i.i.d uniformes sur \\([0,1]\\) (m√©thode d‚Äôinversion): \\[\nX_i \\triangleq {1\\hspace{-3.8pt} 1}_{\\{U_i \\leq p\\}} \\sim \\mathcal{B}(p)\n\\]\n\nLoi binomiale: \\[\n    \\sum_{i=1}^n {1\\hspace{-3.8pt} 1}_{\\{U_i \\leq p\\}} \\sim \\mathcal{B}(n,p)\n\\] en rappelant que \\[\n    X = X_1 + \\cdots + X_n \\sim \\mathcal{B}(n,p)\n\\]\n\n\n\n\n\n\n\n\nNote\n\n\nLa m√©thode d‚Äôinversion marche aussi, mais n√©cessite le calcul de l‚Äôinverse g√©n√©ralis√©e de \\(F\\), donc de coefficients binomiaux‚Ä¶",
    "crumbs": [
      "Slides",
      "Simulation"
    ]
  },
  {
    "objectID": "Slides/slides_simulation.html#loi-de-poisson",
    "href": "Slides/slides_simulation.html#loi-de-poisson",
    "title": "Simulation",
    "section": "Loi de Poisson",
    "text": "Loi de Poisson\nRappel: \\(\\quad X \\sim \\mathcal{P}(\\lambda) \\iff \\mathbb{P}(X = k) = e^{-\\lambda} \\dfrac{\\lambda^k}{k!}\\,, \\quad  \\forall k \\in \\mathbb{N}.\\)\n\nProposition 2 (G√©n√©ration de v.a. de loi de Poisson) \nSoit \\((E_n)_{n \\geq 1}\\) des variables al√©atoires i.i.d. de loi exponentielle de param√®tre \\(\\lambda &gt; 0\\). On pose \\(S_k = E_1 + \\cdots + E_k\\). Alors pour tout \\(n \\in \\mathbb{N}\\) \\[\n    \\mathbb{P}(S_n \\leq 1 &lt; S_{n+1}) =  e^{-\\lambda} \\dfrac{\\lambda^n}{n!}\\enspace .\n\\] Ainsi, \\(T \\triangleq \\sup \\{n \\in \\mathbb{N} : S_n \\leq 1\\}\\) suit une loi de Poisson de param√®tre \\(\\lambda\\) : \\(T \\sim \\mathcal{P}(\\lambda)\\).\n\n\n\n\n\n\n\n\nPoint num√©rique\n\n\nM√©thode adapt√©e par numpy.random.poisson, cf.¬†code source, qui fut propos√©e par D. Knuth; Source: Wikipedia",
    "crumbs": [
      "Slides",
      "Simulation"
    ]
  },
  {
    "objectID": "Slides/slides_simulation.html#loi-de-poisson-suite",
    "href": "Slides/slides_simulation.html#loi-de-poisson-suite",
    "title": "Simulation",
    "section": "Loi de Poisson (suite)",
    "text": "Loi de Poisson (suite)\nLa preuve repose sur le r√©sultat suivant:\n\nLemme 1 (Loi de Erlang) \nSoient \\(E_1, \\dots, E_n\\) des v.a., i.i.d. de loi exponentielle de param√®tre \\(\\lambda &gt;0\\). La somme \\(S_n=E_1+\\dots+E_n\\) suit une loi d‚ÄôErlang de param√®tres \\((n,\\lambda)\\), de fonction de r√©partition \\[\n    F_{n,\\lambda}(t) = 1 - \\sum_{k=0}^{n-1} e^{-\\lambda t} \\frac{(\\lambda t)^k}{k!}\\,.\n\\]\n\n\nPreuve partielle: Transform√©e de Laplace de \\(\\mathcal{E}(\\lambda)\\): \\(\\mathbb{E}\\left(e^{-tE_1}\\right) = \\int_0^{+\\infty} e^{-t x} \\lambda e^{-\\lambda x} \\,  d x = \\frac{\\lambda}{\\lambda+t}\\)\n\n\nDensit√© d‚Äôune loi \\(\\Gamma(\\alpha,\\beta): \\quad f(x) = \\tfrac{\\beta^\\alpha}{\\Gamma(\\alpha)} x^{\\alpha-1} e^{-\\beta x}\\)\n\n\nTransform√©e de Laplace d‚Äôune loi \\(\\Gamma(\\alpha,\\beta): \\quad \\mathbb{E}\\left(e^{-tX}\\right) = \\left(\\tfrac{\\beta}{\\beta+t}\\right)^\\alpha\\)\n\n\nEnfin, \\(\\mathbb{E}\\left(e^{-t S_n}\\right) = \\left(\\mathbb{E}\\left(e^{-t E_1}\\right)\\right)^n=\\left(\\tfrac{\\lambda}{\\lambda+t}\\right)^n\\)",
    "crumbs": [
      "Slides",
      "Simulation"
    ]
  },
  {
    "objectID": "Slides/slides_simulation.html#d√©monstration-avec-le-lemme",
    "href": "Slides/slides_simulation.html#d√©monstration-avec-le-lemme",
    "title": "Simulation",
    "section": "D√©monstration avec le lemme",
    "text": "D√©monstration avec le lemme\nPour \\(n \\in \\mathbb{N}^*\\), on d√©compose la probabilit√© \\(\\mathbb{P}(S_n \\leq 1 &lt; S_{n+1})\\) via \\[\n\\begin{align*}\n    \\mathbb{P}(S_n \\leq 1 &lt; S_{n+1})\n    & = \\mathbb{P}(\\{S_n \\leq 1\\} \\setminus \\{S_{n+1} \\leq 1\\})\\\\\n    & = \\mathbb{P}(S_n \\leq 1) - \\mathbb{P}(S_{n+1} \\leq 1)\\,.\n\\end{align*}\n\\]\n\nDu lemme pr√©c√©dent : \\(\\mathbb{P}(S_n \\leq 1) = 1 - \\displaystyle\\sum_{k=0}^{n-1} e^{-\\lambda} \\dfrac{\\lambda^k}{k!}\\) et \\(\\mathbb{P}(S_{n+1} \\leq 1) = 1 - \\displaystyle\\sum_{k=0}^{n} e^{-\\lambda} \\dfrac{\\lambda^k}{k!}\\).\n\n\nPuis, \\[\n    \\mathbb{P}(S_n \\leq 1 &lt; S_{n+1})\n    = e^{-\\lambda} \\dfrac{\\lambda^n}{n!}\\,.\n\\]\n\n\nOn conclut la preuve de la proposition en remarquant que pour \\(T \\triangleq \\sup \\{k \\in \\mathbb{N}^* : S_k \\leq 1\\}\\) \\[\n    \\mathbb{P}(T=n) = \\mathbb{P}(S_n \\leq 1 &lt; S_{n+1})\\,.\n\\]\n\n\n\nSimulation",
    "crumbs": [
      "Slides",
      "Simulation"
    ]
  },
  {
    "objectID": "TD/TD1.html#test2",
    "href": "TD/TD1.html#test2",
    "title": "TD1:‚Ä¶",
    "section": "test2",
    "text": "test2",
    "crumbs": [
      "TD",
      "TD1:..."
    ]
  },
  {
    "objectID": "TP/TP2.html",
    "href": "TP/TP2.html",
    "title": "TP2: Simulation de lois √©l√©mentaires",
    "section": "",
    "text": "Objectifs de ce TP\n\n\n\n\nUtiliser les g√©n√©rateurs al√©atoires en Python et numpy, savoir afficher un histogramme, une densit√©, etc.\nComprendre au mieux comment utiliser les fonctions al√©atoires (principalement les g√©n√©rateurs) en numpy.",
    "crumbs": [
      "TP",
      "TP2: Simulation de lois √©l√©mentaires"
    ]
  },
  {
    "objectID": "TP/TP2.html#loi-uniforme-et-graine.",
    "href": "TP/TP2.html#loi-uniforme-et-graine.",
    "title": "TP2: Simulation de lois √©l√©mentaires",
    "section": "Loi uniforme et graine.",
    "text": "Loi uniforme et graine.\nAvec numpy, la fonction numpy.random.uniform permet la g√©n√©ration de r√©alisations pseudo-al√©atoires de la loi uniforme sur [0,1].\nOn peut modifier la taille de l‚Äô√©chantillon g√©n√©r√© en modifiant l‚Äôargument de la fonction. Pour obtenir n=4 r√©alisations i.i.d. de loi uniforme, essayez par exemple\n\nimport numpy as np\nnp.random.uniform(size=4)\n\narray([0.80707097, 0.37447255, 0.9741228 , 0.72934639])\n\n\nPour rappel, l‚Äôalgorithme de g√©n√©ration de v.a. est r√©cursif et s‚Äôappuie sur une graine. La graine peut √™tre modifi√©e avec la cr√©ation d‚Äôun g√©n√©rateur, et il suffit d‚Äôentrer un nombre en argument pour fixer cette graine.\n\nrng = np.random.default_rng(seed=34)\nprint(rng.uniform())\nrng = np.random.default_rng(34)\nprint(rng.uniform())\n\n0.004028243493043537\n0.004028243493043537\n\n\nChanger les valeurs de seed et v√©rifier que les tirages ont bien chang√©.",
    "crumbs": [
      "TP",
      "TP2: Simulation de lois √©l√©mentaires"
    ]
  },
  {
    "objectID": "TP/TP2.html#exercice-1-simulation-de-loi-uniforme-et-histogramme",
    "href": "TP/TP2.html#exercice-1-simulation-de-loi-uniforme-et-histogramme",
    "title": "TP2: Simulation de lois √©l√©mentaires",
    "section": "Exercice 1: Simulation de loi uniforme et histogramme",
    "text": "Exercice 1: Simulation de loi uniforme et histogramme\nCr√©ez un vecteur de taille 1000 compos√© de r√©alisations i.i.d. de v.a.uniformes sur [-1,1]. Dans la suite on supposera que l‚Äôon a charg√© matplotlib pour l‚Äôaffichage graphique avec la commande:\n\nimport matplotlib.pylab as plt\nfrom scipy import stats\n\n√Ä l‚Äôaide de la fonction plt.hist, repr√©sentez l‚Äôhistogramme de cet √©chantillon:\nfig, ax = plt.subplots()\nvect = rng.uniform(-1, 1, 1000)\nax.hist(vect, label=\"Histogramme\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\nOn utilisera l‚Äôaide de hist de matplotlibs pour pr√©ciser les options graphiques suivantes:\n\nAnalysez en particulier ce que fait l‚Äôoption bins en entrant l‚Äôoption bins=30 et bins=10.\nModifiez √©galement votre histogramme avec l‚Äôoption density=True, de sorte que l‚Äôaire soit de 1 (on repr√©sente donc une densit√© qui est constante par morceaux)\nAjoutez un titre √† l‚Äôhistogramme gr√¢ce √† la commande plt.title (avec une cha√Æne de caract√®res entre guillemets). On peut √©galement ajouter un nom aux axes avec l‚Äôoption plt.xlabel et plt.ylabel.\nLes options ax.set_xlim et ax.set_ylim permettent de pr√©ciser l‚Äô√©chelle de axes: il faut pr√©ciser un tuple (a,b) o√π a&lt;b sont les deux bornes choisies pour votre axe.\nOn modifiera aussi les options fill et histtype de hist pour obtenir le r√©sultat suivant, en affichant sur un m√™me graphique trois tirages, de tailles 1000, 5000 et 10000.\nLa densit√© de la loi uniforme est obtenue avec la fonction pdf du module scipy.stats. Cr√©er un vecteur √©quir√©parti sur [-2, 2] de longueur 300 √©valuer la fonction sur la m√™me figure: on souhaite superposer cette densit√© √† l‚Äôhistogramme. On utilisera la fonction plot pour tracer la densit√©, et on pourra utiliser l‚Äôoption alpha pour rendre la densit√© plus transparente.\n\nUn exemple de figure de qualit√© acceptable est par exemple celle qui suit:\n\n\n\n\n\n\n\n\n\n\nAttention quand vous tracez des histogrammes pour des r√©alisation de la v.a. non born√©es: pour la gaussienne, les histogrammes sont bons\n\nCode\nvectGauss = np.random.randn(1000)\n\nfig, ax = plt.subplots()\n\nxx = np.linspace(vectGauss.min() - 1, vectGauss.max() + 1, 100)\nax.plot(xx, stats.norm.pdf(xx, loc=0, scale=1),'--', color='k', label=\"Loi th√©orique\", alpha=0.5)\n\nax.hist(vectGauss, histtype='step', density=True, bins=30, label=\"Histogramme 10000\", fill=False, color=colors[0])\n# plot density\n\nplt.title('Histogramme: tirage Gaussien')\nplt.show()\n\n\n\n\n\n\n\n\nPour la Cauchy, les histogrammes se comportent mal\n\nCode\nvectCauchy = np.random.standard_cauchy(10000)\n\nfig, ax = plt.subplots()\n\nxx = np.linspace(vectCauchy.min() - 1, vectCauchy.max() + 1, 100)\nax.plot(xx, stats.cauchy.pdf(xx, loc=0, scale=1),'--', color='k', label=\"Loi th√©orique\", alpha=0.5)\n\nax.hist(vectCauchy, histtype='step', density=True, bins=30, label=\"Histogramme 10000\", fill=False, color=colors[0])\nplt.title('Histogramme na√Øf: tirage Cauchy...')\nplt.show()\n# plot density\n\n\n\n\n\n\n\n\nIl faut tronquer pour retrouver une repr√©sentation fid√®le:\n\nCode\nxmax = 11\nfig, ax = plt.subplots()\n\nxx = np.linspace(-xmax - 1, xmax + 1, 100)\nax.plot(xx, stats.cauchy.pdf(xx, loc=0, scale=1),'--', color='k', label=\"Loi th√©orique\", alpha=0.5)\n\nax.hist(vectCauchy[np.abs(vectCauchy) &lt; xmax], histtype='step', density=True, bins=30, label=\"Histogramme 10000\", fill=False, color=colors[0])\n# plot density\n\nax.set_xlim((-xmax - 1, xmax + 1))\n\nplt.title('Histogramme : tirage Cauchy tronqu√©e...')\nplt.show()",
    "crumbs": [
      "TP",
      "TP2: Simulation de lois √©l√©mentaires"
    ]
  },
  {
    "objectID": "TP/TP2.html#exercice-2-fonction-de-r√©partition-de-la-loi-uniforme",
    "href": "TP/TP2.html#exercice-2-fonction-de-r√©partition-de-la-loi-uniforme",
    "title": "TP2: Simulation de lois √©l√©mentaires",
    "section": "Exercice 2: Fonction de r√©partition de la loi uniforme",
    "text": "Exercice 2: Fonction de r√©partition de la loi uniforme\nLa fonction de r√©partition de la loi uniforme est obtenue via la commande cdf du module scipy.stats.uniform. √Ä l‚Äôaide de la commande plt.plot tracez en bleu la fonction de r√©partition de la loi uniforme sur [-1,1], [-0.7, 0.7] et [-0.5,0.5] et donnez un titre √† votre graphique.\n\nOn contr√¥le avec lw (linewidth) l‚Äô√©paisseur du trait.\nVous pouvez modifier le style et les marqueurs facilement en matplotlib. Une liste exhaustive est donn√©e ici: matplotlib.pyplot.plot.html\nEnfin pour les couleurs on pourra consulter l‚Äôaide en ligne ici: Color tutorial. La mani√®re la plus simple est souvent d‚Äôajouter l‚Äôoption color=nom_couleur dans la fonction plot.\n\nManipulez les diff√©rentes options pour vous familiariser avec les graphes\n\n\n\n\n\n\n\n\n\n\n¬≤ :::{.callout-note}",
    "crumbs": [
      "TP",
      "TP2: Simulation de lois √©l√©mentaires"
    ]
  },
  {
    "objectID": "TP/TP2.html#pour-aller-plus-loin",
    "href": "TP/TP2.html#pour-aller-plus-loin",
    "title": "TP2: Simulation de lois √©l√©mentaires",
    "section": "Pour aller plus loin",
    "text": "Pour aller plus loin\nTenter de reproduire la figure suivante\n\n\n\n\n\n\n\n\n\n\n:::",
    "crumbs": [
      "TP",
      "TP2: Simulation de lois √©l√©mentaires"
    ]
  },
  {
    "objectID": "TP/TP2.html#exercice-3-√©tude-de-la-moyenne-empirique",
    "href": "TP/TP2.html#exercice-3-√©tude-de-la-moyenne-empirique",
    "title": "TP2: Simulation de lois √©l√©mentaires",
    "section": "Exercice 3: √âtude de la moyenne empirique",
    "text": "Exercice 3: √âtude de la moyenne empirique\nCr√©ez un vecteur de taille 100 compos√© de r√©alisations i.i.d. de variables uniformes sur [0,1]. Calculez dans un vecteur la moyenne cumul√©e des valeurs g√©n√©r√©es. Repr√©senter graphiquement l‚Äô√©volution de ces moyennes. Vers quoi semble converger la moyenne quand la taille de l‚Äô√©chantillon augmente ?\nPour ajouter une droite √† un graphe, on utilise la commande ax.axhline. Ajoutez en rouge la droite d‚Äô√©quation y=1/2 sur le graphe pr√©c√©dent. Refaites cet exercice avec un √©chantillon de taille n=1000 pour observer plus finement la convergence.",
    "crumbs": [
      "TP",
      "TP2: Simulation de lois √©l√©mentaires"
    ]
  },
  {
    "objectID": "TP/TP2.html#exercice-4-m√©thode-dinversion-loi-exponentielle-et-loi-de-cauchy",
    "href": "TP/TP2.html#exercice-4-m√©thode-dinversion-loi-exponentielle-et-loi-de-cauchy",
    "title": "TP2: Simulation de lois √©l√©mentaires",
    "section": "Exercice 4: M√©thode d‚Äôinversion, loi exponentielle et loi de Cauchy",
    "text": "Exercice 4: M√©thode d‚Äôinversion, loi exponentielle et loi de Cauchy\n\nRepr√©sentez graphiquement la fonction de r√©partition d‚Äôune loi exponentielle de param√®tre \\lambda=1.\n√âcrivez une fonction dzexpo qui prend en argument une taille d‚Äô√©chantillon n et un param√®tre \\lambda &gt; 0 et qui donne en sortie un √©chantillon de taille n de loi \\mathcal{E}(\\lambda). On utilisera la m√©thode d‚Äôinversion vue en cours et seulement des tirages uniformes sur [0,1]. Attention, le mot clef lambda est un mot r√©serv√© en Python.\nRepr√©sentez graphiquement l‚Äôhistogramme cumul√© (voir l‚Äôoption cumulative de hist) d‚Äôun tel √©chantillon pour n=10^2, n=10^3, puis n=10^4, et pour \\lambda = 1, puis \\lambda = 4. Superposez √† chaque fois le graphe de la densit√© de \\mathcal{E}(\\lambda).\nIllustrez graphiquement la loi des grands nombres avec \\lambda = 1, puis \\lambda = 4. On tracera en particulier la droite d‚Äô√©quation y=\\mathbb{E}[X], o√π X \\sim \\mathcal{E}(\\lambda).\nReprenez les questions pr√©c√©dentes avec la loi de Cauchy (mais repr√©senter la densit√© plut√¥t que les fonctions de r√©partition). Commentez les r√©sultats obtenus.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOn reprend le tout avec la loi de Cauchy:",
    "crumbs": [
      "TP",
      "TP2: Simulation de lois √©l√©mentaires"
    ]
  },
  {
    "objectID": "TP/TP2.html#pour-aller-plus-loin-les-m√©thodes-√†-noyaux-kde",
    "href": "TP/TP2.html#pour-aller-plus-loin-les-m√©thodes-√†-noyaux-kde",
    "title": "TP2: Simulation de lois √©l√©mentaires",
    "section": "Pour aller plus loin: les m√©thodes √† noyaux (KDE)",
    "text": "Pour aller plus loin: les m√©thodes √† noyaux (KDE)\nLe ph√©nom√®ne qui appara√Æt avec la loi de Cauchy est les suivant: les queues de la loi de Cauchy sont tellement √©paisses que la moyenne empirique que l‚Äô√©cart entre \\min_i(X_i) et le \\max_i(X_i) (les X_i √©tant les tirages effectu√©s) est du m√™me ordre de grandeur que le nombre de tirages, n. Comme la fonction hist discr√©tise l‚Äôintervalle [\\min(X_i), \\max(X_i)] en le nombre de bo√Ætes (bins) on observe peu de points dans chaque bo√Æte ce qui rend l‚Äôestimation de la densit√© trop petite.\nCe point n‚Äôest pas un probl√®me pour les lois gaussiennes, car la largeur de l‚Äôintervalle [\\min(X_i), \\max(X_i)] est de l‚Äôordre de \\sqrt{\\log(n)}, et comme on a n points √† placer dans cet intervalle, il y a assez de points dans chaque bo√Æte pour obtenir une bonne estimation.\nNotons que les m√©thodes √† noyaux (KDE) ne souffrent pas de ce d√©faut, car les ‚Äúbo√Ætes‚Äù sont en fait fix√© autour des donn√©es et non sur une grille discr√©tis√©e comme c‚Äôest le cas pour l‚Äôhistogramme. Voir par exemple l‚Äôaide de scikit learn ou encore wikipedia, m√©thode √† noyau.\n\nimport plotly.graph_objects as go\nfrom sklearn.neighbors import KernelDensity\nn_sample = 1000\nx = np.linspace(-2, 10, num=10000)\nX=dzcauchy(n_sample, 1)\n\nkde = KernelDensity(kernel=\"tophat\", bandwidth=0.1).fit(X.reshape(-1, 1))\nlog_dens = kde.score_samples(x.reshape(-1, 1))\nfig=go.Figure()\n\nfig.add_trace(go.Scatter(x=x, y=np.exp(log_dens), mode='lines', line=dict(color='blue', width=2), name=\"Estimation de la densit√©\"))\nfig.add_trace(go.Scatter(x=x, y=stats.cauchy.pdf(x, scale=1, loc=0), mode='lines', opacity=0.6, line=dict(color='black', width=2), name=\"Densit√© de Cauchy\"))\n\nfig.update_layout(\n    template=\"simple_white\",\n    showlegend=True,\n)\nprint(f\"Taille de l'√©chantillon: {n_sample}\")\nprint(f\"√âtendue: {np.max(X)-np.min(X)}\")\n\nTaille de l'√©chantillon: 1000\n√âtendue: 821.8956937401883\n\n\nNotons qu‚Äôun ph√©nom√®ne similaire appara√Æt aussi avec la loi de Pareto, qui est une loi √† queue lourde quand son param√®tre \\alpha est plus petit que 1.\n\n\n\n\n\n\nTaille de l'√©chantillon: 1000\n√âtendue: 6767.926681527171\nTaille de l'√©chantillon: 1000\n√âtendue: 120.26958531004222",
    "crumbs": [
      "TP",
      "TP2: Simulation de lois √©l√©mentaires"
    ]
  },
  {
    "objectID": "TP/quarto.html",
    "href": "TP/quarto.html",
    "title": "D√©marrage en Quarto",
    "section": "",
    "text": "Ce texte est principalement inspir√© du travail d‚ÄôArthur Turrell et de son cours sur Quarto pour les √©conomistes .",
    "crumbs": [
      "TP",
      "D√©marrage en Quarto"
    ]
  },
  {
    "objectID": "TP/quarto.html#introduction",
    "href": "TP/quarto.html#introduction",
    "title": "D√©marrage en Quarto",
    "section": "Introduction",
    "text": "Introduction\nQuarto est un cadre d‚Äô√©dition unifi√© pour la science des donn√©es, qui combine votre code, ses r√©sultats et vos commentaires. Le markdown de Quarto est con√ßu pour √™tre utilis√© de trois mani√®res :\n\nPour communiquer avec des coll√®gues qui souhaitent se concentrer sur les conclusions, et non sur le code derri√®re l‚Äôanalyse.\nPour collaborer avec d‚Äôautres coll√®gues/scientifiques (y compris vous-m√™me dans le futur !), qui s‚Äôint√©ressent √† la fois √† vos conclusions et √† la mani√®re dont vous les avez obtenues (c‚Äôest-√†-dire le code).\nEn tant qu‚Äôenvironnement dans lequel faire de la science des donn√©es, comme un cahier de laboratoire moderne o√π vous pouvez capturer non seulement ce que vous avez fait, mais aussi ce que vous pensiez.\n\nEn combinant le code, les r√©sultats et les commentaires, Quarto permet de cr√©er des documents riches et interactifs qui peuvent √™tre facilement partag√©s et mis √† jour.\n\nExemple 1 (Exemple d‚Äôutilisation) L‚Äô√©criture d‚Äôun rapport de TP ou d‚Äôun projet peut se faire facilement sous quarto, avec un export en .pdf (ou en .html). Vous pouvez d√©cider de masquer ou d‚Äôafficher les parties de code dans les sorties finales. Plus en d√©tail, les cas d‚Äôutilisation comprennent :\n\ndes rapports utilisant des donn√©es et/ou des graphiques et qui sont similaires √† chaque fois qu‚Äôils sont ex√©cut√©s (par exemple, seules les donn√©es sont mises √† jour)\ndes rapports techniques qui montrent ou utilisent les fonctionnalit√©s d‚Äôune base de code existante\ndes pr√©sentations qui r√©sument les donn√©es les plus r√©centes et qui sont produites √† une fr√©quence r√©guli√®re\nl‚Äôenvoi d‚Äôanalyses exploratoires ou de prototypes √† des co-auteurs ou des collaborateurs\nla r√©daction de blogs pour les services de blogging qui acceptent les fichiers .md (assurez-vous d‚Äôexporter vers markdown)\nla cr√©ation de sites web mis √† jour automatiquement de mani√®re relativement simple voir.\n\n\n\nPr√©requis\nInutile pour les machines de l‚Äôuniversit√©!: Vous devez vous rendre sur le site web de Quarto (https://quarto.org/) et suivre les instructions d‚Äôinstallation (https://quarto.org/docs/getting-started/installation.html) avant de commencer. Vous pouvez v√©rifier que vous avez correctement install√© Quarto en utilisant la commande quarto check install sur la ligne de commande.\nVous trouverez l‚Äôextension Quarto pour Visual Studio Code ici: (https://marketplace.visualstudio.com/items?itemName=quarto.quarto). Cette extension cr√©e un bouton sp√©cial dans Visual Studio Code appel√© ‚Äúrender‚Äù qui vous montre √† quoi ressemblera la sortie c√¥te √† c√¥te avec l‚Äôentr√©e, ou encore ‚Äúpreview‚Äù.",
    "crumbs": [
      "TP",
      "D√©marrage en Quarto"
    ]
  },
  {
    "objectID": "TP/quarto.html#rapports-automatis√©s-avec-quarto",
    "href": "TP/quarto.html#rapports-automatis√©s-avec-quarto",
    "title": "D√©marrage en Quarto",
    "section": "Rapports automatis√©s avec Quarto",
    "text": "Rapports automatis√©s avec Quarto\nQuarto peut √™tre utilis√© pour cr√©er des documents et des pr√©sentations de sortie dans une grande vari√©t√© de formats, y compris HTML, PDF et bien d‚Äôautres.\nVous pouvez √©crire les documents d‚Äôentr√©e y compris les extraits de code, de la mani√®re suivante :\n\nCr√©er un fichier markdown sp√©cial, avec l‚Äôextension de fichier .qmd. Pour en savoir plus sur markdown, voir {ref}wrkflow-markdown. Les blocs de code qui ont une syntaxe sp√©ciale sont ex√©cut√©s et leurs r√©sultats sont inclus dans toutes les sorties.\n\nVous pouvez √©galement ajouter du code (Python, R, JavaScript, etc.) aux documents pour cr√©er dynamiquement des figures, des tableaux, etc., puis rendre les documents √† leur format final √† l‚Äôaide de Quarto.\n\nUn exemple minimal d‚Äôun rapport √©crit avec du contenu markdown\nNous allons maintenant essayer un exemple plus minimaliste de la premi√®re approche, un fichier .qmd, qui inclut √©galement du code et des sorties.\nIl y a des avantages et des inconv√©nients √† √©crire votre rapport au format .qmd. L‚Äôavantage est que c‚Äôest un simple fichier texte et donc n‚Äôimporte qui peut l‚Äôouvrir, le regarder et le modifier avec un √©diteur de texte (et c‚Äôest √©galement plus pratique pour le contr√¥le de version, par exemple avec git). Le gros, gros inconv√©nient est que vous ne pouvez pas voir comment le code √©volue au fur et √† mesure que vous l‚Äô√©crivez (vous devez l‚Äôexecuter pour voir les sorties du code, comme nous le verrons dans un instant). Dans la sous-section suivante, nous verrons une fa√ßon d‚Äôobtenir un flux de travail plus efficace.\nCommen√ßons par configurer notre exemple minimal. Le code et le markdown suivants forment le contenu d‚Äôun fichier appel√© rapport_tp.qmd :\n---\ntitle: \"Exemple de rapport\"\nauthor: \"Capitaine Haddock\"\nformat: pdf\ntoc: true\nnumber-sections: true\njupyter: python3\n---\n\n## Histogramme\n\nPour une d√©monstration d'un trac√© d'*histogramme* avec `matplotlib`, voir @fig-hist.\n\n```{python}\n#| label: fig-hist\n#| fig-cap: \"Un histogramme\"\nimport numpy as np\nimport matplotlib.pyplot as plt\nrng = np.random.RandomState(44)\n\nuniform_data = rng.uniform(-1, 1, 100)\n\nfig, ax = plt.subplots()\nax.hist(uniform_data, bins=30)\nplt.show()\n```\net le rendu donne alors:\n\n\n\n\n\n\n\n\nFigure¬†1: Un histogramme\n\n\n\n\n\nCet exemple contient trois types de contenu importants :\n\nUn en-t√™te YAML entour√© de ---.\nDes blocs de code Python entour√©s de ```.\nDu markdown m√©lang√© avec une mise en forme de texte simple comme # heading et _italics_.\n\nDans ce fichier markdown .qmd ‚Äúbrut‚Äù, la commande {python} indique √† Quarto qu‚Äôun bloc de code est en Python et doit √™tre ex√©cut√©, et jupyter: python3 indique √† Quarto quelle installation de Jupyter Notebooks utiliser. Si vous n‚Äô√™tes pas s√ªr du nom de votre installation de Jupyter, vous pouvez voir une liste en ex√©cutant jupyter kernelspec list sur la ligne de commande.\n\n\nRendu dans des documents de sortie\nPour convertir le rapport ci-dessus en un fichier PDF de sortie, enregistrez-le sous report.qmd et ex√©cutez ensuite la commande suivante sur la ligne de commande et dans le m√™me r√©pertoire que le fichier :\nquarto render report.qmd\nN‚Äôoubliez pas que si vous utilisez l‚Äôextension Visual Studio Code quarto (recommand√©e), vous pouvez appuyer sur le bouton ‚Äúrender‚Äù √† la place (mais vous devrez choisir PDF comme format de sortie), ou bien sur ‚Äúpreview‚Äù.\n\nExercice 1 Cr√©er un PDF en enregistrant le markdown ci-dessus dans un fichier appel√© rapport.qmd.\nAttention: si vous obtenez une erreur indiquant que le noyau Jupyter n‚Äôa pas √©t√© trouv√©, v√©rifiez d‚Äôabord que vous avez install√© Jupyter Lab, puis v√©rifiez le nom de votre noyau Jupyter en utilisant jupyter kernelspec list sur la ligne de commande. Vous devez sp√©cifier correctement le nom de votre noyau Jupyter dans l‚Äôen-t√™te du document (dans l‚Äôexemple ci-dessus, il est appel√© ‚Äòpython3‚Äô, qui est le noyau par d√©faut).\n\nMaintenant, puisque nous avons sp√©cifi√© pdf dans l‚Äôen-t√™te de notre fichier, nous avons automatiquement obtenu un PDF. Cependant, une grande vari√©t√© de formats de sortie sont disponibles. Par exemple, HTML :\nquarto render report.qmd --to html\nLa syntaxe de base consiste √† √©crire --to outputformat √† la fin de la commande render.\n\nExercice 2 R√©ussissez √† cr√©er un rapport HTML en enregistrant le markdown ci-dessus dans un fichier appel√© report.qmd, en modifiant l‚Äôent√™te et en ex√©cutant ensuite la commande quarto render avec l‚Äôoption --to html.\nQue se passe-t-il avec le menu sur le c√¥t√© droit lorsque vous ajoutez des en-t√™tes suppl√©mentaires en utilisant la syntaxe markdown ## ?\n\n\n\nOptions d‚Äôex√©cution de bloc de code\nIl existe diff√©rentes options pour l‚Äôex√©cution du bloc de code. Pour inclure un bloc de code qui ne sera pas ex√©cut√©, utilisez simplement la syntaxe markdown r√©guli√®re (c‚Äôest-√†-dire un bloc qui commence par ```python). Sinon, vous avez des options riches pour savoir si vous souhaitez afficher le code d‚Äôentr√©e, uniquement les r√©sultats, les deux ou aucun des deux (tout en ex√©cutant toujours le code).\nPour un exemple de sortie de code o√π l‚Äôentr√©e n‚Äôest pas affich√©e, le code ci-dessous n‚Äôaffichera que la figure de sortie en utilisant l‚Äôoption echo: false.\n```{python}\n#| echo: false\nimport numpy as np\nimport matplotlib.pyplot as plt\nrng = np.random.RandomState(44)\n\nuniform_data = rng.uniform(-1, 1, 100)\n\nfig, ax = plt.subplots()\nax.hist(uniform_data, bins=30)\nplt.show()\n```\nVoici quelques options pour les blocs de code:\n\n\n\n\n\n\n\nOption\nDescription\n\n\n\n\neval\n√âvaluer le bloc de code (si faux, affiche simplement le code dans la sortie).\n\n\necho\nInclure le code source dans la sortie.\n\n\noutput\nInclure les r√©sultats de l‚Äôex√©cution du code dans la sortie (vrai, faux ou asis pour indiquer que la sortie est du markdown brut et ne doit pas avoir de markdown d‚Äôencadrement standard de Quarto).\n\n\nwarning\nInclure les avertissements dans la sortie.\n\n\nerror\nInclure les erreurs dans la sortie (notez que cela implique que les erreurs d‚Äôex√©cution du code ne bloqueront pas le traitement du document).\n\n\ninclude\nOption g√©n√©rale pour emp√™cher toute sortie (code ou r√©sultats) d‚Äô√™tre incluse (par exemple, include: false supprime toute sortie du bloc de code).",
    "crumbs": [
      "TP",
      "D√©marrage en Quarto"
    ]
  },
  {
    "objectID": "TP/quarto.html#diapositives-automatis√©es-avec-quarto",
    "href": "TP/quarto.html#diapositives-automatis√©es-avec-quarto",
    "title": "D√©marrage en Quarto",
    "section": "Diapositives automatis√©es avec Quarto",
    "text": "Diapositives automatis√©es avec Quarto\nCe ne sont pas seulement des rapports que vous pouvez cr√©er, vous pouvez √©galement r√©aliser des pr√©sentations. Vous avez trois principaux formats de sortie √† choisir pour les diapositives :\n\nhtml, via quelque chose appel√© ‚Äòrevealjs‚Äô ; utilisez format: revealjs\npdf, via le package LaTeX beamer ; utilisez format: beamer\n\nTout le reste est identique √† ce que nous avons vu pr√©c√©demment. Voici un exemple minimal montrant √† la fois du code et du texte. Il cr√©e une pr√©sentation au format HTML.\n---\ntitle: \"Ma pr√©sentation\"\nauthor: \"Capitaine Haddock\"\nformat: revealjs\n---\n\n## Introduction\n\n- Voici du texte\n- Ainsi que ceci\n\n## Voici quelques sorties de code\n\n```{python}\n#| echo: false\nimport numpy as np\nimport matplotlib.pyplot as plt\nrng = np.random.RandomState(44)\n\nuniform_data = rng.uniform(-1, 1, 100)\n\nfig, ax = plt.subplots()\nax.hist(uniform_data, bins=30)\nplt.show()\n```\nNotez que cela n‚Äôaffichera pas le code, mais seulement la figure, car nous avons d√©fini #| echo: false pour le bloc de code. Vous pourriez √©galement d√©finir echo: false pour l‚Äôensemble de la pr√©sentation dans l‚Äôen-t√™te.\n\nExercice 3 (Cr√©ation de slides) Rendre cet exemple de diapositive dans les trois principaux formats",
    "crumbs": [
      "TP",
      "D√©marrage en Quarto"
    ]
  }
]