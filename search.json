[
  {
    "objectID": "Courses/simulation.html",
    "href": "Courses/simulation.html",
    "title": "Simulation",
    "section": "",
    "text": "Dans ce chapitre on se demande comment simuler en pratique des variables al√©atoires i.i.d. L‚Äôid√©e est de commencer par le cas de variables al√©atoires de loi uniforme et d‚Äôen d√©duire les autres lois."
  },
  {
    "objectID": "Courses/simulation.html#variables-al√©atoires-uniformes",
    "href": "Courses/simulation.html#variables-al√©atoires-uniformes",
    "title": "Simulation",
    "section": "Variables al√©atoires uniformes",
    "text": "Variables al√©atoires uniformes\nOn rappelle qu‚Äôune variable al√©atoire U suit une loi uniforme sur [0,1], not√© \\mathcal{U}([0,1]) si sa fonction de r√©partition F_U est donn√©e par \nF_U(x)\n=\n\\begin{cases}\n    0, & \\text{si }x &lt; 0\\,,        \\\\\n    x, & \\text{si }x \\in [0,1]\\,,  \\\\\n    1, & \\text{si }x &gt; 1\\,.        \\\\\n\\end{cases}\n\n\n\n\n                                                \n\n\nL‚Äôobjectif est de simuler sur machine une suite U_1, \\ldots, U_n de variables al√©atoires i.i.d. de loi \\mathcal{U}([0,1]). Plusieurs probl√®mes apparaissent alors :\n\nUne machine est d√©terministe.\nLes nombres entre 0 et 1 donn√©s par la machine sont de la forme k/2^p, pour k \\in \\{0, \\ldots, 2^{p-1}\\}. On ne pourra donc jamais g√©n√©rer des nombres qui ne sont pas de cette forme.\nV√©rifier qu‚Äôune suite est bien i.i.d. est un probl√®me difficile.\n\n\nD√©finition 1 (G√©n√©rateur de nombres pseudo-al√©atoires) \nUn g√©n√©rateur de nombres pseudo-al√©atoires (üá¨üáß: Pseudo Random Number Generator, PRNG), est un algorithme d√©terministe r√©cursif qui renvoie une suite U_1, \\ldots, U_n dans [0,1] qui a un ‚Äúcomportement similaire‚Äù √† une suite i.i.d. de loi \\mathcal{U}([0,1]). Pour √™tre plus rigoureux, ces nombres sont en fait des nombres entiers g√©n√©r√©s uniform√©ment sur un certain interval. Dans un second une transformation simple permet d‚Äôobtenir des nombres flottants (üá¨üáß: floats) entre 0 et 1.\n\n\n\n\n\n\n\nPour aller plus loin\n\n\n\nParfois il est utile d‚Äôaller chercher dans le code source certaines information pour savoir comment les fonctions sont cod√©es dans les packages que l‚Äôon utiliser. Par exemple, pour numpy que l‚Äôon utilise fr√©quement, on peut voir l‚Äôop√©ration choisie ici: Random: int -&gt; float en numpy.\n\n\nUn tel algorithme se construit de la mani√®re suivante :\n\nOn part d‚Äôune graine (üá¨üáß: seed) U_0 qui d√©termine la premi√®re valeur de mani√®re la plus arbitraire possible.\nLa proc√©dure r√©cursive s‚Äô√©crit U_{n+1} = f(U_n), o√π f est une transformation d√©terministe, de sorte que U_{n+1} est le plus ind√©pendant possible de U_1, \\dots¬∑, U_n.\n\n\nLa fonction f est d√©terministe et prend ses valeurs dans un ensemble fini, donc l‚Äôalgorithme est p√©riodique. Le but est donc d‚Äôavoir la plus grande p√©riode possible.\nNotons qu‚Äôune fois que la graine est fix√©e, alors l‚Äôalgorithme donne toujours les m√™mes valeurs. Fixer la graine peut donc √™tre tr√®s utile pour r√©p√©ter des simulations dans des conditions identiques et ainsi rep√©rer des erreurs.\n\n\n\n\n\n\n\nExercice: bug ou feature?\n\n\n\nReprendre les widgets du chapitre Th√©or√®mes asymptotiques et faites varier doucement le param√®tre p (de Bernoulli). Que constatez-vous? Proposer une explication potentielle.\n\n\n\nG√©n√©rateur congruentiel lin√©aire\nLa plupart des PRNG s‚Äôappuient sur des r√©sultats arithm√©tiques. Un des plus connus est celui appel√© G√©n√©rateur congruentiel lin√©aire (üá¨üáß Linear congruential generator, LCG). Il est d√©fini comme suit: on construit r√©cursivement une suite d‚Äôentiers X_i via la congruence \n  X_{n+1} = a X_n + b \\quad \\text{mod } m \\enspace,\n o√π a,b,m sont des entiers bien choisis pour que la suite obtenue ait de bonnes propri√©t√©s. Il suffit alors de consid√©rer X_n/m. Par exemple, la fonction rand sur scilab utilise cette congruence avec m=2^{31}, a=843\\; 314\\; 861, et b=453\\; 816\\; 693.\n\n\nG√©n√©rateurs alternatifs\nLes langages Python et R utilisent par d√©faut le g√©n√©rateur Mersenne-Twister qui s‚Äôappuie sur la multiplication vectorielle, mais d‚Äôautres g√©n√©rateurs sont aussi disponibles. Ce g√©n√©rateur a pour p√©riode m =2^{19937}-1, nombre qu‚Äôon peut raisonnablement consid√©rer comme grand.\nPour numpy la m√©thode par d√©faut est PCG64 (cf.¬†documentation de numpy), qui dispose de meilleures garanties statistiques (Voir le site https://www.pcg-random.org pour cela).\n\n\nUsage en numpy\nOn suppose d√©sormais disposer d‚Äôun g√©n√©rateur pseudo-al√©atoire sur [0,1]. En numpy depuis la version 1.17, une bonne mani√®re d‚Äôutiliser des √©l√©ments al√©atoires est d‚Äôutiliser un g√©n√©rateur que l‚Äôon d√©finit soi-m√™me:\n\nseed = 12345  # Toujours √™tre conscient qu'une graine existe\nrng = np.random.default_rng(seed)  #\nprint(rng.random())  ##  un tirage uniforme sur [0,1]\nprint(rng.random(size=5))  ## cinq tirages uniformes sur [0,1]\nprint(rng.random(size=(3, 2)))  ## matrice 3x2, √† entr√©es unif. sur [0,1]\n\n0.22733602246716966\n[0.31675834 0.79736546 0.67625467 0.39110955 0.33281393]\n[[0.59830875 0.18673419]\n [0.67275604 0.94180287]\n [0.24824571 0.94888115]]\n\n\nDans la suite on va voir comment g√©n√©rer d‚Äôautres lois √† partir de la loi uniforme, mais il est clair que les logiciels modernes propose un large √©ventail de distribution classique (gaussienne, exponentielle, etc.). Une liste exhaustive est donn√©e ici pour numpy.\n\n\n\n\n\n\nPour aller plus loin\n\n\n\nUne excellent discussion sur les bonnes pratiques al√©atoires en numpy, et l‚Äôusage de np.random.default_rng est donn√©e dans ce blog post d‚ÄôAlbert Thomas.\n\n\n\n\nPropri√©t√© de la loi uniforme\nOn verra souvent appara√Ætre la variable al√©atoire 1-U o√π U \\sim \\mathcal{U}([0,1]). Il se trouve que 1-U suit aussi une loi uniforme sur [0,1] comme le montre le calcul de sa fonction de r√©partition. Ainsi pour tout x \\in [0,1] on obtien \n\\begin{align*}\n\\mathbb{P}(1-U \\leq x) & = \\mathbb{P}(U \\geq 1-x),\\\\\n                       & = 1-(1-x), \\\\\n                       & = x\\,.\n\\end{align*}\n On peut d√©montrer facilement la m√™me relation pour x&lt;0 et x&gt;1, d‚Äôo√π le r√©sultat."
  },
  {
    "objectID": "Courses/simulation.html#m√©thode-dinversion",
    "href": "Courses/simulation.html#m√©thode-dinversion",
    "title": "Simulation",
    "section": "M√©thode d‚Äôinversion",
    "text": "M√©thode d‚Äôinversion\nL‚Äôid√©e de la m√©thode d‚Äôinversion repose sur le r√©sultat suivant : si F est une fonction de r√©partition bijective et U \\sim\\mathcal{U}([0,1]), alors la variable al√©atoire F^{-1}(U) a pour fonction de r√©partition F. C‚Äôest une cons√©quence de la suite d‚Äô√©galit√©s : \n\\begin{align}\n  \\mathbb{P}( F^{-1}(U) \\leq x ) & = \\mathbb{P}( U \\leq F(x) ) \\\\\n                                 & = F(x)\\,,\n\\end{align}\n\\tag{1}\no√π la deuxi√®me √©galit√© r√©sulte de la bijectivit√© de F. Ainsi, si F est facilement inversible, on peut simuler une variable al√©atoire X de loi F en simulant une variable al√©atoire uniforme U et en posant X = F^{-1}(U).\nMalheureusement, la fonction F n‚Äôest pas toujours inversible (penser aux lois discr√®tes) c‚Äôest donc pourquoi on utilise l‚Äôinverse l‚Äôinverse g√©n√©ralis√©e ou fonction quantile introduite en ?@def-quantile:\n\n  F^\\leftarrow(p)=  \\inf\\{ x \\in \\mathbb{R} \\colon F(x)\\geq p\\} \\enspace.\n\nInterpr√©tation: D√©finir l‚Äôinverse d‚Äôune fonction de r√©partition F revient √† r√©soudre l‚Äô√©quation F(x) = \\alpha d‚Äôinconnue x pour un \\alpha fix√©. Si F n‚Äôest pas bijective, deux probl√®mes apparaissent :\n\nl‚Äô√©quation n‚Äôa aucune solution ce qui revient √† dire que F n‚Äôest pas surjectif (graphiquement, F pr√©sente des sauts) ;\nl‚Äô√©quation a plusieurs solutions ce qui revient √† dire que F n‚Äôest pas injective (graphiquement cela se mat√©rialise par un plateau √† la hauteur \\alpha). Un exemple classique est celui o√π F est la fonction de r√©partition d‚Äôune variable al√©atoire discr√®te.\n\nLe passage √† l‚Äôin√©quation F(x) \\geq u permet de contourner la non-surjectivit√© : on ne regarde non plus les droites horizontales y=u mais la r√©gion \\{y \\geq \\alpha\\}. Le choix de l‚Äô\\inf dans la d√©finition de F^{\\leftarrow} permet de contourner la non-injectivit√© : vu qu‚Äôil y a possiblement plusieurs x tels que F(x) \\geq u, on choisit le ‚Äúpremier‚Äù. Ces consid√©rations sont illustr√©es en Figure XXX TODO.\n\n\n\n\n                                                \nFigure¬†1: ?(caption)\n\n\n\nRemarques additionnelles:\n\nLa fonction F √©tant croissante, la quantit√© F^\\leftarrow(u) correspond au premier instant o√π F d√©passe \\alpha. Si F est bijective (ce qui √©quivaut dans ce cas √† strictement croissante et injective), alors F^\\leftarrow = F^{-1}.\nLa fonction F^\\leftarrow n‚Äôest rien d‚Äôautre que la fonction quantile : si 0 &lt; \\alpha &lt; 1, q_{1-\\alpha} = F^\\leftarrow(1-\\alpha) est le quantile d‚Äôordre (1-\\alpha) de F. Par exemple, F^\\leftarrow(1/2) correspond √† la m√©diane.\nNotons que si u=0, on peut alors naturellement poser F^{\\leftarrow}(0) = -\\infty. De m√™me, avec la convention la convention \\inf \\emptyset = +\\infty, on peut alors √©tendre la d√©finition de F^\\leftarrow √† u=1 (mais F^\\leftarrow(1) n‚Äôest pas toujours √©gal √† \\infty, voir les exemples ci-dessous).\n\n\nD√©finition 2 (Loi √† support fini) \nSoit X une variable al√©atoire discr√®te prenant uniquement les valeurs x_1 &lt; \\dots &lt; x_r avec probabilit√© p_1, \\dots, p_r, et tel que p_1 + \\dots + p_r=1. On v√©rifie que pour tout u \\in ]0,1[, \n        F^\\leftarrow(u) =\n        \\begin{cases}\n            x_1 & \\text{si } 0 &lt; u \\leq p_1\\,,                  \\\\\n            x_2 & \\text{si } p_1 &lt; u \\leq p_1+p_2\\,,            \\\\\n                & \\vdots                                        \\\\\n            x_r & \\text{si }  p_1 + \\dots + p_{r-1} &lt; u &lt; 1\\,.\n        \\end{cases}\n\nSur cet exemple, on peut prolonger la d√©finition de F^\\leftarrow √† u=1 en posant F^\\leftarrow(1) = x_r. L‚Äôinverse g√©n√©ralis√©e se r√©√©crit alors sous la forme \n        F^\\leftarrow(u) = \\sum_{k=1}^r x_k {1\\hspace{-3.8pt} 1}_{p_1 + \\dots + p_{k-1} &lt; u \\leq p_1 + \\dots + p_k}\\enspace,\n o√π on a pos√© p_0=0. Cette expression s‚Äô√©tend directement au cas o√π X prend un nombre infini d√©nombrable de valeurs (la somme devient alors une s√©rie).\n\nXXX TODO: donner un example discret et montrer F^\\leftarrow.\nXXX TODO: show inverse usage using widget from first course and sampling randomly uniformly points in [0,1] to get the one from the distributions at hand.\n\nBibliographie et pour aller plus loin\n\nGenerating Random Floating-Point Numbers by Dividing Integers: a Case Study par Fr√©d√©ric Goualard\nGenerating Pseudo-random Floating-Point Values par Allen Downey."
  },
  {
    "objectID": "Courses/notations.html",
    "href": "Courses/notations.html",
    "title": "Notations et rappels",
    "section": "",
    "text": "On consid√®re un espace probabilis√© (\\Omega, {\\mathcal{F}}, \\mathbb{P}), compos√© d‚Äôun ensemble \\Omega, d‚Äôune tribu \\mathcal{F}, et d‚Äôune mesure de probabilit√© \\mathbb{P}.\nCette d√©finition permet de transposer l‚Äôal√©a qui provient de \\Omega dans l‚Äôespace E. L‚Äôhypoth√®se \\{X \\in B\\} \\in \\mathcal{F} assure que cet ensemble est bien un √©v√®nement et donc que l‚Äôon peut calculer sa probabilit√©.\nUne fois que l‚Äôal√©a a √©t√© transpos√© de \\Omega vers E, on souhaite √©galement transposer la probabilit√© \\mathbb{P} sur E. Ceci motive l‚Äôintroduction de la notion de loi.\nLes propri√©t√©s de \\mathbb{P} assurent que \\mathbb{P}_X est bien une loi de probabilit√© sur l‚Äôespace mesurable (E, \\mathcal{E})."
  },
  {
    "objectID": "Courses/notations.html#loi-discr√®tes",
    "href": "Courses/notations.html#loi-discr√®tes",
    "title": "Notations et rappels",
    "section": "Loi discr√®tes",
    "text": "Loi discr√®tes\nLes variables al√©atoires discr√®tes sont celles √† valeurs dans un ensemble E discret, le plus souvent \\mathbb{N}, muni de la tribu pleine \\mathcal{F} = \\mathcal{P}(E).\n\nExemple 1 (Loi de Bernoulli) La loi la plus simple est la loi de Bernoulli de param√®tre p \\in [0,1], d√©finie sur \\{0,1\\} par \\mathbb{P}(X=1) = 1-\\mathbb{P}(X=0) = p qui mod√©lise une exp√©rience al√©atoire √† deux issues (succ√®s = 1 et √©chec = 0).\n\n\nExemple 2 (Loi binomiale) En sommant des variables al√©atoires ind√©pendantes de loi de Bernoulli on obtient une loi binomiale : \\mathbb{P}(X=k) = \\binom{n}{k} p^k (1-p)^{n-k}, pour k \\in \\{0,\\ldots,n\\}, qui mod√©lise le nombre de succ√®s parmi n lancers.\n\n\nExemple 3 (Loi g√©om√©trique) En observant le nombre d‚Äôexp√©riences n√©cessaires avant d‚Äôobtenir un succ√®s, on obtient une loi g√©om√©trique : \\mathbb{P}(X=k) = p (1-p)^{k-1}, pour k \\geq 1.\n\n\nExemple 4 (Loi de Poisson) La loi de Poisson de param√®tre \\lambda &gt; 0 est d√©finie par \\mathbb{P}(X=k) = e^{-\\lambda} \\lambda^k / k!, pour k \\in \\mathbb{N}, et mod√©lise les √©v√©nements rares."
  },
  {
    "objectID": "Courses/notations.html#lois-continues",
    "href": "Courses/notations.html#lois-continues",
    "title": "Notations et rappels",
    "section": "Lois continues",
    "text": "Lois continues\nParmi les variables al√©atoires r√©elles non discr√®tes, beaucoup peuvent se repr√©senter avec une densit√©, c‚Äôest-√†-dire qu‚Äôil existe une fonction mesurable f : \\mathbb{R} \\to [0, \\infty[ d‚Äôint√©grale 1. La loi d‚Äôune telle variable al√©atoire X est alors donn√©e pour tout A \\in \\mathcal{B}(\\mathbb{R}) par \n    \\mathbb{P}(X \\in A) = \\int_A f(x) \\, \\mathrm d x \\enspace.\n Les propri√©t√©s de l‚Äôint√©grale de Lebesgue assure que cette formule d√©finit bien une loi de probabilit√©.\n\nExemple 5 (Loi uniforme) La loi uniforme sur un ensemble B \\in \\mathcal{B}(\\mathbb{R}), s‚Äôobtient avec la densit√© d√©finie par \nf(x) = {1\\hspace{-3.8pt} 1}_B(x) / \\lambda (B) \\enspace,\n o√π \\lambda (B) repr√©sente la mesure de Lebesgue de l‚Äôensemble B. En particulier pour la loi uniforme sur le segment [0,1] on obtient la fonction suivante: \nf(x) = {1\\hspace{-3.8pt} 1}_{[0,1]}(x)\\enspace.\n Si une variable al√©atoire U suit une telle loi on note U \\sim \\mathcal{U}([0,1]).\n\n\nExemple 6 (Loi exponentielle) La loi exponentielle de param√®tre \\gamma &gt; 0 est obtenue avec la densit√© donn√©e par \nf(x) = \\gamma e^{-\\gamma x} {1\\hspace{-3.8pt} 1}_{\\mathbb{R}_+}(x)\\enspace.\n Si une variable al√©atoire X suit cette loi on note X \\sim \\mathcal{Exp}(\\gamma).\n\n\nExemple 7 (Loi normale/gaussienne univari√©e) On obtient la loi normale de param√®tre \\mu \\in \\mathbb{R} et \\sigma^2 &gt; 0 correspond √† loi dont la densit√© est donn√©e par la fonction r√©elle: \nf(x) = \\frac{1}{\\sqrt{2 \\pi} \\sigma}e^{-\\frac{1}{2 \\sigma^2}(x-\\mu)^2} \\enspace.\n Si une variable al√©atoire X suit une telle loi on note X \\sim \\mathcal{N}(\\mu,\\sigma^2), \\mu correspondant √† l‚Äôesp√©rance de la loi, et \\sigma^2 √† sa variance. On nomme loi normale centr√©e r√©duite le cas correspondant √† \\mu = 0 et \\sigma^2 = 1.\n\n\nExemple 8 (Loi normale multivari√©e) On peut √©tendre les lois normales au cas multi-dimensionnel. Fixons d\\in\\mathbb{N}^* un entier non nul. Pour un vecteur \\mu \\in \\mathbb{R}^d et une matrice sym√©trique-d√©finie positive \\Sigma\\in \\mathbb{R^{d\\times d}}, la densit√© normale mutlivari√©e associ√©e est donn√©e par la fonction: \nf(x) = \\frac{1}{{(2 \\pi)}^{\\frac{d}{2}} {\\rm det}(\\Sigma)} e^{-\\frac{1}{2}(x-\\mu)^\\top \\Sigma ^{-1}(x-\\mu)}\n Notons que \\mu est l‚Äôesp√©rance de la loi et \\Sigma la matrice de variance-covariance."
  },
  {
    "objectID": "Courses/notations.html#fonction-de-r√©partition",
    "href": "Courses/notations.html#fonction-de-r√©partition",
    "title": "Notations et rappels",
    "section": "Fonction de r√©partition",
    "text": "Fonction de r√©partition\nLa notion de variable al√©atoire n‚Äôest pas facile √† manipuler puisqu‚Äôelle part d‚Äôun espace \\Omega dont on ne sait rien. On souhaite donc caract√©riser la loi d‚Äôune variable al√©atoire en ne consid√©rant que l‚Äôespace d‚Äôarriv√©e (E, \\mathcal{E}) .\nPlusieurs outils existent : la fonction de r√©partition (pour des variables al√©atoires r√©elles), la fonction caract√©ristique (pour des variables al√©atoires dans \\mathbb{R}^d), la fonction g√©n√©ratrice des moments (pour des variables al√©atoires discr√®tes), etc. On se contente ici de la fonction de r√©partition qui nous sera utile pour simuler des variables al√©atoires, ainsi que son inverse au sens de Levy.\n\nD√©finition 3 (Fonction de r√©partition üá¨üáß: cumulative distribution function) \nSoit X une variable al√©atoire sur (\\mathbb{R}, \\mathcal{B}(\\mathbb{R})). La fonction de r√©partition de X est la fonction F_X d√©finie sur \\mathbb{R} par \n    F_X(x) = \\mathbb{P}(X \\leq x) = \\mathbb{P}(X \\in ]-\\infty, x]) \\enspace.\n\n\nOn appelle quantile d‚Äôordre p\\in (0,1), la quantit√© F_X^\\leftarrow(p). La m√©diane est √©gale √† F_X^\\leftarrow(1/2), les premiers et troisi√®mes quartiles sont √©gaux √† F_X^\\leftarrow(1/4) et F_X^\\leftarrow(3/4). Enfin, les d√©ciles sont les quantiles F_X^\\leftarrow(k/10) pour k=1,\\dots, 9.\n\nExemple 9 (Cas discret) Soit (x_i)_{i \\in I} une suite ordonn√©e de r√©els, avec I \\subset \\mathbb{N}. Si X est une variable al√©atoire discr√®te prenant les valeurs (x_i)_{i \\in I} et de loi (p_i = \\mathbb{P}(X=x_i))_{i \\in I}, alors \nF_X(x) = \\sum_{i \\in I} p_i {1\\hspace{-3.8pt} 1}_{[x_i, \\infty[}(x) \\enspace.\n\n\n\nExemple 10 (Cas continu) Si X est une variable al√©atoire de densit√© f, alors \n    F_X(x) = \\int_{-\\infty}^x f(t) \\, \\mathrm dt \\enspace.\n\n\nLe graphe des fonctions de r√©partition des loi de Bernoulli, uniforme et normale sont repr√©sent√©es en Figure XXX. Notons que la fonction de r√©partition de la loi normale \\mathcal{N}(0,1), souvent not√©e \\Phi, n‚Äôadmet pas d‚Äôexpression explicite autre que \n\\Phi(x) = \\dfrac{1}{\\sqrt{2\\pi}} \\int_{-\\infty}^x e^{-\\frac{x^2}{2}}\\, \\mathrm d x\\enspace,\n Les valeurs num√©riques de \\Phi(x) √©taient autrefois report√©es dans des tables1. Par transformation affine, si X \\sim \\mathcal{N}(\\mu, \\sigma^2) ‚Äî ce que l‚Äôon peut aussi √©crire : X=\\mu + \\sigma Y, avec Y\\sim \\mathcal{N}(0,1) ‚Äî alors sa fonction de r√©partition est donn√©e par F_X(x)=\\Phi((x-\\mu)/\\sigma).1¬†Wikipedia: loi normale\n\nProposition 1 Soit X une variable al√©atoire de fonction de r√©partition F_X.\n\nF_X est une fonction croissante, de limite 0 en -\\infty et de limite 1 en +\\infty.\nF_X est continue √† droite en tout point.\nPour tout x \\in \\mathbb{R}, on a \\mathbb{P}(X=x) = F_X(x) - F_X(x-), o√π F_X(x-) = \\lim_{\\epsilon \\to 0+} F_X(x- \\epsilon).\nSi X a pour densit√© f, alors F_X est d√©rivable \\lambda-presque partout de d√©riv√©e f.\n\n\nLa propri√©t√© 3. est utile dans le cas discret : les valeurs prises par X correspondent aux points de discontinuit√© de F_X et les probabilit√©s associ√©es correspondent √† la hauteur du saut.\nLa propri√©t√© 4. donne le lien entre la fonction de r√©partition d‚Äôune variable al√©atoire √† densit√© et sa densit√©. On peut donc retrouver la loi de X √† partir de sa fonction de r√©partition. Le th√©or√®me suivant g√©n√©ralise ce r√©sultat √† toute variable al√©atoire r√©elle (pas n√©cessairement discr√®te ou √† densit√©).\n\nTh√©or√®me 1 La fonction de r√©partition d‚Äôune variable al√©atoire caract√©rise sa loi : deux variables al√©atoires ont m√™me loi si et seulement si elles ont m√™me fonction de r√©partition.\n\nXXX source + proof???\nOn rappelle que la tribu des bor√©liens est engendr√©e par la famille d‚Äôensembles \\{]-\\infty,x], x \\in \\mathbb{R}\\}. Le th√©or√®me pr√©c√©dent assure que si on conna√Æt la mesure \\mathbb{P}_X sur cette famille d‚Äôensemble alors on la conna√Æt partout.\nXXX TODO: move this in correct part.\n\nExemple 11 (Loi exponentielle depuis une loi uniforme) On consid√®re une variable al√©atoire U de loi uniforme sur [0,1] et on pose X = -\\ln(1-U). D√©terminons la loi de X en calculant sa fonction de r√©partition. Pour tout x \\in \\mathbb{R}, \n\\begin{align*}\nF_X(x) = & \\mathbb{P}(X \\leq x) \\\\\n       = & \\mathbb{P}(-\\ln(1-U) \\leq x) \\\\\n       = & \\mathbb{P}(U \\leq 1-e^{-x}) \\\\\n       = &\n    \\begin{cases}\n        0           & \\text{ si }x &lt; 0\\,,    \\\\\n        1 - e^{-x} & \\text{ si }x \\geq 0\\,,\n    \\end{cases}\n\\end{align*}\n\no√π on a utilis√© l‚Äô√©galit√© \\mathbb{P}(U \\leq t) = t pour tout t \\in [0,1]. Ainsi la variable al√©atoire X a la m√™me fonction de r√©partition qu‚Äôune loi exponentielle de param√®tre 1. On en conclut que X \\sim \\mathcal{Exp}(1). Notons que l‚Äôon peut aussi montrer que -\\ln(X)\\sim\\mathcal{E}(1), sachant que U et 1-U ont la m√™me loi."
  },
  {
    "objectID": "Courses/notations.html#fonction-quantile-inverse-g√©n√©ralis√©e-√†-gauche",
    "href": "Courses/notations.html#fonction-quantile-inverse-g√©n√©ralis√©e-√†-gauche",
    "title": "Notations et rappels",
    "section": "Fonction quantile, inverse g√©n√©ralis√©e √† gauche",
    "text": "Fonction quantile, inverse g√©n√©ralis√©e √† gauche\nLa fonction de r√©partition √©tant une fonction croissante on peut donner un sens √† son inverse g√©n√©ralis√©e de la mani√®re suivante.\n\nD√©finition 4 (Fonction quantile/ inverse g√©n√©ralis√©e üá¨üáß: quantile distribution function) \nSoit X une variable al√©atoire sur (\\mathbb{R}, \\mathcal{B}(\\mathbb{R})) et F_X sa fonction de r√©partion. La fonction quantile associ√©e F_X^\\leftarrow: ]0,1[\\rightarrow \\mathbb{R} est d√©finie par \n  F_X^\\leftarrow(p)=  \\inf\\{ x \\in \\mathbb{R} \\colon F_X(x)\\geq p\\} \\enspace.\n\n\nOn parle parfois aussi d‚Äôinverse au sens de Levy pour cette inverse g√©n√©ralis√©e.\nDans le cas o√π la fonction de r√©partition F est bijective, alors l‚Äôinverse de la fonction de r√©partition coincide avec la fonction quantile."
  },
  {
    "objectID": "Courses/notations.html#visualisation-densit√©-fonction-de-r√©partition-quantiles-etc.",
    "href": "Courses/notations.html#visualisation-densit√©-fonction-de-r√©partition-quantiles-etc.",
    "title": "Notations et rappels",
    "section": "Visualisation: densit√©, fonction de r√©partition, quantiles, etc.",
    "text": "Visualisation: densit√©, fonction de r√©partition, quantiles, etc.\n\nCas des variables continues\n#| standalone: true\n#| viewerHeight: 830\nimport numpy as np\nfrom scipy import stats\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nfrom shiny import ui, render, App\nfrom shinywidgets import output_widget, render_widget\n\n\ndef keep_no_param_distribution():\n    distributions = stats._continuous_distns._distn_names\n    distributions_0 = []\n    for _, name in enumerate(distributions):\n        dist = getattr(stats, name)\n        if not dist.shapes or len(dist.shapes) == 0:\n            distributions_0.append(name)\n    distributions_0_val = [\n        getattr(stats.distributions, string) for string in distributions_0\n    ]\n    distributions_0_dict = dict(zip(distributions_0, distributions_0_val))\n    return distributions_0_dict\n\n\ndistributions_0_dict = keep_no_param_distribution()\n\nmu = 0\nsigma = 1\n\napp_ui = ui.page_fluid(\n    ui.div(\n        ui.input_slider(\"alpha\", \"Quantile\", 0.01, 0.99, value=0.5, step=0.01),\n        ui.input_slider(\"xrange\", \"x-range\", -10, 10, value=(-5, 5), step=0.2),\n        ui.input_select(\n            \"distrib\",\n            \"Distribution\",\n            list(distributions_0_dict.keys()),\n            selected='norm'\n        ),\n        class_=\"d-flex gap-3\",\n    ),\n    output_widget(\"my_widget\"),\n)\n\n\ndef server(input, output, session):\n    @output\n    @render_widget\n    def my_widget():\n        fig = make_subplots(\n            rows=3,\n            cols=2,\n            vertical_spacing=0.1,\n            horizontal_spacing=0.15,\n            subplot_titles=(\n                \"Fonction quantile\",\n                \"\",\n                \"\",\n                \"Fonction de r√©partition\",\n                \"\",\n                \"Densit√© et quantile\",\n            ),\n            column_widths=[0.2, 0.5],\n            row_heights=[0.35, 0.17, 0.17],\n        )\n\n        alpha = input.alpha()\n        distribution = distributions_0_dict[input.distrib()]\n        x = np.linspace(input.xrange()[0], input.xrange()[1], num=400)\n        cdf_data = distribution.cdf(x, loc=mu, scale=sigma)\n        pdf_data = distribution.pdf(x, loc=mu, scale=sigma)\n        q_alpha = distribution.ppf(alpha, loc=mu, scale=sigma)\n\n        fig.update_layout(autosize=True, height=700)\n\n        # Quantile plot\n        fig.add_trace(\n            go.Scatter(\n                x=cdf_data, y=x, mode=\"lines\", marker={\"color\": \"black\"}\n            ),\n            row=1,\n            col=1,\n        )\n        # Diagonal\n        fig.add_trace(\n            go.Scatter(\n                x=cdf_data, y=cdf_data, mode=\"lines\", marker={\"color\": \"black\"}\n            ),\n            row=2,\n            col=1,\n        )\n        # Cdf part\n        fig.add_trace(\n            go.Scatter(\n                x=x, y=cdf_data, mode=\"lines\", marker={\"color\": \"black\"}\n            ),\n            row=2,\n            col=2,\n        )\n        # pdf part\n        fig.add_scatter(\n            x=x[x &lt; q_alpha],\n            y=pdf_data[x &lt; q_alpha],\n            fill=\"tozeroy\",\n            mode=\"none\",\n            fillcolor=\"rgb(66, 139, 202)\",\n            row=3,\n            col=2,\n        )\n\n        fig.add_trace(\n            go.Scatter(\n                x=x, y=pdf_data, mode=\"lines\", marker={\"color\": \"black\"}\n            ),\n            row=3,\n            col=2,\n        )\n\n        # Dots\n        fig.add_trace(\n            go.Scatter(\n                x=[alpha],\n                y=[q_alpha],\n                mode=\"markers\",\n                marker={\"color\": \"rgb(66, 139, 202)\"},\n                marker_symbol=\"x\",\n                marker_size=8,\n            ),\n            row=1,\n            col=1,\n        )\n\n        fig.add_trace(\n            go.Scatter(\n                x=[alpha],\n                y=[alpha],\n                mode=\"markers\",\n                marker={\"color\": \"rgb(66, 139, 202)\"},\n                marker_symbol=\"x\",\n                marker_size=8,\n            ),\n            row=2,\n            col=1,\n        )\n\n        fig.add_trace(\n            go.Scatter(\n                x=[q_alpha],\n                y=[alpha],\n                mode=\"markers\",\n                marker={\"color\": \"rgb(66, 139, 202)\"},\n                marker_symbol=\"x\",\n                marker_size=8,\n            ),\n            row=2,\n            col=2,\n        )\n\n        # Lines\n        fig.add_trace(\n            go.Scatter(\n                x=[alpha, alpha],\n                y=[x[0], q_alpha],\n                mode=\"lines\",\n                line=dict(dash=\"dash\", color=\"rgb(66, 139, 202)\")\n            ),\n            row=1,\n            col=1\n        )\n\n        fig.add_trace(\n            go.Scatter(\n                x=[alpha, alpha],\n                y=[alpha, 1.],\n                mode=\"lines\",\n                line=dict(dash=\"dash\", color=\"rgb(66, 139, 202)\")\n            ),\n            row=2,\n            col=1\n        )\n        fig.add_trace(\n            go.Scatter(\n                x=[alpha, 1],\n                y=[alpha, alpha],\n                mode=\"lines\",\n                line=dict(dash=\"dash\", color=\"rgb(66, 139, 202)\")\n            ),\n            row=2,\n            col=1\n        )\n        fig.add_trace(\n            go.Scatter(\n                x=[x[0], q_alpha],\n                y=[alpha, alpha],\n                mode=\"lines\",\n                line=dict(dash=\"dash\", color=\"rgb(66, 139, 202)\")\n            ),\n            row=2,\n            col=2\n        )\n        fig.add_trace(\n            go.Scatter(\n                x=[x[0], q_alpha],\n                y=[x[0], 0],\n                mode=\"lines\",\n                line=dict(dash=\"dash\", color=\"rgb(66, 139, 202)\")\n            ),\n            row=2,\n            col=2\n        )\n        # Axes ranges\n        fig.update_xaxes(range=[0, 1.], row=1, col=1)\n        fig.update_yaxes(matches=\"x6\", row=1, col=1)\n\n        fig.update_yaxes(range=[0, 1.], row=2, col=1)\n        fig.update_xaxes(matches=\"x1\", row=2, col=1)\n\n        fig.update_yaxes(rangemode=\"tozero\", row=3, col=2)\n        fig.update_xaxes(range=[x[0], x[-1]], row=3, col=2)\n\n        fig.update_xaxes(matches=\"x6\", row=2, col=2)\n        fig.update_yaxes(matches=\"y3\", row=2, col=2)\n\n        # Add dropdown\n        fig.update_layout(\n            showlegend=False,\n            template=\"simple_white\",\n        )\n        return fig\n\n\napp = App(app_ui, server)\n\n\n\nCas des variables discr√®tes\n#| standalone: true\n#| viewerHeight: 830\nimport numpy as np\nfrom scipy import stats\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nfrom shiny import ui, render, App\nfrom shinywidgets import output_widget, render_widget\n\n\ndef keep_no_param_distribution_disc():\n    distributions = stats._discrete_distns._distn_names\n    distributions_0 = [name for name in distributions if not getattr(stats, name).shapes or len(getattr(stats, name).shapes) in [1, 2]]\n    distributions_0_val = [getattr(stats.distributions, string) for string in distributions_0]\n    distributions_0_dict = dict(zip(distributions_0, distributions_0_val))\n    return distributions_0_dict\n\ndef cdf_tool(x, dtype='int64'):\n    y = np.zeros(2*(len(x)), dtype=dtype)\n    y[::2]=x\n    y[1::2]=x\n    return y[1::], y[:-1], y\n\ndef pmf_tool(x, dtype='int64'):\n    y = np.zeros(2*(len(x)), dtype=dtype)\n    y[::2]=x\n    return y[1::], y[:-1], y\n\ndef insert_nones(my_list):\n    for i, val in enumerate(my_list):\n        if i % 3 == 2:\n            my_list.insert(i, None)\n    return my_list\n\ndistributions_0_dict = keep_no_param_distribution_disc()\n\napp_ui = ui.page_fluid(\n    ui.div(\n        ui.input_slider(\"alpha\", \"Quantile\", 0.01, 0.99, value=0.5, step=0.01),\n        ui.input_slider(\"xrange\", \"x-range\", -10, 10, value=(-5.5, 5.5), step=0.2),\n        ui.input_select(\n            \"distrib\",\n            \"Distribution\",\n            list(distributions_0_dict.keys()),\n            selected='poisson'\n        ),\n        class_=\"d-flex gap-3\",\n    ),\n    output_widget(\"my_widget\"),\n)\n\n\ndef server(input, output, session):\n    @output\n    @render_widget\n    def my_widget():\n        fig = make_subplots(\n            rows=3,\n            cols=2,\n            vertical_spacing=0.1,\n            horizontal_spacing=0.15,\n            subplot_titles=(\n                \"Fonction quantile\",\n                \"\",\n                \"\",\n                \"Fonction de r√©partition\",\n                \"\",\n                \"Fonction de masse et quantile\",\n            ),\n            column_widths=[0.2, 0.5],\n            row_heights=[0.35, 0.17, 0.17],\n        )\n\n\n        alpha = input.alpha()\n        # alpha=0.5\n\n        mu = 0.5  # Param needed for some distribution\n        if input.distrib()=='zipf':\n            mu = 2\n        distribution = distributions_0_dict[input.distrib()]\n        # distribution=distributions_0_dict['poisson']\n        x = np.arange(np.floor(input.xrange()[0]), np.ceil(input.xrange()[1]))\n        # x = np.arange(np.floor(-5.5), np.ceil(5.5))\n\n        cdf_data = distribution.cdf(x, mu)\n        pmf_data = distribution.pmf(x, mu)\n        q_alpha = distribution.ppf(alpha, mu)\n        support = pmf_data.nonzero()[0]\n        fig.update_layout(autosize=True, height=700)\n\n        # Quantile plot\n        new_x, new_y, new_z = cdf_tool(support)\n        _, _, new_pmf = pmf_tool(support)\n\n        fig.add_trace(\n            go.Scatter(\n                x=insert_nones(list(np.append(cdf_data[new_y[::-1]], distribution.cdf(x[0], mu)))),\n                y=insert_nones(list(np.append(x[new_x[::-1]], x[new_x[0]]))),\n                mode=\"lines\",\n                line=dict(color=\"black\")\n            ),\n            row=1,\n            col=1,\n        )\n        fig.add_trace(\n             go.Scatter(\n                x=cdf_data[support], y=x[support],\n                mode=\"markers\", marker={\"color\": \"black\"}\n            ),\n            row=1,\n            col=1,\n        )\n        # Diagonal\n        fig.add_trace(\n            go.Scatter(\n                x=cdf_data, y=cdf_data, mode=\"lines\", marker={\"color\": \"black\"}\n            ),\n            row=2,\n            col=1,\n        )\n        # Cdf part\n        fig.add_trace(\n            go.Scatter(\n                x=x[support], y=cdf_data[support],\n                mode=\"markers\", marker={\"color\": \"black\"}\n            ),\n            row=2,\n            col=2,\n        )\n        fig.add_trace(\n            go.Scatter(\n                x=insert_nones(list(np.append(np.insert(x[new_x], 0, [x[0], x[new_x[0]]]),x[-1]))),\n                y=insert_nones(list(np.append(np.insert(cdf_data[new_y], 0, [0,0]), cdf_data[-1]))),\n                mode=\"lines\",\n                line=dict(color=\"black\")\n            ),\n            row=2,\n            col=2\n        )\n\n        fig.add_trace(\n            go.Scatter(\n                x=x, y=pmf_data, mode=\"markers\", marker={\"color\": \"black\"}\n            ),\n            row=3,\n            col=2,\n        )\n        fig.add_trace(\n            go.Scatter(\n                x=x, y=pmf_data, mode=\"markers\", marker={\"color\": \"black\"}\n            ),\n            row=3,\n            col=2,\n        )\n        x_bar = insert_nones(list(x[new_z]))\n        y_bar = insert_nones(list(pmf_data[new_pmf]))\n        fig.add_trace(\n            go.Scatter(\n                x=x_bar,\n                y=y_bar,\n                mode=\"lines\",\n                line=dict(color=\"black\")\n            ),\n            row=3,\n            col=2\n        )\n        _,_, devil_x = cdf_tool(x[x&lt;=q_alpha])\n        _,_, devil_y = cdf_tool(pmf_data[x&lt;q_alpha], dtype='float64')\n\n        x_bar_blue = insert_nones(list(devil_x))\n        y_bar_blue = np.array(insert_nones(list(devil_y)))\n        y_bar_blue[::-3]=0.\n        y_bar_blue = list(y_bar_blue)\n        fig.add_trace(\n            go.Scatter(\n                x=x_bar_blue,\n                y=y_bar_blue,\n                mode=\"lines\",\n                line=dict(color=\"rgb(66, 139, 202)\")\n            ),\n            row=3,\n            col=2\n        )\n        # pdf part\n        fig.add_scatter(\n            x=x[x &lt;= q_alpha],\n            y=pmf_data[x &lt;= q_alpha],\n            mode=\"markers\",\n            marker={\"color\":\"rgb(66, 139, 202)\"},\n            row=3,\n            col=2,\n        )\n\n        # Dots\n        fig.add_trace(\n            go.Scatter(\n                x=[alpha],\n                y=[q_alpha],\n                mode=\"markers\",\n                marker={\"color\": \"rgb(66, 139, 202)\"},\n                marker_symbol=\"x\",\n                marker_size=8,\n            ),\n            row=1,\n            col=1,\n        )\n\n        fig.add_trace(\n            go.Scatter(\n                x=[alpha],\n                y=[alpha],\n                mode=\"markers\",\n                marker={\"color\": \"rgb(66, 139, 202)\"},\n                marker_symbol=\"x\",\n                marker_size=8,\n            ),\n            row=2,\n            col=1,\n        )\n\n        fig.add_trace(\n            go.Scatter(\n                x=[q_alpha],\n                y=[alpha],\n                mode=\"markers\",\n                marker={\"color\": \"rgb(66, 139, 202)\"},\n                marker_symbol=\"x\",\n                marker_size=8,\n            ),\n            row=2,\n            col=2,\n        )\n\n        # Lines\n        fig.add_trace(\n            go.Scatter(\n                x=[alpha, alpha],\n                y=[x[0], q_alpha],\n                mode=\"lines\",\n                line=dict(dash=\"dash\", color=\"rgb(66, 139, 202)\")\n            ),\n            row=1,\n            col=1\n        )\n\n        fig.add_trace(\n            go.Scatter(\n                x=[alpha, alpha],\n                y=[alpha, 1.],\n                mode=\"lines\",\n                line=dict(dash=\"dash\", color=\"rgb(66, 139, 202)\")\n            ),\n            row=2,\n            col=1\n        )\n        fig.add_trace(\n            go.Scatter(\n                x=[alpha, 1],\n                y=[alpha, alpha],\n                mode=\"lines\",\n                line=dict(dash=\"dash\", color=\"rgb(66, 139, 202)\")\n            ),\n            row=2,\n            col=1\n        )\n        fig.add_trace(\n            go.Scatter(\n                x=[x[0], q_alpha],\n                y=[alpha, alpha],\n                mode=\"lines\",\n                line=dict(dash=\"dash\", color=\"rgb(66, 139, 202)\")\n            ),\n            row=2,\n            col=2\n        )\n        fig.add_trace(\n            go.Scatter(\n                x=[x[0], q_alpha],\n                y=[x[0], 0],\n                mode=\"lines\",\n                line=dict(dash=\"dash\", color=\"rgb(66, 139, 202)\")\n            ),\n            row=2,\n            col=2\n        )\n        # Axes ranges\n        fig.update_xaxes(range=[0, 1.05], row=1, col=1)\n        fig.update_yaxes(matches=\"x6\", row=1, col=1)\n\n        fig.update_yaxes(range=[0, 1.05], row=2, col=1)\n        fig.update_xaxes(matches=\"x1\", row=2, col=1)\n\n        fig.update_yaxes(rangemode=\"tozero\", row=3, col=2)\n        fig.update_xaxes(range=[x[0], x[-1]], row=3, col=2)\n\n        fig.update_xaxes(matches=\"x6\", row=2, col=2)\n        fig.update_yaxes(matches=\"y3\", row=2, col=2)\n\n        # Add dropdown\n        fig.update_layout(\n            showlegend=False,\n            template=\"simple_white\",\n        )\n        return fig\n\n\napp = App(app_ui, server)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction",
    "section": "",
    "text": "Aspects num√©riques de la mod√©lisation al√©atoire et statistiques (cours de Licence 3).\nAttention: site en construction‚Ä¶\n\n\n\nJoseph Salmon: joseph.salmon@umontpellier.fr,\nBenjamin Charlier: benjamin.charlier@umontpellier.fr\n\nCe cours est issu du travail ant√©rieur de la part de:\n\nNicolas Meyer\nBeno√Æte de Saporta\n\n\n\n\nCours de mesure et int√©gration, analyse num√©rique‚Ä¶\n\n\n\n\n\n\n\n\n\n\n\nCCI\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(Data Science): Python Data Science Handbook, With Application to Understanding Data by J. Van DerPlas, 2016; videos: Reproducible Data Analysis in Jupyter"
  },
  {
    "objectID": "index.html#professeurs",
    "href": "index.html#professeurs",
    "title": "Introduction",
    "section": "",
    "text": "Joseph Salmon: joseph.salmon@umontpellier.fr,\nBenjamin Charlier: benjamin.charlier@umontpellier.fr\n\nCe cours est issu du travail ant√©rieur de la part de:\n\nNicolas Meyer\nBeno√Æte de Saporta"
  },
  {
    "objectID": "index.html#pr√©requis",
    "href": "index.html#pr√©requis",
    "title": "Introduction",
    "section": "",
    "text": "Cours de mesure et int√©gration, analyse num√©rique‚Ä¶"
  },
  {
    "objectID": "index.html#modalit√©-de-contr√¥le-des-connaissances",
    "href": "index.html#modalit√©-de-contr√¥le-des-connaissances",
    "title": "Introduction",
    "section": "",
    "text": "CCI"
  },
  {
    "objectID": "index.html#livres-et-ressources",
    "href": "index.html#livres-et-ressources",
    "title": "Introduction",
    "section": "",
    "text": "(Data Science): Python Data Science Handbook, With Application to Understanding Data by J. Van DerPlas, 2016; videos: Reproducible Data Analysis in Jupyter"
  },
  {
    "objectID": "Courses/th_asymptotique.html",
    "href": "Courses/th_asymptotique.html",
    "title": "Th√©or√®mes asymptotiques",
    "section": "",
    "text": "Le premier r√©sultat fondamental en probabilit√©s concerne le comportement asymptotique de la moyenne empirique: \n\\bar X_n = \\dfrac{X_1 + \\cdots + X_n}{n} \\enspace.\n quand on obsever n variables al√©atoires i.i.d X_1,\\dots,X_n, ayant une esp√©rance finie.\n\nTh√©or√®me 1 (Loi forte des grands nombres) \nSoit (X_n)_{n \\geq 1} une suite de variables al√©atoires ind√©pendantes et identiquement distribu√©es (i.i.d.) dans L^1(\\Omega, \\mathcal{F}, \\mathbb{P}). Notons \\mu = \\mathbb{E}[X_1]. Alors \\bar X_n converge vers \\mu presque s√ªrement : \n\\mathbb{P}\\bigg( \\dfrac{X_1 + \\cdots + X_n}{n} \\underset{n \\to \\infty}{\\longrightarrow} \\mu \\bigg) = 1\\,.\n\n\nInterpr√©tation: Intuitivement, la probabilit√© d‚Äôun √©v√©nement A correspond √† la fr√©quence d‚Äôapparition de A quand on r√©p√®te une exp√©rience qui fait intervenir cet √©v√©nement. Par exemple, si on dispose une pi√®ce truqu√©e, on estimera la probabilit√© d‚Äôapparition du c√¥t√© pile en lan√ßant la pi√®ce un grand nombre de fois et en comptant le nombre de pile obtenu. La loi des grands nombres justifie a posteriori cette intuition : si X_1, \\ldots, X_n sont i.i.d. de loi de Bernoulli de param√®tre p, alors \n    \\dfrac{X_1 + \\cdots + X_n}{n} \\xrightarrow[n \\to \\infty]{p.s.} p \\enspace.\n Le membre de gauche correspond au nombre empirique de pile obtenu, celui de droite √† la valeur th√©orique.\nRemarque: Bien qu‚Äôassez intuitif, ce th√©or√®me est difficile √† d√©montrer. XXX TODO reference : Williams (en anglais) ou bien Ouvrard.\n#| standalone: true\n#| viewerHeight: 550\nimport numpy as np\nfrom shiny import ui, render, App, reactive\nfrom shinywidgets import output_widget, register_widget\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objs as go\n\n\nn_init = 42\n\n\napp_ui = ui.page_fluid(\n    ui.tags.head(\n        ui.tags.style(\"\"\"\n            .bslib-sidebar-layout &gt; .sidebar &gt; .sidebar-content {\n            display: flex;\n            flex-direction: column;\n            padding: 0.5rem;\n            }\n            .irs--shiny .irs-line {\n            top: 27px;\n            height: 4px;\n            }\n            .irs--shiny .irs-bar {\n            top: 27px;\n            height: 4px;\n            }\n            .irs--shiny .irs-handle {\n            top: 23px;\n            width: 22px;\n            height: 10px;\n        \"\"\")\n    ),\n    ui.panel_title(\"Loi des grands nombres\"),\n    ui.layout_sidebar(\n        ui.panel_sidebar(\n            ui.input_action_button(\"seed\", \"Nouveau tirage\", class_=\"btn-primary\"),\n            ui.input_slider( \"p\", \"Esp√©rance: p\", 0.01, 0.99, value=0.5, step=0.01, ticks=False,\n            ),\n            ui.input_slider( \"n_samples\", \"√âchantillons: n\", 2, 1000, value=15, step=1, ticks=False),\n        width=3,),\n    ui.panel_main(output_widget(\"my_widget\"), width = 9)\n    )\n)\n\n\n\n\ndef server(input, output, session):\n    seed = reactive.Value(42)\n\n    @reactive.Effect\n    @reactive.event(input.seed)\n    def _():\n        seed.set(np.random.randint(0, 1000))\n\n    subplots = make_subplots(\n        rows=2,\n        cols=1,\n        vertical_spacing=0.5,\n        horizontal_spacing=0.04,\n        row_heights=[8, 2],\n        subplot_titles=(\n            f\"Moyenne empirique &lt;br&gt;(loi de Bernoulli)\",\n            \"Tirages al√©atoires &lt;span style='color:rgb(66, 139, 202)'&gt;bleu: 0&lt;/span&gt;, &lt;span style='color:rgb(255, 0, 0)'&gt;rouge: 1&lt;/span&gt; (seed=\"\n            + f\"{n_init:03}\"\n            + \")\",\n        ),\n    )\n    fig = go.FigureWidget(subplots)\n    fig.add_trace(\n        go.Scatter(\n            mode=\"lines\",\n            line=dict(color=\"black\", width=3),\n            x=[],\n            y=[],\n            name=r\"Moyenne &lt;br&gt; empirique\",\n        ),\n        row=1,\n        col=1,\n    )\n    fig.add_trace(\n        go.Scatter(\n            mode=\"lines\",\n            line=dict(dash=\"dash\", color=\"black\", width=1),\n            marker={},\n            x=[],\n            y=[],\n            name=r\"p\",\n        ),\n        row=1,\n        col=1,\n    )\n    fig.add_trace(\n        go.Heatmap(\n            x=[],\n            z=[],\n            colorscale=[[0, \"rgb(66, 139, 202)\"], [1, \"rgb(255,0,0)\"]],\n            showscale=False,\n        ),\n        row=2,\n        col=1,\n    )\n\n    fig.update_yaxes(range=[0, 1.1], row=1, col=1)\n    fig.update_xaxes(matches=\"x1\", row=2, col=1)\n    fig.update_yaxes(visible=False, row=2, col=1)\n    fig.update_xaxes(visible=False, row=2, col=1)\n\n    fig.update_layout(\n        template=\"simple_white\",\n        showlegend=True,\n        xaxis_title=\"√âchantillons: n\",\n    )\n    fig.update_layout(autosize=True)\n\n    fig.update_layout(\n        legend=dict(\n            yanchor=\"top\",\n            y=0.99,\n            xanchor=\"left\",\n            x=0.95,\n            bgcolor=\"rgba(0,0,0,0)\",\n        )\n    )\n    fig.update_layout(\n        margin=dict(l=0, r=0, b=10, t=70),\n    )\n\n    register_widget(\"my_widget\", fig)\n\n    @reactive.Effect\n    def _():\n        p = input.p()\n        n_samples = input.n_samples()\n\n        rng = np.random.default_rng(seed())\n        iterations = np.arange(1, n_samples + 1)\n        samples = rng.binomial(1, p, size=n_samples)\n        means_samples = np.cumsum(samples) / np.arange(1, n_samples + 1)\n\n        # Update data in fig:\n        fig.data[0].x = iterations\n        fig.data[0].y = means_samples\n\n        fig.data[1].x = iterations\n        fig.data[1].y = np.full((n_samples), p)\n\n        fig.data[2].x = iterations\n        fig.data[2].z = [samples]\n\n        fig.update_xaxes(range=[1, n_samples + 1])\n\n        # Update the subplot titles:\n        fig.layout.annotations[1].update(\n            text=f\"Tirages al√©atoires &lt;span style='color:rgb(66, 139, 202)'&gt;bleu: 0&lt;/span&gt;, &lt;span style='color:rgb(255, 0, 0)'&gt;rouge: 1&lt;/span&gt; (seed=\"\n            + f\"{seed():03}\"\n            + \")\"\n        )\n\n\napp = App(app_ui, server)\n\nXXX TODO: ph√©nom√®ne int√©ressant en bougeant le param√®tre p avec le reste fix√©‚Ä¶les signaux g√©n√©r√©s sont tr√®s tr√®s proches, ce qui ne devrait pas √™tre le cas a priori, saut structuration particuli√®re de la g√©n√©ration."
  },
  {
    "objectID": "Courses/th_asymptotique.html#loi-des-grands-nombres",
    "href": "Courses/th_asymptotique.html#loi-des-grands-nombres",
    "title": "Th√©or√®mes asymptotiques",
    "section": "",
    "text": "Le premier r√©sultat fondamental en probabilit√©s concerne le comportement asymptotique de la moyenne empirique: \n\\bar X_n = \\dfrac{X_1 + \\cdots + X_n}{n} \\enspace.\n quand on obsever n variables al√©atoires i.i.d X_1,\\dots,X_n, ayant une esp√©rance finie.\n\nTh√©or√®me 1 (Loi forte des grands nombres) \nSoit (X_n)_{n \\geq 1} une suite de variables al√©atoires ind√©pendantes et identiquement distribu√©es (i.i.d.) dans L^1(\\Omega, \\mathcal{F}, \\mathbb{P}). Notons \\mu = \\mathbb{E}[X_1]. Alors \\bar X_n converge vers \\mu presque s√ªrement : \n\\mathbb{P}\\bigg( \\dfrac{X_1 + \\cdots + X_n}{n} \\underset{n \\to \\infty}{\\longrightarrow} \\mu \\bigg) = 1\\,.\n\n\nInterpr√©tation: Intuitivement, la probabilit√© d‚Äôun √©v√©nement A correspond √† la fr√©quence d‚Äôapparition de A quand on r√©p√®te une exp√©rience qui fait intervenir cet √©v√©nement. Par exemple, si on dispose une pi√®ce truqu√©e, on estimera la probabilit√© d‚Äôapparition du c√¥t√© pile en lan√ßant la pi√®ce un grand nombre de fois et en comptant le nombre de pile obtenu. La loi des grands nombres justifie a posteriori cette intuition : si X_1, \\ldots, X_n sont i.i.d. de loi de Bernoulli de param√®tre p, alors \n    \\dfrac{X_1 + \\cdots + X_n}{n} \\xrightarrow[n \\to \\infty]{p.s.} p \\enspace.\n Le membre de gauche correspond au nombre empirique de pile obtenu, celui de droite √† la valeur th√©orique.\nRemarque: Bien qu‚Äôassez intuitif, ce th√©or√®me est difficile √† d√©montrer. XXX TODO reference : Williams (en anglais) ou bien Ouvrard.\n#| standalone: true\n#| viewerHeight: 550\nimport numpy as np\nfrom shiny import ui, render, App, reactive\nfrom shinywidgets import output_widget, register_widget\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objs as go\n\n\nn_init = 42\n\n\napp_ui = ui.page_fluid(\n    ui.tags.head(\n        ui.tags.style(\"\"\"\n            .bslib-sidebar-layout &gt; .sidebar &gt; .sidebar-content {\n            display: flex;\n            flex-direction: column;\n            padding: 0.5rem;\n            }\n            .irs--shiny .irs-line {\n            top: 27px;\n            height: 4px;\n            }\n            .irs--shiny .irs-bar {\n            top: 27px;\n            height: 4px;\n            }\n            .irs--shiny .irs-handle {\n            top: 23px;\n            width: 22px;\n            height: 10px;\n        \"\"\")\n    ),\n    ui.panel_title(\"Loi des grands nombres\"),\n    ui.layout_sidebar(\n        ui.panel_sidebar(\n            ui.input_action_button(\"seed\", \"Nouveau tirage\", class_=\"btn-primary\"),\n            ui.input_slider( \"p\", \"Esp√©rance: p\", 0.01, 0.99, value=0.5, step=0.01, ticks=False,\n            ),\n            ui.input_slider( \"n_samples\", \"√âchantillons: n\", 2, 1000, value=15, step=1, ticks=False),\n        width=3,),\n    ui.panel_main(output_widget(\"my_widget\"), width = 9)\n    )\n)\n\n\n\n\ndef server(input, output, session):\n    seed = reactive.Value(42)\n\n    @reactive.Effect\n    @reactive.event(input.seed)\n    def _():\n        seed.set(np.random.randint(0, 1000))\n\n    subplots = make_subplots(\n        rows=2,\n        cols=1,\n        vertical_spacing=0.5,\n        horizontal_spacing=0.04,\n        row_heights=[8, 2],\n        subplot_titles=(\n            f\"Moyenne empirique &lt;br&gt;(loi de Bernoulli)\",\n            \"Tirages al√©atoires &lt;span style='color:rgb(66, 139, 202)'&gt;bleu: 0&lt;/span&gt;, &lt;span style='color:rgb(255, 0, 0)'&gt;rouge: 1&lt;/span&gt; (seed=\"\n            + f\"{n_init:03}\"\n            + \")\",\n        ),\n    )\n    fig = go.FigureWidget(subplots)\n    fig.add_trace(\n        go.Scatter(\n            mode=\"lines\",\n            line=dict(color=\"black\", width=3),\n            x=[],\n            y=[],\n            name=r\"Moyenne &lt;br&gt; empirique\",\n        ),\n        row=1,\n        col=1,\n    )\n    fig.add_trace(\n        go.Scatter(\n            mode=\"lines\",\n            line=dict(dash=\"dash\", color=\"black\", width=1),\n            marker={},\n            x=[],\n            y=[],\n            name=r\"p\",\n        ),\n        row=1,\n        col=1,\n    )\n    fig.add_trace(\n        go.Heatmap(\n            x=[],\n            z=[],\n            colorscale=[[0, \"rgb(66, 139, 202)\"], [1, \"rgb(255,0,0)\"]],\n            showscale=False,\n        ),\n        row=2,\n        col=1,\n    )\n\n    fig.update_yaxes(range=[0, 1.1], row=1, col=1)\n    fig.update_xaxes(matches=\"x1\", row=2, col=1)\n    fig.update_yaxes(visible=False, row=2, col=1)\n    fig.update_xaxes(visible=False, row=2, col=1)\n\n    fig.update_layout(\n        template=\"simple_white\",\n        showlegend=True,\n        xaxis_title=\"√âchantillons: n\",\n    )\n    fig.update_layout(autosize=True)\n\n    fig.update_layout(\n        legend=dict(\n            yanchor=\"top\",\n            y=0.99,\n            xanchor=\"left\",\n            x=0.95,\n            bgcolor=\"rgba(0,0,0,0)\",\n        )\n    )\n    fig.update_layout(\n        margin=dict(l=0, r=0, b=10, t=70),\n    )\n\n    register_widget(\"my_widget\", fig)\n\n    @reactive.Effect\n    def _():\n        p = input.p()\n        n_samples = input.n_samples()\n\n        rng = np.random.default_rng(seed())\n        iterations = np.arange(1, n_samples + 1)\n        samples = rng.binomial(1, p, size=n_samples)\n        means_samples = np.cumsum(samples) / np.arange(1, n_samples + 1)\n\n        # Update data in fig:\n        fig.data[0].x = iterations\n        fig.data[0].y = means_samples\n\n        fig.data[1].x = iterations\n        fig.data[1].y = np.full((n_samples), p)\n\n        fig.data[2].x = iterations\n        fig.data[2].z = [samples]\n\n        fig.update_xaxes(range=[1, n_samples + 1])\n\n        # Update the subplot titles:\n        fig.layout.annotations[1].update(\n            text=f\"Tirages al√©atoires &lt;span style='color:rgb(66, 139, 202)'&gt;bleu: 0&lt;/span&gt;, &lt;span style='color:rgb(255, 0, 0)'&gt;rouge: 1&lt;/span&gt; (seed=\"\n            + f\"{seed():03}\"\n            + \")\"\n        )\n\n\napp = App(app_ui, server)\n\nXXX TODO: ph√©nom√®ne int√©ressant en bougeant le param√®tre p avec le reste fix√©‚Ä¶les signaux g√©n√©r√©s sont tr√®s tr√®s proches, ce qui ne devrait pas √™tre le cas a priori, saut structuration particuli√®re de la g√©n√©ration."
  },
  {
    "objectID": "Courses/th_asymptotique.html#th√©or√®me-central-limite",
    "href": "Courses/th_asymptotique.html#th√©or√®me-central-limite",
    "title": "Th√©or√®mes asymptotiques",
    "section": "Th√©or√®me central limite",
    "text": "Th√©or√®me central limite\nUne fois la loi des grands nombres √©tablie, on peut se demander quel est l‚Äôordre suivant dans le d√©veloppement asymptotique de \\bar X_n - \\mu, ou de mani√®re √©quivalente de S_n - n \\mu, o√π S_n = X_1 + \\cdots + X_n. Le th√©or√®me suivant r√©pond √† cette question, en donnant une convergence en loi d‚Äôune transformation affine de la moyenne empirique:\n\nTh√©or√®me 2 (Th√©or√®me central limite) Soit X_1, \\ldots, X_n une suite de variables al√©atoires i.i.d de variance \\sigma^2 = {\\rm var}(X_1) \\in ]0, \\infty[. On note \\mu = \\mathbb{E}[X_1] leur esp√©rance. Alors \n\\sqrt n \\left(\\tfrac{\\bar X_n - \\mu}{\\sigma} \\right) \\xrightarrow[n \\to +\\infty]{\\mathcal{L}} N\\enspace,\n o√π N suit une loi normale centr√©e r√©duite : N \\sim\\mathcal{N}(0,1).\n\nPreuve XXX TODO: donner une r√©f√©rence.\nOn peut interpr√©ter ce th√©or√®me grossi√®rement de la fa√ßon suivante: la moyenne empirique de variables al√©atoires i.i.d de variance \\sigma^2 se comporte asymptotiquement comme une loi normale \\mathcal{N}(\\mu, \\tfrac{\\sigma^2}{n}), ce que l‚Äôon √©crit avec un abus de notation:\n\n\\bar X_n \\approx \\mathcal{N}(\\mu, \\frac{\\sigma^2}{n}) \\enspace.\n\nEn termes de somme cumul√©e empirique, la convergence se r√©√©crit\n\n    \\tfrac{S_n - n \\mu}{\\sqrt n \\sigma} \\xrightarrow[n \\to +\\infty]{\\mathcal{L}} N \\enspace.\n\nLes hypoth√®ses de ce th√©or√®me sont plut√¥t faibles (il suffit de supposer une variance finie). Pourtant, le r√©sultat est universel : la loi de d√©part peut √™tre aussi farfelue que l‚Äôon veut, elle se rapprochera toujours asymptotiquement d‚Äôune loi normale.\nOn rappelle que la convergence en loi est √©quivalente √† la convergence des fonctions de r√©partition en tout point de continuit√© de la limite. Ainsi, le th√©or√®me central limite se r√©√©crit de la mani√®re suivante : pour tout a &lt; b, notons \\alpha_n=\\mathbb{P} \\left(\\bar X_n \\notin [ \\mu + \\tfrac{a \\sigma}{\\sqrt{n}},  \\mu + \\tfrac{ b \\sigma}{\\sqrt{n}}] \\right). Ainsi \n\\begin{align}\n    1-\\alpha_n& = \\mathbb{P} \\left(\\bar X_n \\in [ \\mu + \\tfrac{a \\sigma}{\\sqrt{n}}, \\mu + \\tfrac{ b \\sigma}{\\sqrt{n}}] \\right)\\nonumber\\\\\n    & =\n    \\mathbb{P} \\left(\\tfrac{\\bar X_n - \\mu}{\\sigma} \\in [ \\tfrac{a}{\\sqrt{n}},\\tfrac{b}{\\sqrt{n}}]\\right) \\nonumber\\\\\n    & =\n    \\mathbb{P} \\bigg( a \\leq \\sqrt n \\left(\\tfrac{\\bar X_n - \\mu}{\\sigma} \\right) \\leq b\\bigg) \\nonumber\\\\\n    & \\underset{n \\to \\infty}{\\longrightarrow}  \\int_a^b \\varphi(x) \\,  dx\\,. \\nonumber\\\\\n\\end{align}\no√π l‚Äôon note \\varphi (resp. \\Phi) la densit√© (resp. la fonction de r√©partition) d‚Äôune loi normale centr√©e r√©duite, d√©finie pour tout x\\in\\mathbb{R} par \\varphi(x)=\\tfrac{e^{-\\frac{x^2}{2}}}{\\sqrt{2\\pi}} (resp. \\Phi(x)= \\int_{-\\infty}^{x}\\varphi(u) du).\nDans le cas classique d‚Äôun intervalle de confiance √† 95%, c‚Äôest-√†-dire quand \\alpha_n=0.05, et en prenant un intervalle de confiance sym√©trique (alors a=-t et b=q) on obtient 1-\\alpha_n= \\int_{-q}^q \\varphi(x) \\, dx=\\Phi(q)-\\Phi(-q)=2 \\Phi(q)-1 \\implies \\boxed{q=\\Phi^{-1}(1-\\tfrac{\\alpha_n}{2})} et q est donc le quantile de niveau 1-\\tfrac{\\alpha_n}{2} de la loi normale centr√©e r√©duite. Num√©riquement on peut facilement √©valuer q et v√©rifier que q\\approx 1.96 avec scipy:\n\nfrom scipy.stats import norm\nq = norm.ppf(1-0.05/2)\nprint(f\"Gaussienne centr√©e r√©duite,\\nQuantile de niveau (1-Œ±/2):\\nq = {q:.2f}\")\n\nGaussienne centr√©e r√©duite,\nQuantile de niveau (1-Œ±/2):\nq = 1.96\n\n\n\nExemple 1 On consid√®re des variables al√©atoires X_1, \\ldots, X_n i.i.d. suivant une loi de Bernoulli de param√®tre p \\in ]0,1[, dont l‚Äôesp√©rance et la variance sont respectivemenbt p et p(1-p). Le th√©or√®me central limite donne alors \n    \\sqrt n \\left(\\frac{\\bar X_n - p}{p (1-p)} \\right) \\xrightarrow[n \\to +\\infty]{\\mathcal{L}} N\\,,\n avec N \\sim \\mathcal{N}(0,1). Cette convergence est illustr√©e en Figure XXX TODO image.\n\n#| standalone: true\n#| viewerHeight: 600\nimport numpy as np\nfrom scipy.stats import norm\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nfrom shiny import ui, render, App, reactive\nfrom shinywidgets import output_widget, render_widget\n\n\napp_ui = ui.page_fluid(\n    ui.tags.head(\n        ui.tags.style(\"\"\"\n            .bslib-sidebar-layout &gt; .sidebar &gt; .sidebar-content {\n            display: flex;\n            flex-direction: column;\n            padding: 0.5rem;\n            }\n            .irs--shiny .irs-line {\n            top: 27px;\n            height: 4px;\n            }\n            .irs--shiny .irs-bar {\n            top: 27px;\n            height: 4px;\n            }\n            .irs--shiny .irs-handle {\n            top: 23px;\n            width: 22px;\n            height: 10px;\n        \"\"\")\n    ),\n    ui.panel_title(\"TCL\"),\n    ui.layout_sidebar(\n        ui.panel_sidebar(\n            ui.input_action_button(\"seed\", \"Nouveau tirage\",class_=\"btn-primary\"),\n            ui.input_slider(\"p\", \"Esp√©rance: p\", 0.01, 0.99, value=0.5, step=0.01, ticks=False),\n            ui.input_slider(\"n_repetitions\", \"R√©p√©titions: t\", 1, 300, value=200, step=1, ticks=False),\n            ui.input_slider(\"n_samples\", \"√âchantillons: n\", 1, 200, value=100, step=1, ticks=False), width=3\n        ),\n    ui.panel_main(output_widget(\"my_widget\"), width = 9)\n    )\n)\n\n\n\ndef server(input, output, session):\n    seed = reactive.Value(42)\n\n\n    @reactive.Effect\n    @reactive.event(input.seed)\n    def _():\n        seed.set(np.random.randint(0, 1000))\n\n\n    @output\n    @render_widget\n    def my_widget():\n\n        rng = np.random.default_rng(seed())\n        p = input.p()\n        n_repetitions = input.n_repetitions()\n        n_samples = input.n_samples()\n        iterations = np.arange(1, n_samples + 1)\n        samples = rng.binomial(1, p, size=(n_repetitions, n_samples))\n        means_samples = np.cumsum(samples, axis=1) / np.arange(1, n_samples + 1)\n        x_hist = np.linspace(0, 1, num=300)\n\n        # Create figure\n        fig = make_subplots(\n                    rows=1,\n                    cols=3,\n                    # vertical_spacing=0.5,\n                    horizontal_spacing=0.02,\n                    column_widths=[9, 1, 3],\n                    subplot_titles=(\"Moyenne empirique:&lt;br&gt; n = \" + str(n_samples), \"        \", \"Histogramme: &lt;br&gt; t = \" + str(n_repetitions) + \" r√©p√©titions\")\n                )\n\n\n        for i in range(n_repetitions):\n            fig.add_trace(\n                    go.Scatter(\n                        mode='lines',\n                        line=dict(color=\"rgba(0,0,0,0.05)\", width=1),\n                        x=iterations,\n                        y=means_samples[i,:],\n                        ),\n                        row=1, col=1,\n            )\n        fig.add_trace(\n                go.Scatter(\n                    mode='lines',\n                    line=dict(dash=\"dash\", color=\"black\", width=1),\n                    marker={},\n                    x=iterations,\n                    y=np.full((n_samples), p),\n                    name='p'),\n                    row=1, col=1,\n        )\n        fig.update_layout(\n            template=\"simple_white\",\n            showlegend=False,\n        )\n        fig.add_trace(\n                go.Scatter(\n                    mode='markers',\n                    marker=dict(color=\"rgba(0,0,0,0.05)\", size=4),\n                    x=np.zeros(n_samples),\n                    y=means_samples[:,-1],\n                ),\n                row=1, col=2,\n\n        )\n        y_hist, bins = np.histogram(means_samples[:,-1], bins=int(np.sqrt(n_repetitions)), density=True)\n        fig.add_trace(\n            go.Bar(x=y_hist, y=bins[:-1] + np.diff(bins)/2,\n                    opacity=0.75,\n                    marker_color = 'black',\n                    orientation='h',\n                    width=np.diff(bins),\n                    name=\"Tirages de moyennes empiriques\",\n                    ),\n                row=1, col=3,\n        )\n        fig.add_trace(\n            go.Scatter(x=norm.pdf(x_hist, p, np.sqrt(p*(1-p) / n_samples)),\n                       y=x_hist,\n                       mode='lines',\n                       line=dict(color=\"red\"),\n                       legendgroup='1',\n                       name=\"TCL\"\n                       ),\n                row=1, col=3,\n        )\n\n        fig.update_xaxes(range=[1, n_samples + 1])\n        fig.update_yaxes(range=[-.05, 1.05], row=1, col=1)\n        fig.update_yaxes(matches=\"y1\",visible = False,  row=1, col=2)\n        fig.update_xaxes(range=[-0.2, 0.2], visible = False, row=1, col=2)\n\n        fig.update_yaxes(matches=\"y1\", row=1, col=3, visible=False)\n        fig.update_xaxes(range=[0, 1.1 / np.sqrt(2*np.pi* p*(1-p) / n_samples)], row=1, col=3)\n        fig.update_xaxes(visible=False, row=1, col=3)\n\n\n\n        for trace in fig['data']:\n            print(trace)\n            if (trace['name'] != 'TCL') and (trace['name'] != 'p'):\n                trace['showlegend'] = False\n        fig.update_layout(\n        margin=dict(l=0, r=0, b=10, t=100),\n        )\n        fig.update_layout(\n            title=dict(text=\"Moyennes empiriques et TCL &lt;br&gt; (loi de Bernoulli)&lt;br&gt;\", yanchor=\"top\", y=0.95),\n            title_x=0.5,\n            showlegend=True,\n\n        )\n        fig['layout']['xaxis']['title']='√âchantillons: n'\n\n        fig.update_layout(autosize=True)\n\n        return fig\n\n\napp = App(app_ui, server)\n\n#| standalone: true\n#| viewerHeight: 550\nimport numpy as np\nfrom shiny import ui, render, App, reactive\nfrom shinywidgets import output_widget, register_widget\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objs as go\nfrom scipy import signal\n\n\napp_ui = ui.page_fluid(\n    ui.tags.head(\n        ui.tags.style(\"\"\"\n            .bslib-sidebar-layout &gt; .sidebar &gt; .sidebar-content {\n            display: flex;\n            flex-direction: column;\n            padding: 0.5rem;\n            }\n            .irs--shiny .irs-line {\n            top: 27px;\n            height: 2px;\n            }\n            .irs--shiny .irs-bar {\n            top: 27px;\n            height: 2px;\n            }\n            .irs--shiny .irs-handle {\n            top: 23px;\n            width: 22px;\n            height: 10px;\n        \"\"\")\n    ),\n    ui.panel_title(\"TCL et convolutions\"),\n    ui.layout_sidebar(\n        ui.panel_sidebar(\n            ui.input_select(\n                \"loi\",\n                \"Loi sous-jacente\",\n                {'uniforme': 'Uniforme', 'laplace' : 'Laplace'},\n            ),\n            ui.input_slider(\n                \"n_iter\",\n                \"√âchantillons: n\",\n                1,\n                10,\n                value=1,\n                step=1,\n                ticks=False,\n            ), width = 3\n    ),\n    ui.panel_main(\n        output_widget(\"my_widget\"), width = 9\n    )\n)\n)\n\n\nnnzeros = 10001\nx_min = -20\nx_max = 20\n\nx = np.linspace(x_min, x_max, nnzeros)\ny = np.zeros(nnzeros)\nmask = np.where(np.abs(x) &lt;= 0.5, 1, 0)\ny[mask == 1] = 1\ndelta = (x_max - x_min) / nnzeros\nvar = np.sum(y * x**2 *(delta)) - (np.sum(y * x * delta))**2\n\ndef convolve(signal_in, n_convolutions, delta):\n    output = np.zeros(len(signal_in))\n    if n_convolutions == 0:\n        return output\n    elif n_convolutions == 1:\n        return signal_in\n    else:\n        output = signal_in.copy()\n        for i in range(n_convolutions - 1):\n            output = signal.fftconvolve(\n                output * delta, signal_in, mode=\"same\"\n            )\n        return output\n\n\ndef server(input, output, session):\n\n    fig = go.FigureWidget()\n    fig.add_trace(\n        go.Scatter(\n            mode=\"lines\",\n            line=dict(color=\"black\", width=3),\n            x=[],\n            y=[],\n            name=r\"Moyenne empirique\",\n        )\n    )\n\n    fig.add_trace(\n        go.Scatter(\n            x=x,\n            y=np.exp(-(x**2) / (2 * var)) / np.sqrt(2 * var * np.pi),\n            mode=\"lines\",\n            line=dict(dash=\"dash\", color=\"red\", width=2),\n            name=f\"Loi normale (variance ad√©quate)\",\n        )\n    )\n    fig.update_xaxes(range=[-3, 3], position=0.)\n    fig.update_yaxes(range=[0, 2 * np.max(y)], position=0.5, showticklabels=False)\n\n\n    fig.update_layout(\n        template=\"simple_white\",\n        showlegend=True,\n        autosize=True,\n        title=dict(text=\"Densit√© : &lt;br&gt; moyenne de n variables al√©atoires\", yanchor=\"top\", y=0.95),\n        title_x=0.5,\n    )\n    fig.update_layout(legend=dict(\n        yanchor=\"top\",\n        y=0.99,\n        xanchor=\"left\",\n        x=0.01\n    ))\n    fig.update_layout(\n    margin=dict(l=0, r=0, b=10, t=100),\n    )\n\n    register_widget(\"my_widget\", fig)\n\n    @reactive.Effect\n    def _():\n        if str(input.loi()) == 'uniforme':\n            y = np.zeros(nnzeros)\n            mask = np.where(np.abs(x) &lt;= 0.5, 1, 0)\n            y[mask == 1] = 1\n\n        else:\n            # else str(input.loi()) == 'laplace':\n            y=np.exp(-np.abs(x))/2\n            # y = np.zeros(nnzeros)\n            # mask = np.where(np.abs(x) &gt;= 0.5, 1, 0)\n            # y[mask == 1] = 0\n            # y = np.cos(np.pi * x) + 1\n            y = y / (np.sum(y) * delta)\n\n        var = np.sum(y * x**2 * delta) - (np.sum(y * x * delta))**2\n\n        y_display = convolve(y, input.n_iter(), delta)\n        # Update data in fig:\n        fig.data[0].x = x / np.sqrt(input.n_iter())\n        fig.data[0].y = y_display * np.sqrt(input.n_iter())\n        fig.data[1].y = np.exp(-(x**2) / (2 * var)) / np.sqrt(2 * var * np.pi)\n        fig.update_yaxes(range=[0, 2 * np.max(y)], position=0.5, showticklabels=False)\n\n\napp = App(app_ui, server)\n\nPour aller plus loins sur les convolutions, voir la vid√©o de 3Blue1Brown √† ce sujet: Convolutions | Why X+Y in probability is a beautiful mess"
  }
]