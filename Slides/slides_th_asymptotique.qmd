---
title: "HAX603X: Modélisation stochastique"
subtitle: "Théorèmes asymptotiques"
format:
  revealjs:
    html-math-method: mathjax
    include-before: [ '<script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {enableAssistiveMml: false}});</script>']
    toc: true
    template-partials:
        - toc-slide.html
    include-after-body: toc-add.html
filters:
  - shinylive
---

# Loi des grands nombres

## Loi forte des grands nombres

- Résultat fondamental: concerne le comportement asymptotique de la moyenne empirique:
$$
\bar X_n = \dfrac{X_1 + \cdots + X_n}{n} \enspace.
$$
quand on observe $n$ variables aléatoires i.i.d $X_1,\dots,X_n$, ayant une espérance finie.

:::: {#thm-lfgn}

## Loi forte des grands nombres

<br>

Soit $(X_n)_{n \geq 1}$ une suite de variables aléatoires indépendantes et identiquement distribuées (i.i.d.) dans $L^1(\Omega, \mathcal{F}, \mathbb{P})$. Notons $\mu = \mathbb{E}[X_1]$. Alors $\bar X_n$ converge vers $\mu$ presque sûrement :
$$
\mathbb{P}\bigg( \dfrac{X_1 + \cdots + X_n}{n} \underset{n \to \infty}{\longrightarrow} \mu \bigg) = 1\,.
$$
::::


## Interprétation

Intuitivement, la probabilité d'un événement $A$ correspond à la fréquence d'apparition de $A$ quand on répète une expérience qui fait intervenir cet événement.


::: {#exm-bernoulli}

## Cas Bernouilli: pile ou face

La probabilité d'apparition du côté pile (noté $p$) peut-être estimée en lançant la pièce un grand nombre de fois et en comptant le nombre de pile obtenu.

La loi des grands nombres justifie cette intuition : si $X_1, \ldots, X_n$ sont i.i.d. de loi de Bernoulli de paramètre $p$, alors
$$
	\dfrac{X_1 + \cdots + X_n}{n} \xrightarrow[n \to \infty]{p.s.} p =\mathbb{E}(X_1) \enspace.
$$

- Membre de gauche :  la fréquence empirique de piles
- Membre de droite :  la fréquence théorique de piles
:::

## Visualisation de l'exemple du pile ou face

```{shinylive-python}
#| echo: false
#| standalone: true
#| viewerHeight: 550
import numpy as np
from shiny import ui, render, App, reactive
from shinywidgets import output_widget, register_widget
from plotly.subplots import make_subplots
import plotly.graph_objs as go


n_init = 42


app_ui = ui.page_fluid(
    ui.tags.head(
        ui.tags.style("""
            .bslib-sidebar-layout > .sidebar > .sidebar-content {
            display: flex;
            flex-direction: column;
            padding: 0.5rem;
            }
            .irs--shiny .irs-line {
            top: 27px;
            height: 4px;
            }
            .irs.irs--shiny .irs-bar {
            top: 27px;
            height: 4px;
            }
            .irs--shiny .irs-handle {
            top: 23px;
            width: 22px;
            height: 10px;
        """)
    ),
    ui.panel_title("Loi des grands nombres"),
    ui.layout_sidebar(
        ui.panel_sidebar(
            ui.input_action_button("seed", "Nouveau tirage", class_="btn-primary"),
            ui.input_slider( "p", "Espérance: p", 0.01, 0.99, value=0.35, step=0.01, ticks=False,
            ),
            ui.input_slider( "n_samples", "Échantillons: n", 2, 1000, value=15, step=1, ticks=False),
        width=3),
    ui.panel_main(output_widget("my_widget"), width = 9)
    )
)




def server(input, output, session):
    seed = reactive.Value(42)

    @reactive.Effect
    @reactive.event(input.seed)
    def _():
        seed.set(np.random.randint(0, 1000))

    subplots = make_subplots(
        rows=2,
        cols=1,
        vertical_spacing=0.45,
        horizontal_spacing=0.04,
        row_heights=[5, 1],
        subplot_titles=(
            f"Moyenne empirique: loi de Bernoulli",
            "Tirages aléatoires <span style='color:rgb(66, 139, 202)'>bleu: 0</span>, <span style='color:rgb(255, 0, 0)'>rouge: 1</span> (seed="
            + f"{n_init:03}"
            + ")",
        ),
    )
    fig = go.FigureWidget(subplots)
    fig.add_trace(
        go.Scatter(
            mode="lines",
            line=dict(color="black", width=3),
            x=[],
            y=[],
            name=r"Moyenne <br> empirique",
        ),
        row=1,
        col=1,
    )
    fig.add_trace(
        go.Scatter(
            mode="lines",
            line=dict(dash="dash", color="black", width=1),
            marker={},
            x=[],
            y=[],
            name=r"p",
        ),
        row=1,
        col=1,
    )
    fig.add_trace(
        go.Heatmap(
            x=[],
            z=[],
            colorscale=[[0, "rgb(66, 139, 202)"], [1, "rgb(255,0,0)"]],
            showscale=False,
        ),
        row=2,
        col=1,
    )

    fig.update_yaxes(range=[0, 1.1], row=1, col=1)
    fig.update_xaxes(matches="x1", row=2, col=1)
    fig.update_yaxes(visible=False, row=2, col=1)
    fig.update_xaxes(visible=False, row=2, col=1)

    fig.update_layout(
        template="simple_white",
        showlegend=True,
        xaxis_title="Échantillons: n",
    )
    fig.update_layout(autosize=True)

    fig.update_layout(
        legend=dict(
            yanchor="top",
            y=0.99,
            xanchor="left",
            x=0.65,
            bgcolor="rgba(0,0,0,0)",
        )
    )
    fig.update_layout(
        margin=dict(l=0, r=0, b=10, t=70),
    )

    register_widget("my_widget", fig)

    @reactive.Effect
    def _():
        p = input.p()
        n_samples = input.n_samples()

        rng = np.random.default_rng(seed())
        iterations = np.arange(1, n_samples + 1)
        samples = rng.binomial(1, p, size=n_samples)
        means_samples = np.cumsum(samples) / np.arange(1, n_samples + 1)

        # Update data in fig:
        fig.data[0].x = iterations
        fig.data[0].y = means_samples

        fig.data[1].x = iterations
        fig.data[1].y = np.full((n_samples), p)

        fig.data[2].x = iterations
        fig.data[2].z = [samples]

        fig.update_xaxes(range=[1, n_samples + 1])

        # Update the subplot titles:
        fig.layout.annotations[1].update(
            text=f"Tirages aléatoires (seed="
            + f"{seed():03}"
            + ") <br> <span style='color:rgb(66, 139, 202)'>bleu: 0</span>, <span style='color:rgb(255, 0, 0)'>rouge: 1</span> "
        )


app = App(app_ui, server)

```

[Pour aller plus loin]{.underline}:

Quant $p$ varie, à $n$ fixé...les signaux générés sont très très proches, ce qui ne devrait pas être le cas sans structuration particulière de la génération. L'aléa est imparfait!


# Théorème central limite


## Au delà de la loi des grands nombres

- 1er ordre d'approximation de la convergence de $\bar{X}_n$: loi des grands nombres
- 2ème ordre d'approximation: théorème central limite

[Enjeu]{.underline}: quantifier les variations de $\bar X_n - \mu$

[Réponse].underline: théorème central limite (TCL), avec la convergence en loi d'une transformation affine de la moyenne empirique

## Théorème central limite

:::: {#thm-tcl}

## Théorème central limite
Soit $X_1, \ldots, X_n$ une suite de variables aléatoires i.i.d de variance $\sigma^2 = {\rm var}(X_1) \in ]0, \infty[$. On note $\mu = \mathbb{E}[X_1]$ leur espérance. Alors
$$
\sqrt n \left(\tfrac{\bar X_n - \mu}{\sigma} \right) \xrightarrow[n \to +\infty]{\mathcal{L}} N\enspace,
$$
où $N$ suit une loi normale centrée réduite : $N \sim\mathcal{N}(0,1)$.
::::

[Interprétation]{.underline}:
la moyenne empirique de v.a. i.i.d de variance $\sigma^2$
se comporte asymptotiquement comme une loi normale $\mathcal{N}(\mu, \tfrac{\sigma^2}{n})$: $\quad \bar X_n \approx \mathcal{N}(\mu, \frac{\sigma^2}{n})$.


:::{.callout-note}

Hypothèses du théorème plutôt faibles: variance finie uniquement

:::

## Formulation de la convergence

Convergence en loi $\iff$ convergence des fonctions de répartition (aux pts de continuité de la cible)

Ré-écriture du TCL : pour tout $a < b$, notons $\alpha_n=\mathbb{P} \left(\bar X_n \notin [ \mu + \tfrac{a \sigma}{\sqrt{n}}, \mu + \tfrac{ b \sigma}{\sqrt{n}}] \right)$.

$$
\mathbb{P} \left(\tfrac{\bar X_n - \mu}{\sigma}\in [ \tfrac{a}{\sqrt{n}},\tfrac{b}{\sqrt{n}}] \right)\\
\begin{align}
    1-\alpha_n& = \mathbb{P} \left(\bar X_n \in [ \mu + \tfrac{a \sigma}{\sqrt{n}}, \mu + \tfrac{ b \sigma}{\sqrt{n}}] \right)\nonumber\\
    & =
    \mathbb{P} \left(\tfrac{\bar X_n - \mu}{\sigma} \in [ \tfrac{a}{\sqrt{n}},\tfrac{b}{\sqrt{n}}]\right) \nonumber\\
    & =
	\mathbb{P} \bigg( a \leq \sqrt n \left(\tfrac{\bar X_n - \mu}{\sigma} \right) \leq b\bigg) \nonumber\\
    & \underset{n \to \infty}{\longrightarrow}  \int_a^b \varphi(x) \,  dx\,. \nonumber\\
\end{align}
$$
où l'on note $\varphi$ (resp. $\Phi$) la densité (resp. la fonction de répartition) d'une loi normale centrée réduite, définie pour tout $x\in\mathbb{R}$ par $\varphi(x)=\tfrac{e^{-\frac{x^2}{2}}}{\sqrt{2\pi}}$ (resp. $\Phi(x)= \int_{-\infty}^{x}\varphi(u) du$).

Dans le cas classique d'un intervalle de confiance à 95\%, c'est-à-dire quand $\alpha_n=0.05$, et en prenant un intervalle de confiance symétrique (alors $a=-t$ et $b=q$) on obtient $1-\alpha_n= \int_{-q}^q \varphi(x) \,  dx=\Phi(q)-\Phi(-q)=2 \Phi(q)-1 \implies \boxed{q=\Phi^{-1}(1-\tfrac{\alpha_n}{2})}$ et $q$ est donc le quantile de niveau $1-\tfrac{\alpha_n}{2}$ de la loi normale centrée réduite. Numériquement on peut facilement évaluer $q$ et vérifier que $q\approx 1.96$ avec `scipy`:


## Bibliographie

::: {#refs}
:::