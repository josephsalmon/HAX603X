---
title: "Simulation"
subtitle: "HAX603X: Modélisation stochastique"
format:
  revealjs:
    toc: true
    template-partials:
        - toc-slide.html
    include-after-body: toc-add.html
filters:
  - shinylive
jupyter: python3
---


<!-- <script type="text/x-mathjax-config">
MathJax.Hub.Config({
  extensions: ["tex2jax.js"],
  jax: ["input/TeX", "output/HTML-CSS"],
  tex2jax: {
    inlineMath: [ ['$','$'] ],
    displayMath: [ ['$$','$$']],
    processEscapes: true
  },
  "HTML-CSS": {
    availableFonts: ["Source Sans Pro", "Helvetica", "sans-serif"],
    preferredFont: "Source Sans Pro",
    webFont: "Source Sans Pro"
  }
});
</script> -->

## Enjeu

<br>
<br>


[[Question]{.underline}: Comment simuler en pratique des variables aléatoires i.i.d?]{.r-fit-text}


<br>
<br>

. . .

[[Approche]{.underline}: Commencer par les v.a. uniformes et en déduire les autres lois]{.r-fit-text}
<br>
<br>


##
```{=html}
<div id='toc'></div>
```

# Variables aléatoires uniformes

## Rappel sur les variables uniformes

[Rappel]{.underline} : $U$ suit une loi uniforme sur $[0,1]$: $U\sim\mathcal{U}([0,1])$ ssi sa fonction de répartition $F_U$ vaut
$$
F_U(x)
=
\begin{cases}
    0, & \text{si }x < 0\,,        \\
    x, & \text{si }x \in [0,1]\,,  \\
    1, & \text{si }x > 1\,.
\end{cases}
$$

::: {.r-stack}

```{python}
#| echo: false
#| layout-ncol: 1

import numpy as np
import scipy
import plotly.graph_objects as go


x = np.linspace(-3, 4, num=300)

fig = go.Figure(layout=go.Layout(autosize=True, width=1000, height=550))

fig.add_trace(
    go.Scatter(
        mode="lines",
        line=dict(color="black", width=3),
        x=x,
        y=scipy.stats.uniform.cdf(x),
        name="F_U",
    )
)


fig.update_xaxes(tickfont_size=18)
fig.update_yaxes(tickfont_size=18)
fig.update_layout(
    template="simple_white",
    showlegend=True,
)
fig.update_layout(
    legend=dict(
        bgcolor="#ffffff",
        font=dict(
            size=18,
            color="black"
        )
    )
)
```
:::

## Challenges

<br>

[Objectif]{.underline}: simuler sur machine une suite $U_1, \dots, U_n$  de v.a., i.i.d., de loi $\mathcal{U}([0,1])$.

<br>
[Difficultés]{.underline}:

:::{.incremental}

- Une machine est déterministe.
- Les nombres flottants entre $0$ et $1$ donnés par la machine sont de la forme $k/2^p$, pour $k \in \{0, \ldots, 2^{p-1}\} \implies$ impossibilité de générer certains nombres.
- Vérifier qu'une suite est bien i.i.d. est un problème difficile.
:::


# Générateurs de nombres pseudo-aléatoires

## Générateurs de nombres pseudo-aléatoires

::: {#def-PRNG}
## Générateur de nombres pseudo-aléatoires

<br>

Un **générateur de nombres pseudo-aléatoires** (:gb:: *Pseudo Random Number Generator*, PRNG), est un algorithme déterministe récursif qui renvoie une suite $U_1, \ldots, U_n$ dans $[0,1]$ qui a un "comportement similaire" à une suite i.i.d. de loi $\mathcal{U}([0,1])$.
:::

<br>

[Remarque]{.underline}: ces nombres sont obtenus depuis des **nombres entiers** générés aléatoirement et uniformément sur grand interval, puis une transformation simple (normalisation) permet d'obtenir des nombres flottants (:gb:: *floats*) entre 0 et 1.


## Détails techniques

Un PRNG se construit ainsi :

1. Initialisation: une graine (:gb:: *seed*) $U_0$, détermine la première valeur (choix arbitraire)
2. On calcule $U_{n+1} = f(U_n)$, où $f$ est une transformation déterministe, telle que $U_{n+1}$ est "le plus indépendant possible" de $U_1, \dots, U_n$.

. . .

- $f$ : à valeur dans un ensemble fini $\implies$ **périodicité** (contrainte: utiliser la plus grande période possible)
- L'algorithme est **déterministe** (une fois la graine fixée). Utilité de fixer la graine: répéter des simulations dans des conditions identiques et ainsi repérer des erreurs


## Le générateur congruentiel linéaire

La plupart des PRNG s'appuient sur des résultats arithmétiques.

:::{.incremental}

- Le plus célèbre: Générateur Congruentiel Linéaire (:gb: Linear Congruential Generator, LCG).

- Récurrence:
$$
  X_{n+1} = a X_n + b \quad \text{mod } m \enspace,
$$
$a,b,m$, entiers bien choisis pour que la suite obtenue ait de bonnes propriétés
- Normalisation: $X_n/m$.
- Exemple: la fonction ```rand``` de ```scilab``` utilisait cette congruence avec $m=2^{31}$, $a=843\; 314\; 861$, et $b=453\; 816\; 693$.
:::

## Exemples de générateurs alternatifs

<br>
<br>

- méthode par défaut pour `Python` et `R`:  **Mersenne-Twister**, s'appuie sur la multiplication vectorielle (période du générateur $m =2^{19937}-1$)

<br>

- méthode par défaut pour `numpy`: **PCG64** (cf. [documentation de `numpy`](https://numpy.org/doc/stable/reference/random/bit_generators/pcg64.html)),
dispose de meilleures garanties statistiques; voir [https://www.pcg-random.org](https://www.pcg-random.org/statistical-tests.html#id4)

<br>

- On suppose désormais disposer d'un générateur pseudo-aléatoire sur $[0,1]$.


## Usage en `numpy`


En `numpy` (version>1.17): utiliser des éléments aléatoires est d'utiliser un générateur

<br>

```{python}
#| echo: true
seed = 12345                       #  choix de la graine
rng = np.random.default_rng(seed)  #  générateur
```

<br>

. . .

```{python}
#| echo: true
print(rng.random())                #  un tirage uniforme sur [0,1]
```

<br>

. . .


```{python}
#| echo: true
print(rng.random(size=5))          #  5 tirages uniformes sur [0,1]
```

<br>

. . .


```{python}
#| echo: true
print(rng.random(size=(3, 2)))     #  matrice 3x2, à entrées unif. sur [0,1]
```

## Plus sur les lois aléatoires et Python

:::{.incremental}

<br>

- Suite du cours: apprendre à générer de nombreuses lois à partir de la loi uniforme

- [En pratique]{.underline}: les logiciels proposent les distributions classiques (gaussiennes, exponentielles, etc.), utiliser plutôt ces fonctions que de les implémenter soi-même.

- Liste exhaustive pour `numpy`:

  [https://numpy.org/doc/stable/reference/random/generator.html#distributions](https://numpy.org/doc/stable/reference/random/generator.html#distributions)

:::

<br>

. . .

::: {.callout-note appearance="simple"}
## Pour aller plus loin
Une excellent discussion sur les bonnes pratiques aléatoires en `numpy`, et l'usage de `np.random.default_rng` est donnée dans ce [blog post d'Albert Thomas](https://albertcthomas.github.io/good-practices-random-number-generators/).
:::


# Méthode d'inversion (unidimensionnel)



## Rappel sur la fonction quantile

[Rappel]{.underline}: Pour $F$ une fonction définie sur $\mathbb{R}$ à valeurs dans $[0, 1]$, croissante, on note

$$
\forall q \in ]0,1[, \quad F^\leftarrow(q) = \inf\{ x \in \mathbb{R} : F(x)\geq q\}
$${#eq-inf}

[Note]{.underline}: Si $x_0 \in \{ x \in \mathbb{R} : F(x)\geq q\}$ alors $[x_0,+\infty[ \subset \{ x \in \mathbb{R} : F(x)\geq q\}$

. . .


:::{#thm-quantile}

## Caratérisation des quantiles
Soit $F$ une fonction définie sur $\mathbb{R}$ à valeurs dans $[0, 1]$, croissante et continue à droite, alors pour tout $q \in ]0, 1[$, on a
$$
\begin{align}
   \{x \in \mathbb{R} :  F(x) \geq q) \} & =
   \{x \in \mathbb{R} : x \geq F^\leftarrow(q)  \}
\end{align}
$$
:::

. . .

[Cas $\subset$:]{.underline} Soit $x \in \mathbb{R}$ t.q. $F(x) \geq q$, alors par définition de l'inf dans @eq-inf, $x \geq F^\leftarrow(q)$

<br>

. . .

[Cas $\supset$:]{.underline} Soient $\epsilon>0$ et $x \in \mathbb{R}$ t.q. $x \geq F^\leftarrow(q)$ alors (def. de l'inf) $\exists x_0 \in \{ x \in \mathbb{R} : F(x)\geq q\}$, t.q. $x + \epsilon > x_0$. Ainsi, $F(x + \epsilon) \geq F(x_0) \geq q$; par continuité à droite de $F$, $F(x) \geq q$


## Méthode d'inversion
:::{#thm-methode_inversion}

## Méthode d'inversion
Soit $X$ une v.a réelle, et  $U \sim\mathcal{U}([0,1])$, alors la variable aléatoire $F_X^\leftarrow(U)$ a même loi que $X$.
:::

. . .

<br>

[Preuve]{.underline}: en utilisant le théorème précédent, on a
$$
\forall x\in\mathbb{R}, \quad \mathbb{P}(x \geq F_X^\leftarrow(U)) = \mathbb{P}(F_X(x) \geq U)
$$

. . .

Puis, comme $U$ est une loi uniforme sur $[0,1]$:

$$
\mathbb{P}(F_X(x) \geq U) = F_X(x)
$$

. . .

Ainsi, $F_X^\leftarrow(U)$ et $X$ ont même loi: les deux v.a. ont la même fonction de répartition



## Symétrie de la loi uniforme


:::{#prp-uni}

## Symétrie de la loi uniforme
Soit $U \sim \mathcal{U}([0,1])$ une variable uniforme sur $[0,1]$. Alors,  $1-U$ suit aussi une loi uniforme sur $[0,1]$.
:::

:::{.fragment fragment-index=1}

[Preuve]{.underline}:

On va décrire la fonction de répartition de $1-U$ et montrer qu'elle est égale à celle d'une loi uniforme sur $[0,1]$.
:::

:::{.fragment fragment-index=2}


Le résultat est facile pour $x \notin [0,1]$, on suppose donc $x \in [0,1]$.
:::

$$
\begin{align*}
\mathbb{P}(1-U \leq x) & \class{fragment}{{}= \mathbb{P}(U \geq 1-x) }\\
                       & \class{fragment}{{} = 1-(1-x)} \\
                       & \class{fragment}{{} = x}
\end{align*}
$$

## Exemple : loi exponentielle

<br>

:::{.incremental}
- Densité d'une loi $\mathcal{E}(\lambda)$ pour $\lambda > 0$ : $f_{\lambda}(x) = \lambda e^{-\lambda x}{1\hspace{-3.8pt} 1}_{\mathbb{R}_+}(x)$
- Fonction de répartition: $F_{\lambda}(x) = (1 - e^{-\lambda x}) {1\hspace{-3.8pt} 1}_{\mathbb{R}_+}(x)$
- $F_{\lambda}$ est bijective (de $\mathbb{R}_+$ dans $]0,1[$) et pour tout $u \in ]0,1[$, $F_{\lambda}^{-1}(u) = -\frac{1}{\lambda} \log(1-u)$
:::

. . .

<br>

Avec le résultat:
$$
U \sim \mathcal{U}([0,1]) \iff 1-U \sim \mathcal{U}([0,1])\enspace,
$$

<br>

Pour simuler une loi exponentielle: simuler $U$ uniforme et appliquer $-\tfrac{1}{\lambda} \log(\cdot)$

$$
\boxed{-\tfrac{1}{\lambda} \log(U) \sim \mathcal{E}(\lambda)}
$$

## Visualisation

Voir animation dans la section [Cours, section "Méthode d'inversion"](../Courses/simulation.html#m%C3%A9thode-dinversion-1).



# Méthode de rejet (unidimensionnel)

## Méthode de rejet: contraintes

[Motivation]{.underline}: simuler une variable aléatoire $X$ de densité $f$  (**loi cible**), mais $f$ est trop compliquée pour la méthode de l'inverse.

. . .

[Idée]{.underline}: tirer suivant une autre loi $g$ (**loi des propositions**) et rejeter certains tirages.


:::{.incremental}

- on sait simuler $Y$ de loi $g$,
- il existe $m > 0$ tel que $f(x) \leq m \cdot g(x)$ (**constante de majoration**)
- on sait évaluer le **rapport d'acceptation** $r(x) = \frac{f(x)}{m\cdot g(x)}$

:::



. . .


[Remarque 1]{.underline}: $g(x)=0 \implies f(x)=0$, ainsi le support de $g$ doit englober celui de $f$

. . .


[Remarque 2]{.underline}: $m \geq 1$ car $m = m \displaystyle\int_\mathbb{R} g(x)\, dx \class{fragment}{{} \geq \displaystyle\int_\mathbb{R} f(x) dx} \class{fragment}{{} = 1}$


## Méthode de rejet: principe

Considérer deux suites i.i.d. de v.a. indépendantes entre elles:

- $(Y_n)_{n \geq 1}$ de loi $g$,
- $(U_n)_{n \geq 1}$ de loi uniforme sur $[0,1]$.

En pratique, $Y_n$ correspond à une **proposition** et $U_n$ permettra de décider l'**acceptation**/**rejet** de la proposition:

. . .

- Si oui, alors on conserve $Y_n$
- Si non, on simule $Y_{n+1}$

Pour simuler $X$ de densité $f$, simuler $Y_n$  (suivant $g$), $U_n$ (suivant $\mathcal{U}[0,1]$) et accepter si 
$$
U_n \leq r(Y_n) = \frac{f(Y_n)}{m\cdot g(Y_n)}
$$



## Exemple simple
<br>
<br>

:::{.incremental}

- $f(x) = 4x^3 {1\hspace{-3.8pt} 1}_{[0,1]}(x)$
- $f$ est majorée par $4$ $\implies$ $g = {1\hspace{-3.8pt} 1}_{[0,1]}$ et $m=4$ conviennent
- $r(x) =f(x) / (m\cdot g(x)) = x^3$, pour $x \in [0,1]$. On simule donc $(Y_n, U_n)$ et on teste si $4 \cdot U_1 \leq 4 Y_1^3$, etc.

:::

. . .

:::{.callout-note}

## Note

Dans la suite on verra qu'on tire des points $(Y_n, 4U_n)$ et qu'on teste si ils sont dans l'ensemble $\{(x,y) \in \mathbb{R}^2: y \leq f(x) \}$

:::


## Code Python

::: {.r-fit-text}
```{python}
#| echo: true
#| code-line-numbers: 1-8|9-11|12|13-17|18-19|20|
def accept_reject(n, f, g, g_sampler, m, rng):
    """
    n: nombre de simulations
    f: loi cible
    g, g_sampler: loi et générateur des propositions                            
    m: constante pour la majoration
    rng: générateur pseudo-aléatoire
    """
    x_samples = np.zeros(n)
    u_samples = np.zeros(n)
    accepted = np.zeros(n)
    for i in range(n):
        x = g_sampler()
        u = rng.uniform()
        alpha = u * m * g(x)  # note: pour le test on peut éviter les divisions
        u_samples [i] = alpha
        x_samples[i] = x
        if  alpha <= f(x):
            accepted[i] = 1
    return x_samples, u_samples, accepted
```
:::

## Visualisation de l'exemple

```{python}
#| echo: false
#| layout-ncol: 1

def plot_accept_reject(n, f, g, g_sampler, m, rng):
    x_samples, u_samples, accepted = accept_reject(n, f, g, g_sampler, m, rng)
    x = np.linspace(0, 1, 100)

    fig = go.Figure(layout=go.Layout(autosize=True, width=1200, height=900))
    fig.add_trace(
        go.Scatter(
            x=x_samples[accepted == 1],
            y=u_samples[accepted == 1],
            mode="markers",
            marker=dict(color="blue", symbol="circle", size=10, opacity=0.5),
            name="Accepté",
        )
    )
    fig.add_trace(
        go.Scatter(
            x=x_samples[accepted == 0],
            y=u_samples[accepted == 0],
            mode="markers",
            marker=dict(color="red", symbol="circle", size=10, opacity=0.5),
            name="Rejeté",
        )
    )
    fig.add_trace(
        go.Scatter(
            x=x, y=f(x), mode="lines", line=dict(color="black", dash="dash"), name="f"
        )
    )
    fig.add_trace(
        go.Scatter(
            x=x, y=m * g(x), mode="lines", line=dict(color="black"), name="m  · g"
        )
    )

    fig.update_xaxes(range=[0, 1.1])
    fig.update_yaxes(range=[0, 5.5])



    fig.update_layout(template="simple_white", showlegend=True)
    fig.update_layout(
        legend=dict(
            yanchor="top",
            y=0.92,
            x=.05,
            bgcolor="#ffffff",
            bordercolor="Black",
            borderwidth=1,
            font=dict(
                size=18,
                color="black"
            )
        )
    )
    ratio = np.sum(accepted) / n
    return fig, ratio, x_samples[accepted == 1]


f = lambda x: 4 * x**3
g = lambda x: np.ones_like(x)
m = 4
seed = 1234
```


::: {.r-stack}

::: {.fragment}
```{python}
#| echo: false
n = 0
rng = np.random.default_rng(seed)
g_sampler = lambda: rng.uniform()
fig1, ratio1,_ = plot_accept_reject(n, f, g, g_sampler, m, rng)
fig1.show()
```
:::

::: {.fragment}
```{python}
#| echo: false
n = 1
rng = np.random.default_rng(seed)
g_sampler = lambda: rng.uniform()
fig1, ratio1,_ = plot_accept_reject(n, f, g, g_sampler, m, rng)
fig1.show()
```
:::

::: {.fragment}
```{python}
#| echo: false
n = 2
rng = np.random.default_rng(seed)
g_sampler = lambda: rng.uniform()
fig1, ratio1,_ = plot_accept_reject(n, f, g, g_sampler, m, rng)
fig1.show()
```
:::


::: {.fragment}
```{python}
#| echo: false
n = 3
rng = np.random.default_rng(seed)
g_sampler = lambda: rng.uniform()
fig1, ratio1,_ = plot_accept_reject(n, f, g, g_sampler, m, rng)
fig1.show()
```
:::



::: {.fragment}
```{python}
#| echo: false
n = 30
rng = np.random.default_rng(seed)
g_sampler = lambda: rng.uniform()
fig1, ratio1,_ = plot_accept_reject(n, f, g, g_sampler, m, rng)
fig1.show()
```
:::



::: {.fragment}
```{python}
#| echo: false
n = 1000
rng = np.random.default_rng(seed)
g_sampler = lambda: rng.uniform()
fig1, ratio1, x_accepted = plot_accept_reject(n, f, g, g_sampler, m, rng)
fig1.show()
```
:::

::: {.fragment}
```{python}

x = np.linspace(0, 1, 100)

fig = go.Figure(layout=go.Layout(autosize=True, width=1200, height=900))
fig.update_xaxes(range=[0, 1.1])
fig.update_yaxes(range=[0, 5.5])



fig.update_layout(template="simple_white", showlegend=True)
fig.update_layout(
    legend=dict(
        yanchor="top",
        y=0.92,
        x=.05,
        bgcolor="#ffffff",
        bordercolor="Black",
        borderwidth=1,
        font=dict(
            size=18,
            color="black"
        )
    )
)
fig.add_trace(
      go.Scatter(
          x=x_accepted,
          y=0.05*np.ones_like(x_accepted),
          mode="markers",
          marker=dict(color="blue", symbol="circle", size=10, opacity=0.5),
          name="Abscisse des points acceptés",
      )
  )

fig.add_trace(
    go.Scatter(
        x=x, y=f(x), mode="lines", line=dict(color="black", dash="dash"), name="f"
    )
)
```
:::


::: {.fragment}

```{python}

fig.add_trace(
    go.Histogram(
        x=x_accepted,
        y=np.zeros_like(x_accepted),
        nbinsx=30,
        histnorm="probability density",
        marker=dict(color="blue", opacity=0.5),
        name="Histogramme des points acceptés",
    )
)

fig.show()
```
:::

:::


## Variante avec une loi triangulaire

<br>
<br>

Supposons disposer d'un générateur de loi triangulaire sur $[0,1]$ (cf. `np.random.triangular(0, 1, 1)`)


:::{.incremental}


- $f(x) = 4x^3 {1\hspace{-3.8pt} 1}_{[0,1]}(x)$
- $f(x)$ est majorée par $4 x \cdot {1\hspace{-3.8pt} 1}_{[0,1]}(x)$ $\implies$ $g = 2x \cdot {1\hspace{-3.8pt} 1}_{[0,1]}$ et $m=2$ conviennent
- $r(x)=f(x) / (m\cdot g(x)) = x^3$, pour $x \in [0,1]$.

:::

## Variante (continuée)

::: {.r-stack}

::: {.fragment}
```{python}
#| echo: false
n = 1000
rng = np.random.default_rng(seed)
g_sampler = lambda: rng.uniform()
fig1, ratio1, x_accepted = plot_accept_reject(n, f, g, g_sampler, m, rng)
fig1.show()
```
:::


::: {.fragment}

```{python}
#| echo: false

g = lambda x: 2 * x
rng = np.random.default_rng(seed)
g_sampler = lambda: np.random.triangular(0, 1, 1)
m = 2

fig2, ratio2, _ = plot_accept_reject(n, f, g, g_sampler, m, rng)
fig2.show()
```
:::

:::


## Comparaison des deux majorants

<br>
```{python}
#| echo: false
ojs_define(ratio1=ratio1)
ojs_define(ratio2=ratio2)
```

::: {.r-fit-text}
```{ojs}
//| echo: false
md`Taux d'acceptation: avec la loi uniforme **${ratio1.toPrecision(5)}**`
```

```{ojs}
//| echo: false
md`Taux d'acceptation: avec la loi triangulaire **${ratio2.toPrecision(5)}**`
```

<br>

[Conclusion]{.underline}: plus le majorant est proche de la loi cible, plus le taux d'acceptation est élevé, et moins de simulations sont nécessaires
:::

<br>


::: {.callout-note appearance="simple"}

## Note

L'exemple est pour l'illustration de la méthode, dans le cas présent méthode de l'inverse fonctionnerait aussi (on peut calculer la fonction quantile explicitement).

:::

## Méthode de rejet: validation théorique

[Rappel]{.underline}:

- il existe $m > 0$ tel que $f(x) \leq m \cdot g(x)$ et $r(x) = \frac{f(x)}{m\cdot g(x)}$
- $(Y_n)_{n \geq 1}$ i.i.d. de loi $g$
- $(U_n)_{n \geq 1}$ i.i.d. de loi uniforme sur $[0,1]$ (indépendamment des $Y_n$)

. . .


:::: {#thm-rejet}

## Méthode de rejet

<br>

Soit $T = \inf \{n \geq 1 : U_n \leq r(Y_n)\}$ le premier instant où le tirage est accepté. Alors :

- $T \sim \mathcal{G}(\frac{1}{m})$ : loi géométrique de paramètre $\frac{1}{m}$
- $Y_T$ a pour densité $f$ et est indépendante de $T$
::::


##  Démonstration

Pour $x \in \mathbb{R}$ et $n \in \mathbb{N}^{*}$, et $X = Y_T$, on écrit
$\mathbb{P}(X \leq x, T=n)= \mathbb{P}(U_1 > r(Y_1), \dots, U_{n-1} > r(Y_{n-1}), U_n \leq r(Y_n), Y_n \leq x)$


::: {.fragment fragment-index=1}
$$
    \mathbb{P}(X \leq x, T=n) = {\color{blue}\mathbb{P}(U_1 > r(Y_1))}^{n-1} \cdot {\color{brown}\mathbb{P}(U_n \leq r(Y_n), Y_n \leq x)}, \quad \textsf{( tirages i.i.d.)}
$$
:::

::: {.fragment fragment-index=2}
[Premier terme]{.underline}:  $Y_1$ et $U_1$ sont indépendantes, leur loi jointe correspond au produit des densités :
:::

::: {.fragment fragment-index=3}

$$
{\color{blue}
\begin{align*}
		\mathbb{P}(U_1 > r(Y_1))
		 & \class{fragment}{{} = \mathbb{P}((U_1, Y_1) \in \{(u,y) \in \mathbb{R}^2 : u > r(y)\})}                             \\
		 & \class{fragment}{{} = \int_{\mathbb{R}^2} \left( {1\hspace{-3.8pt} 1}_{\{u > r(y)\}} \right) \cdot \left({1\hspace{-3.8pt} 1}_{[0,1]}(u) g(y)\right) \,  du  dy}  \\
		 & \class{fragment}{{} = \int_\mathbb{R} \left( \int_0^1 {1\hspace{-3.8pt} 1}_{\{u > r(y)\}} \,  du\right) g(y)\,  d y}
		 \class{fragment}{{} =  \int_\mathbb{R} (1-r(y)) \, g(y)\,  d y}\\
		 & \class{fragment}{{} =  \int_\mathbb{R} g(y) -  \int_\mathbb{R}\frac{f(y)}{m} d y, \quad\quad \text{car }  r(y) = \frac{f(y)}{m \cdot g(y)}}\\
     & \class{fragment}{{} = 1-\tfrac{1}{m}}
\end{align*}
}
$$

:::

## Démonstration (suite)

[Second terme]{.underline}:
$$
{\color{brown}
\begin{align*}
    \mathbb{P}(U_n \leq r(Y_n), Y_n \leq x)
    & \class{fragment}{{ = \int_{\mathbb{R}^2} {1\hspace{-3.8pt} 1}_{\{u \leq r(y)\}} {1\hspace{-3.8pt} 1}_{\{y \leq x\}} ({1\hspace{-3.8pt} 1}_{[0,1]}(u) g(y)) \,  du  dy  }} \\
    & \class{fragment}{{} = \int_\mathbb{R} \left( \int_0^1  {1\hspace{-3.8pt} 1}_{\{u \leq r(y)\}} \,  du\right) {1\hspace{-3.8pt} 1}_{\{y \leq x\}}  g(y)\,  d y} \\
		& \class{fragment}{{} = \int_\mathbb{R} r(y) {1\hspace{-3.8pt} 1}_{\{y \leq x\}}  g(y)\,  d y } \\
		& \class{fragment}{{} = \int_{-\infty}^x \dfrac{f(y)}{m}\,  d y } \\
 		& \class{fragment}{{} = \dfrac{F(x)}{m} , \quad F \textsf{ fonction de répartition associée à} f}
\end{align*}
}
$$

##  Démonstration (suite)
$$
    \mathbb{P}(X \leq x, T=n)
    =
    {\color{blue}\left(1 - \tfrac{1}{m}\right)^{n-1}} \cdot {\color{brown}\tfrac{F(x)}{m}}
$$

. . .

On peut alors obtenir les lois marginales:
$$
\begin{align*}
    \mathbb{P}(T=n)
    & = \lim_{q \to \infty} \mathbb{P}(X \in ]-\infty, q], T=n) = \lim_{q \to \infty} \left(1 - \tfrac{1}{m}\right)^{n-1} \tfrac{F(q)}{m}\\
    & = \left(1 - \tfrac{1}{m}\right)^{n-1} \tfrac{1}{m}
\end{align*}
$$
Ainsi, $T$ suit une loi géométrique de paramètre $1/m$, puis $X$ a pour loi $F$:

. . .

$$
\begin{align*}
    \mathbb{P}(X \leq x)
    & = \mathbb{P}(X \leq x, T \in \mathbb{N}^*)
      = \sum_{n=1}^\infty \mathbb{P}(X \leq x, T=n) \\
    & = \sum_{n=1}^\infty \left(1 - \tfrac{1}{m}\right)^{n-1} \tfrac{F(x)}{m}
      = \tfrac{1}{1-(1-1/m)} \tfrac{F(x)}{m} = F(x) \enspace.
\end{align*}
$$


## Fin de la démonstration

On obtient l'indépendance de $T$ et $X$ car on peut alors écrire:
$$
    \forall x \in \mathbb{R}, \forall n \in \mathbb{N}^*, \quad
    \mathbb{P}(X \leq x, T=n)
    =
    \mathbb{P}(X \leq x) \cdot \mathbb{P}(T=n)
$$


## Cas de densité connue à une constante près : exemple
**Loi de Andrews **(densité proportionnelle à $\mathrm{sinc}$, sinus cardinal):
$$
\forall x \in \mathbb{R},\quad f(x) = \frac{1}{S} \frac{\sin(\pi\cdot x)}{\pi \cdot x}  {1\hspace{-3.8pt} 1}_{[-1,1]}(x)
$$
avec $S = \int_{-1}^{1}\frac{\sin(\pi\cdot x)}{\pi \cdot x}dx$ non explicite. On note parfois: $f(x) \propto \frac{\sin(\pi\cdot x)}{\pi \cdot x}  {1\hspace{-3.8pt} 1}_{[-1,1]}(x)$

<br>

. . .

[Méthode du rejet]{.underline}: prendre $m=2/S$ et $g(x) = \frac{1}{2} {1\hspace{-3.8pt} 1}_{[-1,1]}(x)$:

$$
u \leq r(x) = \frac{f(x)}{m \cdot g(x)} \iff u \leq \frac{1}{S} \frac{\sin(\pi\cdot x)}{\pi \cdot x}  \cdot \frac{1}{\frac{2}{S} g(x)} = \frac{\sin(\pi\cdot x)}{\pi \cdot x}  \cdot \frac{1}{2 \cdot g(x)}
$$

. . .

Ainsi l'évaluation de $r(x)$ est possible sans connaître $S$!


## Loi de Andrews: visualisation

```{python}
#| echo: true
n = 300
m = 2
rng = np.random.default_rng(seed)
g = lambda x: np.ones_like(x) / 2
g_sampler = lambda: 2 * rng.uniform() - 1
x_samples, u_samples, accepted = accept_reject(n, np.sinc, g, g_sampler, m, rng)
```

```{python}
#| echo: false
ratio_andrews = np.sum(accepted) / n
ojs_define(ratio_andrews=ratio_andrews)
# Note: https://stackoverflow.com/questions/70804891/how-to-vectorize-a-function-in-python-that-includes-a-limit-e-g-sinx-x
```




```{python}
from scipy import integrate
S = integrate.quad(np.sinc, -1, 1)[0]
ojs_define(s_int=S)
# print(f"En utilisant la méthode de rejet, on trouve que S = {S:.3f}")
```
```{ojs}
//| echo: false
md`Le taux d'acceptation est ici de **${ratio_andrews.toPrecision(3)}**.`
```

::: {.r-stack}

```{python}
#| echo: false
#| layout-ncol: 1
fig = go.Figure(layout=go.Layout(autosize=True, width=1200, height=550))
fig.add_trace(
    go.Histogram(
        x=x_samples[accepted == 1],
        nbinsx=30,
        marker=dict(color="blue", opacity=0.1),
        histnorm="probability density", name="Histogramme des points acceptés"
    )
)

# Plot the density
x = np.linspace(-1, 1, 100)
fig.add_trace(
    go.Scatter(
        x=x,
        y=np.sinc(x) / S,
        mode="lines",
        line=dict(color="black", dash="dash", width=5),
        name="Densité",
    )
)
# Plot the samples
fig.update_xaxes(range=[-1.1, 1.1])
fig.update_yaxes(range=[0, 1.5])

fig.add_trace(
    go.Scatter(
        x=x_samples[accepted == 1],
        y=0.05 * np.ones_like(x_samples[accepted == 1]),
        mode="markers",
        marker=dict(color="blue", symbol="circle", size=15, opacity=0.59),
        name="Abscisse des points acceptés",
    )
) 

fig.update_layout(template="simple_white", showlegend=True)

fig.update_layout(
    legend=dict(
        y=0.95, x=.74,
        yanchor="top",
        bgcolor="#ffffff",
        bordercolor="Black",
        borderwidth=1,
        font=dict(
            size=18,
            color="black"
        )
    )
)

```

:::

. . .

```{ojs}
//| echo: false
md`Pour la visualization, on approxime S=**${s_int.toPrecision(3)}** en utilisant une méthode de calcul numérique.`
```



## Cas de densité connue à une constante près (cas général)

Soit $\tilde{f}: \mathbb{R} \to [0,+\infty[$ connue et $S \triangleq \int_{\mathbb{R}} \tilde{f}(x) \,  d x  < + \infty$ inconnue (ou dure à évaluer)

[Densité cible]{.underline}: $\quad f(x) = \frac{\tilde{f}(x)}{S}$

<br>

Méthode du rejet pour $f$, en utilisant seulement $\tilde{f}$:  soit $\tilde{m}>0$ un majorant de $\tilde{f}$ t.q.
$$
\begin{align*}
\tilde{f}(x) \leq  \tilde{m} \cdot g(x)
\end{align*}
$$

Application avec $m=\tilde{m}/S$  (sans connaître $S$), le test d'acceptation donne:

. . .

$$
\begin{align*}
U_n & \leq \frac{f(Y_n)}{m \cdot g(Y_n)}\\
  & \class{fragment}{{}\leq \frac{\frac{\tilde{f}(Y_n)}{S}}{\frac{\tilde{m}}{S} \cdot g(Y_n)}} \class{fragment}{{}= \frac{\tilde{f}(Y_n)}{\tilde{m} \cdot g(Y_n)}}
\end{align*}
$$



# Méthode de rejet (multidimensionnel)


## Cas multidimensionnel

<br>

:::{.incremental}

- impossibilité de la méthode de l'inverse: fonction de répartition non disponible (en général)

- la méthode de rejet : généralisable au cas multidimensionnel
  - "fléau de la dimension": plus la dimension est grande, plus la méthode est inefficace (penser au nombre de points nécessaires pour quadriller un hypercube...)
  - difficulté d'écrire une fonction de majoration en toute généralité

:::

## Exemple: loi uniforme sur le disque unité (2D)

<br>

[Loi cible]{.underline}: loi uniforme sur le disque unité, $f(x)\propto {1\hspace{-3.8pt} 1}_{x_1^2+x_2^2 \leq 1}(x)$ pour $x=(x_1,x_2)\in\mathbb{R}^2$

<br>

[Loi majorante]{.underline}: loi uniforme sur le carré $[-1,1]^2$, $g(x)\triangleq \tfrac{1}{2} {1\hspace{-3.8pt} 1}_{[-1, 1]^2}(x)$ et $m=2$

<br>


:::{.callout-note}

La loi uniforme sur le carré est une loi produit: il suffit de savoir générer une loi uniforme sur un segment 1D pour l'obtenir

:::

## Exemple: loi uniforme sur le disque (visualisation)

```{python}
#| echo: false


def cardiode(n_discr=100):
    theta = np.linspace(0, 2 * np.pi, n_discr)
    r = (1 + np.cos(theta))
    x = r * np.cos(theta)
    y = r * np.sin(theta)
    return x, y


def circle(n_discr=100):
    theta = np.linspace(0, 2 * np.pi, n_discr)
    x = np.cos(theta)
    y = np.sin(theta)
    return x, y


def plot_curve(fig, plot_type="circle", n_discr=100):
    if plot_type=="circle":
        x, y = circle(n_discr)
    elif plot_type=="cardioide":
        x, y = cardiode(n_discr)
    fig.add_trace(go.Scatter(x=x, y=y, mode="lines", line=dict(color="black", width=4), name="Limite de surface"))
    fig.update_layout(template="simple_white", showlegend=True,    yaxis_scaleanchor="x")
    return fig
```

```{python}
#| echo: false
def test_cardioide(x, y):
    if (x**2 + y**2 - x) ** 2 <= (x**2 + y**2):
        return 1
    else:
        return 0


def test_unit_circle(x, y):
    if x**2 + y**2 <= 1:
        return 1
    else:
        return 0


color = ["red", "blue"]
status = ["Rejeté", "Accepté"]


def plot_samples(fig, n_sample, plot_type="circle"):
    if plot_type == "circle":
        test_func = test_unit_circle
        x_samples = np.random.uniform(-1, 1, n_sample)
        y_samples = np.random.uniform(-1., 1, n_sample)

    elif plot_type == "cardioide":
        test_func = test_cardioide
        x_samples = np.random.uniform(-2, 3, n_sample)
        y_samples = np.random.uniform(-1.5, 1.5, n_sample)

    test_func = np.vectorize(test_func)
    test_samples = test_func(x_samples, y_samples)
    for i in [0, 1]:
        fig.add_trace(
            go.Scatter(
                x=x_samples[test_samples == i],
                y=y_samples[test_samples == i],
                mode="markers",
                marker=dict(color=color[i], symbol="circle", size=7, opacity=0.5),
                name=status[i],
            )
        )
    fig.update_layout(
        legend=dict(
            yanchor="top",
            bgcolor="#ffffff",
            bordercolor="Black",
            borderwidth=1,
            font=dict(
                size=18,
                color="black"
            )
        )
    )
    return fig, test_samples
```


::: {.r-stack}

::: {.fragment}
```{python}
#| echo: false
#| layout-ncol: 1

n_sample = 3000
plot_type = "circle"

fig = go.Figure(layout=go.Layout(autosize=True, width=1200, height=700))
fig, test_samples_circle = plot_samples(fig, n_sample=n_sample, plot_type=plot_type)
fig = plot_curve(fig, plot_type=plot_type, n_discr=100)
fig.update_xaxes(range=[-2, 2.])
fig.update_yaxes(range=[-1.5, 1.5])
fig.update_layout(legend=dict(y=0.95, x=.74))
ratio = np.sum(test_samples_circle) / n_sample
aire = 2 * 2 * ratio
fig.show()
```

:::

:::

::: {.r-stack}


::: {.fragment}


```{python}
#| echo: false
ojs_define(aire=aire)
ojs_define(ratio=ratio)

```

```{ojs}
//| echo: false
//| layout-ncol: 1
md`Ratio estimée: **${ratio.toPrecision(5)}**`
md`Aire (bleue) estimé: **${aire.toPrecision(5)}**`
```

:::

:::



## Exemple: loi uniforme sur une cardioïde (2D)

<br>

[Loi cible]{.underline}: loi uniforme sur le disque unité, $f(x)\triangleq{1\hspace{-3.8pt} 1}_{(x_1^2+x_2^2 - x_2)^2 \leq x_1^2+ x_2^2}(x)$ pour $x=(x_1,x_2)\in\mathbb{R}^2$
<br>

[Loi majorante]{.underline}: loi uniforme sur le rectangle $[-2,3]\times [-1.5,1.5]$, $g(x)\triangleq \tfrac{1}{15}{1\hspace{-3.8pt} 1}_{[-2,3]\times [-1.5,1.5]}(x)$ et $m=15$

<br>

:::{.callout-note}

La loi uniforme sur un rectangle est une loi produit: il suffit de savoir générer une loi uniforme sur un segment 1D pour l'obtenir

:::

## Exemple: loi uniforme sur une cardioïde (2D)

::: {.r-stack}

::: {.fragment}
```{python}
#| echo: false
#| layout-ncol: 1

n_sample = 3000
plot_type = "cardioide"

fig = go.Figure(layout=go.Layout(autosize=True, width=800, height=700))
fig, test_samples_circle = plot_samples(fig, n_sample=n_sample, plot_type=plot_type)
fig = plot_curve(fig, plot_type=plot_type, n_discr=100)
fig.update_xaxes(range=[-3, 4.])
fig.update_yaxes(range=[-3, 3])
fig.update_layout(legend=dict(y=0.95, x=.74))
ratio_cardioid = np.sum(test_samples_circle) / n_sample
aire_cardioid = 3* 5 * ratio_cardioid
fig.show()
```

:::

:::

::: {.r-stack}


::: {.fragment}


```{python}
#| echo: false
ojs_define(aire_cardioid=aire_cardioid)
ojs_define(ratio_cardioid=ratio_cardioid)

```

```{ojs}
//| echo: false
//| layout-ncol: 1
md`Ratio estimée: **${ratio_cardioid.toPrecision(5)}**`
md`Aire (bleue) estimé: **${aire_cardioid.toPrecision(5)}**`
```

:::

:::


## Théorie


:::{#thm-unif-mesurable}
## Géneration uniforme sur un ensemble

Soient $A\subset B \subset \mathbb{R}^d$, deux ensembles mesurables pour la mesure de Lebesgue.
Pour générer selon $\mathcal{U}(A)$, connaissant un générateur selon $\mathcal{U}(B)$, la méthode du rejet consiste ici à tirer $Y_i \sim \mathcal{U}(B)$ (i.i.d) et à garder $Y_i$ si $Y_i \in A$.
:::


:::{.fragment fragment-index=1}


[Preuve]{.underline}:
On note $f\triangleq \frac{1}{|A|} {1\hspace{-3.8pt} 1}_{A}$, $g\triangleq \frac{1}{|B|}{1\hspace{-3.8pt} 1}_{B}$ et $m\triangleq\frac{|B|}{|A|}$.
Comme $A \subset B$, pour tout $x\in \mathbb{R}^{d}$:
$$
\begin{align*}
\class{fragment}{{}f(x)} \class{fragment}{{}= \frac{1}{|A|} {1\hspace{-3.8pt} 1}_{A}(x)} \class{fragment}{{}\leq \frac{1}{|B|} {1\hspace{-3.8pt} 1}_{B}(x) \cdot \frac{|B|}{|A|}} \class{fragment}{{}\leq g(x) \cdot m}
\end{align*}
$$



<!-- On peut donc utiliser la méthode du rejet avec $Y \sim \mathcal{U}(B)$ et $U \sim \mathcal{U}([0,1])$ et $r$ qui vaut: -->

$$
\begin{align*}
&\class{fragment}{{}Y \sim \mathcal{U}(B),} \quad \class{fragment}{{} U \sim \mathcal{U}([0,1])} \\
&\class{fragment}{{}r(Y)} \class{fragment}{{}= \frac{f(Y)}{m\cdot g(Y)}}
\class{fragment}{{}=
\frac{ \frac{1}{|A|} {1\hspace{-3.8pt} 1}_{A}(Y)}
{\frac{|B|}{|A|}\cdot \frac{1}{|B|} {1\hspace{-3.8pt} 1}_{B}(Y)}}
\class{fragment}{{}=
\frac{  {1\hspace{-3.8pt} 1}_{A}(Y)}
{ {1\hspace{-3.8pt} 1}_{B}(Y)}}
\class{fragment}{{}= {1\hspace{-3.8pt} 1}_{A}(Y)}
\end{align*}
$$

$$
\begin{align*}
\class{fragment}{{}\text{Enfin}, U \leq {1\hspace{-3.8pt} 1}_{A}(Y) \iff {1\hspace{-3.8pt} 1}_{A}(Y)=1}\class{fragment}{{}\iff Y \in A}
\end{align*}
$$

:::


## Remarque sur les constantes

<br>

Dans l'exemple précédent, on a pu appliquer la méthode de rejet sans la connaissance de $m$

<br>

Point important: parfois la connaissance de $m$ est difficile à obtenir, et il est préférable de ne pas l'utiliser (notamment quand les constantes de normalisation des densités sont difficiles à calculer).

<br>
[Exemples]{.underscore}: statistiques bayesiennes, modèles graphiques, etc.


# Autres méthodes



## Sommes de variables aléatoires

[Loi de Bernoulli]{.underline}:  avec $U_1, \ldots, U_n$ i.i.d uniformes sur $[0,1]$ (méthode d'inversion):
$$
X_i \triangleq {1\hspace{-3.8pt} 1}_{\{U_i \leq p\}} \sim \mathcal{B}(p)
$$

. . .

[Loi binomiale]{.underline}:
$$
	\sum_{i=1}^n {1\hspace{-3.8pt} 1}_{\{U_i \leq p\}} \sim \mathcal{B}(n,p)
$$
en rappelant que
$$
	X = X_1 + \cdots + X_n \sim \mathcal{B}(n,p)
$$

. . .

:::{.callout-note}

## Note
La méthode d'inversion marche aussi, mais nécessite le calcul de l'inverse généralisée de $F$, donc de coefficients binomiaux...
:::


## Loi de Poisson

[Rappel]{.underline}: $\quad X \sim \mathcal{P}(\lambda) \iff \mathbb{P}(X = k) = e^{-\lambda} \dfrac{\lambda^k}{k!}\,, \quad  \forall k \in \mathbb{N}.$

:::: {#prp-poisson_exponentielle}

## Génération de v.a. de loi de Poisson

<br>

Soit $(E_n)_{n \geq 1}$ des variables aléatoires i.i.d. de loi exponentielle de paramètre $\lambda > 0$. On pose $S_k = E_1 + \cdots + E_k$. Alors pour tout $n \in \mathbb{N}$
$$
    \mathbb{P}(S_n \leq 1 < S_{n+1}) =  e^{-\lambda} \dfrac{\lambda^n}{n!}\enspace .
$$
Ainsi, $T \triangleq \sup \{n \in \mathbb{N} : S_n \leq 1\}$ suit une loi de Poisson de paramètre $\lambda$ : $T \sim \mathcal{P}(\lambda)$.

::::

<br>

:::{.callout-note}

## Point numérique
Méthode adaptée par `numpy.random.poisson`,
cf. [code source](https://github.com/numpy/numpy/blob/main/numpy/random/src/distributions/distributions.c#L540), qui fut proposée par D. Knuth; Source: [Wikipedia](https://en.wikipedia.org/wiki/Poisson_distribution#cite_note-Knuth1997-68)

:::

## Loi de Poisson (suite)

La preuve repose sur le résultat suivant:

:::: {#lem-erlang}

## Loi de Erlang

<br>


Soient $E_1, \dots, E_n$ des v.a., i.i.d. de loi exponentielle de paramètre $\lambda >0$. La somme $S_n=E_1+\dots+E_n$ suit une loi d'Erlang de paramètres $(n,\lambda)$, de fonction de répartition
$$
    F_{n,\lambda}(t) = 1 - \sum_{k=0}^{n-1} e^{-\lambda t} \frac{(\lambda t)^k}{k!}\,.
$$
::::

. . .

*Preuve partielle*: Transformée de Laplace de $\mathcal{E}(\lambda)$: $\mathbb{E}\left(e^{-tE_1}\right) = \int_0^{+\infty} e^{-t x} \lambda e^{-\lambda x} \,  d x = \frac{\lambda}{\lambda+t}$

. . .

Densité d'une loi  $\Gamma(\alpha,\beta): \quad f(x) = \tfrac{\beta^\alpha}{\Gamma(\alpha)} x^{\alpha-1} e^{-\beta x}$

. . .


Transformée de Laplace d'une loi  $\Gamma(\alpha,\beta): \quad \mathbb{E}\left(e^{-tX}\right) = \left(\tfrac{\beta}{\beta+t}\right)^\alpha$

. . .

Enfin, $\mathbb{E}\left(e^{-t S_n}\right) = \left(\mathbb{E}\left(e^{-t E_1}\right)\right)^n=\left(\tfrac{\lambda}{\lambda+t}\right)^n$

## Démonstration avec le lemme


Pour $n \in \mathbb{N}^*$, on décompose la probabilité $\mathbb{P}(S_n \leq 1 < S_{n+1})$ via
$$
\begin{align*}
    \mathbb{P}(S_n \leq 1 < S_{n+1})
    & = \mathbb{P}(\{S_n \leq 1\} \setminus \{S_{n+1} \leq 1\})\\
    & = \mathbb{P}(S_n \leq 1) - \mathbb{P}(S_{n+1} \leq 1)\,.
\end{align*}
$$

. . .

Du lemme précédent : $\mathbb{P}(S_n \leq 1) = 1 - \displaystyle\sum_{k=0}^{n-1} e^{-\lambda} \dfrac{\lambda^k}{k!}$
et
$\mathbb{P}(S_{n+1} \leq 1) = 1 - \displaystyle\sum_{k=0}^{n} e^{-\lambda} \dfrac{\lambda^k}{k!}$.

. . .

Puis,
$$
    \mathbb{P}(S_n \leq 1 < S_{n+1})
    = e^{-\lambda} \dfrac{\lambda^n}{n!}\,.
$$

. . .

On conclut la preuve de la proposition en remarquant que pour $T \triangleq \sup \{k \in \mathbb{N}^* : S_k \leq 1\}$
$$
    \mathbb{P}(T=n) = \mathbb{P}(S_n \leq 1 < S_{n+1})\,.
$$
