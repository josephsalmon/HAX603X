---
title: "Introduction"
subtitle: "HAX603X: Modélisation stochastique"
format:
  revealjs:
    # preview-links: true
    code-link: true
    highlight-style: a11y

---


# Présentation, informations générales


<!-- ----------------------------------------------------------------------- -->
## {#contact data-menu-title="Contact"}
![](https://raw.githubusercontent.com/josephsalmon/OrganizationFiles/master/inkscape/images/contact_js.svg)

<br>
<br>
<br>

PS: n'oubliez pas de mettre [HAX603X] dans le titre de vos mails!
<!-- ----------------------------------------------------------------------- -->


<!-- ----------------------------------------------------------------------- -->
## {{< fa graduation-cap title="graduation cap" >}} Enseignants
<hr>


::: {.fragment fragment-index=1}
- **Joseph Salmon** : CM et TP
    - Directeur de recherche, Inria
    - Précédemment : Paris Diderot-Paris 7, Duke Univ., Télécom ParisTech, Univ. Washington, Univ. Montpellier
    - Spécialités : statistiques, optimisation, traitement des images, sciences participatives, apprentissage automatique
    - Bureau : 415, Bat. 9
:::
<div style="line-height:4%;">
<br>
</div>


<!-- column -->

::::{.columns}

:::{.column width="50%"}

::: {.fragment fragment-index=2}
- **Jean Baptiste Fermanian** : CM, TD
  - Post-doctorant, Univ. Montpellier
  - Précédemment : ENS Paris-Saclay
  - Spécialités : statistiques
  - Bureau : 121, Bat. 9
:::

:::

:::{.column width="50%"}


::: {.fragment fragment-index=2}
- **Julien Patras** :  TP
  - Doctorant, IRD
  - Précédemment : Ecole Polytechnique
  - Spécialités : Statistiques, écologie
  - Bureau : LIRMM et IRD Sète
:::

:::

::::

<!-- ----------------------------------------------------------------------- -->



<!-- ----------------------------------------------------------------------- -->
## {{< fa globe title="laptop code" >}} Ressources en ligne
<hr>


<br>

[Informations principales]{.underline} : site du cours <http://josephsalmon.github.io/HAX603X>

<br>

:::{.incremental}

- Syllabus
- Cours (détaillé: site web)
- Slides (résumé)
- Feuilles de TD
- Feuilles de TP
- Rendu TP : Moodle de l'université (https://moodle.umontpellier.fr/course/view.php?id=5558)

:::
<!-- ----------------------------------------------------------------------- -->



<!-- ----------------------------------------------------------------------- -->
## {{< fa clipboard-list title="Clipboard list" >}} Validation
<hr>

- TP notés : Rendu = fichier Python **.py** unique

   - TP noté 1 : rendre en fin de session (en S7)
   - TP noté 2 : rendre en fin de session (en S15)

- CC : devoir sur table d'une heure (S16)

<br>

- Coefficients:
    - Note Session 1 : (40% CC + 30% TP 1 + 30% TP 2)
    - Note Session 2 : (30% CC + 35% TP 1 + 35% TP 2)

::: {.callout-important}


Le rendu est individuel pour les TPs notés !
:::
<!-- ----------------------------------------------------------------------- -->


## {{< fa laptop title="Computer" >}} Notation pour les TPs
<hr>


[Rendu]{.underline} : sur Moodle, en déposant un fichier `nom_prenom.py`
dans le dossier adéquat.

Détails de la notation des TPs :

- Qualité des réponses aux questions
- Qualité de rédaction et d'orthographe
- Qualité des graphiques (légendes, couleurs)
- Qualité du code (noms de variables, clarté, commentaires utiles, code synthétique, etc.)
- Code reproductible et absence de bug

<br>

::: {.callout-important}

## [Pénalités]{.underline}


- Envoi par mail : zéro
- Retard : zéro (uploader avant la fin, faites des tests, fermeture automatique de Moodle)

:::
<!-- ----------------------------------------------------------------------- -->




<!-- ----------------------------------------------------------------------- -->
## Prérequis - à revoir seul
<hr>

:::{.incremental}
<br>
<br>

- Bases de probabilités (en particulier "HAX506X - Théorie des Probabilités"): probabilité, densité, espérance, fonction de répartition, mesure, intégration, analyse numérique élémentaire, etc. [@Foata_Fuchs96;@Barbe_Ledoux06;@Ouvrard07;@Ouvrard08]

<br>

- Programmation élémentaire (en Python): `if` ... `elif`... `else` ..., for, while, fonctions, etc. [HLMA310 - Logiciels scientifiques](https://josephsalmon.eu/HLMA310.html), [@Courant_deFalco_Gonnord_Filliatre_Conchon_Dowek_Wack13], [Cours de Python: Univ. Paris Diderot](https://python.sdv.univ-paris-diderot.fr/)

<br>

- Pour aller plus loin: conditionnement, martingales [@Williams91]

:::
<!-- ----------------------------------------------------------------------- -->




<!-- ----------------------------------------------------------------------- -->
## Description du cours HAX603X
<hr>

:::{.incremental}

1. Générer l’aléa
   - générateurs pseudo-aléatoires, simulations de variables aléatoires (inverse, rejet, etc.)
   - illustrations numériques et visualisation en Python (loi des grands nombres, TCL)
2. Méthode de Monte-Carlo
   - méthode de Monte-Carlo pour le calcul approché d’une intégrale
   - réduction de la variance : variables antithétiques, variables de contrôle, etc.
3. Compléments
   - vecteurs gaussiens et lien avec les lois usuelles de la statistique inférentielle (student, chi2 ou $\chi^2$)
   - quantification de l'incertitude, intervalles de confiance
   - marches aléatoires simples, etc.
:::
<!-- ----------------------------------------------------------------------- -->


# Perspectives historiques



<!-- ----------------------------------------------------------------------- -->

##  L'aiguille de Buffon {{< iconify ph:needle >}}
<hr>

::::{.columns}

:::{.column width="60%"}

:::{.fragment fragment-index=1}
- 1733:  **l'aiguille de Buffon**, méthode d'estimation de la valeur de $\pi$. 
:::
:::{.fragment fragment-index=2}
- **Problème initial**: une aiguille de taille $1$ tombe sur un parquet composé de lattes de largeur $1$: quelle est alors la probabilité $P$ que l'aiguille croise une ligne de la trame du parquet ?
:::

:::{.fragment fragment-index=3}
```{python}
#| echo: false
#| layout-ncol: 1

import numpy as np
import plotly.graph_objects as go
from plotly.subplots import make_subplots

# Initialize
rng = np.random.default_rng(44)
n_samples, xmax, xmin = 200, 14.499999, -14.499999

# Create needles in one step
centers_x, angles, centers_y = [rng.uniform(a, b, n_samples) for a, b in 
                              [(xmin, xmax), (0, 2*np.pi), (-2, 2)]]

# Compute borders more efficiently
cos_angle, sin_angle = np.cos(angles)/2, np.sin(angles)/2
borders_right = np.column_stack((centers_x + cos_angle, centers_y + sin_angle))
borders_left = np.column_stack((centers_x - cos_angle, centers_y - sin_angle))

# Calculate overlap
centers_x_round = np.round(centers_x)
overlap = np.where((borders_right[:,0] - centers_x_round) * 
                  (borders_left[:,0] - centers_x_round) < 0, 1, 0)
n_overlap = int(np.sum(overlap))

# Create border arrays
borders_red = np.empty((3 * n_overlap, 2), dtype=object)
borders_blue = np.empty((3 * (n_samples - n_overlap), 2), dtype=object)
borders_red.fill(None)
borders_blue.fill(None)

# Assign borders
borders_red[::3], borders_red[1::3] = borders_right[overlap == 1], borders_left[overlap == 1]
borders_blue[::3], borders_blue[1::3] = borders_right[overlap == 0], borders_left[overlap == 0]

# Create overlap arrays
overlaps = np.tile(overlap, (3,)).reshape(-1)
idx_red, idx_blue = np.cumsum(overlaps), np.cumsum(1 - overlaps)


# Create figure and add vertical lines
fig = make_subplots(rows=1, cols=1)
[fig.add_shape(type="line", x0=i, x1=i, y0=-3, y1=3, line=dict(color="black", width=2))
 for i in range(int(np.round(xmin)), int(np.round(xmax)) + 1)]

# Plot configuration
i = 20
for borders, idx, color, name in [(borders_red, idx_red, "red", "Avec intersection"),
                                 (borders_blue, idx_blue, "darkblue", "Sans intersection")]:
    fig.add_trace(go.Scatter(
        x=borders[:idx[3*i]+1, 0], y=borders[:idx[3*i]+1, 1],
        mode="lines", line=dict(width=2), marker=dict(color=color),
        name=name, visible=True
    ))

# Update layout
fig.update_layout(
    template="simple_white",
    xaxis=dict(range=[xmin, xmax], constrain="domain", showgrid=False, visible=False),
    yaxis=dict(
        scaleanchor="x", 
        visible=False,
        range=[-2.5, 2.5]  # Adjusted range to move figure up
    ),
    legend=dict(x=0.5, y=0.80, xanchor='center', yanchor='top'),  # Moved legend higher
    margin=dict(t=20)  # Reduced top margin
)
fig.show()
```

:::

:::

:::{.column width="40%"}

:::{.fragment fragment-index=1}

[Georges-Louis Leclerc, Comte de Buffon](https://fr.wikipedia.org/wiki/Georges-Louis_Leclerc_de_Buffon)(1707-1788) :
 naturaliste, mathématicien et industriel français du siècle des Lumières<img src="https://upload.wikimedia.org/wikipedia/commons/b/b5/Georges-Louis_Leclerc_de_Buffon.jpg" width="65%" style="display: block; margin-right: auto; margin-left: auto;" alt="Portrait de Georges-Louis Leclerc, comte de Buffon.
Huile sur toile de François-Hubert Drouais, Montbard, musée Buffon." title="Portrait de Georges-Louis Leclerc, comte de Buffon.
Huile sur toile de François-Hubert Drouais, Montbard, musée Buffon."></img>
:::

:::

::::
<!-- ----------------------------------------------------------------------- -->



<!-- ----------------------------------------------------------------------- -->
## L'aiguille de Buffon {{< iconify ph:needle >}}{{< iconify ph:needle >}}
<hr>

::: {.fragment}

**Problème initial**: une aiguille de taille 1 tombe sur un parquet composé de lattes de largeur $1$: quelle est alors la probabilité $P$ que l'aiguille croise une ligne de la trame du parquet ?
:::

<br>

::: {.fragment}
**Réponse probabiliste**:
$$
P = \frac{2}{\pi} \approx 0.6366 \enspace
$$
Une preuve de ce résultat est donnée [ici](http://josephsalmon.github.io/HAX603X/perspective_historique.html)
:::
<!-- ----------------------------------------------------------------------- -->


<!-- ----------------------------------------------------------------------- -->
## Principe de Monte Carlo et estimation (I)
<hr>

Idée sous-jacente de Buffon :

si l'on répète cette expérience un grand nombre de fois, on peut approcher la quantité $P$ numériquement, par exemple en proposant un estimateur $\hat{P}_n$ qui compte la proportion de chevauchement, après avoir effectué $n$ répétitions des lancers

<br>
Estimation de $\pi$:

$$
\pi \approx \frac{2}{\hat{P}_n}
$$
<!-- ----------------------------------------------------------------------- -->


<!-- ----------------------------------------------------------------------- -->
## Principe de Monte Carlo et estimation  (II)
<hr>

<br>

```{python}

#| echo: false
#| layout-ncol: 1

# Create figure and add elements
fig1 = make_subplots(rows=2, cols=1, vertical_spacing=0.1, row_heights=[2, 1])
[fig1.add_shape(type="line", x0=i, x1=i, y0=-3, y1=3, 
                line=dict(color="black", width=2), row=1, col=1)
 for i in range(int(np.round(xmin)), int(np.round(xmax)) + 1)]

# Calculate values
n_samples_array = np.arange(1, n_samples + 1)
pi_estimate = 2 / (np.cumsum(overlap) / n_samples_array)

# Add traces for each iteration
for i in range(3, n_samples):
    for data, color, name in [(borders_red, "red", "Avec intersection"),
                             (borders_blue, "darkblue", "Sans intersection")]:
        idx = idx_red if color == "red" else idx_blue
        fig1.add_trace(go.Scatter(
            x=data[:idx[3*i]+1, 0], y=data[:idx[3*i]+1, 1],
            mode="lines", line=dict(width=2), marker=dict(color=color),
            name=name, visible=False), row=1, col=1)

    fig1.add_trace(go.Scatter(
        x=n_samples_array[:i], y=pi_estimate[:i],
        mode="lines", line=dict(width=1), marker=dict(color="red"),
        showlegend=False, visible=False), row=2, col=1)

# Add annotations and shapes
for ann in [dict(x=0.04, y=0.19, text="Estimation de pi", font=dict(color="red")),
           dict(x=0.99, y=0.19, text="pi")]:
    fig1.add_annotation(**ann, xref="paper", yref="paper", showarrow=False)

fig1.add_shape(type="line", x0=0, x1=n_samples, y0=np.pi, y1=np.pi,
               line=dict(color="black", width=1, dash="dashdot"), row=2, col=1)

# Update layout
fig1.update_layout(
    template="none",
    xaxis=dict(range=[xmin, xmax], constrain="domain", showgrid=False, visible=False),
    yaxis_scaleanchor="x", yaxis_visible=False,
    xaxis2=dict(showgrid=True, zeroline=True, zerolinewidth=3, range=[-0.001, n_samples]),
    yaxis2=dict(showgrid=True, zeroline=True, zerolinewidth=2, range=[0., 6]),
    legend=dict(x=0.5, y=0.91, xanchor='center', yanchor='bottom')
)

# Set initial visible state
[setattr(fig1.data[11*3 + i], 'visible', True) for i in range(3)]

# Create slider
steps = [dict(
    label=str(ii + 4),
    method="update",
    args=[{"visible": [True if j in range(3*ii, 3*ii+3) else False 
           for j in range(len(fig1.data))],},
          {"title": f"Estimation avec {ii+4} aiguilles: pi = {pi_estimate[ii]:.4f}"}]
) for ii in range(len(fig1.data)//3)]

fig1.update_layout(sliders=[dict(
    active=6, currentvalue={"prefix": "Nombre d'aiguilles: "},
    pad={"t": 50}, y=-0.1, steps=steps
)])

fig1.show()
```
<!-- ----------------------------------------------------------------------- -->


<!-- ----------------------------------------------------------------------- -->

## Méthode de Monte-Carlo
<hr>

Méthode de calcul numérique qui consiste à utiliser des nombres aléatoires pour résoudre des problèmes déterministes, plus particulièrement en grande dimension (et pas uniquement en dimension 1)

<br>

Domaines d'applications:

 - la physique
 - la chimie
 - la biologie
 - la finance
 - l'apprentissage automatique
<!-- ----------------------------------------------------------------------- -->


<!-- ----------------------------------------------------------------------- -->
## {{< fa circle-radiation title="circle radiation" >}}  Naissance de la méthode
<hr>

::::{.columns}

:::{.column width="70%"}

:::{.fragment fragment-index=1}

- Lieu: Los Alamos, New Mexico, USA
- Époque: seconde guerre mondial
- Contexte: **Projet Manhattan**, produire une bombe atomique
- Besoins: modéliser les réactions nucléaires en chaîne (combinatoires)

:::


::: {layout-ncol=3}
:::{.fragment fragment-index=3}
![[John von Neumann](https://fr.wikipedia.org/wiki/John_von_Neumann) (1903-1957), mathématicien et physicien américano-hongrois, un des pères de l'informatique](https://upload.wikimedia.org/wikipedia/commons/d/d6/JohnvonNeumann-LosAlamos.jpg){height=200}
:::

:::{.fragment fragment-index=4}
![[Nicholas Metropolis](https://fr.wikipedia.org/wiki/Nicholas_Metropolis) (1915-1999), physicien gréco-américain, un des initiateurs de la méthode de Monte Carlo et du recuit simulé](https://upload.wikimedia.org/wikipedia/commons/5/56/Nicholas_Metropolis_cropped.PNG){height=200}
:::

:::{.fragment fragment-index=5}
![[Stanisław Ulam](https://fr.wikipedia.org/wiki/Stanislaw_Ulam) (1909-1984),  mathématicien polono-américain, un des initiateurs de la méthode de Monte Carlo et de la propulsion nucléaire pulsée](https://upload.wikimedia.org/wikipedia/commons/thumb/8/82/Stanislaw_Ulam.tif/lossy-page1-413px-Stanislaw_Ulam.tif.jpg){height=200}
:::

:::

:::

:::{.column width="30%"}

:::{.fragment fragment-index=2}

![Explosion de Trinity (16 Juillet 1945)](https://upload.wikimedia.org/wikipedia/commons/f/fc/Trinity_Detonation_T%26B.jpg){width=100%}

:::

:::

::::
<!-- ----------------------------------------------------------------------- -->



<!-- ----------------------------------------------------------------------- -->
## {{< iconify game-icons:rolling-dices >}} Origine du nom "Monte-Carlo"
<hr>

Initialement: besoin de confidentialité du projet Manhattan

<br>

Monte-Carlo: connue pour ses jeux de hasard, où l'oncle de Stanisław Ulam aimait se rendre pour assouvir sa soif de jeu.

<br>
Ce serait N. Metropolis qui aurait proposé ce nom, cf. [@metropolis1987beginning]:


En: *It was at that time that I suggested an obvious name for the statistical method—a suggestion not unrelated to the fact that Stan had an uncle who would borrow money from relatives because he “just had to go to Monte Carlo”.*

Fr: *C'est à ce moment-là que j'ai suggéré un nom évident pour la méthode statistique—une suggestion qui n'était pas sans rapport avec le fait que Stan avait un oncle qui empruntait de l'argent à ses proches parce qu'il "devait absolument aller à Monte Carlo".*

<!-- ----------------------------------------------------------------------- -->



<!-- ----------------------------------------------------------------------- -->
## {{< iconify fluent:arrow-growth-24-filled >}} Essor de la méthode de Monte Carlo
<hr>


::::{.columns}

:::{.column width="60%"}

:::{.fragment fragment-index=1}

- Popularisation croissante:

  - Essor de l'informatique (depuis les années 80)
  - Essor du calcul parallèle (GPUs, clusters, etc.)

:::

:::{.fragment fragment-index=2}

- Domaines principaux impactés:

  - Finance : évaluation des prix de produits dérivés
  - Apprentissage automatique: utilisation de l'aléatoire pour générer des scénarios

    [[Exemples]{.underline}: Alphago (2016)]{.fragment fragment-index=3}[, AlphaGeometry (2024)]{.fragment fragment-index=4}
:::

:::

:::{.column width="40%"}

:::{.fragment fragment-index=3}
  ![](../Slides/images/go-board-19x19-stones.svg){width=50%}
:::

:::{.fragment fragment-index=4}

  ![](../Slides/images/alphageometry.png){width=50%}
:::


:::

::::
<br>


:::{.fragment fragment-index=5}

Recherche arborescente Monte-Carlo (:gb: : *Monte Carlo tree search*): analyse des scénarios les plus prometteurs, en élargissant l'arbre de recherche sur la base d'un échantillonnage aléatoire de l'espace entier (ingrédient important d'AlphaGo)

:::


# Bibliographie


<!-- ----------------------------------------------------------------------- -->
## {{< iconify emojione-monotone:books >}} Références
<hr>

::: {#refs}
:::
<!-- ----------------------------------------------------------------------- -->

