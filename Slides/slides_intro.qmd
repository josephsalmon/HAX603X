---
title: "HAX603X: Modélisation stochastique"
subtitle: "Introduction"
---


# ChatGPT, Myths and Realities

# Generalities on Machine Learning and Artificial Intelligence

<!-- ## Historical perspective on AI

AI was all about symbolic reasoning until the 1980s: culmination in “Expert Systems”, which are abandoned:

- based on a fixed and very hard to improve set of rules
- fails miserably in “edge” case and with outliers data
- relies on a bunch of expert domains and general reliability is very hard to assess

$\Rightarrow$ an alternative neuro-computing branch of AI, but…

. . .

… in mid ’80s they seem to have both failed to deliver on their promises. -->

## Historical perspective timeline

::::{layout-ncol="2"}

<!-- - 1950s: perceptron [@rosenblatt1958perceptron] -->
<!-- - 1960s: perceptron convergence theorem [@minsky1969introduction] -->
<!-- - 1970s-1980s: backpropagation [@rumelhart1986learning] -->
<!-- - 1990s: SVM [@cortes1995support] -->
<!-- - 2000s: Boosting [@freund1997decision], Random Forests [@breiman2001random] -->
- 2010s: Deep Learning

::::

## AI domains