---
title: "Loi normale: cas univarié"
subtitle: "HAX603X: Modélisation stochastique"
format:
  revealjs:
    toc: true
    template-partials:
        - toc-slide.html
    include-after-body: toc-add.html
---

# Définitions et propriétés de la loi normale/gaussienne


## Rappel concernant la loi normale

::::{.columns}

:::{.column width="60%"}

Pour $\mu \in \mathbb{R}$ et $\nu > 0$, on note $X \sim \mathcal{N}(\mu, \nu)$, si $X$ est une variable aléatoire ayant pour densité $\varphi_{\mu, \nu}$:

$$
\forall x \in \mathbb{R}, \quad	\varphi_{\mu, \nu}(x)=\frac{1}{\sqrt{2 \pi \nu}}\exp\Big(-\frac{(x-\mu)^2}{2\nu}\Big)\enspace.
$$

:::{.fragment fragment-index=1}

- Espérance: $X$ a pour **espérance** $\mu$, $\mathbb{E}(X)=\mu$,
- Variance: $X$ a pour **variance** $\nu$, $\mathbb{V}(X)=\nu$.
- Cas particulier $\mu=0$ et $\nu=1$ correspond à une variable aléatoire dite **centrée réduite**.

:::

:::


:::{.column width="35%"}

:::{.fragment fragment-index=2}

On parle aussi de loi gaussienne, en hommage au mathématicien [Carl Friedrich Gauss, *le prince des mathématiciens*](https://fr.wikipedia.org/wiki/Carl_Friedrich_Gauss): (1777-1855)
mathématicien, astronome et physicien né à Brunswick, directeur de l'observatoire de [Göttingen](https://www.youtube.com/watch?v=t0sNy1xOhRc) de 1807 jusqu'à sa mort en 1855
<img src="https://upload.wikimedia.org/wikipedia/commons/9/9b/Carl_Friedrich_Gauss.jpg" width="45%" style="display: block; margin-right: auto; margin-left: auto;" alt="Portrait de Carl Friedrich Gauss" title="Tableau de Christian Albrecht Jensen (Moscou, Musée des Beaux-Arts Pouchkine)."></img>

:::

:::

:::


## Visualisation de la densité de la loi normale

 voir [https://josephsalmon.github.io/HAX603X/Courses/loi_normale1D.html](https://josephsalmon.github.io/HAX603X/Courses/loi_normale1D.html)



## Propriétés de la loi normale

:::{.incremental}
- **stabilité par transformation affine** :

  si $X \sim \mathcal{N}(\mu, \nu)$ et si $(\alpha,\beta) \in \mathbb{R}^* \times \mathbb{R}$, alors $\alpha X + \beta \sim \mathcal{N}(\alpha\mu + \beta, \alpha^2 \nu)$.
  - si $X \sim \mathcal{N}(0,1)$, alors $\sqrt{\nu} X + \mu \sim \mathcal{N}(\mu, \nu)$,
  - si $X \sim \mathcal{N}(\mu, \nu)$, alors $(X-\mu)/\sqrt{\nu} \sim \mathcal{N}(0,1)$.
:::

. . .

- [Conséquence]{.underline}: pour simuler selon une loi normale, il suffit de savoir le faire pour le cas centré-réduit, puis d'utiliser la propriété ci-dessus


## Fonction caractéristique

:::{#prp-caract}

# Fonction caractéristique de la loi normale
La fonction caractéristique d'une variable aléatoire $X \sim \mathcal{N}(\mu, \nu)$ est donnée pour tout $t \in \mathbb{R}$ par
$$
\begin{align*}
\phi_{\mu,\nu}(t) & \triangleq \mathbb{E}(e^{i t X}) = \exp\Big( i \mu t - \frac{\nu t^2}{2}\Big)\enspace.
\end{align*}
$$
:::

[Cas particulier]{.underline}: si $X \sim \mathcal{N}(0,1)$, alors $\phi_{0,1}(t) = \exp\Big( - \frac{t^2}{2}\Big)$

. . .


*Éléments de preuve*: pour tout $z \in \mathbb{R}$, on calcule la transformée de Laplace, puis on l'étend ensuite sur $\mathbb{C}$, et on l'instancie pour $z=it$.

. . .

$$
\begin{align*}
\class{fragment}{{}\mathbb{E}[e^{zX}]}& 
\class{fragment}{{}=\frac{1}{\sqrt{2\pi}}\int_{-\infty}^\infty e^{-\frac12x^2}e^{zx}\,dx} \class{fragment}{{}= \frac{e^{\frac12z^2}}{\sqrt{2\pi}}\int_{-\infty}^\infty e^{-\frac12(x-z)^2}\,dx}\\
&\class{fragment}{{}=\frac{e^{\frac12z^2}}{\sqrt{2\pi}}\int_{-\infty}^\infty e^{-\frac12y^2}\,dy}\class{fragment}{{}
=e^{\frac12z^2}}
\end{align*}
$$




# Simulation d'une loi normale


## Idée naïves pour simuler une loi normale

- Méthode de l'inverse: besoin d'un calcul de la fonction de répartition de la loi normale, qui n'a pas de forme analytique simple (analyse numérique, méthode coûteuse).

- TCL: tirer $U_1, \dots, U_n$  i.i.d. et uniforme sur $[0,1]$, puis poser
$$
	\sqrt{n}\frac{(\bar{U}_n - 1/2)}{\sqrt{1/12}}
$$
[Limite]{.underline}: seulement une approximation, et convergence relativement lente (coût élevé)

- Alternatives: nécessite opérations de changement de variables


## Changement de variables en dimension 2

Soit $\phi$ un $C^1$-difféomorphisme de $\mathbb{R}^2$ (bijection dont la réciproque est également de classe $C^1$)

. . .

[Rappel]{.underline}: la **jacobienne** de $\phi^{-1}$ correspond à la matrice (application linéaire) des dérivées partielles.
Ainsi, si $\phi(x,y) = (u,v) \iff (x,y) = \phi^{-1}(u,v)$, alors
$$
\begin{align*}
{\rm{J}}_{\phi^{-1}}: (u,v) & \mapsto
\begin{pmatrix}
  \frac{\partial x}{\partial u} & \frac{\partial x}{\partial v}    \\
  \frac{\partial y}{\partial u} & \frac{\partial y}{\partial v}
\end{pmatrix} \in \mathbb{R}^{2\times 2}
\end{align*}
$$

. . .

::: {#thm-changement-variables}

## Caractérisation de la loi d'une variable aléatoire réelle

Soit $(X,Y)$ un vecteur aléatoire de densité $f_{(X,Y)}$ définie sur l'ouvert $A \subset \mathbb{R}^2$ et $\phi : A \to B \subset \mathbb{R}^2$ un $C^1$-difféomorphisme. Le vecteur aléatoire $(U,V)=\phi(X,Y)$ admet alors pour densité $f_{(U,V)}$ définie sur $B$ pour tout $(u,v) \in \mathbb{R}^2$ par
$$
\begin{align*}
    (u,v) & \mapsto
    f_{(X,Y)} (\phi^{-1}(u,v)) |\det ({\rm{J}}_{\phi^{-1}} (u,v))| {1\hspace{-3.8pt} 1}_B(u,v)
\end{align*}
$$

:::

[Remarque]{.underline}: le résultat s'étend facilement en dimension supérieure


## Preuve


[Rappel]{.underline}: la loi de $(U,V)$ est caractérisée par $\mathbb{E}[h(U,V)]$ pour tout $h$ mesurable bornée.

. . .

Soit un tel $h$ et on applique la formule de transfert :
$$
\begin{align*}
  \mathbb{E}[h(U,V)] & = \mathbb{E}[h(\phi(X,Y))] = \int_{\mathbb{R}^2} h(\phi(x,y)) f_{(X,Y)}(x,y) \, dx dy \\
 & = \int_{A} h(\phi(x,y)) f_{(X,Y)}(x,y) \, d x d y\enspace.
\end{align*}
$$

. . .

On applique alors la formule du changement de variables $(u,v) = \phi(x,y) \iff \phi^{-1}(u,v) = (x,y)$ :
$$
\begin{align*}
   \mathbb{E}[h(U,V)] &
  = \!\int_{B}  \!\!\! h(u,v) f_{(X,Y)}(\phi^{-1}(u,v)) |\det ({\rm{J}}_{\phi^{-1}} (u,v))| \, d u d v\\
  & = \!\int_{\mathbb{R}^2} \!\!\!\! h(u,v) f_{(X,Y)}(\phi^{-1}(u,v)) |\det ({\rm{J}}_{\phi^{-1}} (u,v))| {1\hspace{-3.8pt} 1}_B(u,v)\, d u d v .
\end{align*}
$$
ce qui donne le résultat voulu.


## Méthode de Box-Müller


L'algorithme de Box-Müller est le suivant: si $U$ et $V$ sont des v.a. indépendantes de loi uniforme sur $[0,1]$ et qu'on définit $X$ et $Y$ par
$$
\begin{cases}
  X = \sqrt{-2 \log(U)} \cos(2\pi V)\\
  Y = \sqrt{-2 \log(U)} \sin(2\pi V)\,.
\end{cases}
$$
alors $X$ et $Y$ des variables aléatoires gaussiennes centrées réduites indépendantes.



## Preuve de la méthode de Box-Müller
$$
	\begin{array}{ccccc}
		\phi^{-1} & : & ]0, \infty[ \times ]0, 2\pi[ & \to     & &\mathbb{R}^2 \setminus ([0,\infty[ \times \{0\})  \\
		          &   & ( r , \theta)                   & \mapsto && (r \cos(\theta) , r \sin(\theta))  \\
		\phi & : & \mathbb{R}^2 \setminus ([0,\infty[ \times \{0\}) & \to  && ]0, \infty[ \times ]0, 2\pi[                                                       \\
		     &   & ( x, y )                                            & \mapsto && (\sqrt{x^2+y^2} , 2 \arctan \Big( \frac{y}{x+\sqrt{x^2+y^2}} \Big)
	\end{array}
$$


::: {#thm-box-muller}

## Méthode de Box-Müller

Soit $X$ et $Y$ deux v.a. indépendantes $X,Y \sim \mathcal{N}(0,1)$. Le couple de variables aléatoires polaires $(R, \Theta) = \phi(X,Y)$ a pour densité
$$
			f_{R, \Theta}(r,\theta)
			= \Big( r \cdot e^{-\tfrac{r^2}{2}} {1\hspace{-3.8pt} 1}_{]0, \infty[}(r) \Big) \bigg(\frac{{1\hspace{-3.8pt} 1}_{]0, 2 \pi[}(\theta)}{2 \pi} \bigg)\,.
$$
$R$ et $\Theta$ sont indépendantes, $\Theta \sim \mathcal{U}(]0, 2\pi[)$, $R$ suit une loi de Rayleigh de densité
$$
    f_R(r) =  r \cdot e^{-r^2/2} {1\hspace{-3.8pt} 1}_{]0, \infty[}(r)\,, \quad r > 0\,.
$$

:::

## Preuve de la méthode de Box-Müller (suite)

:::{#lem-rayleigh}

## Simulation selon la loi de Rayleigh
Si $U$ est une variable aléatoire de loi uniforme sur $]0,1[$, alors 
$\sqrt{-2 \log(U)}$ suit une loi de Rayleigh.
:::

<br>

**Preuve**: Pour tout $x > 0$, $F_R(x)=\mathbb{P}(R\leq x) = 1-\exp(-\tfrac{x^2}{2})$, et donc pour tout $q \in ]0,1[, F_R^{^\leftarrow}(q)=\sqrt{-2\log(1-q)}$.
Ainsi par la méthode de l'inverse,
$\sqrt{-2\log(1-U)}$ suit est une v.a. distribuée selon la loi de Rayleigh, et donc aussi $\sqrt{-2\log(U)}$.

. . .

<br>

Enfin, on prouve le bien fondé de la méthode de Box-Müller en utilisant le lemme ci-dessus, et en notant que $~U\sim\mathcal{U}[0,1] \implies 2 \pi U\sim\mathcal{U}[0,2\pi]$

## Alternatives

- l'algorithme de Box-Müller n'est pas utilisé si souvent en pratique (evaluation de fonctions coûteuses: logarithme, cosinus, sinus).
- Pour s’affranchir des fonctions trigonométriques, une version modifiée de l’algorithme de Box-Müller a été proposée : la méthode de Marsaglia, qui s’appuie sur des variables aléatoires uniformes sur le disque unité (voir l’exercice dédié en TD).
- Une autre alternative est la méthode de Ziggurat implémentée dans la librairie `numpy`, notamment.

# Lois autour de la loi normale


## Loi du $\chi^2$

Concernant la prononciation, on prononce "khi-deux" le nom de cette loi.


::: {#def-chi2}

## Loi du $\chi^2$


Soit $X_1, \dots, X_k$ des variables aléatoires i.I.d. de loi normale centrée réduite. La loi de la variable aléatoire $X = X_1^2 + \dots + X_k^2$ est appelée **loi du $\chi^2$ à $k$ degrés de liberté**. Sa densité est donnée par
$$
f(x) = \frac{1}{2^{\frac{k}{2}}\Gamma(\frac{k}{2})} x^{\frac{k}{2}-1} e^{-x/2}\,, \quad x \geq 0\,,
$$
où $\Gamma$ désigne la fonction gamma d'Euler :
$$
\Gamma(x) = \int_0^{\infty} t^{x-1} e^{-t}\,  dt\,.
$$
On note alors $X \sim \chi^2(k)$.

:::



## Loi de Student

::::{.columns}

:::{.column width="60%"}

::: {#def-student}

## Loi de Student

Soit $X \sim \mathcal{N}(0,1)$ et $Y \sim \chi^2(k)$ deux variables aléatoires indépendantes. La loi de la variable aléatoire $V = \frac{X}{\sqrt{Y/k}}$ est appelée **loi de Student à $k$ degrés de liberté**. Elle admet pour densité
$$
	f_V(t)
	= \dfrac{1}{\sqrt{k \pi}} \dfrac{\Gamma(\frac{k+1}{2})}{\Gamma(\frac{k}{2})} \Big(1+\dfrac{t^2}{k}\Big)^{-\frac{k+1}{2}}\,,
	\quad t \in \mathbb{R}\,.
$$

:::


:::{.fragment fragment-index=2}

[Application]{.underline}: elle est utilisée en statistiques pour déterminer l'intervalle de confiance de l’espérance d'une loi normale, quand la variance est inconnue (en lien avec le [théorème de Cochran](https://fr.wikipedia.org/wiki/Théorème_de_Cochran))
:::

:::

:::{.column width="35%"}


:::{.fragment fragment-index=3}

Cette loi a été décrite en 1908 par
[William Gosset](https://fr.wikipedia.org/wiki/William_Gosset): (1876-1937)
statisticien et chimiste anglais. Il était employé à la brasserie Guinness à Dublin. Son employeur lui refusant le droit de publier sous son propre nom, W. Gosset choisit un pseudonyme, *Student* (&#x1F1EB;&#x1F1F7;: étudiant).
<img src="https://upload.wikimedia.org/wikipedia/commons/4/42/William_Sealy_Gosset.jpg" width="45%" style="display: block; margin-right: auto; margin-left: auto;" alt="Photo William Gosset" title="Photographie de William Gosset, prise en 1908."></img>

:::

:::

:::




## Loi de Cauchy

::::{.columns}

:::{.column width="60%"}

::: {#def-cauchy}

## Loi de Cauchy standard

Une v.a. $X$ suit une **loi de Cauchy standard** si sa densité est donnée par
$$
	f_X(x) = \dfrac{1}{\pi(1+x^2)}\,, \quad x \in \mathbb{R}\,.
$$
On note alors $X\sim \mathcal{C}(0,1)$ dans ce cas.
:::



:::{.fragment fragment-index=2}

[Application]{.underline}: Loi souvent utile comme contre-exemple, n'ayant ni espérance (ni variance a fortiori), et ne satisfaisant pas la loi des grands nombres ou le TCL.


:::

:::

:::{.column width="35%"}


:::{.fragment fragment-index=3}

::: {style="font-size: 80%;"}

Loi étudiée en particulier par
[Augustin-Louis Cauchy](https://fr.wikipedia.org/wiki/Augustin_Louis_Cauchy): (1789-1857)
mathématicien et physicien français, connu pour ses travaux fondateurs en analyse complexe et dans l'étude du groupe des permutations.
<img src="https://upload.wikimedia.org/wikipedia/commons/7/7f/Augustin_Louis_Cauchy.jpg" width="45%" style="display: block; margin-right: auto; margin-left: auto;" alt="Photo Augustin-Louis Cauchy" title="Photographie d'Augustin-Louis Cauchy par C. H. Reutlinger."></img>

cf. [@Stigler74] pour plus de détails historiques sur cette loi
:::


:::	

:::

:::






## Loi de Cauchy: fonctions caractéristiques



::: {#def-cauchy}

## Loi de Cauchy
On dit que $Y$ suit une loi de Cauchy de paramètres $(\mu,\sigma)\in \mathbb{R} \times ]0,+\infty[$ si $Y=\mu + \sigma X$, où $X$ suit une loi de Cauchy standard. On note alors $X\sim \mathcal{C}(0,1)$ dans ce cas, et la densité de $Y$ est donnée par
$$
	f_Y(y) = \dfrac{1}{\sigma \pi(1 + \tfrac{1}{\sigma^2}\left(y-\mu\right)^2)}\,, \quad y \in \mathbb{R}\,.
$$

:::


::: {#prp-cauchy}

## Loi de Cauchy et fonction caractéristique


La fonction caractéristique de la loi de Cauchy standard est donnée par
$$
\begin{align*}
\varphi_X(t) & \triangleq \int_{\mathbb{R}} e^{itx} f_X(x) \, dx = e^{-|t|}\,.
\end{align*}
$$
et donc si $Y\sim \mathcal{C}(\mu,\sigma)$, alors pour tout $t \in \mathbb{R}$, $\varphi_Y(t) = e^{i\mu t - \sigma |t|}$.

:::

Pour la preuve voir par exemple [Exemple III.5.5., @Barbe_Ledoux06]


## Loi de Cauchy : stabilité par somme

[Implications]{.underline}:

- la somme de deux variables aléatoires indépendantes de loi de Cauchy est de Cauchy: Si $X_1 \sim \mathcal{C}(\mu_1,\sigma_2)$ et $X_2 \sim \mathcal{C}(\mu_2,\sigma_2)$ sont indépendantes, alors $X_1+X_2 \sim \mathcal{C}(\mu_1+\mu_2,\sigma_1+\sigma_2)$ ([preuve]{.underline}: même fonction caractéristique).

. . .

- la moyenne de variables de Cauchy standard i.i.d suit la loi de Cauchy standard: si $X_1, \ldots, X_n$ sont i.i.d de loi de Cauchy standard alors  $\bar{X}_n \sim \mathcal{C}(0,1)$ ([preuve]{.underline}: pour tout $t \in \mathbb{R}$, $\varphi_{\bar{X}_n}(t) = e^{-|t|}$)


[Conclusion]{.underline}: la moyenne empirique de v.a. $\mathcal{C}(0,1)$ i.i.d ne converge pas en probabilité vers une constante!



## Loi de Cauchy: propriété

::: {#prp-cauchy}

## Loi de Cauchy et loi normale

Soient $X$ et $Y$ deux variables aléatoires indépendantes de loi normale centrée réduite. Alors, $Y/X$ suit une loi de Cauchy standard.

:::

[Remarque]{.underline}: la méthode d'inversion permet aussi de simuler une variable aléatoire de loi de Cauchy (cf. TD/TP).

. . .

<br>

[Conséquence]{.underline}: $X\sim \mathcal{C}(0,1) \implies 1/X \sim \mathcal{C}(0,1)$

## Preuve (rapport de variables gaussiennes):


*Preuve*:
Comme pour la loi de Student, on démontre ce résultat avec un changement de variables. On considère l'application
$$
	\begin{array}{ccccc}
		\phi & : & \mathbb{R}^* \times \mathbb{R} & \to     & \mathbb{R}^* \times \mathbb{R} \\
				&   & (x,y) & \mapsto & \Big(x, \dfrac{y}{x}\Big)\\
        \phi^{-1} & :  & \mathbb{R}^* \times \mathbb{R} & \to     & \mathbb{R}^* \times \mathbb{R}\\
				&   & (u, v) & \mapsto & \Big(u, uv)
\end{array}
$$

$$
	J_{\phi^{-1}} (u,v)
	=
	\begin{pmatrix}
		1 & 0 \\
		v & u
	\end{pmatrix}\,,
$$
et son déterminant vaut $u$. Le reste est calculatoire et laissé en exercice.

## Bibliographie

::: {#refs}
:::