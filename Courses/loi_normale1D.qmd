---
title: "Loi normale: cas 1D"
format:
  html:
      out.width: 50%
filters:
  - shinylive
---

On considère ici $\mathbb{R}^d$ muni du produit scalaire euclidien $\langle \cdot, \cdot \rangle$ et de la norme euclidienne $\|\cdot\|$ associée.


## La loi normale

### Définitions et propriétés

On rappelle que la loi normale de paramètres $\mu \in \mathbb{R}$ et $\nu > 0$ a une densité donnée pour tout $x \in \mathbb{R}$ par

$$
	\varphi_{\mu, \nu}(x)=\frac{1}{\sqrt{2 \pi \nu}}\exp\Big(-\frac{(x-\mu)^2}{2\nu}\Big)\enspace.
$$

On note $X \sim \mathcal{N}(\mu, \nu)$, si $X$ est une variable aléatoire ayant pour densité $\varphi_{\mu, \nu}$. Notons que si $X \sim \mathcal{N}(\mu,\nu)$, alors $X$ a pour **espérance** $\mu$ et pour **variance** $\nu$. Le cas particulier $\mu=0$ et $\nu=1$ correspond à une variable aléatoire dite **centrée réduite**.


La loi normale vérifie la propriété de **stabilité par transformation affine** : si $X \sim \mathcal{N}(\mu, \nu)$ et si $(a,b) \in \mathbb{R}^* \times \mathbb{R}$, alors la variable aléatoire $a X + b$ suit une loi normale $\mathcal{N}(a\mu + b, a^2 \nu)$. On peut donc facilement passer d'une loi normale centrée réduite à une loi normale quelconque via une transformation affine :

- si $X \sim \mathcal{N}(0,1)$, alors $\sqrt{\nu} X + \mu \sim \mathcal{N}(\mu, \nu)$,
- si $X \sim \mathcal{N}(\mu, \nu)$, alors $(X-\mu)/\sqrt{\nu} \sim \mathcal{N}(0,1)$.

Ainsi, savoir simuler une loi normale centrée réduite, permet de simuler n'importe quelle loi normale.

Rappelons enfin que la fonction caractéristique d'une variable aléatoire $X \sim \mathcal{N}(\mu, \nu)$ est donnée pour tout $t \in \mathbb{R}$ par
$$
\begin{align*}
\phi_{\mu, \nu}(t) & \triangleq \mathbb{E}(e^{i t X})  \\
\phi_X(t) & = \exp\Big( i \mu t - \dfrac{\nu t^2}{2}\Big)\enspace.
\end{align*}
$$


### Simulation d'une loi  normale

::: {.callout-note appearance="simple"}
## Une mauvaise piste pour simuler une loi normale


On peut simuler une loi normale à partir de variables aléatoires uniformes $U_1, \ldots, U_n$  iid en appliquant le théorème central limite à
$$
	\dfrac{U_1 + \cdots + U_n - n/2}{\sqrt{n/12}}\,.
$$
Cependant, cette méthode ne donne qu'une approximation d'une loi normale. Par ailleurs, la vitesse de convergence étant relativement lente (de l'ordre de $\sqrt n$), il faudra simuler beaucoup de variables aléatoires uniformes pour avoir une approximation correcte, ce qui demande un temps de calcul assez élevé.

:::


#### Changement de variables

Le théorème suivant permet de passer de la loi d'un couple $(X,Y)$ à celle de $(U,V) = \phi(X,Y)$, où $\phi$ est un $C^1$-difféomorphisme, c'est-à-dire une application bijective dont la réciproque est également de classe $C^1$.

Pour cela rappelons que la **jacobienne** de $\phi^{-1}$ correspond à la matrice (application linéaire) des dérivées partielles. Ainsi, si $\phi(x,y) = (u,v) \iff (x,y) = \phi^{-1}(u,v)$, alors
$$
{\rm{Jac}}~\phi^{-1} (u,v)
=
\begin{pmatrix}
  \frac{\partial x}{\partial u} & \frac{\partial x}{\partial v}    \\
  \frac{\partial y}{\partial u} & \frac{\partial y}{\partial v}
\end{pmatrix} \enspace.
$$

::: {#thm-changement-variables}


## Caractérisation de la loi d'une variable aléatoire réelle

Soit $(X,Y)$ un vecteur aléatoire de densité $f_{(X,Y)}$ définie sur l'ouvert $A \subset \mathbb{R}^2$ et $\phi : A \to B \subset \mathbb{R}^2$ un $C^1$-difféomorphisme. Le vecteur aléatoire $(U,V)=\phi(X,Y)$ admet alors pour densité $f_{(U,V)}$ définie sur $B$ pour tout $(u,v) \in \mathbb{R}^2$ par
$$
\begin{align*}
  f_{(U,V)} (u,v)
  = f_{(X,Y)} (\phi^{-1}(u,v)) |\det ({\rm{Jac}}~\phi^{-1} (u,v))| {1\hspace{-3.8pt} 1}_B(u,v)\enspace.
\end{align*}
$$

:::

On a énoncé le résultat en dimension $2$ par simplicité. Il s'étend bien évidemment à une dimension $d$ quelconque. En particulier, pour $d=1$, on retrouve le changement de variable classique dans le cas de l'intégration d'une fonction à valeurs réelles.

*Démonstration.*

On rappelle que la loi de $(U,V)$ est caractérisée par les quantités $\mathbb{E}[h(U,V)]$ pour tout $h : \mathbb{R}^2 \to \mathbb{R}$ mesurable bornée. On considère donc une telle fonction $h$ et on applique la formule de transfert :
$$
\begin{align*}
  \mathbb{E}[h(U,V)] &
  =\mathbb{E}[h(\phi(X,Y))]\\
 & = \int_{\mathbb{R}^2} h(\phi(x,y)) f_{(X,Y)}(x,y) \, dx dy \\
 & = \int_{A} h(\phi(x,y)) f_{(X,Y)}(x,y) \, d x d y\enspace.
\end{align*}
$$
On applique alors la formule du changement de variables vu en théorie de l'intégration avec $(u,v) = \phi(x,y) \iff \phi^{-1}(u,v) = (x,y)$ :
$$
  \mathbb{E}[h(U,V)]
  = \int_{B} h(u,v)) f_{(X,Y)}(\phi^{-1}(u,v)) |\det ({\rm{Jac}}~\phi^{-1} (u,v))| \, d u d v\,.
$$
On conclut alors que
$$
\mathbb{E}[h(U,V)]
= \int_{\mathbb{R}^2} h(u,v) f_{(X,Y)}(\phi^{-1}(u,v)) |\det ({\rm{Jac}}~\phi^{-1} (u,v))| {1\hspace{-3.8pt} 1}_B(u,v)\, d u d v\,,
$$
ce qui donne le résultat voulu.


<div class="container">
<div class="div1"> </div>
<div class="div-remaining">□</div>
</div>



:::{#exm-cos}

## Exemple : loi de $\cos(X)$, avec $X \sim \mathcal{U}(]0,\pi[)$
Donnons un exemple dans le cas réel. On considère une variable aléatoire $X$ de loi uniforme sur $]0,\pi[$.
Sa densité est donnée par $f_X(x) = {1\hspace{-3.8pt} 1}_{]0,\pi[}(x)/\pi$.
On pose $U = \cos(X)$ et on souhaite déterminer la loi de $U$.

On applique le théorème précédent avec la fonction $\phi^{-1}(u) = \arccos(u)$ sur $]-1,1[$. La densité de $U$ est alors donnée pour tout $u \in \mathbb{R}$ par
$$
\begin{align*}
  f_U(u)
 & = \dfrac{{1\hspace{-3.8pt} 1}_{]0,\pi[}(\arccos(u))}{\pi} \Big| \dfrac{-1}{\sqrt{1-u^2}} \Big| {1\hspace{-3.8pt} 1}_{]-1,1[}(u)\\
 & = \dfrac{1}{\pi \sqrt{1-u^2}} {1\hspace{-3.8pt} 1}_{]-1,1[}(u)\enspace.
\end{align*}
$$


:::


## Méthode de Box-Müller

Un cas particulier fondamental de la formule de changement de variables concerne le passage en coordonnées polaires. Cette transformation est définie via l'application
$$
	\begin{array}{ccccc}
		\phi^{-1} & : & ]0, \infty[ \times ]0, 2\pi[ & \to     & \mathbb{R}^2 \setminus ([0,\infty[ \times \{0\}) \\
		          &   & \begin{pmatrix} r \\ \theta \end{pmatrix}                   & \mapsto & \begin{pmatrix} r \cos(\theta) \\ r \sin(\theta) \end{pmatrix}\,.
	\end{array}
$$
L'expression de $\phi$ ne nous sera pas utile. On peut tout de même la donner au passage :

$$
  	\begin{array}{ccccc}
		\phi & : & \mathbb{R}^2 \setminus ([0,\infty[ \times \{0\}) & \to     & ]0, \infty[ \times ]0, 2\pi[                                                      \\
		     &   & \begin{pmatrix} x \\ y \end{pmatrix}                                            & \mapsto & \begin{pmatrix}\sqrt{x^2+y^2} \\ 2 \arctan \Big( \frac{y}{x+\sqrt{x^2+y^2}} \Big)\end{pmatrix}\,.
	\end{array}
$$

Ici, le jacobien de $\phi^{-1}$ est la matrice
$$
	{\rm{Jac}} \phi^{-1} (r,\theta)
	=
	\begin{pmatrix}
		\cos(\theta) & -r \sin(\theta) \\
		\sin(\theta) & r \cos(\theta)
	\end{pmatrix}\,,
$$
qui vérifie $|\det({\rm{Jac}} \phi^{-1} (r, \theta))| = r$. Ainsi, si $(X,Y)$ a pour densité $f_{(X,Y)}$, alors $(R, \Theta) = \phi(X,Y)$ a pour densité
$$
f_{(R, \Theta)} (r, \theta)
= f_{(X,Y)}(r \cos(\theta), r \sin(\theta)) \cdot r \cdot {1\hspace{-3.8pt} 1}_{]0, \infty[}(r) \cdot {1\hspace{-3.8pt} 1}_{]0, 2 \pi[}(\theta)\,.
$$

Dans le cas où $X$ et $Y$ sont des variables aléatoires gaussiennes indépendantes, on obtient le résultat suivant.

::: {#thm-box-muller}

## Méthode de Box-Müller

Soit $X$ et $Y$ deux variables aléatoires indépendantes de loi normales centrées réduites : $X,Y \sim \mathcal{N}(0,1)$. Le couple de variables aléatoires polaires $(R, \Theta) = \phi^{-1}(X,Y)$ a pour densité
$$
			f_{R, \Theta}(r,\theta)
			= \Big( r \cdot e^{-\tfrac{r^2}{2}} {1\hspace{-3.8pt} 1}_{]0, \infty[}(r) \Big) \bigg(\dfrac{{1\hspace{-3.8pt} 1}_{]0, 2 \pi[}(\theta)}{2 \pi} \bigg)\,.
$$
Autrement dit, elles sont indépendantes, l'angle $\Theta$ suit une loi uniforme sur $]0, 2\pi[$ et la distance à l'origine $R$ suit une loi de Rayleigh donnée par la densité
$$
    f_R(r) =  r \cdot e^{-r^2/2} {1\hspace{-3.8pt} 1}_{]0, \infty[}(r)\,, \quad r > 0\,.
$$

:::

*Démonstration:*
La densité du couple $(X,Y)$ est donnée par
$$
  f_{(X,Y)}(x,y) = \dfrac{1}{2\pi} e^{-\frac{x^2+y^2}{2}}\,, \quad x,y \in \mathbb{R}\,.
$$
Le théorème précédent donne alors la densité de $(R, \Theta)$ :
$$
\begin{align*}
  f_{(R, \Theta)} &
  = f_{(X,Y)}(r \cos(\theta), r \sin(\theta)) \cdot r \cdot {1\hspace{-3.8pt} 1}_{]0, \infty[}(r) \cdot {1\hspace{-3.8pt} 1}_{]0, 2 \pi[}(\theta)\\
  &= \dfrac{1}{2\pi} e^{-\frac{r^2}{2}} \cdot r \cdot {1\hspace{-3.8pt} 1}_{]0, \infty[}(r) \cdot {1\hspace{-3.8pt} 1}_{]0, 2 \pi[}(\theta)\,,
\end{align*}
$$
ce qui conclut la preuve.
<div class="container">
<div class="div1"> </div>
<div class="div-remaining">□</div>
</div>

Notons que $R^2$ est à valeurs dans $]0,\infty[$ et vérifie pour $x > 0$:
$$
\begin{align*}
	\mathbb{P}(R^2 > x)
	&= \mathbb{P}(R > \sqrt x)\\
	&= \int_{\sqrt{x}}^\infty r e^{-r^2/2}\, \mathrm dr\\
	&= \Big[- e^{-r^2/2} \Big]_{\sqrt{x}}^\infty\\
	&= e^{-x/2}\,.
\end{align*}
$$
On reconnaît le fonction de survie d'une loi exponentielle de paramètre $1/2$. Or, on a vu que si $U$ suit une loi uniforme sur $[0,1]$, alors $-2 \ln(U)$ suit une loi exponentielle de paramètre $1/2$, donc $\sqrt{-2 \ln(U)}$ a la même loi que $R$.

L'algorithme de Box-Müller s'en suit: si $U$ et $V$ sont des v.a. indépendantes de loi uniforme sur $[0,1]$ et qu'on définit $X$ et $Y$ par
$$
\begin{cases}
  X = \sqrt{-2 \ln(U)} \cos(2\pi V)\\
  Y = \sqrt{-2 \ln(U)} \sin(2\pi V)\,.
\end{cases}
$$
alors $X$ et $Y$ des variables aléatoires gaussiennes centrées réduites indépendantes.

:::{.callout-note}

## Note
Cet algorithme n'est en fait pas souvent utilisé en pratique : il fait appel à des fonctions dont l'évaluation est coûteuse (logarithme, cosinus, sinus).
Pour s'affranchir des fonctions trigonométriques, une version modifiée de l'algorithme de Box-Müller a été proposée : la méthode de Marsaglia, qui s'appuie sur des variables aléatoires uniformes sur le disque unité (voir l'exercice dédié en TD). Une autre alternative est la méthode de Ziggurat.
:::