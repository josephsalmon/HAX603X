---
title: "Loi normale: cas 1D"
format:
  html:
      out.width: 50%
filters:
  - shinylive
---

On considère ici $\mathbb{R}^d$ muni du produit scalaire euclidien $\langle \cdot, \cdot \rangle$ et de la norme euclidienne $\|\cdot\|$ associée.


## La loi normale

## Définitions et propriétés de la loi normale

On rappelle que la loi normale de paramètres $\mu \in \mathbb{R}$ et $\nu > 0$ a une densité donnée pour tout $x \in \mathbb{R}$ par

$$
	\varphi_{\mu, \nu}(x)=\frac{1}{\sqrt{2 \pi \nu}}\exp\Big(-\frac{(x-\mu)^2}{2\nu}\Big)\enspace.
$$

On parle aussi souvent de loi gaussienne, en hommage au mathématicien [Carl Friedrich Gauss, *le prince des mathématiciens*](https://fr.wikipedia.org/wiki/Carl_Friedrich_Gauss)^[[Carl Friedrich Gauss](https://fr.wikipedia.org/wiki/Carl_Friedrich_Gauss): (1777-1855)
mathématicien, astronome et physicien né à Brunswick, directeur de l'observatoire de [Göttingen](https://www.youtube.com/watch?v=t0sNy1xOhRc) de 1807 jusqu'à sa mort en 1855
<img src="https://upload.wikimedia.org/wikipedia/commons/9/9b/Carl_Friedrich_Gauss.jpg" width="65%" style="display: block; margin-right: auto; margin-left: auto;" alt="Portrait de Carl Friedrich Gauss" title="Tableau de Christian Albrecht Jensen (Moscou, Musée des Beaux-Arts Pouchkine)."></img>].

On note $X \sim \mathcal{N}(\mu, \nu)$, si $X$ est une variable aléatoire ayant pour densité $\varphi_{\mu, \nu}$. Notons que si $X \sim \mathcal{N}(\mu,\nu)$, alors $X$ a pour **espérance** $\mu$ et pour **variance** $\nu$. Le cas particulier $\mu=0$ et $\nu=1$ correspond à une variable aléatoire dite **centrée réduite**.


La loi normale vérifie la propriété de **stabilité par transformation affine** : si $X \sim \mathcal{N}(\mu, \nu)$ et si $(a,b) \in \mathbb{R}^* \times \mathbb{R}$, alors la variable aléatoire $a X + b$ suit une loi normale $\mathcal{N}(a\mu + b, a^2 \nu)$. On peut donc facilement passer d'une loi normale centrée réduite à une loi normale quelconque via une transformation affine :

- si $X \sim \mathcal{N}(0,1)$, alors $\sqrt{\nu} X + \mu \sim \mathcal{N}(\mu, \nu)$,
- si $X \sim \mathcal{N}(\mu, \nu)$, alors $(X-\mu)/\sqrt{\nu} \sim \mathcal{N}(0,1)$.

Ainsi, savoir simuler une loi normale centrée réduite, permet de simuler n'importe quelle loi normale.

Rappelons enfin que la fonction caractéristique d'une variable aléatoire $X \sim \mathcal{N}(\mu, \nu)$ est donnée pour tout $t \in \mathbb{R}$ par
$$
\begin{align*}
\phi_{\mu, \nu}(t) & \triangleq \mathbb{E}(e^{i t X})  \\
\phi_X(t) & = \exp\Big( i \mu t - \frac{\nu t^2}{2}\Big)\enspace.
\end{align*}
$$


### Simulation d'une loi  normale

::: {.callout-note appearance="simple"}
## Une mauvaise piste pour simuler une loi normale


On peut simuler une loi normale à partir de variables aléatoires uniformes $U_1, \dots, U_n$  iid en appliquant le théorème central limite à
$$
	\frac{U_1 + \cdots + U_n - n/2}{\sqrt{n/12}}\,.
$$
Cependant, cette méthode ne donne qu'une approximation d'une loi normale. Par ailleurs, la vitesse de convergence étant relativement lente (de l'ordre de $\sqrt n$), il faudra simuler beaucoup de variables aléatoires uniformes pour avoir une approximation correcte, ce qui demande un temps de calcul assez élevé.

:::


#### Changement de variables

Le théorème suivant permet de passer de la loi d'un couple $(X,Y)$ à celle de $(U,V) = \phi(X,Y)$, où $\phi$ est un $C^1$-difféomorphisme, c'est-à-dire une application bijective dont la réciproque est également de classe $C^1$.

Pour cela rappelons que la **jacobienne** de $\phi^{-1}$ correspond à la matrice (application linéaire) des dérivées partielles. Ainsi, si $\phi(x,y) = (u,v) \iff (x,y) = \phi^{-1}(u,v)$, alors
$$
\begin{align*}
{\rm{J}}_{\phi^{-1}}: (u,v) & \mapsto
\begin{pmatrix}
  \frac{\partial x}{\partial u} & \frac{\partial x}{\partial v}    \\
  \frac{\partial y}{\partial u} & \frac{\partial y}{\partial v}
\end{pmatrix} \enspace.
\end{align*}
$$

::: {#thm-changement-variables}


## Caractérisation de la loi d'une variable aléatoire réelle

Soit $(X,Y)$ un vecteur aléatoire de densité $f_{(X,Y)}$ définie sur l'ouvert $A \subset \mathbb{R}^2$ et $\phi : A \to B \subset \mathbb{R}^2$ un $C^1$-difféomorphisme. Le vecteur aléatoire $(U,V)=\phi(X,Y)$ admet alors pour densité $f_{(U,V)}$ définie sur $B$ pour tout $(u,v) \in \mathbb{R}^2$ par
$$
\begin{align*}
    (u,v) & \mapsto
    f_{(X,Y)} (\phi^{-1}(u,v)) |\det ({\rm{J}}_{\phi^{-1}} (u,v))| {1\hspace{-3.8pt} 1}_B(u,v)\enspace.
\end{align*}
$$

:::

On a énoncé le résultat en dimension $2$ par simplicité. Il s'étend bien évidemment à une dimension $d$ quelconque. En particulier, pour $d=1$, on retrouve le changement de variable classique dans le cas de l'intégration d'une fonction à valeurs réelles.

*Démonstration.*

On rappelle que la loi de $(U,V)$ est caractérisée par les quantités $\mathbb{E}[h(U,V)]$ pour tout $h : \mathbb{R}^2 \to \mathbb{R}$ mesurable bornée. On considère donc une telle fonction $h$ et on applique la formule de transfert :
$$
\begin{align*}
  \mathbb{E}[h(U,V)] &
  =\mathbb{E}[h(\phi(X,Y))]\\
 & = \int_{\mathbb{R}^2} h(\phi(x,y)) f_{(X,Y)}(x,y) \, dx dy \\
 & = \int_{A} h(\phi(x,y)) f_{(X,Y)}(x,y) \, d x d y\enspace.
\end{align*}
$$
On applique alors la formule du changement de variables vu en théorie de l'intégration avec $(u,v) = \phi(x,y) \iff \phi^{-1}(u,v) = (x,y)$ :
$$
\begin{align*}
  & \mathbb{E}[h(U,V)]\\
  & = \!\int_{B}  \!\!\! h(u,v) f_{(X,Y)}(\phi^{-1}(u,v)) |\det ({\rm{J}}_{\phi^{-1}} (u,v))| \, d u d v\\
  & = \!\int_{\mathbb{R}^2} \!\!\!\! h(u,v) f_{(X,Y)}(\phi^{-1}(u,v)) |\det ({\rm{J}}_{\phi^{-1}} (u,v))| {1\hspace{-3.8pt} 1}_B(u,v)\, d u d v .
\end{align*}
$$
<!-- On conclut alors que
$$
\mathbb{E}[h(U,V)]
= \int_{\mathbb{R}^2} h(u,v) f_{(X,Y)}(\phi^{-1}(u,v)) |\det ({\rm{J}}_{\phi^{-1}} (u,v))| {1\hspace{-3.8pt} 1}_B(u,v)\, d u d v\,,
$$ -->
ce qui donne le résultat voulu.
<div class="container">
<div class="div1"> </div>
<div class="div-remaining">□</div>
</div>

<br>

:::{#exm-cos}

## Exemple : loi de $\cos(X)$, avec $X \sim \mathcal{U}(]0,\pi[)$
Donnons un exemple dans le cas réel. On considère une variable aléatoire $X$ de loi uniforme sur $]0,\pi[$.
Sa densité est donnée par $f_X(x) = {1\hspace{-3.8pt} 1}_{]0,\pi[}(x)/\pi$.
On pose $U = \cos(X)$ et on souhaite déterminer la loi de $U$.

On applique le théorème précédent avec la fonction $\phi^{-1}(u) = \arccos(u)$ sur $]-1,1[$. La densité de $U$ est alors donnée pour tout $u \in \mathbb{R}$ par
$$
\begin{align*}
  f_U(u)
 & = \frac{{1\hspace{-3.8pt} 1}_{]0,\pi[}(\arccos(u))}{\pi} \Big| \frac{-1}{\sqrt{1-u^2}} \Big| {1\hspace{-3.8pt} 1}_{]-1,1[}(u)\\
 & = \frac{1}{\pi \sqrt{1-u^2}} {1\hspace{-3.8pt} 1}_{]-1,1[}(u)\enspace.
\end{align*}
$$


:::


## Méthode de Box-Müller

Un cas particulier fondamental de la formule de changement de variables concerne le passage en coordonnées polaires. Cette transformation est définie via l'application
$$
	\begin{array}{ccccc}
		\phi^{-1} & : & ]0, \infty[ \times ]0, 2\pi[ & \to     & \mathbb{R}^2 \setminus ([0,\infty[ \times \{0\}) \\
		          &   & \begin{pmatrix} r \\ \theta \end{pmatrix}                   & \mapsto & \begin{pmatrix} r \cos(\theta) \\ r \sin(\theta) \end{pmatrix}\,.
	\end{array}
$$
L'expression de $\phi$ ne nous sera pas utile. On peut tout de même la donner au passage :

$$
  	\begin{array}{ccccc}
		\phi & : & \mathbb{R}^2 \setminus ([0,\infty[ \times \{0\}) & \to     & ]0, \infty[ \times ]0, 2\pi[                                                      \\
		     &   & \begin{pmatrix} x \\ y \end{pmatrix}                                            & \mapsto & \begin{pmatrix}\sqrt{x^2+y^2} \\ 2 \arctan \Big( \frac{y}{x+\sqrt{x^2+y^2}} \Big)\end{pmatrix}\,.
	\end{array}
$$

Ici, le jacobien de $\phi^{-1}$ est la matrice
$$
	{\rm{J}}_{\phi^{-1}} (r,\theta)
	=
	\begin{pmatrix}
		\cos(\theta) & -r \sin(\theta) \\
		\sin(\theta) & r \cos(\theta)
	\end{pmatrix}\,,
$$
qui vérifie $|\det({\rm{J}}_{\phi^{-1}} (r, \theta))| = r$. Ainsi, si $(X,Y)$ a pour densité $f_{(X,Y)}$, alors $(R, \Theta) = \phi(X,Y)$ a pour densité
$$
  f_{(R, \Theta)} (r, \theta)
  = r\cdot f_{(X,Y)}(r \cos(\theta), r \sin(\theta)) \!\cdot\! {1\hspace{-3.8pt} 1}_{]0, \infty[}(r)  {1\hspace{-3.8pt} 1}_{]0, 2 \pi[}(\theta).
$$

Dans le cas où $X$ et $Y$ sont des variables aléatoires gaussiennes indépendantes, on obtient le résultat suivant.

::: {#thm-box-muller}

## Méthode de Box-Müller

Soit $X$ et $Y$ deux variables aléatoires indépendantes de loi normales centrées réduites : $X,Y \sim \mathcal{N}(0,1)$. Le couple de variables aléatoires polaires $(R, \Theta) = \phi^{-1}(X,Y)$ a pour densité
$$
			f_{R, \Theta}(r,\theta)
			= \Big( r \cdot e^{-\tfrac{r^2}{2}} {1\hspace{-3.8pt} 1}_{]0, \infty[}(r) \Big) \bigg(\frac{{1\hspace{-3.8pt} 1}_{]0, 2 \pi[}(\theta)}{2 \pi} \bigg)\,.
$$
Autrement dit, elles sont indépendantes, l'angle $\Theta$ suit une loi uniforme sur $]0, 2\pi[$ et la distance à l'origine $R$ suit une loi de Rayleigh donnée par la densité
$$
    f_R(r) =  r \cdot e^{-r^2/2} {1\hspace{-3.8pt} 1}_{]0, \infty[}(r)\,, \quad r > 0\,.
$$

:::

*Démonstration:*
La densité du couple $(X,Y)$ est donnée par
$$
  f_{(X,Y)}(x,y) = \frac{1}{2\pi} e^{-\frac{x^2+y^2}{2}}\,, \quad x,y \in \mathbb{R}\,.
$$
Le théorème précédent donne alors la densité de $(R, \Theta)$ :
$$
\begin{align*}
  f_{(R, \Theta)} (r, \theta) &
  = r\cdot f_{(X,Y)}(r \cos(\theta), r \sin(\theta)) \!\cdot\! {1\hspace{-3.8pt} 1}_{]0, \infty[}(r)  {1\hspace{-3.8pt} 1}_{]0, 2 \pi[}(\theta)\\
  &= r \cdot\frac{1}{2\pi} e^{-\frac{r^2}{2}} \cdot  {1\hspace{-3.8pt} 1}_{]0, \infty[}(r) {1\hspace{-3.8pt} 1}_{]0, 2 \pi[}(\theta)\,,
\end{align*}
$$
ce qui conclut la preuve.
<div class="container">
<div class="div1"> </div>
<div class="div-remaining">□</div>
</div>

Notons que $R^2$ est à valeurs dans $]0,\infty[$ et vérifie pour $x > 0$:
$$
\begin{align*}
	\mathbb{P}(R^2 > x)
	&= \mathbb{P}(R > \sqrt x)\\
	&= \int_{\sqrt{x}}^{\infty} r e^{-r^2/2}\, dr\\
	&= \Big[- e^{-\tfrac{r^2}{2}} \Big]_{\sqrt{x}}^{\infty}\\
	&= e^{-\tfrac{x}{2}}\,.
\end{align*}
$$
On reconnaît le fonction de survie d'une loi exponentielle de paramètre $1/2$. Or, on a vu (cf. méthode de l'inverse) que si $U$ suit une loi uniforme sur $[0,1]$, alors $-2 \ln(U)$ suit une loi exponentielle de paramètre $1/2$, donc $\sqrt{-2 \ln(U)}$ a la même loi que $R$.

L'algorithme de Box-Müller s'en suit: si $U$ et $V$ sont des v.a. indépendantes de loi uniforme sur $[0,1]$ et qu'on définit $X$ et $Y$ par
$$
\begin{cases}
  X = \sqrt{-2 \ln(U)} \cos(2\pi V)\\
  Y = \sqrt{-2 \ln(U)} \sin(2\pi V)\,.
\end{cases}
$$
alors $X$ et $Y$ des variables aléatoires gaussiennes centrées réduites indépendantes.

:::{.callout-note}

## Note
Cet algorithme n'est en fait pas souvent utilisé en pratique : il fait appel à des fonctions dont l'évaluation est coûteuse (logarithme, cosinus, sinus).
Pour s'affranchir des fonctions trigonométriques, une version modifiée de l'algorithme de Box-Müller a été proposée : la méthode de Marsaglia, qui s'appuie sur des variables aléatoires uniformes sur le disque unité (voir l'exercice dédié en TD). Une autre alternative est la méthode de Ziggurat.
:::


```{ojs}
//| echo: false
//| layout-ncol: 1
//| width: 50%

// Plotly = require("https://cdn.plot.ly/plotly-latest.min.js")
Plotly = require('plotly.js-dist');
dists = require( 'https://cdn.jsdelivr.net/gh/stdlib-js/stats-base-dists@umd/browser.js' );
jstat = require('jstat');
math = require("mathjs");
// see source here: https://github.com/stdlib-js/stats-base-dists/tree/umd
// continuous case

viewof inputs = Inputs.form([
  Inputs.range([-2, 2], {label: tex`\mu`, step: 0.1}, {value: 0}),
  Inputs.range([0.1, 10], {label: tex`\nu`, step: 0.1, value: 1}),
  Inputs.range([1, 1000], {label: tex`n`, step: 1}),
  Inputs.button("Re-Tirage")
])


mu = inputs[0];
nu = inputs[1];
n_samples = inputs[2];

{

function mvnpdf(x, mu, nu){
    return 1.0 / (2*math.pi *nu)**(0.5)* math.exp(-0.5*(x-mu)**2 / nu);
}


function normal_rng(mu, nu, n=100){
    var samples = Array.from({length: n}, () => {
        var x = jstat.normal.sample(mu, nu**0.5);
        return x;
    });

    return samples;
}

var samples = normal_rng(mu, nu, 1000);
var samples_jitter = normal_rng(0, 0.03, 1000)
var npoints=500, mini = -10, maxi=10, x = new Array(npoints), z = new Array(npoints), i, j;

//  Densité:
for(var i = 0; i < npoints; i++) {
    x[i] = mini + i * (maxi - mini) / (npoints - 1);
	z[i] = mvnpdf(x[i], mu, nu);
    }

{
var trace1 = {
        x: samples.slice(0, n_samples),
        y: samples_jitter.slice(0, n_samples),
        mode: 'markers',
        type: 'scatter',
        marker: {
            color: 'rgba(0,0,0,0.5)',
            size: 5,
        },

  }

var trace22 = {
        x: x,
        y: z,
		type: "scatter",
		mode: "lines",
		name: 'Pdf',
		line: {color: 'black'},
		yaxis: 'y2',
		xaxis: 'x2',
        }


var data = [
  trace1,
  trace22,
  ];


var layout = {

	yaxis: {domain: [0, 0.2],
			showticklabels: false,
			range: [-0.6, 0.6],
            autorange: false},
	xaxis2: {matches: 'x',
	          range: [-10, 10],
          autorange: false},
	yaxis2: {domain: [0.29, 0.99]},

  showlegend: false,

};

    var config = {responsive: true}
    const div = DOM.element('div');
    Plotly.newPlot(div, data, layout, config);
    return div;
	}
}
```


## Lois autour de la loi normale


### Loi du $\chi^2$

Concernant la prononciation, on prononce "khi-deux" le nom de cette loi.

::: {#def-chi2}

## Loi du $\chi^2$


Soit $X_1, \dots, X_k$ des variables aléatoires iid de loi normale centrée réduite. La loi de la variable aléatoire $X = X_1^2 + \dots + X_k^2$ est appelée **loi du $\chi^2$ à $k$ degrés de liberté**. Sa densité est donnée par
$$
f(x) = \frac{1}{2^{k/2}\Gamma(\frac{k}{2})} x^{k/2-1} e^{-x/2}\,, \quad x \geq 0\,,
$$
où $\Gamma$ désigne la fonction gamma d'Euler :
$$
\Gamma(x) = \int_0^{\infty} t^{x-1} e^{-t}\,  dt\,.
$$
On note alors $X \sim \chi^2(k)$.

:::

Au vu de sa définition, la simulation d'une loi du $\chi^2$ est claire : on simule $k$ variables aléatoires gaussiennes centrées réduites indépendantes et on somme leur carrés.


*Preuve*:
Montrons pour $k=1$ que la densité est bien de la forme précédente, c'est-à-dire
$$
	f(x) = \frac{1}{\sqrt{2\pi}} \frac{e^{-x/2}}{\sqrt x}\,, \quad x \geq 0\,,
$$
où on a utilisé la relation $\Gamma(1/2) = \sqrt \pi$ (intégrale de Gauss).

Soit $h : \mathbb{R} \to \mathbb{R}$ une fonction mesurable bornée. On a
$$
\begin{align*}
	\mathbb{E}[h(X_1^2)]
	& = \int_\mathbb{R} h(x^2) \frac{e^{-x^2/2}}{\sqrt{2 \pi}} \, dx\\
	& = \int_{-\infty}^0 h(x^2) \frac{e^{-x^2/2}}{\sqrt{2 \pi}} \, dx
	+ \int_0^{\infty} h(x^2) \frac{e^{-x^2/2}}{\sqrt{2 \pi}} \, dx\,.
\end{align*}
$$
En effectuant le changement de variable $x=-\sqrt u$ dans la première intégrale et $x=\sqrt u$ dans la deuxième, on obtient
$$
	\mathbb{E}[h(X_1^2)]
	= \int_{\infty}^0 h(u) \frac{e^{-u/2}}{\sqrt{2 \pi}} \frac{ du}{2 \sqrt u}
	+ \int_0^{\infty} h(u) \frac{e^{-u/2}}{\sqrt{2 \pi}} \frac {du}{2 \sqrt u}\,.
$$
Les deux intégrales étant égales, on conclut que
$$
	\mathbb{E}[h(X_1^2)] = \int_0^{\infty} h(u) \frac{e^{-u/2}}{\sqrt{2 \pi}\sqrt u}\, du\,,
$$
ce qui prouve le résultat pour $k=1$.

La généralisation à $k$ quelconque se fait par récurrence: on utilise la formule de convolution des la loi pour obtenir la loi pour $k+1$:

$$
\begin{align*}
	X_1^2 + \dots + X_k^2 + X_{k+1}^2
	& = (X_1^2 + \dots + X_k^2) + X_{k+1}^2\\
	& = \chi^2(k) + X_{k+1}^2\,.
\end{align*}
$$
Ainsi,
$$
\begin{align*}
	f_{\chi^2(k+1)}(x)
	& = \int_0^{\infty} f_{\chi^2(k)}(x-y) f_{X_{k+1}^2}(y) \, dy\\
	& = \int_0^x \frac{1}{2^{k/2}\Gamma(\frac{k}{2})} (x-y)^{k/2-1} e^{-\frac{x-y}{2}} \tfrac{e^{-y/2}}{\sqrt{2\pi y}} \, dy\\
	& = \frac{e^{-\frac{x}{2}}}{2^{\frac{k+1}{2}} \Gamma(\frac{k}{2})  \Gamma(\frac{1}{2})}\int_0^x  (x-y)^{k/2-1}  \tfrac{1}{\sqrt{ y}} \, dy\\
	& = \frac{e^{-\frac{x}{2}}}{2^{\frac{k+1}{2}} \Gamma(\frac{k}{2})  \Gamma(\frac{1}{2})} x \int_0^1  (x-ux)^{k/2-1}  \tfrac{1}{\sqrt{xu}} \, du\\
\end{align*}
$$
avec le changement de variable $y=ux$. Ensuite,
$$
\begin{align*}
	f_{\chi^2(k+1)}(x)
	& = \frac{x^{\frac{k+1}{2}} e^{-\frac{x}{2}}}{2^{\frac{k+1}{2}} \Gamma(\frac{k}{2})  \Gamma(\frac{1}{2})}  \int_0^1  (1-u)^{k/2-1}  u^{1/2-1} du\enspace.
\end{align*}
$$
Or rappelons que si $\Beta(a,b) =  \int_0^1  (1-u)^{a-1}  u^{b-1} \, du$, alors pour tout $a,b \in [0,+\infty[, \Beta(a,b)=\frac{\Gamma(a)\Gamma(b)}{\Gamma(a+b)}$. En effet, en faisant le changement de variable $t+s=r$ et $t=rw$ dans l'intégrale double suivante, on obtient:
$$
\begin{align*}
\Gamma(a)\Gamma(b) & = \int_0^{\infty} t^{a-1} e^{-t} \, dt \int_0^{\infty} s^{b-1} e^{-s} \, ds\\
& = \int_0^{\infty} \int_0^{\infty} e^{-t-s} t^{a-1} s^{b-1} \, dt \, ds\\
& = \int_0^{1} \int_0^{\infty} e^{-r} (rw)^{a-1} (r(1-w))^{b-1} r \, dr \, dw\\
& = \int_0^1 w^{a-1} (1-w)^{b-1} \int_0^{\infty} e^{-r} r^{a+b-1} \, dr \, dw\\
\end{align*}
$$
et donc  $\Beta(a,b)=\frac{\Gamma(a)\Gamma(b)}{\Gamma(a+b)}$.
En appliquant cette relation pour $a=k/2$ et $b=1/2$, on obtient

$$
\begin{align*}
	f_{\chi^2(k+1)}(x)
	& = \frac{x^{\frac{k+1}{2}} e^{-\frac{x}{2}}}{2^{\frac{k+1}{2}} \Gamma(\frac{k+1}{2})} \enspace.
\end{align*}
$$
Le résultat est donc prouvé par récurrence.
<div class="container">
<div class="div1"> </div>
<div class="div-remaining">□</div>
</div>

<br>