---
title: "Loi normale: cas univarié"
bibliography: ../Courses/references.bib
filters:
  - shinylive
---



## Définitions et propriétés de la loi normale

On rappelle que la loi normale de paramètres $\mu \in \mathbb{R}$ et $\nu > 0$ a une densité donnée pour tout $x \in \mathbb{R}$ par
$$
	\varphi_{\mu, \nu}(x)=\frac{1}{\sqrt{2 \pi \nu}}\exp\Big(-\frac{(x-\mu)^2}{2\nu}\Big)\enspace.
$$

On note $X \sim \mathcal{N}(\mu, \nu)$, si $X$ est une variable aléatoire ayant pour densité $\varphi_{\mu, \nu}$. Notons que si $X \sim \mathcal{N}(\mu,\nu)$, alors $X$ a pour **espérance** $\mu$ et pour **variance** $\nu$. Le cas particulier $\mu=0$ et $\nu=1$ correspond à une variable aléatoire dite **centrée réduite**.

::: {.content-visible when-format="html"}

```{ojs}
//| echo: false
//| layout-ncol: 1
//| figalign: "center"

// Plotly = require("https://cdn.plot.ly/plotly-latest.min.js")
Plotly = require('plotly.js-dist');
dists = require( 'https://cdn.jsdelivr.net/gh/stdlib-js/stats-base-dists@umd/browser.js' );
jstat = require('jstat');
math = require("mathjs");
// see source here: https://github.com/stdlib-js/stats-base-dists/tree/umd
// continuous case

viewof inputs = Inputs.form([
  Inputs.range([-2, 2], {label: tex`\mu`, step: 0.1}, {value: 0}),
  Inputs.range([0.1, 10], {label: tex`\nu`, step: 0.1, value: 1}),
  Inputs.range([1, 1000], {label: tex`n`, step: 1}),
  Inputs.button("Re-Tirage")
])

mu = inputs[0];
nu = inputs[1];
n_samples = inputs[2];

{

function mvnpdf(x, mu, nu){
    return 1.0 / (2*math.pi *nu)**(0.5)* math.exp(-0.5*(x-mu)**2 / nu);
}


function normal_rng(mu, nu, n=100){
    var samples = Array.from({length: n}, () => {
        var x = jstat.normal.sample(mu, nu**0.5);
        return x;
    });

    return samples;
}

var samples = normal_rng(mu, nu, 1000);
var samples_jitter = normal_rng(0, 0.03, 1000);
var npoints=500, mini = -10, maxi=10, x = new Array(npoints), z = new Array(npoints), i, j;

//  Densité:s
for(var i = 0; i < npoints; i++) {
    x[i] = mini + i * (maxi - mini) / (npoints - 1);
	z[i] = mvnpdf(x[i], mu, nu);
    }

{
var trace1 = {
        x: samples.slice(0, n_samples),
        y: samples_jitter.slice(0, n_samples),
        mode: 'markers',
        type: 'scatter',
        marker: {
            color: 'rgba(0,0,0,0.5)',
            size: 5,
        },

  }

var trace22 = {
        x: x,
        y: z,
		type: "scatter",
		mode: "lines",
		name: 'Pdf',
		line: {color: 'black'},
		yaxis: 'y2',
		xaxis: 'x2',
        }


var data = [
  trace1,
  trace22,
  ];


var layout = {
	width: 600,
	yaxis: {domain: [0, 0.2],
			showticklabels: false,
			range: [-0.6, 0.6],
            autorange: false},
	xaxis2: {matches: 'x',
	          range: [-10, 10],
          autorange: false},
	yaxis2: {domain: [0.29, 0.99]},

  showlegend: false,

};

    var config = {responsive: true}
    const div = DOM.element('div');
    Plotly.react(div, data, layout, config);
    return div;
	}
}
```
:::

On parle aussi souvent de loi gaussienne, en hommage au mathématicien [Carl Friedrich Gauss, *le prince des mathématiciens*](https://fr.wikipedia.org/wiki/Carl_Friedrich_Gauss)^[[Carl Friedrich Gauss](https://fr.wikipedia.org/wiki/Carl_Friedrich_Gauss): (1777-1855)
mathématicien, astronome et physicien né à Brunswick, directeur de l'observatoire de [Göttingen](https://www.youtube.com/watch?v=t0sNy1xOhRc) de 1807 jusqu'à sa mort en 1855
<img src="https://upload.wikimedia.org/wikipedia/commons/9/9b/Carl_Friedrich_Gauss.jpg" width="65%" style="display: block; margin-right: auto; margin-left: auto;" alt="Portrait de Carl Friedrich Gauss" title="Tableau de Christian Albrecht Jensen (Moscou, Musée des Beaux-Arts Pouchkine)."></img>
].



La loi normale vérifie la propriété de **stabilité par transformation affine** : si $X \sim \mathcal{N}(\mu, \nu)$ et si $(a,b) \in \mathbb{R}^* \times \mathbb{R}$, alors la variable aléatoire $a X + b$ suit une loi normale $\mathcal{N}(a\mu + b, a^2 \nu)$. On peut donc facilement passer d'une loi normale centrée réduite à une loi normale quelconque via une transformation affine :

- si $X \sim \mathcal{N}(0,1)$, alors $\sqrt{\nu} X + \mu \sim \mathcal{N}(\mu, \nu)$,
- si $X \sim \mathcal{N}(\mu, \nu)$, alors $(X-\mu)/\sqrt{\nu} \sim \mathcal{N}(0,1)$.

Ainsi, savoir simuler une loi normale centrée réduite, permet de simuler n'importe quelle loi normale.

:::{#prp-caract}

## Fonction caractéristique de la loi normale
La fonction caractéristique d'une variable aléatoire $X \sim \mathcal{N}(\mu, \nu)$ est donnée pour tout $t \in \mathbb{R}$ par

\begin{align*}
\phi_{\mu,\nu}(t) & \triangleq \mathbb{E}(e^{i t X})  \\
& = \exp\Big( i \mu t - \frac{\nu t^2}{2}\Big)\enspace.
\end{align*}

:::


::: {.proof}
on remarque d'abord que si $X \sim \mathcal{N}(0,1)$ alors pour tout $z \in \mathbb{R}$, on a


\begin{align*}
\mathbb{E}[e^{zX}]&=\frac{1}{\sqrt{2\pi}}\int_{-\infty}^\infty e^{-\frac12x^2}e^{zx}\,dx\\
&= \frac{e^{\frac12z^2}}{\sqrt{2\pi}}\int_{-\infty}^\infty e^{-\frac12(x-z)^2}\,dx\\
&=\frac{e^{\frac12z^2}}{\sqrt{2\pi}}\int_{-\infty}^\infty e^{-\frac12y^2}\,dy\\
&=e^{\frac12z^2}.
\end{align*}

En utilisant le théorème de prolongement analytique (voir par exemple [Théorème I.10., @Queffelec_Zuily13] on peut donc étendre cette formule à tout $z \in \mathbb{C}$, et particulier au cas $z=it$ pour $t \in \mathbb{R}$. On obtient alors $\phi_{\mu,\nu}(t)=e^{-\frac{t^2}{2}}$.
Enfin, on utilise la linéarité de l'espérance pour obtenir le résultat pour $X \sim \mathcal{N}(\mu,\nu)$. En effet, si $X \sim \mathcal{N}(0,1)$, alors $X=\mu+\sqrt{\nu}Z$ avec $Z \sim \mathcal{N}(0,1)$, et donc $\phi_{\mu,\nu}(t)=e^{i\mu t}\phi_{0,1}(\sqrt{\nu}t)=e^{i\mu t-\frac{\nu t^2}{2}}$.
:::

### Simulation d'une loi  normale

::: {.callout-note appearance="simple"}
## Une mauvaise piste pour simuler une loi normale


On peut simuler une loi normale à partir de variables aléatoires uniformes $U_1, \dots, U_n$  iid en appliquant le théorème central limite à
$$
	\frac{U_1 + \cdots + U_n - n/2}{\sqrt{n/12}}\,.
$$
Cependant, cette méthode ne donne qu'une approximation d'une loi normale. Par ailleurs, la vitesse de convergence étant relativement lente (de l'ordre de $\sqrt n$), il faudra simuler beaucoup de variables aléatoires uniformes pour avoir une approximation correcte, ce qui demande un temps de calcul assez élevé.

:::


#### Changement de variables

Le théorème suivant permet de passer de la loi d'un couple $(X,Y)$ à celle de $(U,V) = \phi(X,Y)$, où $\phi$ est un $C^1$-difféomorphisme, c'est-à-dire une application bijective dont la réciproque est également de classe $C^1$.

Pour cela rappelons que la **jacobienne** de $\phi^{-1}$ correspond à la matrice (application linéaire) des dérivées partielles. Ainsi, si $\phi(x,y) = (u,v) \iff (x,y) = \phi^{-1}(u,v)$, alors

\begin{align*}
{\mathrm{J}}_{\phi^{-1}}: (u,v) & \mapsto
\begin{pmatrix}
  \frac{\partial x}{\partial u} & \frac{\partial x}{\partial v}    \\
  \frac{\partial y}{\partial u} & \frac{\partial y}{\partial v}
\end{pmatrix} \enspace.
\end{align*}


::: {#thm-changement-variables}


## Caractérisation de la loi d'une variable aléatoire réelle

Soit $(X,Y)$ un vecteur aléatoire de densité $f_{(X,Y)}$ définie sur l'ouvert $A \subset \mathbb{R}^2$ et $\phi : A \to B \subset \mathbb{R}^2$ un $C^1$-difféomorphisme. Le vecteur aléatoire $(U,V)=\phi(X,Y)$ admet alors pour densité $f_{(U,V)}$ définie sur $B$ pour tout $(u,v) \in \mathbb{R}^2$ par

\begin{align*}
    (u,v) & \mapsto
    f_{(X,Y)} (\phi^{-1}(u,v)) |\det ({\mathrm{J}}_{\phi^{-1}} (u,v))| {1\hspace{-3.8pt} 1}_B(u,v)\enspace.
\end{align*}


:::

On a énoncé le résultat en dimension $2$ par simplicité. Il s'étend bien évidemment à une dimension $d$ quelconque. En particulier, pour $d=1$, on retrouve le changement de variable classique dans le cas de l'intégration d'une fonction à valeurs réelles.

::: {.proof}

On rappelle que la loi de $(U,V)$ est caractérisée par les quantités $\mathbb{E}[h(U,V)]$ pour tout $h : \mathbb{R}^2 \to \mathbb{R}$ mesurable bornée. On considère donc une telle fonction $h$ et on applique la formule de transfert :

\begin{align*}
  \mathbb{E}[h(U,V)] &
  =\mathbb{E}[h(\phi(X,Y))]\\
 & = \int_{\mathbb{R}^2} h(\phi(x,y)) f_{(X,Y)}(x,y) \, dx dy \\
 & = \int_{A} h(\phi(x,y)) f_{(X,Y)}(x,y) \, d x d y\enspace.
\end{align*}
La dernière égalité est due au fait que $f_{(X,Y)}$ est nulle en dehors de $A$.

On applique alors la formule du changement de variables vu en théorie de l'intégration avec $(u,v) = \phi(x,y) \iff \phi^{-1}(u,v) = (x,y)$ :

\begin{align*}
  & \mathbb{E}[h(U,V)]\\
  & = \!\int_{B}  \!\!\! h(u,v) f_{(X,Y)}(\phi^{-1}(u,v)) |\det ({\mathrm{J}}_{\phi^{-1}} (u,v))| \, d u d v\\
  & = \!\int_{\mathbb{R}^2} \!\!\!\! h(u,v) f_{(X,Y)}(\phi^{-1}(u,v)) |\det ({\mathrm{J}}_{\phi^{-1}} (u,v))| {1\hspace{-3.8pt} 1}_B(u,v)\, d u d v .
\end{align*}

<!-- On conclut alors que
$$
\mathbb{E}[h(U,V)]
= \int_{\mathbb{R}^2} h(u,v) f_{(X,Y)}(\phi^{-1}(u,v)) |\det ({\mathrm{J}}_{\phi^{-1}} (u,v))| {1\hspace{-3.8pt} 1}_B(u,v)\, d u d v\,,
$$ -->
ce qui donne le résultat voulu.
:::


:::{#exm-cos}

## Exemple : loi de de cos(X)
Donnons un exemple dans le cas réel. On considère une variable aléatoire $X$ de loi uniforme sur $]0,\pi[$.
Sa densité est donnée par $f_X(x) = {1\hspace{-3.8pt} 1}_{]0,\pi[}(x)/\pi$.
On pose $U = \cos(X)$ et on souhaite déterminer la loi de $U$.

On applique le théorème précédent avec la fonction $\phi^{-1}(u) = \arccos(u)$ sur $]-1,1[$. La densité de $U$ est alors donnée pour tout $u \in \mathbb{R}$ par

\begin{align*}
  f_U(u)
 & = \frac{{1\hspace{-3.8pt} 1}_{]0,\pi[}(\arccos(u))}{\pi} \Big| \frac{-1}{\sqrt{1-u^2}} \Big| {1\hspace{-3.8pt} 1}_{]-1,1[}(u)\\
 & = \frac{1}{\pi \sqrt{1-u^2}} {1\hspace{-3.8pt} 1}_{]-1,1[}(u)\enspace.
\end{align*}

:::


## Méthode de Box-Müller

Un cas particulier fondamental de la formule de changement de variables concerne le passage en coordonnées polaires. Cette transformation est définie via l'application
$$
	\begin{array}{ccccc}
		\phi^{-1} & : & ]0, \infty[ \times ]0, 2\pi[ & \to   & \mathbb{R}^2 \setminus ([0,\infty[ \times \{0\}) \\
		          &   & \begin{pmatrix} r \\ \theta \end{pmatrix}                   & \mapsto & \begin{pmatrix} r \cos(\theta) \\ r \sin(\theta) \end{pmatrix}\,.
	\end{array}
$$
L'expression de $\phi$ ne nous sera pas utile. On peut tout de même la donner au passage :

$$
  	\begin{array}{ccccc}
		\phi & : & \mathbb{R}^2 \setminus ([0,\infty[ \times \{0\}) & \to     & ]0, \infty[ \times ]0, 2\pi[                                                      \\
		     &   & \begin{pmatrix} x \\ y \end{pmatrix}                                            & \mapsto & \begin{pmatrix}\sqrt{x^2+y^2} \\ 2 \arctan \Big( \frac{y}{x+\sqrt{x^2+y^2}} \Big)\end{pmatrix}\,.
	\end{array}
$$

Ici, le jacobien de $\phi^{-1}$ est la matrice
$$
	{\mathrm{J}}_{\phi^{-1}} (r,\theta)
	=
	\begin{pmatrix}
		\cos(\theta) & -r \sin(\theta) \\
		\sin(\theta) & r \cos(\theta)
	\end{pmatrix}\,,
$$
qui vérifie $|\det({\mathrm{J}}_{\phi^{-1}} (r, \theta))| = r$. Ainsi, si $(X,Y)$ a pour densité $f_{(X,Y)}$, alors $(R, \Theta) = \phi(X,Y)$ a pour densité
$$
  f_{(R, \Theta)} (r, \theta)
  = r\cdot f_{(X,Y)}(r \cos(\theta), r \sin(\theta)) \!\cdot\! {1\hspace{-3.8pt} 1}_{]0, \infty[}(r)  {1\hspace{-3.8pt} 1}_{]0, 2 \pi[}(\theta).
$$

Dans le cas où $X$ et $Y$ sont des variables aléatoires gaussiennes indépendantes, on obtient le résultat suivant.

::: {#thm-box-muller}

## Méthode de Box-Müller

Soit $X$ et $Y$ deux variables aléatoires indépendantes de loi normales centrées réduites : $X,Y \sim \mathcal{N}(0,1)$. Le couple de variables aléatoires polaires $(R, \Theta) = \phi(X,Y)$ a pour densité
$$
			f_{R, \Theta}(r,\theta)
			= \Big( r \cdot e^{-\tfrac{r^2}{2}} {1\hspace{-3.8pt} 1}_{]0, \infty[}(r) \Big) \bigg(\frac{{1\hspace{-3.8pt} 1}_{]0, 2 \pi[}(\theta)}{2 \pi} \bigg)\,.
$$
Autrement dit, elles sont indépendantes, l'angle $\Theta$ suit une loi uniforme sur $]0, 2\pi[$ et la distance à l'origine $R$ suit une loi de Rayleigh donnée par la densité
$$
    f_R(r) =  r \cdot e^{-r^2/2} {1\hspace{-3.8pt} 1}_{]0, \infty[}(r)\,, \quad r > 0\,.
$$

:::

::: {.proof}
La densité du couple $(X,Y)$ est donnée par
$$
  f_{(X,Y)}(x,y) = \frac{1}{2\pi} e^{-\frac{x^2+y^2}{2}}\,, \quad x,y \in \mathbb{R}\,.
$$
Le théorème précédent donne alors la densité de $(R, \Theta)$ :

\begin{align*}
  f_{(R, \Theta)} (r, \theta) &
  = r\cdot f_{(X,Y)}(r \cos(\theta), r \sin(\theta)) \!\cdot\! {1\hspace{-3.8pt} 1}_{]0, \infty[}(r)  {1\hspace{-3.8pt} 1}_{]0, 2 \pi[}(\theta)\\
  &= r \cdot\frac{1}{2\pi} e^{-\frac{r^2}{2}} \cdot  {1\hspace{-3.8pt} 1}_{]0, \infty[}(r) {1\hspace{-3.8pt} 1}_{]0, 2 \pi[}(\theta)\,,
\end{align*}

ce qui conclut la preuve.
:::

Si $R$ suit une loi de Rayleigh alors $\sqrt{-2 \log(U)}$ a la même loi que $R$, où $U$ est une variable aléatoire de loi uniforme sur $]0,1[$:
Pour cela il suffit de remarquer que pour tout $x > 0$, $F_R(x)=\mathbb{P}(R\leq x) = 1-\exp(-\tfrac{x^2}{2})$, et donc que pour tout $q \in ]0,1[, F_R^{^\leftarrow}(q)=\sqrt{-2\log(1-q)}$ et donc $\sqrt{-2\log(1-U)}$, puis $\sqrt{-2\log(U)}$ on donc la même loi que $R$.

L'algorithme de Box-Müller s'en suit: si $U$ et $V$ sont des v.a. indépendantes de loi uniforme sur $[0,1]$ et qu'on définit $X$ et $Y$ par
$$
\begin{cases}
  X = \sqrt{-2 \log(U)} \cos(2\pi V)\\
  Y = \sqrt{-2 \log(U)} \sin(2\pi V)\,.
\end{cases}
$$
alors $X$ et $Y$ des variables aléatoires gaussiennes centrées réduites indépendantes.

:::{.callout-note}

## Note
Cet algorithme n'est en fait pas souvent utilisé en pratique : il fait appel à des fonctions dont l'évaluation est coûteuse (logarithme, cosinus, sinus).
Pour s'affranchir des fonctions trigonométriques, une version modifiée de l'algorithme de Box-Müller a été proposée : la méthode de Marsaglia, qui s'appuie sur des variables aléatoires uniformes sur le disque unité (voir l'exercice dédié en TD). Une autre alternative est la méthode de Ziggurat.
:::



## Lois autour de la loi normale


### Loi du $\chi^2$

Concernant la prononciation, on prononce "khi-deux" le nom de cette loi.

::: {#def-chi2}

## Loi du $\chi^2$


Soit $X_1, \dots, X_k$ des variables aléatoires i.i.d. de loi normale centrée réduite. La loi de la variable aléatoire $X = X_1^2 + \dots + X_k^2$ est appelée **loi du $\chi^2$ à $k$ degrés de liberté**. Sa densité est donnée par
$$
f(x) = \frac{1}{2^{\frac{k}{2}}\Gamma(\frac{k}{2})} x^{\frac{k}{2}-1} e^{-x/2}\,, \quad x \geq 0\,,
$$
où $\Gamma$ désigne la fonction gamma d'Euler :
$$
\Gamma(x) = \int_0^{\infty} t^{x-1} e^{-t}\,  dt\,.
$$
On note alors $X \sim \chi^2(k)$.

:::

Au vu de sa définition, la simulation d'une loi du $\chi^2$ est claire : on simule $k$ variables aléatoires gaussiennes centrées réduites indépendantes et on somme leur carrés.


::: {.proof}
Montrons pour $k=1$ que la densité est bien de la forme précédente, c'est-à-dire
$$
	f(x) = \frac{1}{\sqrt{2\pi}} \frac{e^{-x/2}}{\sqrt x}\,, \quad x \geq 0\,,
$$
où on a utilisé la relation $\Gamma(1/2) = \sqrt \pi$ (intégrale de Gauss).

Soit $h : \mathbb{R} \to \mathbb{R}$ une fonction mesurable bornée. On a

\begin{align*}
	\mathbb{E}[h(X_1^2)]
	& = \int_{\mathbb{R}} h(x^2) \frac{e^{-\frac{x^2}{2}}}{\sqrt{2 \pi}} \, dx\\
	& = \int_{-\infty}^0 h(x^2) \frac{e^{-\frac{x^2}{2}}}{\sqrt{2 \pi}} \, dx
	+ \int_0^{\infty} h(x^2) \frac{e^{-\frac{x^2}{2}}}{\sqrt{2 \pi}} \, dx\,.
\end{align*}

En effectuant le changement de variable $x=-\sqrt u$ dans la première intégrale et $x=\sqrt u$ dans la deuxième, on obtient
$$
	\mathbb{E}[h(X_1^2)]
	= \int_{\infty}^0 h(u) \frac{e^{-u/2}}{\sqrt{2 \pi}} \frac{ du}{2 \sqrt u}
	+ \int_0^{\infty} h(u) \frac{e^{-u/2}}{\sqrt{2 \pi}} \frac {du}{2 \sqrt u}\,.
$$
Les deux intégrales étant égales, on conclut que
$$
	\mathbb{E}[h(X_1^2)] = \int_0^{\infty} h(u) \frac{e^{-u/2}}{\sqrt{2 \pi}\sqrt u}\, du\,,
$$
ce qui prouve le résultat pour $k=1$.

La généralisation à $k$ quelconque se fait par récurrence: on utilise la formule de convolution des la loi pour obtenir la loi pour $k+1$:


\begin{align*}
	X_1^2 + \dots + X_k^2 + X_{k+1}^2
	& = (X_1^2 + \dots + X_k^2) + X_{k+1}^2\\
	& = \chi^2(k) + X_{k+1}^2\,.
\end{align*}

Ainsi,

\begin{align*}
	f_{\chi^2(k+1)}(x)
	& = \int_0^{\infty} f_{\chi^2(k)}(x-y) f_{X_{k+1}^2}(y) \, dy\\
	& = \int_0^x \frac{1}{2^{\frac{k}{2}}\Gamma(\frac{k}{2})} (x-y)^{\frac{k}{2}-1} e^{-\frac{x-y}{2}} \tfrac{e^{-\frac{y}{2}}}{\sqrt{2\pi y}} \, dy\\
	& = \frac{e^{-\frac{x}{2}}}{2^{\frac{k+1}{2}} \Gamma(\frac{k}{2})  \Gamma(\frac{1}{2})}\int_0^x  (x-y)^{\frac{k}{2}-1}  \tfrac{1}{\sqrt{ y}} \, dy\\
	& = \frac{e^{-\frac{x}{2}}}{2^{\frac{k+1}{2}} \Gamma(\frac{k}{2})  \Gamma(\frac{1}{2})} x \int_0^1  (x-ux)^{\frac{k}{2}-1}  \tfrac{1}{\sqrt{xu}} \, du\\
\end{align*}

avec le changement de variable $y=ux$. Ensuite,

\begin{align*}
	f_{\chi^2(k+1)}(x)
	& = \frac{x^{\frac{k+1}{2}} e^{-\frac{x}{2}}}{2^{\frac{k+1}{2}} \Gamma(\frac{k}{2})  \Gamma(\frac{1}{2})}  \int_0^1  (1-u)^{\frac{k}{2}-1}  u^{1/2-1} du\enspace.
\end{align*}

Or rappelons que si $\Beta(a,b) =  \int_0^1  (1-u)^{a-1}  u^{b-1} \, du$, alors pour tout $a,b \in [0,+\infty[, \Beta(a,b)=\frac{\Gamma(a)\Gamma(b)}{\Gamma(a+b)}$. En effet, en faisant le changement de variable dans l'intégrale double qui suit:
$$
\begin{array}{ccccc}
	\psi & : & \mathbb{R}^+ \times \mathbb{R}^+ & \to     & \mathbb{R}^+\times ]0,1[                 \\
			&   & (s,t)                          & \mapsto & \Big(s+t, \frac{t}{s+t}\Big)\,,
\end{array}
$$
c'est-à-dire $\psi^{-1}(r,w) = (r(1-w), rw)$, et le jacobien est donné par $J_{\psi^{-1}}(r,w) = \begin{pmatrix} 1-w & -r \\ w & r \end{pmatrix}$, et donc $J_{\psi^{-1}}(r,w)=r$, on obtient:

\begin{align*}
\Gamma(a)\Gamma(b) & = \int_0^{\infty} t^{a-1} e^{-t} \, dt \int_0^{\infty} s^{b-1} e^{-s} \, ds\\
& = \int_0^{\infty} \int_0^{\infty} e^{-t-s} t^{a-1} s^{b-1} \, dt \, ds\\
& = \int_0^{1} \int_0^{\infty} e^{-r} (rw)^{a-1} (r(1-w))^{b-1} r \, dr \, dw\\
& = \int_0^1 w^{a-1} (1-w)^{b-1} \int_0^{\infty} e^{-r} r^{a+b-1} \, dr \, dw\\
\end{align*}

et donc  $\Beta(a,b)=\frac{\Gamma(a)\Gamma(b)}{\Gamma(a+b)}$.
En appliquant cette relation pour $a=\frac{k}{2}$ et $b=1/2$, on obtient


\begin{align*}
	f_{\chi^2(k+1)}(x)
	& = \frac{x^{\frac{k+1}{2}} e^{-\frac{x}{2}}}{2^{\frac{k+1}{2}} \Gamma(\frac{k+1}{2})} \enspace.
\end{align*}

Le résultat est donc prouvé par récurrence.
:::


### Loi de Student

::: {#def-student}

## Loi de Student

Soit $X \sim \mathcal{N}(0,1)$ et $Y \sim \chi^2(k)$ deux variables aléatoires indépendantes. La loi de la variable aléatoire $V = \frac{X}{\sqrt{Y/k}}$ est appelée **loi de Student à $k$ degrés de liberté**. Elle admet pour densité
$$
	f_V(t)
	= \dfrac{1}{\sqrt{k \pi}} \dfrac{\Gamma(\frac{k+1}{2})}{\Gamma(\frac{k}{2})} \Big(1+\dfrac{t^2}{k}\Big)^{-\frac{k+1}{2}}\,,
	\quad t \in \mathbb{R}\,.
$$

:::

La loi de Student correspond donc au ratio d'une loi normale par la racine carrée d'une loi du $\chi^2(k)$ normalisée.
Ce ratio apparaît souvent en statistique lors de la construction d'intervalles de confiance.
Cette loi a été décrite en 1908 par
[William Gosset](https://fr.wikipedia.org/wiki/William_Gosset)^[[William Gosset](https://fr.wikipedia.org/wiki/William_Gosset): (1876-1937)
statisticien et chimiste anglais. Il était employé à la brasserie Guinness à Dublin, chargé du contrôle qualité. Son employeur lui refusant le droit de publier sous son propre nom, W. Gosset choisit un pseudonyme, *Student* (&#x1F1EB;&#x1F1F7;: étudiant).
<img src="https://upload.wikimedia.org/wikipedia/commons/4/42/William_Sealy_Gosset.jpg" width="65%" style="display: block; margin-right: auto; margin-left: auto;" alt="Photo William Gosset" title="Photographie de William Gosset, prise en 1908."></img>].


Au vu de la proposition précédente, simuler une loi de Student est assez simple : on simule $k+1$ loi normales indépendantes $X_1, \ldots, X_{k+1}$ et on considère
$$
	V = \dfrac{\sqrt{k} X_{k+1}}{\sqrt{X_1^2+\cdots + X_k^2}}\,.
$$

::: {.proof}

## Formule de la densité

On applique pour cela la formule du changement de variables avec la transformation
$$
	\begin{array}{ccccc}
		\phi & : & \mathbb{R}^* \times ]0, \infty[ & \to     & \mathbb{R}^* \times \mathbb{R}^*      \\
		     &   & (x,y)                           & \mapsto & \Big(x, \dfrac{x}{\sqrt{y/k}}\Big)\,,
	\end{array}
$$
c'est-à-dire
$$
	\phi^{-1}(u,v) = \Big(u, k\dfrac{u^2}{v^2}\Big)\,.
$$
La fonction $\phi^{-1}$ a pour matrice jacobienne
$$
	J_{\phi^{-1}} (u,v)
	=
	\begin{pmatrix}
		1              & 0                  \\
		\frac{2k}{v^2} & \frac{-2ku^2}{v^3}
	\end{pmatrix}\,,
$$
dont le déterminant vaut $\frac{-2ku^2}{v^3}$. Par ailleurs, les variables aléatoires $X$ et $Y$ étant indépendantes, la densité du couple $(X,Y)$ correspond au produit des densités :
$$
	f_{(X,Y)}(x,y) = \dfrac{e^{-\frac{x^2}{2}}}{\sqrt{2 \pi}}  \dfrac{1}{2^{\frac{k}{2}2} \Gamma(\frac{k}{2})} y^{\frac{k}{2}-1} e^{-\frac{y}{2}} {1\hspace{-3.8pt} 1}_{]0, \infty[}(y)\, \quad x,y \in \mathbb{R}\,.
$$
Tout est prêt pour appliquer la théorème du changement de variables qui assure que la densité du couple $(U,V)$ est donnée par
$$
	f_{(U,V)}(u,v)
	= \dfrac{e^{-\frac{u^2}{2}}}{\sqrt{2 \pi}} \dfrac{1}{2^{\frac{k}{2}} \Gamma(\frac{k}{2})} \bigg(\dfrac{k u^2}{v^2}\bigg)^{\frac{k}{2}-1} e^{-\frac{1}{2} \frac{k u^2}{v^2}} \dfrac{2 k u^2}{(v^2)^{\frac{3}{2}}} {1\hspace{-3.8pt} 1}_{\mathbb{R}^*}(v)\,.
$$
Il suffit alors de marginaliser pour obtenir la densité de $V$, ce qui s'effectue en calculant l'intégrale
$$
	\int_{\mathbb{R}} f_{(U,V)} (u,v) \, du\,.
$$
Les termes en $u$ de l'expression précédente s'intègre en

\begin{align*}
	\int_{-\infty}^\infty e^{-\frac{u^2}{2}(1+\frac{k}{v^2})} (u^2)^{\frac{k}{2}} \, d u
	 & = \int_0^\infty e^{-s} \bigg( \dfrac{2 s}{1+\frac{k}{v^2}} \bigg)^{\frac{k}{2}} \sqrt{\dfrac{2}{1+\frac{k}{v^2}}} \dfrac{ds}{2\sqrt{s}} \\
	 & = \frac{2^{\frac{k+1}{2}}}{2} \frac{1}{\left(1+\tfrac{k}{v^2}\right)^{ \frac{k+1}{2}}} \!\!\! \int_0^\infty e^{-s} s^{\frac{k}{2} - \frac{1}{2}} \, ds\,,
\end{align*}

où la première égalité résulte du changement de variable
$$
	s = \dfrac{u^2}{2} \left( 1 + \dfrac{k}{v^2} \right) \iff \sqrt{\dfrac{2s }{1+\frac{k}{v^2}}} = u\,.
$$
On reconnaît dans l'intégrale la valeur de $\Gamma(\frac{k+1}{2})$ ce qui conduit à
$$
	f_V(v)
	=
	\dfrac{1}{\sqrt{2 \pi}} \dfrac{1}{2^{\frac{k}{2}} \Gamma(\frac{k}{2})} \bigg(\dfrac{k}{v^2}\bigg)^{\frac{k}{2}-1} \dfrac{2 k}{(v^2)^{\frac{3}{2}}}
	\frac{2^{\frac{k+1}{2}}}{2} \frac{\Gamma\left(\tfrac{k+1}{2}\right)}{\left(1+\tfrac{k}{v^2}\right)^{ \frac{k+1}{2}}}\,.
$$
On réécrit alors les termes en $k/v^2$ via

\begin{align*}
\left(\tfrac{k}{v^2}\right)^{\frac{k}{2}-1} \tfrac{k}{(v^2)^{\frac{3}{2}}}
	\frac{1}{\left(1+\tfrac{k}{v^2}\right)^{ \frac{k+1}{2}}}
	& =
	\left(\tfrac{k}{v^2}\right)^{\frac{k}{2}-1} \tfrac{1}{\sqrt{k}} \left(\tfrac{k}{v^2}\right)^{\frac{3}{2}}
	\frac{1}{\left(1+\tfrac{k}{v^2}\right)^{ \frac{k+1}{2}}}\\
	& = \tfrac{1}{\sqrt{k}}\left(1+\tfrac{v^2}{k}\right)^{- \frac{k+1}{2}}\,,
\end{align*}

ce qui permet de conclure :
$$
	f_V(v)
	=
	\dfrac{1}{\sqrt{k \pi}} \dfrac{\Gamma(\frac{k+1}{2})}{\Gamma(\frac{k}{2})} \Big(  1+\dfrac{v^2}{k}\Big)^{-\frac{k+1}{2}}\,.
$$
:::

### Loi de Cauchy

::: {#def-cauchy}

## Loi de Cauchy

Une variable aléatoire $X$ suit une **loi de Cauchy** standard si sa densité est donnée par
$$
	f_X(x) = \dfrac{1}{\pi(1+x^2)}\,, \quad x \in \mathbb{R}\,.
$$
:::

On note alors $X\sim \mathcal{C}(0,1)$ dans ce cas.
Plus généralement on dit que $Y$ suit une loi de Cauchy de paramètres $(\mu,\sigma)\in \mathbb{R} \times ]0,+\infty[$ si $Y=\mu+\sigma X$ où $X$ suit une loi de Cauchy standard. Sa densité est alors donnée par
$$
	f_Y(y) = \dfrac{1}{\sigma \pi(1 + \tfrac{(y-\mu)^2}{\sigma^2})}\,, \quad y \in \mathbb{R}\,.
$$

Pour rappel cette loi est importante comme exemple de loi qui n'admet ni espérance, ni variance a fortiori.
En effet, si $X$ suit une loi de Cauchy standard,
$$
\int_{\mathbb{R}} |x| f_X(x) \, dx = \int_{\mathbb{R}} \frac{|x|}{\pi(1+x^2)} \, dx.
$$
Or $\frac{|x|}{\pi(1+x^2)}\sim \frac{1}{\pi x}$ quand $|x|\to \infty$, et $x\mapsto \frac{|x|}{\pi(1+x^2)}$ n'est donc pas intégrable sur $\mathbb{R}$ au sens de Lebesgue. C'est donc un exemple de loi pour laquelle la loi des grands nombres et le théorème central limite ne s'appliquent pas.


#### Fonction caractéristique


La fonction caractéristique de la loi de Cauchy standard est donnée par

\begin{align*}
\varphi_X(t) & \triangleq \int_{\mathbb{R}} e^{itx} f_X(x) \, dx = e^{-|t|}\,.
\end{align*}

et donc si $X\sim \mathcal{C}(\mu,\sigma)$, alors pour tout $t \in \mathbb{R}$, $\varphi_X(t) = e^{i\mu t - \sigma |t|}$.
Pour la preuve voir par exemple [Exemple III.5.5., @Barbe_Ledoux06]
Une conséquence directe est que la somme de deux variables aléatoires indépendantes de loi de Cauchy reste une loi de Cauchy:
Si $X_1 \sim \mathcal{C}(\mu_1,\sigma_2)$ et $X_2 \sim \mathcal{C}(\mu_2,\sigma_2)$ sont indépendantes, alors $X_1+X_2 \sim \mathcal{C}(\mu_1+\mu_2,\sigma_1+\sigma_2)$ (car elles ont la même fonction caractéristique).

On en déduit que la moyenne de variables de Cauchy standard i.i.d suit la loi de Cauchy standard. En effet, si $X_1, \ldots, X_n$ sont i.i.d de loi de Cauchy standard, alors pour tout $t \in \mathbb{R}$, $\varphi_{\frac{1}{n}\sum_{i=1}^n X_i}(t) = e^{-|t|}$, et donc $\frac{1}{n}\sum_{i=1}^n X_i \sim \mathcal{C}(0,1)$. Ainsi la moyenne empirique ne converge pas en probabilité vers une constante!


#### Fonction de répartition et simulation


La fonction de répartition de $X$ correspond, à une constante près, à la fonction arctangente qui est bijective de $\mathbb{R}$ sur $]\frac{-\pi}{2},\frac{\pi}{2}[$.
La méthode d'inversion permet donc de simuler une variable aléatoire de loi de Cauchy.
La proposition suivante donne un autre moyen.



::: {#prp-cauchy}

## Loi de Cauchy et loi normale

Soit $X$ et $Y$ deux variables aléatoires indépendantes de loi normale centrée réduite. Alors la variable aléatoire $Y/X$ suit une loi de Cauchy.

:::

Notons que $Y/X$ est bien définie puisque $X$ est différent de $0$ presque sûrement.

::: {.proof}

Comme pour la loi de Student, on démontre ce résultat avec un changement de variables. On considère l'application
$$
	\begin{array}{ccccc}
		\phi & : & \mathbb{R}^* \times \mathbb{R} & \to     & \mathbb{R}^2                 \\
				&   & (x,y)                          & \mapsto & \Big(x, \dfrac{y}{x}\Big)\,,
	\end{array}
$$
c'est-à-dire
$$
	\phi^{-1}(u,v) = (u, uv)\,.
$$
La matrice jacobienne de $\phi^{-1}$ est donnée par
$$
	J_{\phi^{-1}} (u,v)
	=
	\begin{pmatrix}
		1 & 0 \\
		v & u
	\end{pmatrix}\,,
$$
dont le déterminant vaut $u$. Rappelons également que la densité du couple $(X,Y)$ vaut
$$
	f_{(X,Y)}(x,y) = \dfrac{1}{2 \pi} e^{- \frac{x^2 + y^2}{2}}\,, \quad x,y \in \mathbb{R}\,.
$$
La formule du changement de variables donne alors la densité de $f_{(U,V)}$ :
$$
	f_{(U,V)}
	= \dfrac{1}{2 \pi} e^{- \frac{u^2 + u^2v^2}{2}} |u|\,.
$$
On obtient alors la densité de $V$ en intégrant par rapport à $u$ :

\begin{align*}
	f_V(v)
	& = \dfrac{1}{2 \pi} \int_{\mathbb{R}} e^{- \frac{u^2 + u^2v^2}{2}} |u| \, \mathrm du \\
	& = \dfrac{1}{\pi} \int_0^\infty e^{- \frac{u^2(1 + v^2)}{2}} u \, \mathrm du\\
	& = \dfrac{1}{\pi} \bigg[ -\dfrac{e^{- \frac{u^2(1 + v^2)}{2}}}{1+v^2} \bigg]_0^\infty\\
	& = \dfrac{1}{\pi(1+v^2)}\enspace.
\end{align*}

:::

On obtient ainsi une autre manière de simuler une loi de Cauchy, en prenant le ratio de deux  gaussiennes indépendantes.
Cependant, la simulation via la méthode d'inversion peut-être moins coûteuse puisqu'elle ne fait appel qu'à une variable aléatoire uniforme et à la fonction tangente.


## Bibliographie

::: {#refs}
:::