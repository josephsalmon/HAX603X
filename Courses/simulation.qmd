---
title: "Simulation"
format:
  html:
    out.width: 50%
jupyter: python3
---


Dans ce chapitre on se demande comment simuler en pratique des variables aléatoires i.i.d. L'idée est de commencer par le cas de variables aléatoires de loi uniforme et d'en déduire les autres lois.


## Variables aléatoires uniformes

On rappelle qu'une variable aléatoire $U$ suit une loi uniforme sur $[0,1]$, noté $\mathcal{U}([0,1])$ si sa fonction de répartition $F_U$ est donnée par
$$
F_U(x)
=
\begin{cases}
    0, & \text{si }x < 0\,,        \\
    x, & \text{si }x \in [0,1]\,,  \\
    1, & \text{si }x > 1\,.        \\
\end{cases}
$$

```{python}
#| echo: false
#| label: fig-repartition-uniforme
#| fig-cap:
#|       - Fonction de répartition de la loi uniforme
import numpy as np
import scipy
import plotly.graph_objects as go
import plotly.io as pio

pio.renderers.default = "notebook"

x = np.linspace(-3, 4, num=300)

fig = go.Figure()

fig.add_trace(
    go.Scatter(
        mode="lines",
        line=dict(color="black", width=3),
        x=x,
        y=scipy.stats.uniform.cdf(x),
        name=r"$F_U$",
    )
)

fig.update_layout(
    template="simple_white",
    showlegend=True,
)

# XXX known bug with plotly and latex labels.
```

L'objectif est de simuler sur machine une suite $U_1, \ldots, U_n$  de variables aléatoires i.i.d. de loi $\mathcal{U}([0,1])$. Plusieurs problèmes apparaissent alors :

- Une machine est déterministe.
- Les nombres entre $0$ et $1$ donnés par la machine sont de la forme $k/2^p$, pour $k \in \{0, \ldots, 2^{p-1}\}$. On ne pourra donc jamais générer des nombres qui ne sont pas de cette forme.
- Vérifier qu'une suite est bien i.i.d. est un problème difficile.

::: {#def-PRNG}
## Générateur de nombres pseudo-aléatoires

<br>

Un **générateur de nombres pseudo-aléatoires** (&#127468;&#127463;: *Pseudo Random Number Generator*, PRNG), est un algorithme déterministe récursif qui renvoie une suite $U_1, \ldots, U_n$ dans $[0,1]$ qui a un "comportement similaire" à une suite i.i.d. de loi $\mathcal{U}([0,1])$.
Pour être plus rigoureux, ces nombres sont en fait des nombres entiers générés uniformément sur un certain interval. Dans un second temps, une transformation simple (normalisation) permet d'obtenir des nombres flottants (&#127468;&#127463;: *floats*) entre 0 et 1.
:::


::: {.callout-note appearance="simple"}
## Pour aller plus loin
Parfois il est utile d'aller chercher dans le code source certaines information pour savoir comment les fonctions sont codées dans les packages que l'on utiliser. Par exemple, pour `numpy` que l'on utilise fréquement, on peut voir l'opération choisie ici: [Random: int -> float en `numpy`](https://github.com/numpy/numpy/blob/d50fc570a9e15ea4d8ec35add245d5d791fa4596/numpy/random/src/mt19937/randomkit.c#L479).
:::



Un tel algorithme se construit de la manière suivante :

1. On part d'une graine (&#127468;&#127463;: *seed*) $U_0$ qui détermine la première valeur de manière la plus arbitraire possible.
2. La procédure récursive s'écrit $U_{n+1} = f(U_n)$, où $f$ est une transformation déterministe, de sorte que $U_{n+1}$ est le plus indépendant possible de $U_1, \dots·, U_n$.


- La fonction $f$ est déterministe et prend ses valeurs dans un ensemble fini, donc l'algorithme est périodique. Le but est donc d'avoir la plus grande période possible.

- Notons qu'une fois que la graine est fixée, alors l'algorithme donne toujours les mêmes valeurs. Fixer la graine peut donc être très utile pour répéter des simulations dans des conditions identiques et ainsi repérer des erreurs.


::: {.callout-important appearance='default' icon="false"}
##  Exercice: bug ou feature?
Reprendre les widgets du chapitre [Théorèmes asymptotiques](th_asymptotique.qmd) et faites varier doucement le paramètre $p$ (de Bernoulli). Que constatez-vous? Proposer une explication potentielle.
:::

### Générateur congruentiel linéaire

La plupart des PRNG s'appuient sur des résultats arithmétiques. Un des plus connus est celui appelé Générateur congruentiel linéaire (&#127468;&#127463; Linear congruential generator, LCG).
Il est défini comme suit: on construit récursivement une suite d'entiers $X_i$ via la congruence
$$
  X_{n+1} = a X_n + b \quad \text{mod } m \enspace,
$$
où $a,b,m$ sont des entiers bien choisis pour que la suite obtenue ait de bonnes propriétés.
Il suffit alors de considérer $X_n/m$. Par exemple, la fonction ```rand``` sur ```scilab``` utilise cette congruence avec $m=2^{31}$, $a=843\; 314\; 861$, et $b=453\; 816\; 693$.

### Générateurs alternatifs
Les langages ```Python``` et ```R``` utilisent par défaut le générateur Mersenne-Twister qui s'appuie sur la multiplication vectorielle, mais d'autres générateurs sont aussi disponibles.
Ce générateur a pour période $m =2^{19937}-1$, nombre qu'on peut raisonnablement considérer comme grand.

Pour ```numpy``` la méthode par défaut est PCG64 (cf. [documentation de `numpy`](https://numpy.org/doc/stable/reference/random/bit_generators/pcg64.html)),
qui dispose de meilleures garanties statistiques (Voir le site [https://www.pcg-random.org](https://www.pcg-random.org/statistical-tests.html#id4) pour cela).


### Usage en `numpy`

On suppose désormais disposer d'un générateur pseudo-aléatoire sur $[0,1]$.
En `numpy` depuis la version 1.17, une bonne manière d'utiliser des éléments aléatoires est d'utiliser un générateur que l'on définit soi-même:

```{python}
seed = 12345  # Toujours être conscient qu'une graine existe
rng = np.random.default_rng(seed)  #
print(rng.random())  ##  un tirage uniforme sur [0,1]
print(rng.random(size=5))  ## cinq tirages uniformes sur [0,1]
print(rng.random(size=(3, 2)))  ## matrice 3x2, à entrées unif. sur [0,1]
```

Dans la suite on va voir comment générer d'autres lois à partir de la loi uniforme, mais il est clair que les logiciels modernes proposent un large éventail de distribution classique (gaussienne, exponentielle, etc.).
Une liste exhaustive est donnée [ici](https://numpy.org/doc/stable/reference/random/generator.html#distributions) pour `numpy`.

::: {.callout-note appearance="simple"}
## Pour aller plus loin
Une excellent discussion sur les bonnes pratiques aléatoires en `numpy`, et l'usage de `np.random.default_rng` est donnée dans ce [blog post d'Albert Thomas](https://albertcthomas.github.io/good-practices-random-number-generators/).
:::



### Propriété de la loi uniforme
On verra souvent apparaître la variable aléatoire $1-U$ où $U \sim \mathcal{U}([0,1])$. Il se trouve que $1-U$ suit aussi une loi uniforme sur $[0,1]$ comme le montre le calcul de sa fonction de répartition. Ainsi pour tout $x \in [0,1]$ on obtient
$$
\begin{align*}
\mathbb{P}(1-U \leq x) & = \mathbb{P}(U \geq 1-x),\\
                       & = 1-(1-x), \\
                       & = x\,.
\end{align*}
$$
On peut démontrer facilement la même relation pour $x<0$ et $x>1$, d'où le résultat.


## Méthode d'inversion


L'idée de la méthode d'inversion repose sur le résultat suivant :


## Rappel sur la fonction quantile

Rappel : Pour $F$ une fonction définie sur $\mathbb{R}$ à valeurs dans $[0, 1]$, croissante, on note

$$
\forall q \in ]0,1[, \quad F^\leftarrow(q) = \inf\{ x \in \mathbb{R} : F(x)\geq q\}
$${#eq-inf}



:::{#thm-quantile}

## Caratérisation des quantiles
Soit $F$ une fonction définie sur $\mathbb{R}$ à valeurs dans $[0, 1]$, croissante et continue à droite, alors pour tout $q \in ]0, 1[$, on a
$$
\begin{align}
   \{x \in \mathbb{R} :  F(x) \geq q) \} & =
   \{x \in \mathbb{R} : x \geq F^\leftarrow(q)  \}
\end{align}
$$ 
:::

::: {.proof}

- Cas $\subset$: Soit $x \in \mathbb{R}$ t.q. $F(x) \geq q$, alors par définition de l'inf dans @eq-inf, $x \geq F^\leftarrow(q)$.
- Cas $\supset$: Soit  $x \in \mathbb{R}$ t.q. $x \geq F_X^\leftarrow(q)$ alors pour tout $\epsilon > 0$, $x + \epsilon > F^\leftarrow(q)$, donc $F(x + \epsilon) \geq q$. Puis, par continuité à droite de $F$, $F(x) \geq q$.
:::

<br>


## Méthode d'inversion
:::{#thm-methode_inversion}

## Méthode d'inversion
Soit $X$ une v.a réelle, et  $U \sim\mathcal{U}([0,1])$, alors la variable aléatoire $F_X^{\leftarrow}(U)$ a même loi que $X$.
:::


::: {.proof}

En utilisant le théorème précédent, on a
$\mathbb{P}(F_X^{\leftarrow}(U) \leq x) = \mathbb{P}(U \leq F_X(x))$ pour tout $x\in\mathbb{R}$.
Puis, comme $U$ est une loi uniforme sur $[0,1],  \mathbb{P}(U\leq F_X(x))=F_X(x)$.

On en déduit donc que la loi de $F_X^{-1}(U)$ est la même que celle de $X$, car les deux v.a. ont la même fonction de répartition.
:::

<br>



::: {#exm-inverse-exponentielle}

## Simulation d'une loi exponentielle

On rappelle que la loi exponentielle de paramètre $\lambda > 0$ a pour densité
$$
f_{\lambda}(x) = \lambda e^{-\lambda x} {1\hspace{-3.8pt} 1}_{\mathbb{R}_+}(x)\enspace.
$$
et donc pour fonction de répartition
$$
F_{\lambda}(x) = (1 - e^{-\lambda x}) {1\hspace{-3.8pt} 1}_{\mathbb{R}_+}(x)\enspace.
$$
On vérifie que $F_{\lambda}$ est bijective de $\mathbb{R}_+$ dans $]0,1[$ et que son inverse est donnée pour tout $u \in ]0,1[$ par
$$
F_{\lambda}^{-1}(u) = -\frac{1}{\lambda} \log(1-u)\enspace.
$$
:::

<br>

::: {#exm-inverse-weibull}

## Simulation d'une loi de Weibull
La loi de Weibull de paramètre $\lambda > 0$ et $k >0$ est caractérisée par la fonction de répartition
$$
F(x) = (1 - e^{-(x/\lambda)^k}){1\hspace{-3.8pt} 1}_{\mathbb{R}_+}(x)
$$
C'est une loi utilisée dans différents domaines, notamment en gestion des risques (hydrologie, finance, assurance, etc.). Le calcul de l'inverse généralisée $F^\leftarrow$ est immédiat car $F$ est bijective sur $]0, \infty[$ :
$$
    F^\leftarrow (u) = \lambda (-\ln(1-u))^{\frac{1}{k}}\,, \quad u \in ]0,1[\,.
$$
La méthode d'inversion s'applique : si $U \sim \mathcal{U}([0,1])$, alors la variable aléatoire $X = \lambda (-\ln(U))^{\frac{1}{k}}$ suit une loi de Weibull de paramètres $\lambda$ et $k$.
:::

Malheureusement, la fonction $F$ n'est pas toujours inversible (penser aux lois discrètes) c'est donc pourquoi on utilise l'inverse l'inverse généralisée ou fonction quantile introduite dans la section [Notations](notations.qmd):
<!-- @def-quantile: -->

$$
  F^\leftarrow(p)=  \inf\{ x \in \mathbb{R} \colon F(x)\geq p\} \enspace.
$$


**Interprétation:** Définir l'inverse d'une fonction de répartition $F$ revient à résoudre l'équation $F(x) = \alpha$ d'inconnue $x$ pour un $\alpha$ fixé.
Si $F$ n'est pas bijective, deux problèmes apparaissent :

- l'équation n'a aucune solution ce qui revient à dire que $F$ n'est pas surjectif (graphiquement, $F$ présente des sauts) ;
- l'équation a plusieurs solutions ce qui revient à dire que $F$ n'est pas injective (graphiquement cela se matérialise par un plateau à la hauteur $\alpha$). Un exemple classique est celui où $F$ est la fonction de répartition d'une variable aléatoire discrète.

Le passage à l'inéquation $F(x) \geq u$ permet de contourner la non-surjectivité : on ne regarde non plus les droites horizontales $y=u$ mais la région $\{y \geq \alpha\}$. Le choix de l'$\inf$ dans la définition de $F^{\leftarrow}$ permet de contourner la non-injectivité : vu qu'il y a possiblement plusieurs $x$ tels que $F(x) \geq u$, on choisit le "premier".
Ces considérations sont illustrées en Figure @fig-3-quantiles.



```{python}
#| echo: false
#| label: fig-3-quantiles
from scipy import stats
from plotly.subplots import make_subplots


def keep_no_param_distribution():
    distributions = stats._continuous_distns._distn_names
    distributions_0 = [
        name for name in distributions if not getattr(stats, name).shapes
    ]
    distributions_0_val = [
        getattr(stats.distributions, string) for string in distributions_0
    ]
    distributions_0_dict = dict(zip(distributions_0, distributions_0_val))
    return distributions_0_dict


def keep_no_param_distribution_disc():
    distributions = stats._discrete_distns._distn_names
    distributions_0 = [
        name
        for name in distributions
        if not getattr(stats, name).shapes or len(getattr(stats, name).shapes) in [1, 2]
    ]
    distributions_0_val = [
        getattr(stats.distributions, string) for string in distributions_0
    ]
    distributions_0_dict = dict(zip(distributions_0, distributions_0_val))
    return distributions_0_dict


def cdf_tool(x, dtype="int64"):
    y = np.zeros(2 * (len(x)), dtype=dtype)
    y[::2] = x
    y[1::2] = x
    return y[1::], y[:-1], y


def pmf_tool(x, dtype="int64"):
    y = np.zeros(2 * (len(x)), dtype=dtype)
    y[::2] = x
    return y[1::], y[:-1], y


def insert_nones(my_list):
    for i, val in enumerate(my_list):
        if i % 3 == 2:
            my_list.insert(i, None)
    return my_list


x = np.linspace(-5, 5, num=300)
alpha = 0.75
mu = 0.5
fig = make_subplots(
    rows=1,
    cols=3,
    horizontal_spacing=0.15,
    subplot_titles=(
        "solution unique",
        "pas de solution",
        "infinité de solutions",
    ),
)

fig.update_layout(title=go.layout.Title(text="Cas F(x)=u : ", x=0))
# Quantile plot
color_blue = "rgb(66, 139, 202)"
fig.update_layout(
    autosize=True,
    height=340,
    showlegend=False,
    template="simple_white",
)

for i, name_distrib in enumerate(["norm", "poisson", "geom"]):
    if name_distrib == "norm":
        distributions_0_dict = keep_no_param_distribution()
        distribution = distributions_0_dict[name_distrib]

        cdf_data = distribution.cdf(x)
        q_alpha = distribution.ppf(alpha)
        # Cdf part
        fig.add_trace(
            go.Scatter(
                x=x,
                y=cdf_data,
                mode="lines",
                marker={"color": "black"},
                name="F",
                showlegend=True,
            ),
            row=1,
            col=i + 1,
        )

    else:
        distributions_0_dict = keep_no_param_distribution_disc()
        distribution = distributions_0_dict[name_distrib]
        q_alpha = distribution.ppf(alpha, mu)
        x_int = np.arange(np.floor(x.min()), np.ceil(x.max()))

        cdf_data = distribution.cdf(x_int, mu)
        pmf_data = distribution.pmf(x_int, mu)
        # Quantile plot
        support = pmf_data.nonzero()[0]
        new_x, new_y, new_z = cdf_tool(support)
        _, _, new_pmf = pmf_tool(support)

        # Cdf part
        fig.add_trace(
            go.Scatter(
                x=x_int[support],
                y=cdf_data[support],
                mode="markers",
                marker={"color": "black"},
            ),
            row=1,
            col=i + 1,
        )
        fig.add_trace(
            go.Scatter(
                x=insert_nones(
                    list(
                        np.append(
                            np.insert(x_int[new_x], 0, [x_int[0], x_int[new_x[0]]]),
                            x_int[-1],
                        )
                    )
                ),
                y=insert_nones(
                    list(np.append(np.insert(cdf_data[new_y], 0, [0, 0]), cdf_data[-1]))
                ),
                mode="lines",
                line=dict(color="black"),
            ),
            row=1,
            col=i + 1,
        )

    fig.add_trace(
        go.Scatter(
            x=[q_alpha, q_alpha],
            y=[0, alpha],
            mode="lines",
            line=dict(color=color_blue, dash="dash"),
            showlegend=False,
        ),
        row=1,
        col=i + 1,
    )

    fig.add_trace(
        go.Scatter(
            x=[x[0], q_alpha],
            y=[alpha, alpha],
            mode="lines",
            line=dict(color=color_blue, dash="dash"),
            showlegend=False,
        ),
        row=1,
        col=i + 1,
    )

    fig.add_trace(
        go.Scatter(
            x=[q_alpha],
            y=[alpha],
            mode="markers",
            marker=dict(color=color_blue, symbol="x", size=8),
            name="Quantile",
            showlegend=False,
        ),
        row=1,
        col=i + 1,
    )

    fig.add_trace(
        go.Scatter(
            x=[x[0] + x.ptp() / 12],
            y=[alpha + 0.05],
            text="u",
            mode="text",
            showlegend=False,
            textfont_color=color_blue,
        ),
        row=1,
        col=i + 1,
    )

    fig.update_yaxes(range=[0, 1.0], row=1, col=i + 1)
    fig.update_xaxes(range=[x.min(), x.max()], row=1, col=i + 1)


fig.update_yaxes(matches="y1", row=1, col=2)
fig.update_yaxes(matches="y1", row=1, col=3)

fig.update_xaxes(matches="x1", row=1, col=2)
fig.update_xaxes(matches="x1", row=1, col=3)

fig.show()
```

**Remarques additionnelles**:

- La fonction $F$ étant croissante, la quantité $F^\leftarrow(u)$ correspond au premier instant où $F$ dépasse $\alpha$.
  Si $F$ est bijective (ce qui équivaut dans ce cas à strictement croissante et injective), alors $F^\leftarrow = F^{-1}$.
- La fonction $F^\leftarrow$ n'est rien d'autre que la fonction quantile : si $0 < \alpha < 1$, $q_{1-\alpha} = F^\leftarrow(1-\alpha)$ est le quantile d'ordre $(1-\alpha)$ de $F$. Par exemple, $F^\leftarrow(1/2)$ correspond à la médiane.
- Notons que si $u=0$, on peut alors naturellement poser $F^{\leftarrow}(0) = -\infty$. De même, avec la convention la convention $\inf \emptyset = +\infty$, on peut alors étendre la définition de $F^\leftarrow$ à $u=1$ (mais $F^\leftarrow(1)$ n'est pas toujours égal à $\infty$, voir les exemples ci-dessous).


::: {#exm-inverse-bernoulli}

## Simulation d'une loi de Bernoulli

La fonction de répartition $F$ d'une loi de Bernoulli de paramètre $p \in ]0,1[$ est donnée par
$$
    F(x) =
    \Bigg\{ \begin{array}{ll}
        0   & \text{ si } x < 0\,,        \\
        1-p & \text{ si } 0 \leq x < 1\,, \\
        1   & \text{ si } x \leq 1\,.
    \end{array}
$$
L'inverse généralisée de $F$ peut ainsi être calculée via la formule de l'exemple \ref{ex:inversion_discrete} :
$$
    F^\leftarrow(u) =
    \bigg\{ \begin{array}{ll}
        0 & \text{ si } 0 < u \leq 1-p\,, \\
        1 & \text{ si } 1-p < u \leq 1\,.
    \end{array}
$$
ce qui se réécrit plus simplement $F^\leftarrow(u) = {1\hspace{-3.8pt} 1}_{\{1-p < u\}}$.

Ainsi, si $U$ suit une loi uniforme sur $[0,1]$ alors ${1\hspace{-3.8pt} 1}_{\{1-p < U\}}$ suit une loi de Bernoulli de paramètre $p$.
Comme $1-U$ suit aussi une loi uniforme sur $[0,1]$, on en déduit que ${1\hspace{-3.8pt} 1}_{\{U < p\}}$ suit une loi de Bernoulli de paramètre $p$.
Notons que l'on peut remplacer l'inégalité stricte par une inégalité large.
:::

<br>


::: {#prp-inversion_discrete}
##  Loi à support fini

<br>

Soit $X$ une variable aléatoire discrète prenant uniquement les valeurs $x_1 < \dots < x_r$ ($r$ modalité possibles) avec probabilité $p_1, \dots, p_r$ (donc $p_1 + \dots + p_r=1$).
On vérifie que pour tout $u \in ]0,1[$,
$$
		F^\leftarrow(u) =
		\begin{cases}
			x_1 & \text{si } 0 < u \leq p_1\,,                  \\
			x_2 & \text{si } p_1 < u \leq p_1+p_2\,,            \\
			    & \vdots                                        \\
			x_r & \text{si }  \sum_{i=1}^{r-1} p_i < u < 1\,.
		\end{cases}
$$

Sur cet exemple, on peut prolonger la définition de $F^\leftarrow$ à $u=1$ en posant $F^\leftarrow(1) = x_r$.
L'inverse généralisée se réécrit alors sous la forme
$$
		F^\leftarrow(u) = \sum_{k=1}^r x_k {1\hspace{-3.8pt} 1}_{ \{  \sum_{i=1}^{k-1}p_i < u \leq \sum_{i=1}^{k}p_i \} }\enspace,
$$
où on a posé $p_0=0$.
:::

L'expression précédente s'étend directement au cas où $X$ prend un nombre (infini) dénombrable de valeurs, la somme devenant alors une série.

La méthode est illustré ci-dessous pour quelques lois intéressantes:


```{ojs}
//| echo: false
viewof dist = Inputs.select(['normal','cauchy','laplace','bimodal'], {value: "bimodal", label: "Loi"})
viewof replay = html`<button>Relancer`
```



```{ojs}
//| layout-ncol: 1
//| layout-align: center
//| echo: false

// Hack to make ojs load a github js file…
// the repo is https://github.com/fradav/matter-playground
// invcdfboard = {
//   const response = await fetch("https://raw.githubusercontent.com/fradav/matter-playground/main/dist/invcdfboard.umd.cjs");
//   const blob = await response.blob();
//   const url = URL.createObjectURL(blob);
//   return require(url).catch(() => window.invcdfboard);
// }

// Or load from local file
invcdfboard = require(await FileAttachment("../inverse-vizu/dist/invcdfboard.umd.cjs").url())

{
  replay
  const canvas = DOM.canvas(500, 500);
  const galton = invcdfboard.galton(canvas,dist);
  
  return html`${galton.canvas}`
}
```


## Méthode de rejet

L'idée de la méthode de rejet est la suivante. On souhaite simuler une variable aléatoire $X$ de densité $f$, appelée **loi cible**, mais $f$ est trop compliquée pour que la simulation puisse se faire directement.
On dispose cependant d'une autre densité $g$ possédant les propriétés suivantes :

- on sait simuler $Y$ de loi $g$,
- il existe $m > 0$ tel que $f(x) \leq m \cdot g(x)$,
- on sait évaluer le **rapport d'acceptation** $r(x) = \frac{f(x)}{mg(x)}$.


Remarquons d'ores et déjà que la constante $m$ est nécessairement plus grande que $1$ car
$$
	1 = \int_\mathbb{R} f(x) \, dx \leq m \int_\mathbb{R} g(x)\, dx = m\,.
$$


L'idée est alors de considérer deux suites i.i.d. de variables aléatoires indépendantes entre elles:

- $(Y_n)_{n \geq 1}$ de loi $g$,
- $(U_n)_{n \geq 1}$ de loi uniforme sur $[0,1]$.

En pratique, $Y_n$ correspond à une proposition et $U_n$ permettra de décider si on accepte la proposition ou non. Si oui, alors on conserve $Y_n$, sinon on simule $Y_{n+1}$. Le rapport d'acceptation, c'est-à-dire la proportion de $Y_n$ acceptées, correspond à $r(x)$.


Autrement dit, pour simuler $X$ de densité $f$, il suffit de simuler $Y$ de densité $g$ et $U$ uniforme jusqu'à ce que $U \leq r(Y)$.
La proposition suivante assure que cette méthode donne bien le résultat voulu.

:::: {#prp-rejet}

## Méthode de rejet

<br>

Soit $T = \inf \{n \geq 1 : U_n \leq r(Y_n)\}$ le premier instant où le tirage est accepté. Alors :

- $T$ suit une loi géométrique de paramètre $1/m$,
- la variable aléatoire $X = Y_T$ a pour densité $f$ et est indépendante de $T$.
::::



::: {.proof}

Il s'agit d'étudier la loi du couple $(X,T)$. Pour $x \in \mathbb{R}$ et $n \in \mathbb{N}^{*}$, on écrit
$\mathbb{P}(X \leq x, T=n)= \mathbb{P}(U_1 > r(Y_1), \dots, U_{n-1} > r(Y_{n-1}), U_n \leq r(Y_n), Y_n \leq x).$
Les $n$ tirages étant iid, on obtient
$$
    \mathbb{P}(X \leq x, T=n) = \mathbb{P}(U_1 > r(Y_1))^{n-1} \mathbb{P}(U_n \leq r(Y_n), Y_n \leq x)\,.
$$

Concernant le premier terme, les variables aléatoires $Y_1$ et $U_1$ sont indépendantes donc leur loi jointe correspond au produit des densités :
$$\begin{align*}
		\mathbb{P}(U_1 > r(Y_1))
		 & = \mathbb{P}((U_1, Y_1) \in \{(u,y) \in \mathbb{R}^2 : u > r(y)\})                             \\
		 & = \int_{\mathbb{R}^2} {1\hspace{-3.8pt} 1}_{\{u > r(y)\}} ({1\hspace{-3.8pt} 1}_{[0,1]}(u) g(y)) \, du dy  \\
		 & = \int_\mathbb{R} \bigg( \int_0^1 {1\hspace{-3.8pt} 1}_{\{u > r(y)\}} \, du\bigg) g(y)\, d y \\
		 & =  \int_\mathbb{R} (1-r(y)) \, g(y)\, d y\,,
	\end{align*}
$$
ce qui se réécrit, comme $f$ et $g$ sont des densités et que $r(y) = f(y)/(m \cdot g(y))$:
$$\begin{align*}
		\mathbb{P}(U_1 > r(Y_1))
		& = \int_\mathbb{R} g(y)\, d y - \int_\mathbb{R} \dfrac{f(y)}{m}\, dy \\
		& = 1 - \dfrac{1}{m}\,.
	\end{align*}
$$
Le deuxième terme se calcule de manière analogue :
$$
\begin{align*}
    \mathbb{P}(U_n \leq r(Y_n), Y_n \leq x)
        & = \int_{\mathbb{R}^2} {1\hspace{-3.8pt} 1}_{\{u \leq r(y)\}} {1\hspace{-3.8pt} 1}_{\{y \leq x\}} ({1\hspace{-3.8pt} 1}_{[0,1]}(u) g(y)) \, du dy       \\
        & = \int_\mathbb{R} \bigg( \int_0^1  {1\hspace{-3.8pt} 1}_{\{u \leq r(y)\}} \, du\bigg) {1\hspace{-3.8pt} 1}_{\{y \leq x\}}  g(y)\, d y\,,
\end{align*}
$$
c'est-à-dire
$$
\begin{align*}
        \mathbb{P}(U_n \leq r(Y_n), Y_n \leq x)
		& = \int_\mathbb{R} r(y) {1\hspace{-3.8pt} 1}_{\{y \leq x\}}  g(y)\, d y \\
		& = \int_{-\infty}^x \dfrac{f(y)}{m}\, d y \\
		& = \dfrac{F(x)}{m}\,,
\end{align*}
$$
où $F$ est la fonction de répartition de la loi de densité $f$. On peut ainsi conclure que
$$
    \mathbb{P}(X \leq x, T=n)
    =
    \bigg(1 - \dfrac{1}{m}\bigg)^{n-1} \dfrac{F(x)}{m}\,.
$$
Il ne reste plus qu'à étudier les lois marginales.
D'une part, par continuité monotone croissante,
$$
    \mathbb{P}(T=n)
    = \lim_{q \to \infty} \mathbb{P}(X \in ]-\infty, q], T=n)\,,
$$
ce qui donne
$$
\begin{align*}
    \mathbb{P}(T=n)
    & = \lim_{q \to \infty} \bigg(1 - \dfrac{1}{m}\bigg)^{n-1} \dfrac{F(q)}{m}\\
    & = \bigg(1 - \dfrac{1}{m}\bigg)^{n-1} \dfrac{1}{m}\,.
\end{align*}
$$
On en déduit que $T$ suit une loi géométrique de paramètre $1/m$. D'autre part, par $\sigma$-additivité,
$$
\begin{align*}
    \mathbb{P}(X \leq x)
    & = \mathbb{P}(X \leq x, T \in \mathbb{N}^*)\\
    & = \sum_{n=1}^\infty \mathbb{P}(X \leq x, T=n)\,,
\end{align*}
$$
ce qui donne
$$
\begin{align*}
    \mathbb{P}(X \leq x)
    & = \sum_{n=1}^\infty \bigg(1 - \dfrac{1}{m}\bigg)^{n-1} \dfrac{F(x)}{m}\\
    & = \dfrac{1}{1-(1-1/m)} \dfrac{F(x)}{m}\\
    & = F(x)\,,
\end{align*}
$$
ce qui prouve que $X$ a pour loi $F$.

Enfin, la loi du couple $(X,T)$ est égale au produit des lois
$$
\begin{align*}
    \mathbb{P}(X \leq x, T=n)
    & = \bigg(1 - \dfrac{1}{m}\bigg)^{n-1} \dfrac{F(x)}{m}\\
    & = \mathbb{P}(T=n) \mathbb{P}(X \leq x)\,,
\end{align*}
$$
:::


```{python}
#| echo: true

def accept_reject(n, f, g, g_sampler, m):
    """
    n: nombre de simulations
    f: densité cible
    g: densité des propositions, g_sampler: simulateur selon g
    m: constante pour la majoration
    """
    x_samples = np.zeros(n)
    u_samples = np.zeros(n)
    accepted = np.zeros(n)
    n_accepted = 0
    while n_accepted < n:
        x = g_sampler()
        u = np.random.uniform()
        alpha = u * m * g(x)
        u_samples [n_accepted] = alpha
        x_samples[n_accepted] = x
        if  alpha <= f(x):
            accepted[n_accepted] = 1
        n_accepted += 1
    return x_samples, u_samples, accepted
```


::: {.callout-note appearance="simple"}
## En pratique...
On simule $U_1$ et $Y_1$. Si $U_1 \leq r(Y_1)$ c'est gagné, on pose $X=Y_1$. Sinon, on simule $U_2$ et $Y_2$ et on teste à nouveau l'inégalité $U_2 \leq r(Y_2)$. Et ainsi de suite. Comme $T$ suit une loi géométrique de paramètre $1/m$, son espérance vaut $m$ : il faut en moyenne $m$ tentatives pour obtenir une simulation de la loi de densité $f$. L'objectif est alors de choisir un couple $(g, m)$ de sorte que $m$ soit le plus proche possible de $1$.
:::

::: {#exm-rejet_polynome}

## Rejet d'une loi polynomiale
Donnons un exemple jouet (on étudiera des exemples plus pertinents en TD). On considère la densité $f(x) = 4x^3 {1\hspace{-3.8pt} 1}_{[0,1]}(x)$. Comme $f$ est majorée par $4$, on peut choisir pour $g$ la densité de la loi uniforme sur $[0,1]$ et $m=4$. Alors, $r(x) =f(x) / (mg(x)) = x^3$, pour $x \in [0,1]$. On simule donc $(Y_1, U_1)$ et on teste si $U_1 \leq Y_1^3$, etc.

Bien évidemment, on privilégiera ici une simulation via $F^\leftarrow$ qui permet de générer des variables aléatoires de loi $f$ plus rapidement.

```{python}
#| echo: false

def plot_accept_reject(n, f, g, g_sampler, m):
    x_samples, u_samples, accepted = accept_reject(n, f, g, g_sampler, m)
    x = np.linspace(0, 1, 100)

    fig = go.Figure()
    fig.add_trace(
        go.Scatter(
            x=x_samples[accepted == 1],
            y=u_samples[accepted == 1],
            mode="markers",
            marker=dict(color="blue", symbol="circle", size=5),
            name="Accepté",
        )
    )
    fig.add_trace(
        go.Scatter(
            x=x_samples[accepted == 0],
            y=u_samples[accepted == 0],
            mode="markers",
            marker=dict(color="red", symbol="circle", size=5),
            name="Rejeté",
        )
    )
    fig.add_trace(
        go.Scatter(
            x=x, y=f(x), mode="lines", line=dict(color="black", dash="dash"), name="f"
        )
    )
    fig.add_trace(
        go.Scatter(
            x=x, y=m * g(x), mode="lines", line=dict(color="black"), name="m  · g"
        )
    )
    fig.update_layout(template="simple_white", showlegend=True)
    ratio = np.sum(accepted) / n
    return fig, ratio


n = 1000
f = lambda x: 4 * x**3

g = lambda x: np.ones_like(x)
g_sampler = lambda: np.random.uniform()
m = 4
```

```{python}
#| echo: false
#| label: fig-rejet-polynome-unif
#| fig-cap:
#|       - Visualisation des zones d'acceptations/rejet (g uniforme)
fig1,ratio1 = plot_accept_reject(n, f, g, g_sampler, m)
fig1.show()
```
Nous pouvons facilement améliorer la proportion de point acceptés en proposant par exemple $g$ définie par $g(x) = 2x {1\hspace{-3.8pt} 1}_{[0, 1]}(x)$, et $m=2$.

```{python}
#| echo: false
#| label: fig-rejet-polynome-tri
#| fig-cap:
#|       - Visualisation des zones d'acceptations/rejet (g triangulaire)
g = lambda x: 2*x 
g_sampler = lambda: np.random.triangular(0, 1, 1)
m = 2

fig2, ratio2 = plot_accept_reject(n, f, g, g_sampler, m)
fig2.show()
```

```{python}
#| echo: false
ojs_define(ratio1=ratio1)
ojs_define(ratio2=ratio2)
```

```{ojs}
//| echo: false
md`Le taux d'acceptation est passé de **${ratio1}** à
**${ratio2}** en utilisant une loi triangulaire au lieu d'une loi uniforme.`
```
:::

<br>


::: {#exm-rejet_andrews}

## Rejet d'une loi de densité d'Andrews
Considérons la densité d'Andrews définie par $f(x) = \frac{1}{S} \frac{\sin(\pi\cdot x)}{\pi \cdot x}  {1\hspace{-3.8pt} 1}_{[-1,1]}(x)$, avec $S = \int_{-1}^{1}\frac{\sin(\pi\cdot x)}{\pi \cdot x}dx$.
Dans ce contexte, on ne connait pas la valeur exacte de $S$, et on va donc utiliser la méthode de rejet pour simuler des variables aléatoires de loi $f$ sans cette information.
On peut l'adapter le test de la manière suivante:
si l'on prend $m=2/S$ et $g(x) = \frac{1}{2} {1\hspace{-3.8pt} 1}_{[-1,1]}(x)$, on observe que tester $u\leq \frac{f(x)}{m \cdot g(x)}$ est équivalent à tester $u \leq r(x)=\frac{1}{S} \frac{\sin(\pi\cdot x)}{\pi \cdot x}  \cdot \frac{1}{\frac{2}{S} g(x)} = \frac{\sin(\pi\cdot x)}{\pi \cdot x}  \cdot \frac{1}{2 \cdot g(x)}$, ce qui peut se faire sans connaissance de $S$.
De plus on peut vérifier que $g(x) = \frac{1}{2} {1\hspace{-3.8pt} 1}_{[-1,1]}(x)$ définit une densité et que $f(x) \leq m \cdot g(x)$ pour tout $x\in \mathbb{R}$.

```{python}

n = 10000
g = lambda x: np.ones_like(x) / 2
g_sampler = lambda: 2 * np.random.uniform() - 1
m = 2

x_samples, u_samples, accepted = accept_reject(n, np.sinc, g, g_sampler, m)
ratio = np.sum(accepted) / n
# Note: https://stackoverflow.com/questions/70804891/how-to-vectorize-a-function-in-python-that-includes-a-limit-e-g-sinx-x
```

```{python} 
#| echo: false
ojs_define(ratio=ratio)
```

On peut approcher numériquement la valeur exacte de $S$ en utilisant une
méthode de calcul approchée, ce qui permet de comparer ici notre méthode de rejet avec la densité sous-jacente:

```{python}
from scipy import integrate
S = integrate.quad(np.sinc, -1, 1)[0]
print(f"En utilisant la méthode de rejet, on trouve que S = {S:.3f}")
```

Enfin, on peut visualiser la qualité l'approximation de la densité par la méthode de rejet en comparant la densité approchée (avec un histogramme) avec la densité exacte:

```{python}
#| echo: false
#| label: fig-rejet-Andrews
#| fig-cap:
#|      - Méthode de rejet pour simuler une loi de densité de type Andrews, sans connaissance de la valeur exacte de la constante de normalisation.
fig = go.Figure()
fig.add_trace(
    go.Histogram(
        x=x_samples[accepted == 1], histnorm="probability density", name="Échantillons"
    )
)

# Plot the density
x = np.linspace(-1, 1, 100)
fig.add_trace(
    go.Scatter(
        x=x,
        y=np.sinc(x) / S,
        mode="lines",
        line=dict(color="black", dash="dash"),
        name="Densité",
    )
)

fig.update_layout(template="simple_white", showlegend=True)
```

```{ojs}
//| echo: false
md`Dans cet example, le taux d'acceptation est ici de **${ratio}**.`
```
:::



### Cas mutlidimensionnel


Commençons par un cas de dimension deux.

Pour cela on va utiliser la méthode de rejet pour simuler une loi de densité $f$ sur $\mathbb{R}^2$. En particulier, un exemple classique est de tirer des points dans le disque unité, c'est-à-dire de simuler une loi uniforme sur le disque unité. Pour cela, on va utiliser la méthode de rejet avec $g$ la densité de la loi uniforme sur le carré $[-1,1]^2$.

Mais prenons un autre exemple, à savoir tirer des points uniformément dans la surface délimité par une cardioïde. Pour cela, on va utiliser la méthode de rejet avec $g$ la densité de la loi uniforme sur le carré $[-2,2]^2$.

```{python}
#| echo: false

# plot a cardioid curve with polar coordinates in plotly

def cardiode(n_discr=100):
    theta = np.linspace(0, 2 * np.pi, n_discr)
    r = (1 + np.cos(theta))
    x = r * np.cos(theta)
    y = r * np.sin(theta)
    return x, y


def circle(n_discr=100):
    theta = np.linspace(0, 2 * np.pi, n_discr)
    x = np.cos(theta)
    y = np.sin(theta)
    return x, y


def plot_curve(fig, plot_type="circle", n_discr=100):
    if plot_type=="circle":
        x, y = circle(n_discr)
    elif plot_type=="cardioide":
        x, y = cardiode(n_discr)
    fig.add_trace(go.Scatter(x=x, y=y, mode="lines", line=dict(color="black", width=4), name="Limite de surface"))
    fig.update_layout(template="simple_white", showlegend=True,    yaxis_scaleanchor="x")
    if type=="circle":
        fig.update_xaxes(range=[-1.5, 1.5])
    elif type=="cardioide":
        fig.update_xaxes(range=[-2, 3])
    return fig
```

```{python}
#| echo: false
def test_cardioide(x, y):
    if (x**2 + y**2 - x) ** 2 <= (x**2 + y**2):
        return 1
    else:
        return 0


def test_unit_circle(x, y):
    if x**2 + y**2 <= 1:
        return 1
    else:
        return 0


color = ["red", "blue"]
status = ["Rejeté", "Accepté"]


def plot_samples(fig, n_sample, plot_type="circle"):
    if plot_type == "circle":
        test_func = test_unit_circle
        x_samples = np.random.uniform(-1, 1, n_sample)
        y_samples = np.random.uniform(-1., 1, n_sample)
    elif plot_type == "cardioide":
        test_func = test_cardioide
        x_samples = np.random.uniform(-2, 3, n_sample)
        y_samples = np.random.uniform(-1.5, 1.5, n_sample)

    test_func = np.vectorize(test_func)
    test_samples = test_func(x_samples, y_samples)
    for i in [0, 1]:
        fig.add_trace(
            go.Scatter(
                x=x_samples[test_samples == i],
                y=y_samples[test_samples == i],
                mode="markers",
                marker=dict(color=color[i], symbol="circle", size=3),
                name=status[i],
            )
        )
    return fig, test_samples
```

```{python}
#| echo: false
#| label: fig-rejet-cercle
#| fig-cap:
#|    - Méthode de rejet pour simuler une loi uniforme sur un disque unité. 

n_sample = 3000
plot_type = "circle"

fig = go.Figure()
fig, test_samples_circle = plot_samples(fig, n_sample=n_sample, plot_type=plot_type)
fig = plot_curve(fig, plot_type=plot_type, n_discr=100) 
fig.show()
ratio = np.sum(test_samples_circle) / n_sample
print(f"Aire estimée: {ratio * 4}")
```

```{python}
#| echo: false
#| label: fig-rejet-cardioide
#| fig-cap:
#|     - Méthode de rejet pour simuler une loi uniforme sur une surface délimitée par une cardioïde.

plot_type = "cardioide"

fig = go.Figure()
fig, test_samples_cardioide = plot_samples(fig, n_sample=n_sample, plot_type=plot_type)
fig = plot_curve(fig, plot_type=plot_type, n_discr=100)
fig.show()
ratio = np.sum(test_samples_cardioide) / n_sample
print(f"Aire estimée: {ratio * 15}")


```




::: {.callout-important appearance='default' icon="false"}

##  EXERCICE loi uniforme sur un cylindre

Proposer une méthode pour simuler une loi uniforme sur un cylindre de rayon $1$ et de hauteur $10$.
:::




## Autres méthodes

### Sommation de variables aléatoires

Pour simuler une variable aléatoire de loi binomiale $\mathcal{B}(n,p)$, on peut utiliser la méthode d'inversion. Cependant, cela nécessite le calcul de l'inverse généralisée de $F$, donc de coefficients binomiaux et de puissances de $p$ et $1-p$. À la place, on utilisera plutôt la relation bien connue suivante : si $X_1, \ldots, X_n$ est une suite iid de variables aléatoires de loi de Bernoulli de paramètre $p$, alors
$$
	X = X_1 + \cdots + X_n \sim \mathcal{B}(n,p)\,.
$$

Pour simuler des variables aléatoires de Bernoulli, on utilise la méthode d'inversion (voir Exemple \ref{ex:bernoulli}). Ainsi, si $U_1, \ldots, U_n$ sont des variables aléatoires iid de loi uniforme sur $[0,1]$, alors
$$
	\sum_{i=1}^n {1\hspace{-3.8pt} 1}_{\{U_i \leq p\}} \sim \mathcal{B}(n,p)\,.
$$

### Loi de Poisson

Rappelons qu'une variable aléatoire $X$ suit une loi de Poisson de paramètre $\lambda > 0$, notée $X \sim \mathcal{P}(\lambda)$ si
$$
	\mathbb{P}(X = k) = e^{-\lambda} \dfrac{\lambda^k}{k!}\,, \quad k \in \mathbb{N}\,.
$$
Une méthode pour simuler une variable aléatoire de loi de Poisson est donnée par la proposition suivante.


:::: {#prp-poisson_exponentielle}

## Génération de v.a. de loi de Poisson

<br>

Soit $(E_n)_{n \geq 1}$ des variables aléatoires i.i.d. de loi exponentielle de paramètre $\lambda > 0$. On pose $S_k = E_1 + \cdots + E_k$. Alors, pour tout $n \in \mathbb{N}$
$$
    \mathbb{P}(S_n \leq 1 < S_{n+1}) =  e^{-\lambda} \dfrac{\lambda^n}{n!}\enspace .
$$
Ainsi, la variable aléatoire $T$ définie par
$$
    T \triangleq \sup \{n \in \mathbb{N} : S_n \leq 1\}
$$
suit une loi de Poisson de paramètre $\lambda$ : $T \sim \mathcal{P}(\lambda)$.

::::

La preuve repose sur le lemme suivant.

:::: {#lem-erlang}

## Loi de Erlang

<br>


Soit $n$ variables aléatoires $E_1, \dots, E_n$ i.i.d. de loi exponentielle de paramètre $\lambda >0$. La somme $E_1+\dots+E_n$ suit une loi d'Erlang de paramètres $(n,\lambda)$, donnée par la fonction de répartition
$$
    F_{n,\lambda}(t) = 1 - \sum_{k=0}^{n-1} e^{-\lambda t} \frac{(\lambda t)^k}{k!}\,.
$$
::::

::: {.proof}

On montre le résultat pour $n=2$. La généralisation à $k$ quelconque se fait par récurrence. 
Soit $t > 0$, et $f_{\lambda}(x)={1\hspace{-3.8pt} 1}_{\{x \geq 0 \}} \lambda e^{-\lambda x}$ la densité d'une loi exponentielle de paramètre $\lambda$. Les variables aléatoires $E_1$ et $E_2$ étant indépendantes et suivant des lois exponentielles de paramètre $\lambda_1$ et $\lambda_2$, on a
$$
\begin{align*}
    \mathbb{P}(E_1+E_2 \leq t)
        & = \int_{\mathbb{R}^2} {1\hspace{-3.8pt} 1}_{\{x_1 + x_2 \leq t\}} f_{\lambda}(x_1) f_{\lambda}(x_2)\, d x_1 d x_2 \\
        & = \int_{\mathbb{R}^2} {1\hspace{-3.8pt} 1}_{\{x_1 + x_2 \leq t\}} \lambda^2 e^{-\lambda (x_1+x_2)} {1\hspace{-3.8pt} 1}_{\{x_1 \geq 0\}} {1\hspace{-3.8pt} 1}_{\{x_2 \geq 0\}}\, d x_1 d x_2 \\
        & = \int_{\mathbb{R}^2} {1\hspace{-3.8pt} 1}_{\{0 \leq x_1 \leq t\}} {1\hspace{-3.8pt} 1}_{\{0 \leq x_2 \leq t-x_1\}} \lambda^2 e^{-\lambda x_1} e^{-\lambda x_2}\, d x_1 d x_2             \\
        & = \int_0^t \lambda e^{-\lambda x_1} \bigg(\int _0^{t-x_1} \lambda e^{-\lambda x_2}\, d x_2\bigg)  d x_1\,.
\end{align*}
$$
La première intégrale se calcule alors facilement :
$$
    \int _0^{t-x_1} \lambda e^{-\lambda x_2}\, d x_2 = 1 - e^{-\lambda(t-x_1)}\,.
$$
On obtient alors
$$
\begin{align*}
    \mathbb{P}(E_1+E_2 \leq t)
   & = \int_0^t \lambda e^{-\lambda x_1}dx_1 -  \int_0^t e^{-\lambda t} d x_1\\
   & = 1 - e^{-\lambda t} - \lambda t e^{-\lambda t}\,.
\end{align*}
$$
Si $t<0$, alors comme les $E_i$ ne prennent que des valeurs positives on trouve $\mathbb{P}(E_1 + E_2 \leq t) = 0$. Ceci prouve le résultat pour $n=2$.

:::

On peut désormais prouver le résultat de la @{prp-poisson_exponentielle}.

::: {.proof}

Pour $n \in \mathbb{N}$, on décompose la probabilité $\mathbb{P}(S_n \leq 1 < S_{n+1})$ via
$$
\begin{align*}
    \mathbb{P}(S_n \leq 1 < S_{n+1})
    & = \mathbb{P}(\{S_n \leq 1\} \setminus \{S_{n+1} \leq 1\})\\
    & = \mathbb{P}(S_n \leq 1) - \mathbb{P}(S_{n+1} \leq 1)\,.
\end{align*}
$$
Le lemme précédent donne
$$
    \mathbb{P}(S_n \leq 1) = 1 - \sum_{k=0}^{n-1} e^{-\lambda} \dfrac{\lambda^k}{k!}
$$
et
$$
    \mathbb{P}(S_{n+1} \leq 1) = 1 - \sum_{k=0}^{n} e^{-\lambda} \dfrac{\lambda^k}{k!}\,.
$$
On obtient alors le résultat souhaité :
$$
    \mathbb{P}(S_n \leq 1 < S_{n+1})
    = e^{-\lambda} \dfrac{\lambda^n}{n!}\,.
$$

On conclut la preuve de la proposition en remarquant que
$$
    \mathbb{P}(T=n) = \mathbb{P}(S_n \leq 1 < S_{n+1})\,.
$$
:::

La simulation d'une variable aléatoire de Poisson repose donc sur la simulation de lois exponentielles qui se fait via la méthode d'inversion, comme vu dans @exm-inverse-exponentielle. En pratique, on simule $E_1$ et on teste si $E_1 > 1$. Si oui, on pose alors $T=0$. Si non, on simule $E_2$ et on teste si $E_1 + E_2 > 1$. Si oui, on pose $T=1$. Sinon on continue la procédure.

### Bibliographie et pour aller plus loin

- [Generating Random Floating-Point Numbers by Dividing Integers: a Case Study](https://hal.science/hal-02427338/document) par Frédéric Goualard
- [Generating Pseudo-random Floating-Point Values](https://allendowney.com/research/rand/downey07randfloat.pdf) par Allen Downey.
